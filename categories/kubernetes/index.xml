<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on GeekyRyan</title><link>https://rnemeth90.github.io/categories/kubernetes/</link><description>Recent content in Kubernetes on GeekyRyan</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sat, 29 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://rnemeth90.github.io/categories/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>Mounting Multiple Kubernetes Secrets into One Directory</title><link>https://rnemeth90.github.io/posts/2024-06-29-mount-multiple-kubernetes-secrets-into-one-directory/</link><pubDate>Sat, 29 Jun 2024 00:00:00 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2024-06-29-mount-multiple-kubernetes-secrets-into-one-directory/</guid><description>&lt;h1 id="introduction">
 Introduction
 &lt;a class="heading-link" href="#introduction">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>Combining multiple Kubernetes secrets into a single directory can streamline secret management in your applications. This guide walks you through the process of achieving this in Kubernetes, ensuring efficient and organized secret management.&lt;/p>
&lt;h1 id="creating-secrets">
 Creating Secrets
 &lt;a class="heading-link" href="#creating-secrets">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>First, create your secrets using the &lt;code>kubectl create secret&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl create secret generic secret-one --from-literal=key1=value1
kubectl create secret generic secret-two --from-literal=key2=value2
&lt;/code>&lt;/pre>&lt;p>Each secret can contain multiple key-value pairs, and you can add more secrets as needed.&lt;/p></description></item><item><title>Handling Graceful Shutdown in a .NET App Hosted in Kubernetes</title><link>https://rnemeth90.github.io/posts/2022-12-28-graceful-shutdown-in-kubernetes-dotnet-pod/</link><pubDate>Wed, 28 Dec 2022 00:00:00 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2022-12-28-graceful-shutdown-in-kubernetes-dotnet-pod/</guid><description>&lt;p>I was recently involved with troubleshooting some API&amp;rsquo;s hosted in Kubernetes throwing http/502&amp;rsquo;s. This was incredibly difficult to diagnose because it seemingly happened at random, and I had never encountered anything like this. Being that I had never dealt with this in the past, and I (nor my team) was able to figure it out within a reasonable amount of time, I turned to google. My searches resulted in various blogs and SO posts of other people experiencing similar issues, but none of their resolutions worked for us. It was actually a combination of these blogs (and the resolutions posted) that ended up resolving our issue.&lt;/p></description></item><item><title>AKS Scale Down Mode</title><link>https://rnemeth90.github.io/posts/2022-07-19-aks-scale-down-mode/</link><pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2022-07-19-aks-scale-down-mode/</guid><description>&lt;p>By default, scale-out operations performed manually or by cluster autoscale rules require the allocation and provisioning of new nodes, and scale-in operations delete nodes. Scale-down mode is a relatively newer concept that allows us to choose whether to delete &lt;em>or&lt;/em> deallocate nodes.&lt;/p>
&lt;p>Having the ability to deallocate, rather than delete, nodes is a major performance benefit, as the time it takes to spin up new nodes will be significantly decreased. You will not be charged when nodes are deallocated. However, you will still need to pay for any storage that the node is using. Having persistent storage also means that any container images that were cached on the node will still be there when the node starts back up. This can be a major performance benefit if you are using Windows containers because the images for these containers are typically very large.&lt;/p></description></item><item><title>Scheduled Kubernetes Worker Node Maintenance with Kured</title><link>https://rnemeth90.github.io/posts/2022-07-15-scheduled-kubernetes-worker-node-maintenance-with-kured/</link><pubDate>Fri, 15 Jul 2022 18:18:50 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2022-07-15-scheduled-kubernetes-worker-node-maintenance-with-kured/</guid><description>&lt;p>If you manage Linux nodes, you know how vital performing regular maintenance is. Installing software patches that modify Linux kernel headers requires a reboot. Normally, as in the past, we would cordon and drain the node and then manually reboot, wait for it to come back online, verify its health, and add it back to the cluster. That’s a lot of manual work! How can we automate this?&lt;/p>
&lt;p>Weaveworks created a great tool for simplifying these steps: Kured (the &lt;em>&lt;strong>Ku&lt;/strong>bernetes &lt;strong>Re&lt;/strong>boot &lt;strong>D&lt;/strong>aemon&lt;/em>). Let’s start by deploying Kured to our cluster.&lt;/p></description></item><item><title>Running Docker in WSL v1</title><link>https://rnemeth90.github.io/posts/2022-06-26-running-docker-in-wsl-v1/</link><pubDate>Sun, 26 Jun 2022 15:00:28 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2022-06-26-running-docker-in-wsl-v1/</guid><description>&lt;p>I have somewhat of a niche issue, where I have no network connectivity while connecting to my work VPN inside of WSL v2. I have found others complaining about this issue on Github. Though no one seems to know how to fix it and I have not had the time to properly investigate.&lt;/p>
&lt;p>Because of this, I’m required to continue using WSL v1. Though, with WSL v1, Docker does not work. I receive this nice message:&lt;/p></description></item><item><title>Remove Kubernetes Namespace Stuck in the Terminating State</title><link>https://rnemeth90.github.io/posts/2022-06-04-remove-kubernetes-namespace-stuck-in-the-terminating-state/</link><pubDate>Sat, 04 Jun 2022 18:29:41 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2022-06-04-remove-kubernetes-namespace-stuck-in-the-terminating-state/</guid><description>&lt;p>In this post, we will discuss how to remove a Kubernetes namespace that is stuck in the ‘terminating’ state.&lt;/p>
&lt;p>A namespace is like a container. You can use it to store related objects in a Kubernetes environment. Maybe you are hosting a blog in Kubernetes. This blog will likely have a database, a frontend website, a load balancer (service) to spread the incoming traffic among ‘x’ number of frontend containers (pods), and maybe some middle-tier or utility applications. One day, you decide you no longer want this blog, so you plan to delete it. Rather than tediously deleting all of the various entities associated with this blog, you can delete the namespace that contains these entities. This will essentially ‘cascade delete’ the resources within the namespace as well.&lt;/p></description></item><item><title>Kubernetes Storage Simplified</title><link>https://rnemeth90.github.io/posts/2022-03-01-kubernetes-storage-simplified/</link><pubDate>Tue, 01 Mar 2022 08:37:58 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2022-03-01-kubernetes-storage-simplified/</guid><description>&lt;p>In this blog post, we will attempt to explain the current storage options that exist in Kubernetes. If you are new to Kubernetes, learning about its capabilities of managing the application state can be a daunting task.&lt;/p>
&lt;p>Container images are built-in layers, with the runtime layer being writable. However, any files on this writable layer are only available for the container’s lifetime. We can mount a volume to a directory inside the container to have persistent data.&lt;/p></description></item><item><title>Kubernetes Pod Eviction</title><link>https://rnemeth90.github.io/posts/2022-02-05-kubernetes-pod-eviction/</link><pubDate>Sat, 05 Feb 2022 23:49:16 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2022-02-05-kubernetes-pod-eviction/</guid><description>&lt;p>In this article, we will dive into the process of pod eviction in a Kubernetes cluster, how you can pod prevent pod eviction, and how you can recover from such a situation.&lt;/p>
&lt;h2 id="what-is-pod-eviction">
 What is Pod Eviction?
 &lt;a class="heading-link" href="#what-is-pod-eviction">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h2>
&lt;p>Kubernetes pod eviction is a type of involuntary service disruption in which a pod is forcefully stopped on a node or fails to be scheduled on a node. Pod eviction can happen for a variety of reasons. The most common of which is resource starvation on a node. This is referred to as “node-pressure eviction.”&lt;/p></description></item></channel></rss>