<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Linux on GeekyRyan</title><link>https://rnemeth90.github.io/tags/linux/</link><description>GeekyRyan (Linux)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 15 Jul 2022 18:18:50 +0000</lastBuildDate><atom:link href="https://rnemeth90.github.io/tags/linux/index.xml" rel="self" type="application/rss+xml"/><item><title>Scheduled Kubernetes Worker Node Maintenance with Kured</title><link>https://rnemeth90.github.io/posts/2022-07-15-scheduled-kubernetes-worker-node-maintenance-with-kured/</link><pubDate>Fri, 15 Jul 2022 18:18:50 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2022-07-15-scheduled-kubernetes-worker-node-maintenance-with-kured/</guid><description>&lt;p>If you manage Linux nodes, you know how vital performing regular maintenance is. Installing software patches that modify Linux kernel headers requires a reboot. Normally, as in the past, we would cordon and drain the node and then manually reboot, wait for it to come back online, verify its health, and add it back to the cluster. That’s a lot of manual work! How can we automate this?&lt;/p>
&lt;p>Weaveworks created a great tool for simplifying these steps: Kured (the &lt;em>&lt;strong>Ku&lt;/strong>bernetes &lt;strong>Re&lt;/strong>boot &lt;strong>D&lt;/strong>aemon&lt;/em>). Let’s start by deploying Kured to our cluster.&lt;/p>
&lt;p>Kured can be deployed in one of several ways. In this article, we’ll focus on deploying it via Helm. This is the simplest and quickest way to get it running in our cluster.&lt;/p>
&lt;p>Follow these steps to install the Helm chart:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 1) Add the Kured Helm repository&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm repo add kured https://weaveworks.github.io/kured
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 2) Update your local Helm chart repository cache&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm repo update
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 3) Create a dedicated namespace where you would like to deploy kured&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl create namespace kured
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 4) Install kured in that namespace with Helm 3 (only on Linux nodes)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm install kured kured/kured --namespace kured --set nodeSelector.&lt;span style="color:#e6db74">&amp;#34;kubernetes\.io/os&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>linux
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If all went well with the command above, that’s it, you’re done! Have a nice day! 🙂&lt;/p>
&lt;p>If you want to test Kured, login to one of your Linux nodes, and install some patches with your package manager of choice (any patch that requires a reboot, typically patches that modify kernel headers). Then, check for a file named ‘reboot-required’ in /var/run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>ls -lisa /var/run/reboot-required
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you installed patches, and this file does not exist, none of your patches require a reboot. We can still fake the system into thinking a reboot is required by manually creating the ‘reboot-required’ file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>sudo touch /var/run/reboot-required
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2022/07/image-1.png" alt="">&lt;/p>
&lt;p>Then, we’ll use Kubetail to tail the logs of all our Kured pods:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubetail -label kured --namespace kured
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2022/07/image-1024x749.png" alt="">&lt;/p>
&lt;p>By default, Kured checks for the existence of the sentinel file every 60 minutes. However, this behavior can be changed. See the github repo for more info:&lt;/p>
&lt;p>&lt;img src="https://github.com/weaveworks/kured#reboot-sentinel-file--period" alt="weaveworks/kured: Kubernetes Reboot Daemon">&lt;/p>
&lt;p>Scheduling on the node should be disabled if you are within the Kured schedule&lt;/p>
&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2022/07/image-2.png" alt="">&lt;/p>
&lt;p>Now that the node is cordoned off, running pods on the node are drained, and the node is rebooted.&lt;/p>
&lt;p>That’s it for this article. Have a great day!&lt;/p></description></item><item><title>Running Docker in WSL v1</title><link>https://rnemeth90.github.io/posts/2022-06-26-running-docker-in-wsl-v1/</link><pubDate>Sun, 26 Jun 2022 15:00:28 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2022-06-26-running-docker-in-wsl-v1/</guid><description>&lt;p>I have somewhat of a niche issue, where I have no network connectivity while connecting to my work VPN inside of WSL v2. I have found others complaining about this issue on Github. Though no one seems to know how to fix it and I have not had the time to properly investigate.&lt;/p>
&lt;p>Because of this, I’m required to continue using WSL v1. Though, with WSL v1, Docker does not work. I receive this nice message:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ docker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>The command &lt;span style="color:#e6db74">&amp;#39;docker&amp;#39;&lt;/span> could not be found in this WSL &lt;span style="color:#ae81ff">1&lt;/span> distro.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>We recommend to convert this distro to WSL &lt;span style="color:#ae81ff">2&lt;/span> and activate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>the WSL integration in Docker Desktop settings.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>See https://docs.docker.com/docker-for-windows/wsl/ &lt;span style="color:#66d9ef">for&lt;/span> details.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>So I’m in somewhat of a catch-22 here…&lt;/p>
&lt;p>To work around this problem until a proper solution is found, I was able to get Docker working with WSL v1.&lt;/p>
&lt;p>If you happen to be having a similar issue (and it seems like quite a few people are, considering the number of Github posts I found), just follow these steps:&lt;/p>
&lt;ul>
&lt;li>Expose the Docker daemon in docker desktop settings:&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://rnemeth90.github.io/wp-content/uploads/2022/02/image-10-1024x585.png">&lt;/a>&lt;/p>
&lt;p>Install the stand-alone Docker client in WSL v1:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ wget https://download.docker.com/linux/static/stable/x86_64/docker-20.10.5.tgz
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ tar zxvf docker-20.10.5.tgz
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cd docker
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Set the default Docker daemon in WSL v1:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>export DOCKER_HOST&lt;span style="color:#f92672">=&lt;/span>tcp://localhost:2375
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Verify you can connect to Docker running on Windows from within WSL:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>./docker info
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is also beneficial in that you only have one Docker host to manage your containers, network, etc., rather than two.&lt;/p></description></item><item><title>Remove Kubernetes Namespace Stuck in the Terminating State</title><link>https://rnemeth90.github.io/posts/2022-06-04-remove-kubernetes-namespace-stuck-in-the-terminating-state/</link><pubDate>Sat, 04 Jun 2022 18:29:41 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2022-06-04-remove-kubernetes-namespace-stuck-in-the-terminating-state/</guid><description>&lt;p>In this post, we will discuss how to remove a Kubernetes namespace that is stuck in the ‘terminating’ state.&lt;/p>
&lt;p>A namespace is like a container. You can use it to store related objects in a Kubernetes environment. Maybe you are hosting a blog in Kubernetes. This blog will likely have a database, a frontend website, a load balancer (service) to spread the incoming traffic among ‘x’ number of frontend containers (pods), and maybe some middle-tier or utility applications. One day, you decide you no longer want this blog, so you plan to delete it. Rather than tediously deleting all of the various entities associated with this blog, you can delete the namespace that contains these entities. This will essentially ‘cascade delete’ the resources within the namespace as well.&lt;/p>
&lt;p>After deleting the namespace for your blog, you notice that it still exists, but the state of it is ‘Terminating’, and it has been like this for a long time (hours or maybe even days).&lt;/p>
&lt;p>&lt;a href="https://geekyryan.com/wp-content/uploads/2022/06/image.png">&lt;/a>&lt;/p>
&lt;p>Kubernetes will occassionally fail to delete third-party resources when deleting a namespace, causing the namespace to linger. This can happen if the third-party API managing the resource is not responding to requests. To verify if any of these resources still exist, use this command:&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --show-all --ignore-not-found -n &amp;lt;terminating-namespace&amp;gt;
&lt;/code>&lt;/pre>&lt;p>If you happen to see any resources in the output, you can try force deleting them and then try to delete the namespace again.&lt;/p>
&lt;p>In my experience, the majority of the time you will not find any resources still hanging around. Rather, the namespace will be completely empty. What is going on here?&lt;/p>
&lt;p>Let’s take a look at the namespace:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">$ kubectl get namespace darn-c101 -o yaml&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Namespace&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">annotations&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kubectl.kubernetes.io/last-applied-configuration&lt;/span>: &lt;span style="color:#ae81ff">|&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kubernetes.io/metadata.name&lt;/span>: &lt;span style="color:#ae81ff">darn-c101&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">finalizers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">kubernetes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">status&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">conditions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">lastTransitionTime&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2022-06-01T19:05:31Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">message: &amp;#39;Some content in the namespace has finalizers remaining&lt;/span>: &lt;span style="color:#ae81ff">darn-c101.geekyryan.io/finalizer in 1 resource instances&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">reason&lt;/span>: &lt;span style="color:#ae81ff">SomeFinalizersRemain&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">status&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;True&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">NamespaceFinalizersRemaining&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">phase&lt;/span>: &lt;span style="color:#ae81ff">Terminating&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Notice the inclusion of the finalizers field in the above JSON. Some namespaces have a finalizer defined under spec.&lt;/p>
&lt;p>A finalizer is a special metadata key that tells Kubernetes to wait until a specific condition is met before it fully deletes a resource. Much like a finalizer in the .NET framework (does Java have those too? 😀 )&lt;/p>
&lt;p>So when you run a command like &lt;code>kubectl delete namespace &amp;lt;namespace&amp;gt;&lt;/code>, Kubernetes checks for a finalizer in the &lt;code>metadata.finalizers&lt;/code> field. If the resource defined in the finalizer cannot be deleted, then the namespace is not deleted either. This puts the namespace into a perpetual terminating state and is never actually deleted.&lt;/p>
&lt;p>When an object has been terminating for an excessive time, check its finalizers by inspecting the &lt;code>metadata.finalizers&lt;/code> field in its YAML.&lt;/p>
&lt;p>So we now know what the problem is. How can we solve it? Well, it’s actually quite simple. If you are using bash, use this script:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#!/bin/bash
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>namespaces&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>kubectl get ns --field-selector&lt;span style="color:#f92672">=&lt;/span>status.phase&lt;span style="color:#f92672">==&lt;/span>Terminating -o jsonpath&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;{range .items[*]}{.metadata.name}{&amp;#34;\n&amp;#34;}{end}&amp;#39;&lt;/span>&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">[&lt;/span> -z &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$namespaces&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">then&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> echo &lt;span style="color:#e6db74">&amp;#34;No namespaces to delete.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exit
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">else&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> namespace in $namespaces
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">do&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> echo &lt;span style="color:#e6db74">&amp;#34;[Removing Namespace]: &lt;/span>$namespace&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubectl get namespace $namespace -o json | tr -d &lt;span style="color:#e6db74">&amp;#34;\n&amp;#34;&lt;/span> | sed &lt;span style="color:#e6db74">&amp;#34;s/\&amp;#34;finalizers\&amp;#34;: \[[^]]\+\]/\&amp;#34;finalizers\&amp;#34;: []/&amp;#34;&lt;/span> | kubectl replace --raw /api/v1/namespaces/$namespace/finalize -f - &amp;gt; /dev/null 2&amp;gt;&amp;amp;&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">done&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">fi&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;a href="https://gist.github.com/rnemeth90/e83bb4c8808f0d28412cb40edb2487d3">Delete Terminating Kubernetes Namespaces with Bash (github.com)&lt;/a>&lt;/p>
&lt;p>It will search for any namespace that is stuck in the terminating state and forcefully remove it by removing the finalizers field and then using &lt;code> kubectl replace&lt;/code> to commit the change back to the Kube API.&lt;/p>
&lt;p>If you prefer Powershell, you can use this script:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-Powershell" data-lang="Powershell">&lt;span style="display:flex;">&lt;span>$terminatingNamespaces = kubectl get ns --field-selector=status.phase==Terminating -o jsonpath=&lt;span style="color:#e6db74">&amp;#34;{range .items[*]}{.metadata.name}{&amp;#39;\n&amp;#39;}{end}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">foreach&lt;/span> ($ns &lt;span style="color:#66d9ef">in&lt;/span> $terminatingNamespaces) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Write-Verbose &lt;span style="color:#e6db74">&amp;#39;[FOUND]: Forcefully removing $ns&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> $jsonObj = kubectl get namespace $ns -o json | ConvertFrom-Json | foreach-object { $_.spec.finalizers = @(); $_ } |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> convertto-json | kubectl replace --raw /api/v1/namespaces/$namespace/finalize &lt;span style="color:#f92672">-f&lt;/span> -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;a href="https://gist.github.com/rnemeth90/19d7de622a5009c1cf908c5d4deb5358">Delete Terminating Kubernetes Namespaces with Powershell (github.com)&lt;/a>&lt;/p>
&lt;p>It does the same thing as the bash script, just in more of a Window-zy way.&lt;/p>
&lt;p>It’s that simple. I hope this was helpful. If you have any questions, comments, or concerns, please feel free to reach out.&lt;/p></description></item><item><title>Azure VM Scale Set &amp;#8211; Get Instance IP Address</title><link>https://rnemeth90.github.io/posts/2020-11-19-azure-vm-scale-set-get-instance-ip/</link><pubDate>Thu, 19 Nov 2020 18:07:00 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2020-11-19-azure-vm-scale-set-get-instance-ip/</guid><description>&lt;p>If you are using VM Scale Sets in Azure, you know how important it can be to quickly obtain an instance IP address. This can of course be done using the Azure Portal. However, I am often working in a shell or VSCode, and I do not want to leave the comfort of my shell to login to the portal.&lt;/p>
&lt;p>There are a few options we have for retrieving information about a VMSS and its instances without using the Azure Portal. We can use PowerShell or the Azure CLI. Being that I am constantly flipping between Windows and Linux, I will detail both here.&lt;/p>
&lt;p>You will need to have the AZ module installed. To install this module, simple open PowerShell (as admin) and type in “Install-Module -Name az”. To get the IP address of the instances within a scale set, use the following script:&lt;/p>
&lt;p>&lt;a href="https://github.com/rnemeth90/Get-VmssInstanceIpAddress">https://github.com/rnemeth90/Get-VmssInstanceIpAddress&lt;/a>&lt;/p>
&lt;p>You can also use the Azure CLI to obtain individual instance IP addresses. This method is much simpler than PowerShell, and only requires one line of code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>az vmss nic list –resource-group myResourceGroup –vmss-name myVmss | grep –w “privateIpAddress”
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Reset GRUB/root Password for vCenter/PSC Appliance</title><link>https://rnemeth90.github.io/posts/2020-10-31-reset-grubroot-password-for-vcenterpsc/</link><pubDate>Sat, 31 Oct 2020 01:22:00 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2020-10-31-reset-grubroot-password-for-vcenterpsc/</guid><description>&lt;p>In Redhat/Fedora/Cent, GRUB can be protected by running the grub-md5-crypt command and pasting the outputted password hash into the grub.conf file. vSphere 6.0 password protects grub by default. If you change the root password in the VAMI, the GRUB password is changed to match. If you do not change the root password, the GRUB password is “vmware”.&lt;/p>
&lt;p>To reset the GRUB password, we need to boot into a Cent or Redhat live CD. The ISO can be obtained here: &lt;a href="https://www.centos.org/download/">https://www.centos.org/download/&lt;/a>. Its best to upload the ISO to a datastore that the appliance has access to.&lt;/p>
&lt;p>Stop the appliance and attach the ISO:&lt;/p>
&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2020/10/image.png" alt="">&lt;/p>
&lt;p>Be sure to select the “Connect at Power On” option. Boot the VM into the ISO and select the “Troubleshooting” option.&lt;/p>
&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2020/10/image-1.png" alt="">
Next, choose “Rescure a Red hat (or CentOS depending on your ISO) Enterprise Linux System”&lt;/p>
&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2020/10/image-2.png" alt="">&lt;/p>
&lt;p>Select “Continue” to mount the VCSA 6.0’s root filesystem in Read/write mode under /mnt/sysimage. RHEL 7.2 is capable to detect the VCSA’s root volume and mounts it.&lt;/p>
&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2020/10/image-3.png" alt="">&lt;/p>
&lt;p>The VCSA root filesystem is mounted under /mnt/sysimage and you can now access (and modify) it using the shell. Navigate to /mnt/sysimage/boot and list the contents. You’ll see we now have access to the grub directory:&lt;/p>
&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2020/10/image-4.png" alt="">&lt;/p>
&lt;p>cd to the grub directory and list the contents. Look for a file called “menu.lst”. This file holds the grub boot loader password. Open this file with vi by typing “vi menu.lst”. Navigate to the line beginning with “password” using the arrow keys, and then type “dd” to remove the line.&lt;/p>
&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2020/10/image-5.png" alt="">
You can then save the file by pressing “:wq” (without quotes). You can now cat the file and see that the password has been removed.&lt;/p>
&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2020/10/image-6.png" alt="">&lt;/p>
&lt;p>Exit the shell (this will reboot the server). Detach the ISO and boot the appliance. Once the system is booted, stop the VCSA in the GRUB menu (by pressing the escape key during boot) to break the OS root password.&lt;/p>
&lt;p>&lt;a href="https://rnemeth90.github.io/wp-content/uploads/2020/10/image-7.png">&lt;/a>
Press “e” to edit the boot commands for the kernel.&lt;/p>
&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2020/10/image-8.png" alt="">&lt;/p>
&lt;p>Append “init=/bin/bash” to the line in this step and press enter.&lt;/p>
&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2020/10/image-9.png" alt="">&lt;/p>
&lt;p>Press “b” to boot the system.&lt;/p>
&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2020/10/image-10.png" alt="">&lt;/p>
&lt;p>You will now boot into a bash shell where you can set the root password.&lt;/p>
&lt;p>&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2020/10/image-11.png" alt="">&lt;/p>
&lt;p>Once this is done, exit the shell by typing “exit”. You can now boot the appliance and login with your new root password.&lt;/p></description></item><item><title>Ping Sweeping with FPing</title><link>https://rnemeth90.github.io/posts/2015-03-09-ping-sweeping-with-fping/</link><pubDate>Mon, 09 Mar 2015 01:08:00 +0000</pubDate><guid>https://rnemeth90.github.io/posts/2015-03-09-ping-sweeping-with-fping/</guid><description>&lt;p>I generally use NMAP for any type of host discovery, but recently started experimenting with FPing. One thing I found is that, when performing a ping sweep, not only do I see the hosts that replied to the ping, but FPing also sends any unreachable IP addresses to stdout (which is super annoying and ugly if you ask me…).&lt;/p>
&lt;p>&lt;a href="https://rnemeth90.github.io/wp-content/uploads/2015/03/2015-03-08_21h04_50.png">&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2015/03/2015-03-08_21h04_50.png" alt="">&lt;/a>&lt;/p>
&lt;p>Anyway, after a bit of research, I found a nifty way to suppress these messages. Linux allows us to redirect all error messages to /dev/null. So instead of just running the vanilla fping -a -g…. you would run the program and output all error messages /dev/null, like so:&lt;/p>
&lt;p>&lt;a href="https://rnemeth90.github.io/wp-content/uploads/2015/03/2015-03-08_21h07_14.png">&lt;img src="https://rnemeth90.github.io/wp-content/uploads/2015/03/2015-03-08_21h07_14.png" alt="">&lt;/a>&lt;/p></description></item></channel></rss>